{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Albumentations documentation","text":"<p>Albumentations is a fast and flexible image augmentation library. The library is widely used in industry, deep learning research, machine learning competitions, and open source projects. Albumentations is written in Python, and it is licensed under the MIT license. The source code is available at https://github.com/albumentations-team/albumentations.</p> <p>If you are new to image augmentation, start with articles in the \"Introduction to image augmentation\" section. They describe what image augmentation is, how it can boost deep neural networks' performance, and why you should use Albumentations.</p> <p>Articles in the \"Getting started with Albumentations\" section show how you can use the library for different computer vision tasks: image classification, semantic segmentation, instance segmentation, and object detection, keypoint detection.</p> <p>The \"Examples\" section contains Jupyter Notebooks that demonstrate how to use various features of Albumentations. Each notebook includes a link to Google Colab, where you can run the code by yourself.</p> <p>\"API Reference\" contains the description of Albumentations' methods and classes.</p>"},{"location":"#introduction-to-image-augmentation","title":"Introduction to image augmentation","text":"<ul> <li>What is image augmentation and how it can improve the performance of deep neural networks</li> <li>Why you need a dedicated library for image augmentation</li> <li>Why Albumentations</li> </ul>"},{"location":"#getting-started-with-albumentations","title":"Getting started with Albumentations","text":"<ul> <li>Installation</li> <li>Frequently Asked Questions</li> <li>Image augmentation for classification</li> <li>Mask augmentation for segmentation</li> <li>Bounding boxes augmentation for object detection</li> <li>Keypoints augmentation</li> <li>Simultaneous augmentation of multiple targets: masks, bounding boxes, keypoints</li> <li>A list of transforms and their supported targets</li> <li>Setting probabilities for transforms in an augmentation pipeline</li> </ul>"},{"location":"#integrations","title":"Integrations","text":"<ul> <li>HuggingFace</li> <li>FiftyOne</li> <li>Roboflow</li> </ul>"},{"location":"#examples","title":"Examples","text":"<ul> <li>Defining a simple augmentation pipeline for image augmentation</li> <li>Using Albumentations to augment bounding boxes for object detection tasks</li> <li>How to use Albumentations for detection tasks if you need to keep all bounding boxes</li> <li>Using Albumentations for a semantic segmentation task</li> <li>Using Albumentations to augment keypoints</li> <li>Applying the same augmentation with the same parameters to multiple images, masks, bounding boxes, or keypoints</li> <li>Weather augmentations in Albumentations</li> <li>Example of applying XYMasking transform</li> <li>Example of applying MixUp transform</li> <li>Example of applying ChromaticAberration transform</li> <li>Example of applying Morphological transform</li> <li>Example of applying D4 transform</li> <li>Example of applying RandomGridShuffle transform</li> <li>Example of applying OverlayElements transform</li> <li>Example of applying TextImage transform</li> <li>Migrating from torchvision to Albumentations</li> <li>Debugging an augmentation pipeline with ReplayCompose</li> <li>How to save and load parameters of an augmentation pipeline</li> <li>Showcase. Cool augmentation examples on diverse set of images from various real-world tasks.</li> <li>How to save and load transforms to HuggingFace Hub.</li> </ul>"},{"location":"#examples-of-how-to-use-albumentations-with-different-deep-learning-frameworks","title":"Examples of how to use Albumentations with different deep learning frameworks","text":"<ul> <li>PyTorch and Albumentations for image classification</li> <li>PyTorch and Albumentations for semantic segmentation</li> <li>Using Albumentations with Tensorflow</li> </ul>"},{"location":"#external-resources","title":"External resources","text":"<ul> <li>Blog posts, podcasts, talks, and videos about Albumentations</li> <li>Books that mention Albumentations</li> <li>Online courses that cover Albumentations</li> </ul>"},{"location":"#other-topics","title":"Other topics","text":"<ul> <li>Release notes</li> <li>Contributing</li> </ul>"},{"location":"#api-reference","title":"API Reference","text":"<ul> <li>Full API Reference on a single page</li> <li>Index</li> <li>Core API (albumentations.core)</li> <li>Augmentations (albumentations.augmentations)</li> <li>PyTorch Helpers (albumentations.pytorch)</li> </ul>"},{"location":"CONTRIBUTING/","title":"Contributing to Albumentations","text":"<p>Thank you for your interest in contributing to Albumentations! This guide is designed to make it easier for you to get involved and help us build a powerful, efficient, and easy-to-use image augmentation library.</p> <p>For small changes (e.g., bug fixes), feel free to submit a PR.</p> <p>For larger changes, consider creating an issue outlining your proposed change. You can also join us on Discord to discuss your idea with the community.</p>"},{"location":"CONTRIBUTING/#getting-started","title":"Getting Started","text":""},{"location":"CONTRIBUTING/#setting-up-your-development-environment","title":"Setting Up Your Development Environment","text":""},{"location":"CONTRIBUTING/#fork-and-clone-the-repository","title":"Fork and clone the repository","text":"<p>Start by forking the project repository to your GitHub account, then clone your fork to your local machine:</p> Bash<pre><code>git clone https://github.com/albumentations/albumentations.git\ncd albumentations\n</code></pre>"},{"location":"CONTRIBUTING/#create-a-virtual-environment","title":"Create a virtual environment","text":"<p>We recommend using a virtual environment to isolate project dependencies. Ensure you have Python 3.8 or higher installed on your machine, as it is the minimum supported version for Albumentations. To create and activate a virtual environment, run the following commands:</p>"},{"location":"CONTRIBUTING/#linux-macos","title":"Linux / macOS","text":"Bash<pre><code>python3 -m venv env\nsource env/bin/activate\n</code></pre>"},{"location":"CONTRIBUTING/#windows-cmdexe","title":"Windows cmd.exe","text":"Bash<pre><code>python -m venv env\nenv\\Scripts\\activate.bat\n</code></pre>"},{"location":"CONTRIBUTING/#windows-powershell","title":"Windows PowerShell","text":"Bash<pre><code>python -m venv env\nenv\\Scripts\\activate.ps1\n</code></pre>"},{"location":"CONTRIBUTING/#install-development-dependencies","title":"Install development dependencies","text":"<p>Install the project's dependencies by running:</p> Bash<pre><code>pip install -e .\n</code></pre> <p>Additionally, to ensure you have all the necessary tools for code formatting, linting, and additional development utilities, install the requirements from <code>requirements-dev.txt</code>:</p> Bash<pre><code>pip install -r requirements-dev.txt\n</code></pre>"},{"location":"CONTRIBUTING/#navigating-the-project","title":"Navigating the Project","text":"<ul> <li>The main source code is located in the <code>albumentations/</code> directory.</li> <li>Tests are located in the <code>tests/</code> directory. Every pull request should include tests for new features or bug fixes.</li> </ul>"},{"location":"CONTRIBUTING/#how-to-contribute","title":"How to Contribute","text":""},{"location":"CONTRIBUTING/#types-of-contributions","title":"Types of Contributions","text":"<ul> <li>Code Contributions: Whether it's fixing a bug, adding a new feature, or improving performance, your code contributions are valuable.</li> <li>Documentation: Help us improve the project's documentation for better usability and accessibility.</li> <li>Bug Reports and Feature Requests: Use GitHub Issues to report bugs, request features, or suggest improvements.</li> </ul>"},{"location":"CONTRIBUTING/#contribution-process","title":"Contribution Process","text":"<ol> <li>Find an issue to work on: Look for open issues or propose a new one. For newcomers, look for issues labeled \"good first issue.\"</li> <li>Fork the repository (if you haven't already).</li> <li>Create a new branch for your changes: <code>git checkout -b feature/my-new-feature</code>.</li> <li>Implement your changes: Write clean, readable, and well-documented code. Note that we do not use np.random module directly but call corresponding functions from the albumentations/random_utils.py module to ensure consistency and control over randomness.</li> <li>Add or update tests as necessary.</li> <li>Ensure all tests pass and your code adheres to the existing style guidelines.</li> <li>Submit a Pull Request (PR): Open a PR from your forked repository to the main Albumentations repository. Provide a clear description of the changes and any relevant issue numbers.</li> </ol>"},{"location":"CONTRIBUTING/#code-review-process","title":"Code Review Process","text":"<ul> <li>Once you submit a PR, the Albumentations maintainers will review your contribution.</li> <li>Engage in the review process if the maintainers have feedback or questions.</li> <li>Once your PR is approved, a maintainer will merge it into the main codebase.</li> </ul>"},{"location":"CONTRIBUTING/#coding-guidelines","title":"Coding Guidelines","text":""},{"location":"CONTRIBUTING/#using-pre-commit-hooks","title":"Using Pre-commit Hooks","text":"<p>To maintain code quality and consistency, we use pre-commit hooks. Follow these steps to set up pre-commit hooks in your local repository:</p> <p>Install pre-commit: If you haven't already, you need to install pre-commit on your machine. You can do this using pip:</p> Bash<pre><code>pip install pre-commit\n</code></pre> <p>Initialize pre-commit:</p> <p>Navigate to the root of your cloned repository and run:</p> Bash<pre><code>pre-commit install\n</code></pre> <p>This command sets up the pre-commit hooks based on the configurations found in <code>.pre-commit-config.yaml</code> at the root of the repository.</p> <p>Running pre-commit hooks:</p> <p>Pre-commit will automatically run the configured hooks on each commit. You can also manually run the hooks on all files in the repository with:</p> Bash<pre><code>pre-commit run --all-files\n</code></pre> <p>Ensure to fix any issues detected by the pre-commit hooks before submitting your pull request.</p>"},{"location":"CONTRIBUTING/#running-tests","title":"Running Tests","text":"<p>Before submitting your contributions, it's important to ensure that all tests pass. This helps maintain the stability and reliability of Albumentations. Here's how you can run the tests:</p> <p>Install test dependencies:</p> <p>If you haven't installed the development dependencies, make sure to do so. These dependencies include <code>pytest</code>, which is required to run the tests.</p> Bash<pre><code>pip install -e .\n</code></pre> Bash<pre><code>pip install -r requirements-dev.txt\n</code></pre> <p>Run the tests:</p> <p>With <code>pytest</code> installed, you can run all tests using the following command from the root of the repository:</p> Bash<pre><code>pytest\n</code></pre> <p>This will execute all the tests and display the results, indicating whether each test passed or failed.</p> <p>Tip: If you've made changes to a specific area of the library, you can run a subset of the tests related to your changes. This can save time and make it easier to debug issues. Use the <code>pytest</code> documentation to learn more about running specific tests.</p>"},{"location":"CONTRIBUTING/#ensuring-your-contribution-is-ready","title":"Ensuring Your Contribution is Ready","text":"<ul> <li>After setting up pre-commit hooks and ensuring all tests pass, your contribution is nearly ready for submission.</li> <li>Review your changes one last time, ensuring they meet the project's coding guidelines and documentation standards.</li> <li>If your changes affect how users interact with Albumentations, update the documentation accordingly.</li> </ul>"},{"location":"CONTRIBUTING/#adding-or-modifying-transforms","title":"Adding or Modifying Transforms","text":""},{"location":"CONTRIBUTING/#validation-with-initschema","title":"Validation with InitSchema","text":"<p>Each transform includes an <code>InitSchema</code> class responsible for validating and modifying input parameters before the execution of the <code>__init__</code> method. This step ensures that all parameter manipulations, such as converting a single value into a range, are handled consistently and appropriately.</p>"},{"location":"CONTRIBUTING/#simplifying-parameter-types","title":"Simplifying Parameter Types","text":"<p>Historically, our transforms have used <code>Union</code> types to allow flexibility in parameter input\u2014accepting either a single value or a tuple. While flexible, this approach can lead to confusion about how parameters are interpreted and used within the transform. For example, when a single value is provided, it is unclear whether and how it will be expanded into a tuple, which can lead to unpredictable behavior and difficult-to-understand code.</p> <p>To improve clarity and predictability:</p> <p>Explicit Definitions: Parameters should be explicitly defined as either a single value or a tuple. This change avoids ambiguity and ensures that the intent and behavior of the transform are clear to all users.</p>"},{"location":"CONTRIBUTING/#serialization-considerations","title":"Serialization Considerations","text":"<p>Even if a parameter defined as <code>Tuple</code>, the transform should work correctly with a <code>List</code> that have similar values. This is required as <code>JSON</code> and <code>YAML</code> serialization formats do not distinguish between lists and tuples and if you serialize and then deserialize it back, you will get a list instead of a tuple. If it is not the case test will fail, but, just in case, keep this in mind while creating or modifying the transform.</p> <p>List Compatibility: Because these formats do not distinguish between lists and tuples, using List in type definitions ensures that transforms work correctly post-serialization, which treats tuples as lists.</p>"},{"location":"CONTRIBUTING/#probability-handling","title":"Probability Handling","text":"<p>To maintain determinism and reproducibility, handle all probability calculations within the <code>get_params</code> or <code>get_params_dependent_on_targets</code> methods. These calculations should not occur in the <code>apply_xxx</code> or <code>__init__</code> methods, as it is crucial to separate configuration from execution in our codebase.</p>"},{"location":"CONTRIBUTING/#using-random-number-generators","title":"Using Random Number Generators","text":"<p>When you need to use random number generation in your contributions:</p> <ul> <li>Prefer <code>random</code> from the standard library: Use <code>random</code> whenever possible as it generally offers faster performance compared to <code>np.random</code>.</li> <li>Use <code>random_utils</code> for functions from <code>np.random</code>: When you need specific functionality provided by <code>np.random</code>, use the corresponding functions from <code>albumentations/random_utils.py</code> to ensure consistency and control over randomness.</li> </ul> <p>By following this approach, we maintain the efficiency and consistency of random operations across the codebase.</p>"},{"location":"CONTRIBUTING/#specific-guidelines-for-method-definitions","title":"Specific Guidelines for Method Definitions","text":""},{"location":"CONTRIBUTING/#handling-apply_xxx-methods","title":"Handling <code>apply_xxx</code> Methods","text":"<p>When contributing code related to transformation methods, specifically methods that start with <code>apply_</code> (e.g., <code>apply_to_mask</code>, <code>apply_to_bbox</code>), please adhere to the following guidelines:</p> <p>No Default Arguments: Do not use default arguments in <code>apply_xxx</code> methods. Every parameter should be explicitly required, promoting clarity and reducing hidden behaviors that can arise from default values.</p>"},{"location":"CONTRIBUTING/#examples","title":"Examples","text":"<p>Here are a few examples to illustrate these guidelines:</p> <p>Incorrect method definition:</p> Python<pre><code>def apply_to_mask(self, mask, fill_value=0):  # Default value not allowed\n    # implementation\n</code></pre> <p>Correct method definition:</p> Python<pre><code>def apply_to_mask(self, mask, fill_value):  # No default values\n    # implementation\n</code></pre>"},{"location":"CONTRIBUTING/#guidelines-for-modifying-existing-code","title":"Guidelines for Modifying Existing Code","text":"<p>Maintaining the stability and usability of Albumentations for all users is a priority. When contributing, it's important to follow these guidelines for modifying existing code:</p>"},{"location":"CONTRIBUTING/#transform-modifications","title":"Transform Modifications","text":"<p>Transform Interfaces: Changes to transform interfaces or the removal of old transforms should be handled delicately. Introduce changes through a deprecation warning phase that lasts several months. This provides users ample time to adapt to new changes.</p>"},{"location":"CONTRIBUTING/#custom-transformations","title":"Custom Transformations","text":"<p>Support for Customization: We highly value the ability to create custom transformations based on ImageOnlyTransform and DualTransform. Significant changes, like the removal of methods such as get_params_depend_on_targets, should also proceed through a deprecation phase to preserve backward compatibility.</p>"},{"location":"CONTRIBUTING/#helper-functions","title":"Helper Functions","text":"<p>Flexibility with Helpers: Helper functions can be modified more freely. While these functions may be directly used by some users, typically they are power users who are capable of handling such changes. Thus, adding, removing, or moving helper functions can be done with relative freedom.</p>"},{"location":"CONTRIBUTING/#private-methods-and-internal-logic","title":"Private Methods and Internal Logic","text":"<p>Internal Changes: Private methods and functions within transform_interface and Compose that do not affect inheritance from ImageOnlyTransform, DualTransform, or alter the behavior of transformations can be changed or optimized boldly. These modifications do not require a deprecation phase.</p>"},{"location":"CONTRIBUTING/#handling-broken-features","title":"Handling Broken Features","text":"<p>Rapid Response: If it becomes evident that a transformation or feature is fundamentally broken, take decisive action to fix or overhaul it. This may involve substantial changes or relocations within the codebase to correct the issue efficiently.</p>"},{"location":"CONTRIBUTING/#application-of-changes","title":"Application of Changes","text":"<ul> <li>Always document your changes thoroughly, especially if they affect how users interact with the library.</li> <li>Use the pull request description to explain your changes and the reasons behind them. This helps maintainers understand your decisions and facilitates the review process. By adhering to these guidelines, contributors can ensure that their enhancements and fixes are integrated smoothly and maintain the high standards of the Albumentations library.</li> </ul>"},{"location":"CONTRIBUTING/#additional-resources","title":"Additional Resources","text":"<p>Albumentations Documentation</p>"},{"location":"CONTRIBUTING/#acknowledgements","title":"Acknowledgements","text":"<p>Your contributions are appreciated and recognized. Contributors who have significantly impacted the project will be mentioned in our documentation and releases.</p>"},{"location":"CONTRIBUTING/#contact-information","title":"Contact Information","text":"<p>For any questions or concerns about contributing, please reach out to the maintainers via GitHub Issues.</p>"},{"location":"benchmarking_results/","title":"Benchmarking results","text":""},{"location":"benchmarking_results/#benchmarking-results_1","title":"Benchmarking results","text":"<p>To run the benchmark yourself, follow the instructions in benchmark/README.md</p> <p>Results for running the benchmark on the first 2000 images from the ImageNet validation set using an AMD Ryzen Threadripper 3970X CPU. The table shows how many images per second can be processed on a single core; higher is better.</p> Library Version Python 3.10.13 (main, Sep 11 2023, 13:44:35) [GCC 11.2.0] albumentations 1.4.11 imgaug 0.4.0 torchvision 0.18.1+rocm6.0 numpy 1.26.4 opencv-python-headless 4.10.0.84 scikit-image 0.24.0 scipy 1.14.0 pillow 10.4.0 kornia 0.7.3 augly 1.0.0 albumentations1.4.11 torchvision0.18.1+rocm6.0 kornia0.7.3 augly1.0.0 imgaug0.4.0 HorizontalFlip 8017 \u00b1 12 2436 \u00b1 2 935 \u00b1 3 3575 \u00b1 4 4806 \u00b1 7 VerticalFlip 7366 \u00b1 7 2563 \u00b1 8 943 \u00b1 1 4949 \u00b1 5 8159 \u00b1 21 Rotate 570 \u00b1 12 152 \u00b1 2 207 \u00b1 1 633 \u00b1 2 496 \u00b1 2 Affine 1382 \u00b1 31 162 \u00b1 1 201 \u00b1 1 - 682 \u00b1 2 Equalize 1027 \u00b1 2 336 \u00b1 2 77 \u00b1 1 - 1183 \u00b1 1 RandomCrop64 19986 \u00b1 57 15336 \u00b1 16 811 \u00b1 1 19882 \u00b1 356 5410 \u00b1 5 RandomResizedCrop 2308 \u00b1 7 1046 \u00b1 3 187 \u00b1 1 - - ShiftRGB 1240 \u00b1 3 - 425 \u00b1 2 - 1554 \u00b1 6 Resize 2314 \u00b1 9 1272 \u00b1 3 201 \u00b1 3 431 \u00b1 1 1715 \u00b1 2 RandomGamma 2552 \u00b1 2 232 \u00b1 1 211 \u00b1 1 - 1794 \u00b1 1 Grayscale 7313 \u00b1 4 1652 \u00b1 2 443 \u00b1 2 2639 \u00b1 2 1171 \u00b1 23 ColorJitter 396 \u00b1 1 51 \u00b1 1 50 \u00b1 1 224 \u00b1 1 - PlankianJitter 449 \u00b1 1 - 598 \u00b1 1 - - RandomPerspective 471 \u00b1 1 123 \u00b1 1 114 \u00b1 1 - 478 \u00b1 2 GaussianBlur 2099 \u00b1 2 113 \u00b1 2 79 \u00b1 2 165 \u00b1 1 1244 \u00b1 2 MedianBlur 538 \u00b1 1 - 3 \u00b1 1 - 565 \u00b1 1 MotionBlur 2197 \u00b1 9 - 102 \u00b1 1 - 508 \u00b1 1 Posterize 2449 \u00b1 1 2587 \u00b1 3 339 \u00b1 6 - 1547 \u00b1 1 JpegCompression 827 \u00b1 1 - 50 \u00b1 2 684 \u00b1 1 428 \u00b1 4 GaussianNoise 78 \u00b1 1 - - 67 \u00b1 1 128 \u00b1 1 Elastic 127 \u00b1 1 3 \u00b1 1 1 \u00b1 1 - 130 \u00b1 1 Normalize 971 \u00b1 2 449 \u00b1 1 415 \u00b1 1 - -"},{"location":"faq/","title":"Frequently Asked Questions","text":""},{"location":"faq/#installation","title":"Installation","text":"<ul> <li>I am receiving an error message <code>Failed building wheel for imagecodecs</code> when I am trying to install Albumentations. How can I fix the problem?</li> <li>I successfully installed the library, but when I am trying to import it I receive an error <code>ImportError: libXrender.so.1: cannot open shared object file: No such file or directory</code>.</li> </ul>"},{"location":"faq/#examples","title":"Examples","text":"<ul> <li>Why do you call <code>cv2.cvtColor(image, cv2.COLOR_BGR2RGB)</code> in your examples?</li> </ul>"},{"location":"faq/#usage","title":"Usage","text":"<ul> <li>Frequently Asked Questions</li> <li>Installation</li> <li>Examples</li> <li>Usage</li> <li>Installation<ul> <li>I am receiving an error message <code>Failed building wheel for imagecodecs</code> when I am trying to install Albumentations. How can I fix the problem?</li> <li>How to disable automatic checks for new versions?</li> <li>I successfully installed the library, but when I am trying to import it I receive an error <code>ImportError: libXrender.so.1: cannot open shared object file: No such file or directory</code></li> </ul> </li> <li>Examples<ul> <li>Why do you call <code>cv2.cvtColor(image, cv2.COLOR_BGR2RGB)</code> in your examples?</li> </ul> </li> <li>Usage<ul> <li>Supported Image Types</li> <li>How can I find which augmentations were applied to the input data and which parameters they used?</li> <li>How to save and load augmentation transforms to HuggingFace Hub?</li> <li>My computer vision pipeline works with a sequence of images. I want to apply the same augmentations with the same parameters to each image in the sequence. Can Albumentations do it?</li> <li>How to perform balanced scaling?</li> <li>Augmentations have a parameter named <code>p</code> that sets the probability of applying that augmentation. How does <code>p</code> work in nested containers?</li> <li>When I use augmentations with the <code>border_mode</code> parameter (such as <code>Rotate</code>) and set <code>border_mode</code> to <code>cv2.BORDER_REFLECT</code> or <code>cv2.BORDER_REFLECT_101</code> Albumentations mirrors regions of images and masks but doesn't mirror bounding boxes and keypoints. Is it a bug?</li> <li>I created annotations for bounding boxes using labeling service or labeling software. How can I use those annotations in Albumentations?</li> </ul> </li> </ul>"},{"location":"faq/#installation_1","title":"Installation","text":""},{"location":"faq/#i-am-receiving-an-error-message-failed-building-wheel-for-imagecodecs-when-i-am-trying-to-install-albumentations-how-can-i-fix-the-problem","title":"I am receiving an error message <code>Failed building wheel for imagecodecs</code> when I am trying to install Albumentations. How can I fix the problem?","text":"<p>Try to update <code>pip</code> by running the following command:</p> Bash<pre><code>python -m pip install --upgrade pip\n</code></pre>"},{"location":"faq/#how-to-disable-automatic-checks-for-new-versions","title":"How to disable automatic checks for new versions?","text":"<p>To disable automatic checks for new versions, set the environment variable <code>NO_ALBUMENTATIONS_UPDATE</code> to <code>1</code>.</p>"},{"location":"faq/#i-successfully-installed-the-library-but-when-i-am-trying-to-import-it-i-receive-an-error-importerror-libxrenderso1-cannot-open-shared-object-file-no-such-file-or-directory","title":"I successfully installed the library, but when I am trying to import it I receive an error <code>ImportError: libXrender.so.1: cannot open shared object file: No such file or directory</code>","text":"<p>Probably your system doesn't have <code>libXrender</code>. To install the <code>libXrender</code> package on Ubuntu or Debian run:</p> Bash<pre><code> sudo apt-get update\n sudo apt-get install libxrender1\n</code></pre> <p>To install the package on other operating systems, consult the documentation for the OS' package manager.</p>"},{"location":"faq/#examples_1","title":"Examples","text":""},{"location":"faq/#why-do-you-call-cv2cvtcolorimage-cv2color_bgr2rgb-in-your-examples","title":"Why do you call <code>cv2.cvtColor(image, cv2.COLOR_BGR2RGB)</code> in your examples?","text":"<p>For historical reasons, OpenCV reads an image in BGR format (so color channels of the image have the following order: Blue, Green, Red). Albumentations uses the most common and popular RGB image format. So when using OpenCV, we need to convert the image format to RGB explicitly.</p>"},{"location":"faq/#usage_1","title":"Usage","text":""},{"location":"faq/#supported-image-types","title":"Supported Image Types","text":"<p>Albumentations works with images of type uint8 and float32. uint8 images should be in the <code>[0, 255]</code> range, and float32 images should be in the <code>[0, 1]</code> range. If float32 images lie outside of the <code>[0, 1]</code> range, they will be automatically clipped to the <code>[0, 1]</code> range.</p>"},{"location":"faq/#how-can-i-find-which-augmentations-were-applied-to-the-input-data-and-which-parameters-they-used","title":"How can I find which augmentations were applied to the input data and which parameters they used?","text":"<p>To save and inspect parameters of augmentations, you can replace Compose with ReplayCompose. ReplayCompose behaves just like regular Compose, but it also saves information about which augmentations were applied and which parameters were uses. Take a look at the example that shows how you can use ReplayCompose.</p>"},{"location":"faq/#how-to-save-and-load-augmentation-transforms-to-huggingface-hub","title":"How to save and load augmentation transforms to HuggingFace Hub?","text":"Python<pre><code>import albumentations as A\nimport numpy as np\n\ntransform = A.Compose([\n    A.RandomCrop(256, 256),\n    A.HorizontalFlip(),\n    A.RandomBrightnessContrast(),\n    A.RGBShift(),\n    A.Normalize(),\n])\n\ntransform.save_pretrained(\"qubvel-hf/albu\", key=\"train\")\n# The 'key' parameter specifies the context or purpose of the saved transform,\n# allowing for organized and context-specific retrieval.\n# ^ this will save the transform to a directory \"qubvel-hf/albu\" with filename \"albumentations_config_train.json\"\n\ntransform.save_pretrained(\"qubvel-hf/albu\", key=\"train\", push_to_hub=True)\n# ^ this will save the transform to a directory \"qubvel-hf/albu\" with filename \"albumentations_config_train.json\"\n# + push the transform to the Hub to the repository \"qubvel-hf/albu\"\n\ntransform.push_to_hub(\"qubvel-hf/albu\", key=\"train\")\n# Use `save_pretrained` to save the transform locally and optionally push to the Hub.\n# Use `push_to_hub` to directly push the transform to the Hub without saving it locally.\n# ^ this will push the transform to the Hub to the repository \"qubvel-hf/albu\" (without saving it locally)\n\nloaded_transform = A.Compose.from_pretrained(\"qubvel-hf/albu\", key=\"train\")\n# ^ this will load the transform from local folder if exist or from the Hub repository \"qubvel-hf/albu\"\n</code></pre> <p>See this example for more info.</p>"},{"location":"faq/#my-computer-vision-pipeline-works-with-a-sequence-of-images-i-want-to-apply-the-same-augmentations-with-the-same-parameters-to-each-image-in-the-sequence-can-albumentations-do-it","title":"My computer vision pipeline works with a sequence of images. I want to apply the same augmentations with the same parameters to each image in the sequence. Can Albumentations do it?","text":"<p>Yes. You can define additional images, masks, bounding boxes, or keypoints through the <code>additional_targets</code> argument to <code>Compose</code>. You can then pass those additional targets to the augmentation pipeline, and Albumentations will augment them in the same way. See this example for more info.</p>"},{"location":"faq/#how-to-perform-balanced-scaling","title":"How to perform balanced scaling?","text":"<p>The default scaling logic in <code>RandomScale</code>, <code>ShiftScaleRotate</code>, and <code>Affine</code> transformations is biased towards upscaling.</p> <p>For example, if <code>scale_limit = (0.5, 2)</code>, a user might expect that the image will be scaled down in half of the cases and scaled up in the other half. However, in reality, the image will be scaled up in 75% of the cases and scaled down in only 25% of the cases. This is because the default behavior samples uniformly from the interval <code>[0.5, 2]</code>, and the interval <code>[0.5, 1]</code> is three times smaller than <code>[1, 2]</code>.</p> <p>To achieve balanced scaling, you can use <code>Affine</code> with <code>balanced_scale=True</code>, which ensures that the probability of scaling up and scaling down is equal.</p> Python<pre><code>balanced_scale_transform = A.Affine(scale=(0.5, 2), balanced_scale=True)\n</code></pre> <p>or use <code>OneOf</code> transform as follows:</p> Python<pre><code>balanced_scale_transform = A.OneOf([\n  A.Affine(scale=(0.5, 1), p=0.5),\n  A.Affine(scale=(1, 2), p=0.5)])\n</code></pre> <p>This approach ensures that exactly half of the samples will be upscaled and half will be downscaled.</p>"},{"location":"faq/#augmentations-have-a-parameter-named-p-that-sets-the-probability-of-applying-that-augmentation-how-does-p-work-in-nested-containers","title":"Augmentations have a parameter named <code>p</code> that sets the probability of applying that augmentation. How does <code>p</code> work in nested containers?","text":"<p>The <code>p</code> parameter sets the probability of applying a specific augmentation. When augmentations are nested within a top-level container like <code>Compose</code>, the effective probability of each augmentation is the product of the container's probability and the augmentation's probability.</p> <p>Let's look at an example when a container <code>Compose</code> contains one augmentation <code>Resize</code>:</p> Python<pre><code>transform = A.Compose([\n    A.Resize(height=256, width=256, p=1.0),\n], p=0.9)\n</code></pre> <p>In this case, <code>Resize</code> has a 90% chance to be applied. This is because there is a 90% chance for <code>Compose</code> to be applied (p=0.9). If <code>Compose</code> is applied, then <code>Resize</code> is applied with 100% probability <code>(p=1.0)</code>.</p> <p>To visualize:</p> <ul> <li>Probability of <code>Compose</code> being applied: 0.9</li> <li>Probability of <code>Resize</code> being applied given <code>Compose</code> is applied: 1.0</li> <li>Effective probability of <code>Resize</code> being applied: 0.9 * 1.0 = 0.9 (or 90%)</li> </ul> <p>This means that the effective probability of <code>Resize</code> being applied is the product of the probabilities of <code>Compose</code> and <code>Resize</code>, which is <code>0.9 * 1.0 = 0.9</code> or 90%. This principle applies to other transformations as well, where the overall probability is the product of the individual probabilities within the transformation pipeline.</p> <p>Here\u2019s another example:</p> Python<pre><code>transform = A.Compose([\n    A.Resize(height=256, width=256, p=0.5),\n], p=0.9)\n</code></pre> <p>In this example, Resize has an effective probability of being applied as <code>0.9 * 0.5</code> = 0.45 or 45%. This is because <code>Compose</code> is applied 90% of the time, and within that 90%, <code>Resize</code> is applied 50% of the time.</p>"},{"location":"faq/#when-i-use-augmentations-with-the-border_mode-parameter-such-as-rotate-and-set-border_mode-to-cv2border_reflect-or-cv2border_reflect_101-albumentations-mirrors-regions-of-images-and-masks-but-doesnt-mirror-bounding-boxes-and-keypoints-is-it-a-bug","title":"When I use augmentations with the <code>border_mode</code> parameter (such as <code>Rotate</code>) and set <code>border_mode</code> to <code>cv2.BORDER_REFLECT</code> or <code>cv2.BORDER_REFLECT_101</code> Albumentations mirrors regions of images and masks but doesn't mirror bounding boxes and keypoints. Is it a bug?","text":"<p>Unfortunately, adding extra bounding boxes or keypoints to reflected regions of the image is not supported. You can change <code>border_mode</code> mode to <code>cv2.BORDER_CONSTANT</code> if this causes a significant impact on the training of your model.</p>"},{"location":"faq/#i-created-annotations-for-bounding-boxes-using-labeling-service-or-labeling-software-how-can-i-use-those-annotations-in-albumentations","title":"I created annotations for bounding boxes using labeling service or labeling software. How can I use those annotations in Albumentations?","text":"<p>You need to convert those annotations to one of the formats, supported by Albumentations. For the list of formats, please refer to this article. Consult the documentation of the labeling service to see how you can export annotations in those formats.</p>"},{"location":"frameworks_and_libraries/","title":"Frameworks and libraries that use Albumentations","text":""},{"location":"frameworks_and_libraries/#mmdetection","title":"MMDetection","text":"<p>https://github.com/open-mmlab/mmdetection</p> <p>MMDetection is an open source object detection toolbox based on PyTorch. It is a part of the OpenMMLab project.</p> <ul> <li>To install MMDetection with Albumentations follow the installation instructions.</li> <li>MMDetection has an example config with augmentations from Albumentations.</li> </ul>"},{"location":"frameworks_and_libraries/#yolov5","title":"YOLOv5","text":"<p>https://github.com/ultralytics/yolov5</p> <p>YOLOv5 \ud83d\ude80 is a family of object detection architectures and models pretrained on the COCO dataset, and represents Ultralytics open-source research into future vision AI methods, incorporating lessons learned and best practices evolved over thousands of hours of research and development.</p> <ul> <li>To use Albumentations along with YOLOv5 simply <code>pip install -U albumentations</code> and then update the augmentation pipeline as you see fit in the Albumentations class in <code>utils/augmentations.py</code>. An example is available in the YOLOv5 repository.</li> </ul>"},{"location":"frameworks_and_libraries/#other-frameworks-and-libraries","title":"Other frameworks and libraries","text":"<p>Other you can see find at GitHub</p>"},{"location":"release_notes/","title":"Release notes","text":""},{"location":"release_notes/#albumentations-148-release-notes","title":"Albumentations 1.4.8 Release Notes","text":"<ul> <li>Support our work</li> <li>Documentation</li> <li>Deprecations</li> <li>Improvements and bug fixes</li> </ul>"},{"location":"release_notes/#support-our-work","title":"Support Our Work","text":"<ol> <li>Love the library? You can contribute to its development by becoming a  sponsor for the library. Your support is invaluable, and every contribution makes a difference.</li> <li>Haven't starred our repo yet? Show your support with a \u2b50! It's just only one mouse click.</li> <li>Got ideas or facing issues? We'd love to hear from you. Share your thoughts in our issues or join the conversation on our Discord server for Albumentations</li> </ol>"},{"location":"release_notes/#documentation","title":"Documentation","text":"<p>Added to the documentation links to the UI on HuggingFace to explore hyperparameters visually.</p>"},{"location":"release_notes/#deprecations","title":"Deprecations","text":""},{"location":"release_notes/#randomsnow","title":"RandomSnow","text":"<p>Updated interface:</p> <p>Old way:</p> Python<pre><code>transform = A.Compose([A.RandomSnow(\n  snow_point_lower=0.1,\n  snow_point_upper=0.3,\n  p=0.5\n)])\n</code></pre> <p>New way: Python<pre><code>transform = A.Compose([A.RandomSnow(\n  snow_point_range=(0.1, 0.3),\n  p=0.5\n)])\n</code></pre></p> <p>by @MarognaLorenzo</p>"},{"location":"release_notes/#randomrain","title":"RandomRain","text":"<p>Old way Python<pre><code>transform = A.Compose([A.RandomSnow(\n  slant_lower=-10,\n  slant_upper=10,\n  p=0.5\n)])\n</code></pre></p> <p>New way: Python<pre><code>transform = A.Compose([A.RandomRain(\n  slant_range=(-10, 10),\n  p=0.5\n)])\n</code></pre></p> <p>by @MarognaLorenzo</p>"},{"location":"release_notes/#improvements","title":"Improvements","text":"<p>Created library with core functions albucore. Moved a few helper functions there. We need this library to be sure that transforms are: 1. At least as fast as <code>numpy</code> and <code>opencv</code>. For some functions it is possible to be faster than both of them. 2. Easier to debug. 3. Could be used in other projects, not related to Albumentations.</p>"},{"location":"release_notes/#bugfixes","title":"Bugfixes","text":"<p>Bugfix in <code>check_for_updates</code>. Previously, the pipeline would throw an error if it failed to check for updates due to network issues or server unavailability. Now, it handles these exceptions gracefully and continues without interruption. - Bugfix in <code>RandomShadow</code>. Does not create unexpected purple color on bright white regions with shadow overlay anymore. - BugFix in <code>Compose</code>. Now <code>Compose([])</code> does not throw an error, but just works as <code>NoOp</code> by @ayasyrev - Bugfix in <code>min_max</code> normalization. Now return 0 and not NaN on constant images. by @ternaus  Bugfix in <code>CropAndPad</code>. Now we can sample pad/crop values for all sides with interface like <code>((-0.1, -0.2), (-0.2, -0.3), (0.3, 0.4), (0.4, 0.5))</code>, allowing for more flexible and precise control over padding and cropping dimensions by @christian-steinmeyer - Small refactoring to decrease tech debt by @ternaus and @ayasyrev</p>"},{"location":"release_notes/#albumentations-147-release-notes","title":"Albumentations 1.4.7 Release Notes","text":"<ul> <li>Support our work</li> <li>Documentation</li> <li>Deprecations</li> <li>Improvements and bug fixes</li> </ul>"},{"location":"release_notes/#support-our-work_1","title":"Support Our Work","text":"<ol> <li>Love the library? You can contribute to its development by becoming a  sponsor for the library. Your support is invaluable, and every contribution makes a difference.</li> <li>Haven't starred our repo yet? Show your support with a \u2b50! It's just only one mouse click.</li> <li>Got ideas or facing issues? We'd love to hear from you. Share your thoughts in our issues or join the conversation on our Discord server for Albumentations</li> </ol>"},{"location":"release_notes/#documentation_1","title":"Documentation","text":"<ul> <li>Added to the website tutorial on how to use Albumentations with Hugginigface for object Detection. Based on the tutorial by @qubvel</li> </ul>"},{"location":"release_notes/#deprecations_1","title":"Deprecations","text":""},{"location":"release_notes/#imagecompression","title":"ImageCompression","text":"<p>Old way:</p> Python<pre><code>transform = A.Compose([A.ImageCompression(\n  quality_lower=75,\n  quality_upper=100,\n  p=0.5\n)])\n</code></pre> <p>New way:</p> <p>Python<pre><code>transform = A.Compose([A.ImageCompression(\n  quality_range=(75, 100),\n  p=0.5\n)])\n</code></pre> by @MarognaLorenzo</p>"},{"location":"release_notes/#downscale","title":"Downscale","text":"<p>Old way: Python<pre><code>transform = A.Compose([A.Downscale(\n  scale_min=0.25,\n  scale_max=1,\n  interpolation= {\"downscale\": cv2.INTER_AREA, \"upscale\": cv2.INTER_CUBIC},\n  p=0.5\n)])\n</code></pre></p> <p>New way: Python<pre><code>transform = A.Compose([A.Downscale(\n  scale_range=(0.25, 1),\n interpolation_pair = {\"downscale\": cv2.INTER_AREA, \"upscale\": cv2.INTER_CUBIC},\n  p=0.5\n)])\n</code></pre></p> <p>As of now both ways work and will provide the same result, but old functionality will be removed in later releases.</p> <p>by @ternaus</p>"},{"location":"release_notes/#improvements_1","title":"Improvements","text":"<ul> <li>Buggix in <code>Blur</code>.</li> <li>Bugfix in <code>bbox clipping</code>, it could be not intuitive, but boxes should be clipped by <code>height, width</code> and not <code>height - 1, width -1</code> by @ternaus</li> <li>Allow to compose only keys, that are required there. Any extra unnecessary key will give an error by @ayasyrev</li> <li>In <code>PadIfNeeded</code> if value parameter is not None, but border mode is reflection, border mode is changed to <code>cv2.BORDER_CONSTANT</code> by @ternaus</li> </ul>"},{"location":"release_notes/#albumentations-146-release-notes","title":"Albumentations 1.4.6 Release Notes","text":""},{"location":"release_notes/#this-is-out-of-schedule-release-with-a-bugfix-that-was-introduced-in-version-145","title":"This is out of schedule release with a bugfix that was introduced in version 1.4.5","text":"<p>In version 1.4.5 there was a bug that went unnoticed - if you used pipeline that consisted only of <code>ImageOnly</code> transforms but pass bounding boxes into it, you would get an error.</p> <p>If you had in such pipeline at least one non <code>ImageOnly</code> transform, say <code>HorizontalFlip</code> or <code>Crop</code>, everything would work as expected.</p> <p>We fixed the issue and added tests to be sure that it will not happen in the future.</p>"},{"location":"release_notes/#albumentations-145-release-notes","title":"Albumentations 1.4.5 Release Notes","text":"<ul> <li>Support our work</li> <li>Highlights</li> <li>Deprecations</li> <li>Improvements and bug fixes</li> </ul>"},{"location":"release_notes/#support-our-work_2","title":"Support Our Work","text":"<ol> <li>Love the library? You can contribute to its development by becoming a  sponsor for the library. Your support is invaluable, and every contribution makes a difference.</li> <li>Haven't starred our repo yet? Show your support with a \u2b50! It's just only one mouse click.</li> <li>Got ideas or facing issues? We'd love to hear from you. Share your thoughts in our issues or join the conversation on our Discord server for Albumentations</li> </ol>"},{"location":"release_notes/#highlights","title":"Highlights","text":""},{"location":"release_notes/#bbox-clipping","title":"Bbox clipping","text":"<p>Before version 1.4.5 it was assumed that bounding boxes that are fed into the augmentation pipeline should not extend outside of the image.</p> <p>Now we added an option to clip boxes to the image size before augmenting them. This makes pipeline more robust to inaccurate labeling</p> <p>Example:</p> <p>Will fail if boxes extend outside of the image: Python<pre><code>transform = A.Compose([\n    A.HorizontalFlip(p=0.5)\n], bbox_params=A.BboxParams(format='coco'))\n</code></pre></p> <p>Clipping bounding boxes to the image size:</p> Python<pre><code>transform = A.Compose([\n    A.HorizontalFlip(p=0.5)\n], bbox_params=A.BboxParams(format='coco', clip=True))\n</code></pre> <p>by @ternaus</p>"},{"location":"release_notes/#selectivechanneltransform","title":"SelectiveChannelTransform","text":"<p>Added SelectiveChannelTransform that allows to apply transforms to a selected number of channels.</p> <p>For example it could be helpful when working with multispectral images, when RGB is a subset of the overall multispectral stack which is common when working with satellite imagery.</p> <p>Example:</p> <p>Python<pre><code>aug = A.Compose(\n        [A.HorizontalFlip(p=0.5),\n        A.SelectiveChannelTransform(transforms=[A.ColorJItter(p=0.5),\n        A.ChromaticAberration(p=0.5))], channels=[1, 2, 18], p=1)],\n    )\n</code></pre> Here HorizontalFlip applied to the whole multispectral image, but pipeline of <code>ColorJitter</code> and <code>ChromaticAberration</code> only to channels <code>[1, 2, 18]</code></p> <p>by @ternaus</p>"},{"location":"release_notes/#deprecations_2","title":"Deprecations","text":""},{"location":"release_notes/#coarsedropout","title":"CoarseDropout","text":"<p>Old way: Python<pre><code>transform = A.Compose([A.CoarseDropout(\n  min_holes = 5,\n  max_holes = 8,\n  min_width = 3,\n  max_width = 12,\n  min_height = 4,\n  max_height = 5\n)])\n</code></pre></p> <p>New way: Python<pre><code>transform = A.Compose([A.CoarseDropout(\n  num_holes_range=(5, 8),\n  hole_width_range=(3, 12),\n  hole_height_range=(4, 5)\n)])\n</code></pre></p> <p>As of now both ways work and will provide the same result, but old functionality will be removed in later releases.</p> <p>@ternaus</p>"},{"location":"release_notes/#improvements-and-bug-fixes","title":"Improvements and bug fixes","text":"<ul> <li>Number of fixes and speedups in the core of the library <code>Compose</code> and <code>BasicTransform</code> by @ayasyrev</li> <li>Extended <code>Contributor's guide</code> by @ternaus</li> <li>Can use <code>random</code> for <code>fill_value</code> in <code>CoarseDropout</code>by @ternaus</li> <li>Fix in ToGray docstring by @wilderrodrigues</li> <li>BufFix in D4 - now works not only with square, but with rectangular images as well. By @ternaus</li> <li>BugFix in RandomCropFromBorders by @ternaus</li> </ul>"},{"location":"release_notes/#albumentations-144-release-notes","title":"Albumentations 1.4.4 Release Notes","text":"<ul> <li>Support our work</li> <li>Highlights</li> <li>Transforms</li> <li>Improvements and bug fixes</li> </ul>"},{"location":"release_notes/#support-our-work_3","title":"Support Our Work","text":"<ol> <li>Love the library? You can contribute to its development by becoming a  sponsor for the library. Your support is invaluable, and every contribution makes a difference.</li> <li>Haven't starred our repo yet? Show your support with a \u2b50! It's just only one mouse click.</li> <li>Got ideas or facing issues? We'd love to hear from you. Share your thoughts in our issues or join the conversation on our Discord server for Albumentations</li> </ol>"},{"location":"release_notes/#transforms","title":"Transforms","text":""},{"location":"release_notes/#added-d4-transform","title":"Added D4 transform","text":"<p>Applies one of the eight possible D4 dihedral group transformations to a square-shaped input, maintaining the square shape. These transformations correspond to the symmetries of a square, including rotations and reflections by @ternaus</p> <p>The D4 group transformations include:     - <code>e</code> (identity): No transformation is applied.     - <code>r90</code> (rotation by 90 degrees counterclockwise)     - <code>r180</code> (rotation by 180 degrees)     - <code>r270</code> (rotation by 270 degrees counterclockwise)     - <code>v</code> (reflection across the vertical midline)     - <code>hvt</code> (reflection across the anti-diagonal)     - <code>h</code> (reflection across the horizontal midline)     - <code>t</code> (reflection across the main diagonal)</p> <p>Could be applied to: - image - mask - bounding boxes - key points</p> <p>Does not generate interpolation artifacts as there is no interpolation.</p> <p>Provides the most value in tasks where data is invariant to rotations and reflections like: - Top view drone and satellite imagery - Medical images</p> <p>Example:</p> <p></p>"},{"location":"release_notes/#added-new-normalizations-to-normalize-transform","title":"Added new normalizations to Normalize transform","text":"<ul> <li><code>standard</code> - <code>subtract</code> fixed mean, divide by fixed <code>std</code></li> <li><code>image</code> -  the same as <code>standard</code>, but <code>mean</code> and <code>std</code> computed for each image independently.</li> <li><code>image_per_channel</code> -  the same as before, but per channel</li> <li><code>min_max</code> - subtract <code>min(image)</code>and divide by <code>max(image) - min(image)</code></li> <li><code>min_max_per_channel</code> - the same, but per channel by @ternaus</li> </ul>"},{"location":"release_notes/#changes-in-the-interface-of-randomshadow","title":"Changes in the interface of RandomShadow","text":"<p>New, preferred wat is to use <code>num_shadows_limit</code> instead of <code>num_shadows_lower</code> / <code>num_shadows_upper</code> by @ayasyrev</p>"},{"location":"release_notes/#improvements-and-bug-fixes_1","title":"Improvements and bug fixes","text":""},{"location":"release_notes/#added-check-for-input-parameters-to-transforms-with-pydantic","title":"Added check for input parameters to transforms with Pydantic","text":"<p>Now all input parameters are validated and prepared with Pydantic. This will prevent bugs, when transforms are initialized without errors with parameters that are outside of allowed ranges. by @ternaus</p>"},{"location":"release_notes/#updates-in-randomgridshuffle","title":"Updates in RandomGridShuffle","text":"<ol> <li>Bugfix by @ayasyrev</li> <li>Transform updated to work even if side is not divisible by the number of tiles. by @ternaus</li> </ol> <p>Example: </p>"},{"location":"release_notes/#new-way-to-add-additional-targets","title":"New way to add additional targets","text":"<p>Standard way uses <code>additional_targets</code></p> Python<pre><code>transform = A.Compose(\n    transforms=[A.Rotate(limit=(90.0, 90.0), p=1.0)],\n    keypoint_params=A.KeypointParams(\n        angle_in_degrees=True,\n        check_each_transform=True,\n        format=\"xyas\",\n        label_fields=None,\n        remove_invisible=False,\n    ),\n    additional_targets={\"keypoints2\": \"keypoints\"},\n)\n</code></pre> <p>Now you can also add them using <code>add_targets</code>:</p> Python<pre><code>transform = A.Compose(\n    transforms=[A.Rotate(limit=(90.0, 90.0), p=1.0)],\n    keypoint_params=A.KeypointParams(\n        angle_in_degrees=True,\n        check_each_transform=True,\n        format=\"xyas\",\n        label_fields=None,\n        remove_invisible=False,\n    ),\n)\ntransform.add_targets({\"keypoints2\": \"keypoints\"})\n</code></pre> <p>by @ayasyrev</p>"},{"location":"release_notes/#small-fixes","title":"Small fixes","text":"<ul> <li>Small speedup in the code for transforms that use <code>add_weighted</code> function by @gogetron</li> <li>Fix in error message in Affine transform by @matsumotosan</li> <li>Bugfix in Sequential by @ayasyrev</li> </ul>"},{"location":"release_notes/#documentation_2","title":"Documentation","text":"<ul> <li>Updated Contributor's guide. by @ternaus</li> <li>Added example notebook on how to apply D4 to images, masks, bounding boxes and key points. by @ternaus</li> <li>Added example notebook on how to apply RandomGridShuffle to images, masks and keypoints. by @ternaus</li> </ul>"},{"location":"release_notes/#albumentations-143-release-notes","title":"Albumentations 1.4.3 Release Notes","text":"<ul> <li>Request</li> <li>Highlights</li> <li>New transform</li> <li>Minor improvements and bug fixes</li> </ul>"},{"location":"release_notes/#request","title":"Request","text":"<ol> <li>If you enjoy using the library as an individual developer or a company representative, please consider becoming a sponsor for the library. Every dollar helps.</li> <li>If you did not give our repo a \u2b50, it is only one mouse click</li> <li>If you have feature requests or proposals or encounter issues - submit your request to issues or ask in Discord server for Albumentations</li> </ol>"},{"location":"release_notes/#new-transform","title":"New transform","text":"<ul> <li>Added <code>Morphological</code> transform that modifies the structure of the image. Dilation expands the white (foreground) regions in a binary or grayscale image, while erosion shrinks them.</li> </ul>"},{"location":"release_notes/#minor-improvements-and-bug-fixes","title":"Minor improvements and bug fixes","text":"<ul> <li>Updated benchmark for uint8 images, processed on CPU. Added Kornia and Augly. LINK by @ternaus</li> <li>Bugfix in FDA transform by @ternaus</li> <li>Now RandomSizedCrop supports the same signature as analogous transform in torchvision by @zetyquickly</li> </ul>"},{"location":"release_notes/#albumentations-142-release-notes","title":"Albumentations 1.4.2 Release Notes","text":"<ul> <li>Request</li> <li>Highlights</li> <li>New transform</li> <li>New functionality</li> <li>Improvements and bug fixes</li> </ul>"},{"location":"release_notes/#request_1","title":"Request","text":"<ol> <li>If you enjoy using the library as an individual developer or as a representative of the company please consider becoming a sponsor for the library. Every dollar helps.</li> <li>If you did not give our repo a \u2b50, it is only one mouse click</li> <li>If you have feature requests or proposals or encounter issues - submit your request to issues or ask in Discord server for Albumentations</li> </ol>"},{"location":"release_notes/#new-transform_1","title":"New transform","text":"<p> Left: Original, Middle: Chromatic aberration (default args, mode=\"green_purple\"), Right:  Chromatic aberration (default args, mode=\"red_blue\")     (Image is from our internal mobile mapping dataset)   </p> <ul> <li>Added <code>ChromaticAbberation</code> transform that adds chromatic distortion to the image. Wiki by @mrsmrynk</li> </ul>"},{"location":"release_notes/#new-functionality","title":"New functionality","text":"<ul> <li>Return <code>mixing parameter</code> for <code>MixUp</code> transform by @Dipet. For more details Tutorial on MixUp</li> </ul>"},{"location":"release_notes/#improvements-and-bugfixes","title":"Improvements and Bugfixes","text":"<ul> <li>Do not throw deprecation warning when people do not use deprecated parameters in <code>AdvancedBlur</code> by @Aloqeely</li> <li>Updated <code>CONTRIBUTORS.md</code> for Windows users by @Aloqeely</li> <li>Fixed Docstring for <code>DownScale</code> transform by @ryoryon66</li> <li>Bugfix in <code>PadIfNeeded</code> serialization @ternaus</li> </ul>"},{"location":"release_notes/#albumentations-141-release-notes-4-march-2024","title":"Albumentations 1.4.1 Release Notes (4 March 2024)","text":"<ul> <li>Request</li> <li>Highlights</li> <li>New transform</li> <li>Improvements</li> <li>Bug fixes</li> </ul>"},{"location":"release_notes/#request_2","title":"Request","text":"<ol> <li>If you enjoy using the library as an individual developer or during the day job as a part of the company, please consider becoming a sponsor for the library. Every dollar helps.</li> <li>If you did not give our repo a \u2b50, it is only one mouse click</li> <li>If you have feature requests or proposals or encounter issues - submit your request to issues or our new initiative, - Discord server for albumentations</li> </ol>"},{"location":"release_notes/#new-transform_2","title":"New transform","text":"<ul> <li>Added <code>MixUp</code> transform: which linearly combines an input (image, mask, and class label) with another set from a predefined reference dataset. The mixing degree is controlled by a parameter \u03bb (lambda), sampled from a Beta distribution. This method is known for improving model generalization by promoting linear behavior between classes and smoothing decision boundaries.</li> </ul>"},{"location":"release_notes/#minor-changes-and-bug-fixes","title":"Minor changes and Bug Fixes","text":"<ul> <li>Moved from <code>isort</code>, <code>flake8</code>, <code>black</code> to <code>ruff</code></li> <li>Added extra checks for docstrings to match Google Style.</li> <li>Updated Who's using</li> <li>Removed quidda dependency, which addresses <code>opencv</code> library inconsistencies issues</li> <li>New, updated version of benchmark.</li> </ul>"},{"location":"release_notes/#albumentations-140-release-notes-17-february-2024","title":"Albumentations 1.4.0 Release Notes (17 February 2024)","text":"<ul> <li>Request</li> <li>Highlights</li> <li>New transform</li> <li>Backwards Incompatible Changes</li> <li>Improvements</li> <li>Bug fixes</li> </ul>"},{"location":"release_notes/#request_3","title":"Request","text":"<ol> <li>If you enjoy using the library as an individual developer or during the day job as a part of the company, please consider becoming a sponsor for the library. Every dollar helps.</li> <li>If you did not give our repo a \u2b50, it is [only one mouse click].(https://github.com/albumentations-team/albumentations)</li> <li>If you have feature requests, proposals, or encounter issues - submit your request to issues or, our new initiative, - Discord server for albumentations</li> </ol>"},{"location":"release_notes/#highlights_1","title":"Highlights","text":"<p>In this release, we mainly focused on the technical debt as its decrease allows faster iterations and bug fixes in the codebase. We added only one new transform, did not work on speeding up transforms, and other changes are minor.</p> <ol> <li>We are removing the dependency on the imgaug library. The library was one of our inspirations when we created Albumentations, but maintainers of imgaug ceased its support which caused inconsistencies in library versions. It was done in 2021, say commit https://github.com/albumentations-team/albumentations/commit/ba44effb0369ba5eae1e8eb4909105eac9709230 by @Dipet .</li> </ol> <p>But, somehow, we are cutting this dependency only in 2024.</p> <ol> <li>Added typing in all of the codebase. When we started the library, Python 2 was still widely used; hence, none of the original codebases had types specified for function arguments and return types. Since the end of the support for Python 2, we added types to the new or updated code, but only now have we covered all the codebase.</li> </ol>"},{"location":"release_notes/#new-transform_3","title":"New transform","text":"<ul> <li>Added <code>XYMasking</code> transform: applies masking strips to an image, either horizontally (X axis) or vertically (Y axis), simulating occlusions. This transform is helpful for training models to recognize images with varied visibility conditions. It's particularly effective for spectrogram images, allowing spectral and frequency masking to improve model robustness. As other dropout transforms CoarseDropout, MaskDropout, GridDropout it supports images, masks and keypoints as targets. (https://github.com/albumentations-team/albumentations/commit/004fabbf90794fbc21ee356e2dde6637b7fecbd4 by @ternaus )</li> </ul>"},{"location":"release_notes/#backward-incompatible-changes","title":"Backward Incompatible Changes","text":"<p>The deprecated code, including 15 transforms, was removed. Dependency on the imgaug library was removed.</p> <p>(https://github.com/albumentations-team/albumentations/commit/be6a217b207b3d7ebe792caabb438d660b45f2a5 by @ternaus )</p>"},{"location":"release_notes/#deleted-transforms","title":"Deleted Transforms","text":"<ol> <li><code>JpegCompression</code>. Use ImageCompression instead.</li> <li><code>RandomBrightness</code>. Use RandomBrigtnessContrast instead.</li> <li><code>RandomContrast</code>. Use RandomBrigtnessContrast instead.</li> <li><code>Cutout</code>. Use CoarseDropout instead.</li> <li><code>ToTensor</code>. Use ToTensorV2 instead.</li> <li><code>IAAAdditiveGaussianNoise</code>. Use GaussNoise instead.</li> <li><code>IAAAffine</code>. Use Affine instead.</li> <li>IAACropAndPad. Use CropAndPad instead.</li> <li><code>IAAEmboss</code>. Use Emboss instead.</li> <li><code>IAAFliplr</code>. Use HorizontalFlip instead.</li> <li><code>IAAFlipud</code>. Use VerticalFlip instead.</li> <li><code>IAAPerspective</code>. Use Perspective instead.</li> <li><code>IAAPiecewiseAffine</code>. Use PiecewiseAffine instead.</li> <li><code>IAASharpen</code>. Use Sharpen instead.</li> <li><code>IAASuperpixels</code>. Use Superpixels instead.</li> </ol>"},{"location":"release_notes/#other-deprecated-functionality","title":"Other deprecated functionality","text":"<ul> <li>Removed  <code>eps</code> parameter in RandomGamma</li> <li>Removed <code>lambda_transforms</code>in <code>serialization.from_dict</code> function.</li> </ul> <p>## Minor changes and Bug Fixes  * Added details Contributor's guide  * Added support for <code>matrix=None</code> case for Piecewise affine transform (https://github.com/albumentations-team/albumentations/commit/c70e664e060bfd7463c20674927aed217f72d437 @Dipet )  * Bugfix - Eliminated the possibility of the Perspective transform collapsing (https://github.com/albumentations-team/albumentations/commit/a919a772d763e0c62b674ca490a97c89e0b9c5a3 @alicangok )  * Fixes in docstrings (@domef, @aaronzs, @Dipet, @ternaus  )  * Added checks for python 3.12</p>"},{"location":"release_notes/#052-29-november-2020","title":"0.5.2 (29 November 2020)","text":""},{"location":"release_notes/#minor-changes","title":"Minor changes","text":"<ul> <li>ToTensorV2 now automatically expands grayscale images with the shape <code>[H, W]</code> to the shape <code>[H, W, 1]</code>. PR #604 by @Ingwar.</li> <li>CropNonEmptyMaskIfExists  now also works with multiple masks that are provided by the <code>masks</code> argument to the transform function. Previously this augmentation worked only with a single mask provided by the <code>mask</code> argument. PR #761.</li> </ul>"},{"location":"release_notes/#051-2-november-2020","title":"0.5.1 (2 November 2020)","text":""},{"location":"release_notes/#breaking-changes","title":"Breaking changes","text":"<ul> <li>API for <code>A.FDA</code> is changed to resemble API of <code>A.HistogramMatching</code>. Now, both transformations expect to receive a list of reference images, a function to read those image, and additional augmentation parameters. (#734)</li> <li><code>A.HistogramMatching</code> now uses<code>read_rgb_image</code> as a default <code>read_fn</code>. This function reads an image from the disk as an RGB NumPy array. Previously, the default <code>read_fn</code> was <code>cv2.imread</code> which read an image as a BGR NumPy array. (#734)</li> </ul>"},{"location":"release_notes/#new-transformations","title":"New transformations","text":"<ul> <li><code>A.Sequential</code> transform that can apply augmentations in a sequence. This transform is not intended to be a replacement for <code>A.Compose</code>. Instead, it should be used inside <code>A.Compose</code> the same way <code>A.OneOf</code> or <code>A.OneOrOther</code>. For instance, you can combine <code>A.OneOf</code> with <code>A.Sequential</code> to create an augmentation pipeline containing multiple sequences of augmentations and apply one randomly chosen sequence to input data. (#735)</li> </ul>"},{"location":"release_notes/#minor-changes_1","title":"Minor changes","text":"<ul> <li><code>A.ShiftScaleRotate</code> now has two additional optional parameters: <code>shift_limit_x</code> and <code>shift_limit_y</code>. If either of those parameters (or both of them) is set <code>A.ShiftScaleRotate</code> will use the set values to shift images on the respective axis. (#735)</li> <li><code>A.ToTensorV2</code> now supports an additional argument <code>transpose_mask</code> (<code>False</code> by default). If the argument is set to <code>True</code> and an input mask has 3 dimensions, <code>A.ToTensorV2</code> will transpose dimensions of a mask tensor in addition to transposing dimensions of an image tensor. (#735)</li> </ul>"},{"location":"release_notes/#bugfixes_1","title":"Bugfixes","text":"<ul> <li><code>A.FDA</code> now correctly uses coordinates of the center of an image. (#730)</li> <li>Fixed problems with grayscale images for <code>A.HistogramMatching</code>. (#734)</li> <li>Fixed a bug that led to an exception when <code>A.load()</code> was called to deserialize a pipeline that contained <code>A.ToTensor</code> or <code>A.ToTensorV2</code>, but those transforms were not imported in the code before the call. (#735)</li> </ul>"},{"location":"release_notes/#050-19-october-2020","title":"0.5.0 (19 October 2020)","text":""},{"location":"release_notes/#breaking-changes_1","title":"Breaking changes","text":"<ul> <li>Albumentations now explicitly checks that all inputs to augmentations are named arguments and raise an exception otherwise. So if an augmentation receives input like aug(image) instead of aug(image=image), Albumentations will raise an exception. (#560)</li> <li>Dropped support of Python 3.5 (#709)</li> <li>Keypoints and bboxes are checked for visibility after each transform (#566)</li> </ul>"},{"location":"release_notes/#new-transformations_1","title":"New transformations","text":"<ul> <li><code>A.FDA</code> transform for Fourier-based domain adaptation. (#685)</li> <li><code>A.HistogramMatching</code> transform that applies histogram matching. (#708)</li> <li><code>A.ColorJitter</code> transform that behaves similarly to <code>ColorJitter</code> from torchvision (though there are some minor differences due to different internal logic for working with HSV colorspace in Pillow, which is used in torchvision and OpenCV, which is used in Albumentations). (#705)</li> </ul>"},{"location":"release_notes/#minor-changes_2","title":"Minor changes","text":"<ul> <li><code>A.PadIfNeeded</code> now accepts additional <code>pad_width_divisor</code>, <code>pad_height_divisor</code> (<code>None</code> by default) to ensure image has width &amp; height that is dividable by given values. (#700)</li> <li>Added support to apply <code>A.CoarseDropout</code> to masks via <code>mask_fill_value</code>. (#699)</li> <li><code>A.GaussianBlur</code> now supports the sigma parameter that sets standard deviation for Gaussian kernel. (#674, #673) .</li> </ul>"},{"location":"release_notes/#bugfixes_2","title":"Bugfixes","text":"<ul> <li>Fixed bugs in <code>A.HueSaturationValue</code> for float dtype. (#696, #710)</li> <li>Fixed incorrect rounding error on bboxes in <code>YOLO</code> format. (#688)</li> </ul>"},{"location":"release_notes/#046-19-july-2020","title":"0.4.6 (19 July 2020)","text":""},{"location":"release_notes/#improvements_2","title":"Improvements","text":"<ul> <li>Change the ImgAug dependency version from \u201cimgaug&gt;=0.2.5,&lt;0.2.7\u201d to \u201cimgaug&gt;=0.4.0\". Now Albumentations won\u2019t downgrade your existing ImgAug installation to the old version. PR #658.</li> <li>Do not try to resize an image if it already has the required height and width. That eliminates the redundant call to the OpenCV function that requires additional copying of the input data. PR #639. <code>ReplayCompose</code> is now serializable. PR #623 by IlyaOvodov</li> <li>Documentation fixes and updates.</li> </ul>"},{"location":"release_notes/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Fix a bug that causes some keypoints and bounding boxes to lie outside the visible part of the augmented image if an augmentation pipeline contained augmentations that increase the height and width of an image (such as <code>PadIfNeeded</code>). That happened because Albumentations checked which bounding boxes and keypoints lie outside the image only after applying all augmentations. Now Albumentations will check and remove keypoints and bounding boxes that lie outside the image after each augmentation. If, for some reason, you need the old behavior, pass <code>check_each_transform=False</code> in your <code>KeypointParams</code> or <code>BboxParams</code>. Issue #565 and PR #566.</li> <li>Fix a bug that causes an exception when Albumentations received images with the number of color channels that are even but are not multiples of 4 (such as 6, 10, etc.). PR #638.</li> <li>Fix the off-by-one error in applying steps for GridDistortion. Commit 9c225a9</li> <li>Fix bugs that prevent serialization of <code>ImageCompression</code> and <code>GaussNoise</code>. PR #569</li> <li>Fix a bug that causes errors with some values for <code>label_fields</code> in <code>BboxParams</code>. PR #504 by IlyaOvodov</li> <li>Fix a bug that prevents HueSaturationValue for working with grayscale images. PR #500.</li> </ul>"},{"location":"api_reference/","title":"Index","text":"<ul> <li>Full API Reference on a single page</li> <li>Core API (albumentations.core)<ul> <li>Composition API (albumentations.core.composition)</li> <li>Serialization API (albumentations.core.serialization)</li> <li>Transforms Interface (albumentations.core.transforms_interface)</li> <li>Helper functions for working with bounding boxes (albumentations.core.bbox_utils)</li> <li>Helper functions for working with keypoints (albumentations.core.keypoints_utils)</li> </ul> </li> <li>Augmentations (albumentations.augmentations)<ul> <li>Transforms (albumentations.augmentations.transforms)</li> <li>Functional transforms (albumentations.augmentations.functional)</li> </ul> </li> <li>PyTorch Helpers (albumentations.pytorch)<ul> <li>Transforms (albumentations.pytorch.transforms)</li> </ul> </li> </ul>"},{"location":"api_reference/full_reference/","title":"Full API Reference on a single page","text":""},{"location":"api_reference/full_reference/#pixel-level-transforms","title":"Pixel-level transforms","text":"<p>Here is a list of all available pixel-level transforms. You can apply a pixel-level transform to any target, and under the hood, the transform will change only the input image and return any other input targets such as masks, bounding boxes, or keypoints unchanged.</p> <ul> <li>AdvancedBlur</li> <li>Blur</li> <li>CLAHE</li> <li>ChannelDropout</li> <li>ChannelShuffle</li> <li>ChromaticAberration</li> <li>ColorJitter</li> <li>Defocus</li> <li>Downscale</li> <li>Emboss</li> <li>Equalize</li> <li>FDA</li> <li>FancyPCA</li> <li>FromFloat</li> <li>GaussNoise</li> <li>GaussianBlur</li> <li>GlassBlur</li> <li>HistogramMatching</li> <li>HueSaturationValue</li> <li>ISONoise</li> <li>ImageCompression</li> <li>InvertImg</li> <li>MedianBlur</li> <li>MotionBlur</li> <li>MultiplicativeNoise</li> <li>Normalize</li> <li>PixelDistributionAdaptation</li> <li>PlanckianJitter</li> <li>Posterize</li> <li>RGBShift</li> <li>RandomBrightnessContrast</li> <li>RandomFog</li> <li>RandomGamma</li> <li>RandomGravel</li> <li>RandomRain</li> <li>RandomShadow</li> <li>RandomSnow</li> <li>RandomSunFlare</li> <li>RandomToneCurve</li> <li>RingingOvershoot</li> <li>Sharpen</li> <li>Solarize</li> <li>Spatter</li> <li>Superpixels</li> <li>TemplateTransform</li> <li>TextImage</li> <li>ToFloat</li> <li>ToGray</li> <li>ToRGB</li> <li>ToSepia</li> <li>UnsharpMask</li> <li>ZoomBlur</li> </ul>"},{"location":"api_reference/full_reference/#spatial-level-transforms","title":"Spatial-level transforms","text":"<p>Here is a table with spatial-level transforms and targets they support. If you try to apply a spatial-level transform to an unsupported target, Albumentations will raise an error.</p> Transform Image Mask BBoxes Keypoints Global Label Affine \u2713 \u2713 \u2713 \u2713 BBoxSafeRandomCrop \u2713 \u2713 \u2713 \u2713 CenterCrop \u2713 \u2713 \u2713 \u2713 CoarseDropout \u2713 \u2713 \u2713 Crop \u2713 \u2713 \u2713 \u2713 CropAndPad \u2713 \u2713 \u2713 \u2713 CropNonEmptyMaskIfExists \u2713 \u2713 \u2713 \u2713 D4 \u2713 \u2713 \u2713 \u2713 ElasticTransform \u2713 \u2713 \u2713 GridDistortion \u2713 \u2713 \u2713 GridDropout \u2713 \u2713 GridElasticDeform \u2713 \u2713 HorizontalFlip \u2713 \u2713 \u2713 \u2713 Lambda \u2713 \u2713 \u2713 \u2713 \u2713 LongestMaxSize \u2713 \u2713 \u2713 \u2713 MaskDropout \u2713 \u2713 MixUp \u2713 \u2713 \u2713 Morphological \u2713 \u2713 NoOp \u2713 \u2713 \u2713 \u2713 \u2713 OpticalDistortion \u2713 \u2713 \u2713 OverlayElements \u2713 \u2713 PadIfNeeded \u2713 \u2713 \u2713 \u2713 Perspective \u2713 \u2713 \u2713 \u2713 PiecewiseAffine \u2713 \u2713 \u2713 \u2713 PixelDropout \u2713 \u2713 RandomCrop \u2713 \u2713 \u2713 \u2713 RandomCropFromBorders \u2713 \u2713 \u2713 \u2713 RandomGridShuffle \u2713 \u2713 \u2713 RandomResizedCrop \u2713 \u2713 \u2713 \u2713 RandomRotate90 \u2713 \u2713 \u2713 \u2713 RandomScale \u2713 \u2713 \u2713 \u2713 RandomSizedBBoxSafeCrop \u2713 \u2713 \u2713 \u2713 RandomSizedCrop \u2713 \u2713 \u2713 \u2713 Resize \u2713 \u2713 \u2713 \u2713 Rotate \u2713 \u2713 \u2713 \u2713 SafeRotate \u2713 \u2713 \u2713 \u2713 ShiftScaleRotate \u2713 \u2713 \u2713 \u2713 SmallestMaxSize \u2713 \u2713 \u2713 \u2713 Transpose \u2713 \u2713 \u2713 \u2713 VerticalFlip \u2713 \u2713 \u2713 \u2713 XYMasking \u2713 \u2713 \u2713"},{"location":"api_reference/full_reference/#albumentations.augmentations","title":"<code>augmentations</code>  <code>special</code>","text":""},{"location":"api_reference/full_reference/#albumentations.augmentations.blur","title":"<code>blur</code>  <code>special</code>","text":""},{"location":"api_reference/full_reference/#albumentations.augmentations.blur.transforms","title":"<code>transforms</code>","text":""},{"location":"api_reference/full_reference/#albumentations.augmentations.blur.transforms.AdvancedBlur","title":"<code>class  AdvancedBlur</code> <code>     (blur_limit=(3, 7), sigma_x_limit=(0.2, 1.0), sigma_y_limit=(0.2, 1.0), sigmaX_limit=None, sigmaY_limit=None, rotate_limit=90, beta_limit=(0.5, 8.0), noise_limit=(0.9, 1.1), always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Blurs the input image using a Generalized Normal filter with randomly selected parameters.</p> <p>This transform also adds multiplicative noise to the generated kernel before convolution, affecting the image in a unique way that combines blurring and noise injection for enhanced data augmentation.</p> <p>Parameters:</p> Name Type Description <code>blur_limit</code> <code>ScaleIntType</code> <p>Maximum Gaussian kernel size for blurring the input image. Must be zero or odd and in range [0, inf). If set to 0, it will be computed from sigma as <code>round(sigma * (3 if img.dtype == np.uint8 else 4) * 2 + 1) + 1</code>. If a single value is provided, <code>blur_limit</code> will be in the range (0, blur_limit). Defaults to (3, 7).</p> <code>sigma_x_limit</code> <code>ScaleFloatType</code> <p>Gaussian kernel standard deviation for the X dimension. Must be in range [0, inf). If a single value is provided, <code>sigma_x_limit</code> will be in the range (0, sigma_limit). If set to 0, sigma will be computed as <code>sigma = 0.3*((ksize-1)*0.5 - 1) + 0.8</code>. Defaults to (0.2, 1.0).</p> <code>sigma_y_limit</code> <code>ScaleFloatType</code> <p>Gaussian kernel standard deviation for the Y dimension. Must follow the same rules as <code>sigma_x_limit</code>. Defaults to (0.2, 1.0).</p> <code>rotate_limit</code> <code>ScaleIntType</code> <p>Range from which a random angle used to rotate the Gaussian kernel is picked. If limit is a single int, an angle is picked from (-rotate_limit, rotate_limit). Defaults to (-90, 90).</p> <code>beta_limit</code> <code>ScaleFloatType</code> <p>Distribution shape parameter. 1 represents the normal distribution. Values below 1.0 make distribution tails heavier than normal, and values above 1.0 make it lighter than normal. Defaults to (0.5, 8.0).</p> <code>noise_limit</code> <code>ScaleFloatType</code> <p>Multiplicative factor that controls the strength of kernel noise. Must be positive and preferably centered around 1.0. If a single value is provided, <code>noise_limit</code> will be in the range (0, noise_limit). Defaults to (0.75, 1.25).</p> <code>p</code> <code>float</code> <p>Probability of applying the transform. Defaults to 0.5.</p> <p>Reference</p> <p>\"Real-ESRGAN: Training Real-World Blind Super-Resolution with Pure Synthetic Data\", available at https://arxiv.org/abs/2107.10833</p> <p>Targets</p> <p>image</p> <p>Image types:     uint8, float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/blur/transforms.py</code> Python<pre><code>class AdvancedBlur(ImageOnlyTransform):\n    \"\"\"Blurs the input image using a Generalized Normal filter with randomly selected parameters.\n\n    This transform also adds multiplicative noise to the generated kernel before convolution,\n    affecting the image in a unique way that combines blurring and noise injection for enhanced\n    data augmentation.\n\n    Args:\n        blur_limit (ScaleIntType, optional): Maximum Gaussian kernel size for blurring the input image.\n            Must be zero or odd and in range [0, inf). If set to 0, it will be computed from sigma\n            as `round(sigma * (3 if img.dtype == np.uint8 else 4) * 2 + 1) + 1`.\n            If a single value is provided, `blur_limit` will be in the range (0, blur_limit).\n            Defaults to (3, 7).\n        sigma_x_limit ScaleFloatType: Gaussian kernel standard deviation for the X dimension.\n            Must be in range [0, inf). If a single value is provided, `sigma_x_limit` will be in the range\n            (0, sigma_limit). If set to 0, sigma will be computed as `sigma = 0.3*((ksize-1)*0.5 - 1) + 0.8`.\n            Defaults to (0.2, 1.0).\n        sigma_y_limit ScaleFloatType: Gaussian kernel standard deviation for the Y dimension.\n            Must follow the same rules as `sigma_x_limit`.\n            Defaults to (0.2, 1.0).\n        rotate_limit (ScaleIntType, optional): Range from which a random angle used to rotate the Gaussian kernel\n            is picked. If limit is a single int, an angle is picked from (-rotate_limit, rotate_limit).\n            Defaults to (-90, 90).\n        beta_limit (ScaleFloatType, optional): Distribution shape parameter. 1 represents the normal distribution.\n            Values below 1.0 make distribution tails heavier than normal, and values above 1.0 make it\n            lighter than normal.\n            Defaults to (0.5, 8.0).\n        noise_limit (ScaleFloatType, optional): Multiplicative factor that controls the strength of kernel noise.\n            Must be positive and preferably centered around 1.0. If a single value is provided,\n            `noise_limit` will be in the range (0, noise_limit).\n            Defaults to (0.75, 1.25).\n        p (float, optional): Probability of applying the transform.\n            Defaults to 0.5.\n\n    Reference:\n        \"Real-ESRGAN: Training Real-World Blind Super-Resolution with Pure Synthetic Data\",\n        available at https://arxiv.org/abs/2107.10833\n\n    Targets:\n        image\n\n    Image types:\n        uint8, float32\n\n    \"\"\"\n\n    class InitSchema(BlurInitSchema):\n        sigma_x_limit: NonNegativeFloatRangeType = (0.2, 1.0)\n        sigma_y_limit: NonNegativeFloatRangeType = (0.2, 1.0)\n        beta_limit: NonNegativeFloatRangeType = (0.5, 8.0)\n        noise_limit: NonNegativeFloatRangeType = (0.75, 1.25)\n        rotate_limit: SymmetricRangeType = (-90, 90)\n\n        @field_validator(\"beta_limit\")\n        @classmethod\n        def check_beta_limit(cls, value: ScaleFloatType) -&gt; tuple[float, float]:\n            result = to_tuple(value, low=0)\n            if not (result[0] &lt; 1.0 &lt; result[1]):\n                msg = \"beta_limit is expected to include 1.0.\"\n                raise ValueError(msg)\n            return result\n\n        @model_validator(mode=\"after\")\n        def validate_limits(self) -&gt; Self:\n            if (\n                isinstance(self.sigma_x_limit, (tuple, list))\n                and self.sigma_x_limit[0] == 0\n                and isinstance(self.sigma_y_limit, (tuple, list))\n                and self.sigma_y_limit[0] == 0\n            ):\n                msg = \"sigma_x_limit and sigma_y_limit minimum value cannot be both equal to 0.\"\n                raise ValueError(msg)\n            return self\n\n    def __init__(\n        self,\n        blur_limit: ScaleIntType = (3, 7),\n        sigma_x_limit: ScaleFloatType = (0.2, 1.0),\n        sigma_y_limit: ScaleFloatType = (0.2, 1.0),\n        sigmaX_limit: ScaleFloatType | None = None,  # noqa: N803\n        sigmaY_limit: ScaleFloatType | None = None,  # noqa: N803\n        rotate_limit: ScaleIntType = 90,\n        beta_limit: ScaleFloatType = (0.5, 8.0),\n        noise_limit: ScaleFloatType = (0.9, 1.1),\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p, always_apply)\n\n        if sigmaX_limit is not None:\n            warnings.warn(\"sigmaX_limit is deprecated; use sigma_x_limit instead.\", DeprecationWarning, stacklevel=2)\n            sigma_x_limit = sigmaX_limit\n\n        if sigmaY_limit is not None:\n            warnings.warn(\"sigmaY_limit is deprecated; use sigma_y_limit instead.\", DeprecationWarning, stacklevel=2)\n            sigma_y_limit = sigmaY_limit\n\n        self.blur_limit = cast(Tuple[int, int], blur_limit)\n        self.sigma_x_limit = cast(Tuple[float, float], sigma_x_limit)\n        self.sigma_y_limit = cast(Tuple[float, float], sigma_y_limit)\n        self.rotate_limit = cast(Tuple[int, int], rotate_limit)\n        self.beta_limit = cast(Tuple[float, float], beta_limit)\n        self.noise_limit = cast(Tuple[float, float], noise_limit)\n\n    def apply(self, img: np.ndarray, kernel: np.ndarray, **params: Any) -&gt; np.ndarray:\n        return fmain.convolve(img, kernel=kernel)\n\n    def get_params(self) -&gt; dict[str, np.ndarray]:\n        ksize = random.randrange(self.blur_limit[0], self.blur_limit[1] + 1, 2)\n        sigma_x = random.uniform(*self.sigma_x_limit)\n        sigma_y = random.uniform(*self.sigma_y_limit)\n        angle = np.deg2rad(random.uniform(*self.rotate_limit))\n\n        # Split into 2 cases to avoid selection of narrow kernels (beta &gt; 1) too often.\n        beta = (\n            random.uniform(self.beta_limit[0], 1) if random.random() &lt; HALF else random.uniform(1, self.beta_limit[1])\n        )\n\n        noise_matrix = random_utils.uniform(self.noise_limit[0], self.noise_limit[1], size=[ksize, ksize])\n\n        # Generate mesh grid centered at zero.\n        ax = np.arange(-ksize // 2 + 1.0, ksize // 2 + 1.0)\n        # &gt; Shape (ksize, ksize, 2)\n        grid = np.stack(np.meshgrid(ax, ax), axis=-1)\n\n        # Calculate rotated sigma matrix\n        d_matrix = np.array([[sigma_x**2, 0], [0, sigma_y**2]])\n        u_matrix = np.array([[np.cos(angle), -np.sin(angle)], [np.sin(angle), np.cos(angle)]])\n        sigma_matrix = np.dot(u_matrix, np.dot(d_matrix, u_matrix.T))\n\n        inverse_sigma = np.linalg.inv(sigma_matrix)\n        # Described in \"Parameter Estimation For Multivariate Generalized Gaussian Distributions\"\n        kernel = np.exp(-0.5 * np.power(np.sum(np.dot(grid, inverse_sigma) * grid, 2), beta))\n        # Add noise\n        kernel *= noise_matrix\n\n        # Normalize kernel\n        kernel = kernel.astype(np.float32) / np.sum(kernel)\n        return {\"kernel\": kernel}\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, str, str, str, str, str]:\n        return (\n            \"blur_limit\",\n            \"sigma_x_limit\",\n            \"sigma_y_limit\",\n            \"rotate_limit\",\n            \"beta_limit\",\n            \"noise_limit\",\n        )\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.blur.transforms.AdvancedBlur.apply","title":"<code>apply (self, img, kernel, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/blur/transforms.py</code> Python<pre><code>def apply(self, img: np.ndarray, kernel: np.ndarray, **params: Any) -&gt; np.ndarray:\n    return fmain.convolve(img, kernel=kernel)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.blur.transforms.AdvancedBlur.get_params","title":"<code>get_params (self)</code>","text":"<p>Returns parameters independent of input.</p> Source code in <code>albumentations/augmentations/blur/transforms.py</code> Python<pre><code>def get_params(self) -&gt; dict[str, np.ndarray]:\n    ksize = random.randrange(self.blur_limit[0], self.blur_limit[1] + 1, 2)\n    sigma_x = random.uniform(*self.sigma_x_limit)\n    sigma_y = random.uniform(*self.sigma_y_limit)\n    angle = np.deg2rad(random.uniform(*self.rotate_limit))\n\n    # Split into 2 cases to avoid selection of narrow kernels (beta &gt; 1) too often.\n    beta = (\n        random.uniform(self.beta_limit[0], 1) if random.random() &lt; HALF else random.uniform(1, self.beta_limit[1])\n    )\n\n    noise_matrix = random_utils.uniform(self.noise_limit[0], self.noise_limit[1], size=[ksize, ksize])\n\n    # Generate mesh grid centered at zero.\n    ax = np.arange(-ksize // 2 + 1.0, ksize // 2 + 1.0)\n    # &gt; Shape (ksize, ksize, 2)\n    grid = np.stack(np.meshgrid(ax, ax), axis=-1)\n\n    # Calculate rotated sigma matrix\n    d_matrix = np.array([[sigma_x**2, 0], [0, sigma_y**2]])\n    u_matrix = np.array([[np.cos(angle), -np.sin(angle)], [np.sin(angle), np.cos(angle)]])\n    sigma_matrix = np.dot(u_matrix, np.dot(d_matrix, u_matrix.T))\n\n    inverse_sigma = np.linalg.inv(sigma_matrix)\n    # Described in \"Parameter Estimation For Multivariate Generalized Gaussian Distributions\"\n    kernel = np.exp(-0.5 * np.power(np.sum(np.dot(grid, inverse_sigma) * grid, 2), beta))\n    # Add noise\n    kernel *= noise_matrix\n\n    # Normalize kernel\n    kernel = kernel.astype(np.float32) / np.sum(kernel)\n    return {\"kernel\": kernel}\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.blur.transforms.AdvancedBlur.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/blur/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, str, str, str, str, str]:\n    return (\n        \"blur_limit\",\n        \"sigma_x_limit\",\n        \"sigma_y_limit\",\n        \"rotate_limit\",\n        \"beta_limit\",\n        \"noise_limit\",\n    )\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.blur.transforms.Blur","title":"<code>class  Blur</code> <code>     (blur_limit=7, p=0.5, always_apply=None)                 </code>  [view source on GitHub]","text":"<p>Blur the input image using a random-sized kernel.</p> <p>Parameters:</p> Name Type Description <code>blur_limit</code> <code>ScaleIntType</code> <p>maximum kernel size for blurring the input image. Should be in range [3, inf). Default: (3, 7).</p> <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 0.5.</p> <p>Targets</p> <p>image</p> <p>Image types:     uint8, float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/blur/transforms.py</code> Python<pre><code>class Blur(ImageOnlyTransform):\n    \"\"\"Blur the input image using a random-sized kernel.\n\n    Args:\n        blur_limit: maximum kernel size for blurring the input image.\n            Should be in range [3, inf). Default: (3, 7).\n        p: probability of applying the transform. Default: 0.5.\n\n    Targets:\n        image\n\n    Image types:\n        uint8, float32\n\n    \"\"\"\n\n    class InitSchema(BlurInitSchema):\n        pass\n\n    def __init__(self, blur_limit: ScaleIntType = 7, p: float = 0.5, always_apply: bool | None = None):\n        super().__init__(p, always_apply)\n        self.blur_limit = cast(Tuple[int, int], blur_limit)\n\n    def apply(self, img: np.ndarray, kernel: int, **params: Any) -&gt; np.ndarray:\n        return fblur.blur(img, kernel)\n\n    def get_params(self) -&gt; dict[str, Any]:\n        return {\"kernel\": random_utils.choice(list(range(self.blur_limit[0], self.blur_limit[1] + 1, 2)))}\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return (\"blur_limit\",)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.blur.transforms.Blur.apply","title":"<code>apply (self, img, kernel, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/blur/transforms.py</code> Python<pre><code>def apply(self, img: np.ndarray, kernel: int, **params: Any) -&gt; np.ndarray:\n    return fblur.blur(img, kernel)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.blur.transforms.Blur.get_params","title":"<code>get_params (self)</code>","text":"<p>Returns parameters independent of input.</p> Source code in <code>albumentations/augmentations/blur/transforms.py</code> Python<pre><code>def get_params(self) -&gt; dict[str, Any]:\n    return {\"kernel\": random_utils.choice(list(range(self.blur_limit[0], self.blur_limit[1] + 1, 2)))}\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.blur.transforms.Blur.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/blur/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return (\"blur_limit\",)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.blur.transforms.Defocus","title":"<code>class  Defocus</code> <code>     (radius=(3, 10), alias_blur=(0.1, 0.5), always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Apply defocus transform.</p> <p>Parameters:</p> Name Type Description <code>radius</code> <code>int, int) or int</code> <p>range for radius of defocusing. If limit is a single int, the range will be [1, limit]. Default: (3, 10).</p> <code>alias_blur</code> <code>float, float) or float</code> <p>range for alias_blur of defocusing (sigma of gaussian blur). If limit is a single float, the range will be (0, limit). Default: (0.1, 0.5).</p> <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 0.5.</p> <p>Targets</p> <p>image</p> <p>Image types:     unit8, float32</p> <p>Reference</p> <p>https://arxiv.org/abs/1903.12261</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/blur/transforms.py</code> Python<pre><code>class Defocus(ImageOnlyTransform):\n    \"\"\"Apply defocus transform.\n\n    Args:\n        radius ((int, int) or int): range for radius of defocusing.\n            If limit is a single int, the range will be [1, limit]. Default: (3, 10).\n        alias_blur ((float, float) or float): range for alias_blur of defocusing (sigma of gaussian blur).\n            If limit is a single float, the range will be (0, limit). Default: (0.1, 0.5).\n        p (float): probability of applying the transform. Default: 0.5.\n\n    Targets:\n        image\n\n    Image types:\n        unit8, float32\n\n    Reference:\n        https://arxiv.org/abs/1903.12261\n    \"\"\"\n\n    class InitSchema(BaseTransformInitSchema):\n        radius: OnePlusIntRangeType = (3, 10)\n        alias_blur: NonNegativeFloatRangeType = (0.1, 0.5)\n\n    def __init__(\n        self,\n        radius: ScaleIntType = (3, 10),\n        alias_blur: ScaleFloatType = (0.1, 0.5),\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p, always_apply)\n        self.radius = cast(Tuple[int, int], radius)\n        self.alias_blur = cast(Tuple[float, float], alias_blur)\n\n    def apply(self, img: np.ndarray, radius: int, alias_blur: float, **params: Any) -&gt; np.ndarray:\n        return fblur.defocus(img, radius, alias_blur)\n\n    def get_params(self) -&gt; dict[str, Any]:\n        return {\n            \"radius\": random.randint(self.radius[0], self.radius[1]),\n            \"alias_blur\": random.uniform(self.alias_blur[0], self.alias_blur[1]),\n        }\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, str]:\n        return (\"radius\", \"alias_blur\")\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.blur.transforms.Defocus.apply","title":"<code>apply (self, img, radius, alias_blur, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/blur/transforms.py</code> Python<pre><code>def apply(self, img: np.ndarray, radius: int, alias_blur: float, **params: Any) -&gt; np.ndarray:\n    return fblur.defocus(img, radius, alias_blur)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.blur.transforms.Defocus.get_params","title":"<code>get_params (self)</code>","text":"<p>Returns parameters independent of input.</p> Source code in <code>albumentations/augmentations/blur/transforms.py</code> Python<pre><code>def get_params(self) -&gt; dict[str, Any]:\n    return {\n        \"radius\": random.randint(self.radius[0], self.radius[1]),\n        \"alias_blur\": random.uniform(self.alias_blur[0], self.alias_blur[1]),\n    }\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.blur.transforms.Defocus.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/blur/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, str]:\n    return (\"radius\", \"alias_blur\")\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.blur.transforms.GaussianBlur","title":"<code>class  GaussianBlur</code> <code>     (blur_limit=(3, 7), sigma_limit=0, always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Blur the input image using a Gaussian filter with a random kernel size.</p> <p>Parameters:</p> Name Type Description <code>blur_limit</code> <code>int, (int, int</code> <p>maximum Gaussian kernel size for blurring the input image. Must be zero or odd and in range [0, inf). If set to 0 it will be computed from sigma as <code>round(sigma * (3 if img.dtype == np.uint8 else 4) * 2 + 1) + 1</code>. If set single value <code>blur_limit</code> will be in range (0, blur_limit). Default: (3, 7).</p> <code>sigma_limit</code> <code>float, (float, float</code> <p>Gaussian kernel standard deviation. Must be in range [0, inf). If set single value <code>sigma_limit</code> will be in range (0, sigma_limit). If set to 0 sigma will be computed as <code>sigma = 0.3*((ksize-1)*0.5 - 1) + 0.8</code>. Default: 0.</p> <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 0.5.</p> <p>Targets</p> <p>image</p> <p>Image types:     uint8, float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/blur/transforms.py</code> Python<pre><code>class GaussianBlur(ImageOnlyTransform):\n    \"\"\"Blur the input image using a Gaussian filter with a random kernel size.\n\n    Args:\n        blur_limit (int, (int, int)): maximum Gaussian kernel size for blurring the input image.\n            Must be zero or odd and in range [0, inf). If set to 0 it will be computed from sigma\n            as `round(sigma * (3 if img.dtype == np.uint8 else 4) * 2 + 1) + 1`.\n            If set single value `blur_limit` will be in range (0, blur_limit).\n            Default: (3, 7).\n        sigma_limit (float, (float, float)): Gaussian kernel standard deviation. Must be in range [0, inf).\n            If set single value `sigma_limit` will be in range (0, sigma_limit).\n            If set to 0 sigma will be computed as `sigma = 0.3*((ksize-1)*0.5 - 1) + 0.8`. Default: 0.\n        p (float): probability of applying the transform. Default: 0.5.\n\n    Targets:\n        image\n\n    Image types:\n        uint8, float32\n\n    \"\"\"\n\n    class InitSchema(BlurInitSchema):\n        sigma_limit: NonNegativeFloatRangeType = 0\n\n        @field_validator(\"blur_limit\")\n        @classmethod\n        def process_blur(cls, value: ScaleIntType, info: ValidationInfo) -&gt; tuple[int, int]:\n            return process_blur_limit(value, info, min_value=0)\n\n        @model_validator(mode=\"after\")\n        def validate_limits(self) -&gt; Self:\n            if (\n                isinstance(self.blur_limit, (tuple, list))\n                and self.blur_limit[0] == 0\n                and isinstance(self.sigma_limit, (tuple, list))\n                and self.sigma_limit[0] == 0\n            ):\n                self.blur_limit = 3, max(3, self.blur_limit[1])\n                warnings.warn(\n                    \"blur_limit and sigma_limit minimum value can not be both equal to 0. \"\n                    \"blur_limit minimum value changed to 3.\",\n                    stacklevel=2,\n                )\n\n            if isinstance(self.blur_limit, tuple):\n                for v in self.blur_limit:\n                    if v != 0 and v % 2 != 1:\n                        raise ValueError(f\"Blur limit must be 0 or odd. Got: {self.blur_limit}\")\n\n            return self\n\n    def __init__(\n        self,\n        blur_limit: ScaleIntType = (3, 7),\n        sigma_limit: ScaleFloatType = 0,\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p, always_apply)\n        self.blur_limit = cast(Tuple[int, int], blur_limit)\n        self.sigma_limit = cast(Tuple[float, float], sigma_limit)\n\n    def apply(self, img: np.ndarray, ksize: int, sigma: float, **params: Any) -&gt; np.ndarray:\n        return fblur.gaussian_blur(img, ksize, sigma=sigma)\n\n    def get_params(self) -&gt; dict[str, float]:\n        ksize = random.randrange(self.blur_limit[0], self.blur_limit[1] + 1)\n        if ksize != 0 and ksize % 2 != 1:\n            ksize = (ksize + 1) % (self.blur_limit[1] + 1)\n\n        return {\"ksize\": ksize, \"sigma\": random.uniform(*self.sigma_limit)}\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, str]:\n        return (\"blur_limit\", \"sigma_limit\")\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.blur.transforms.GaussianBlur.apply","title":"<code>apply (self, img, ksize, sigma, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/blur/transforms.py</code> Python<pre><code>def apply(self, img: np.ndarray, ksize: int, sigma: float, **params: Any) -&gt; np.ndarray:\n    return fblur.gaussian_blur(img, ksize, sigma=sigma)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.blur.transforms.GaussianBlur.get_params","title":"<code>get_params (self)</code>","text":"<p>Returns parameters independent of input.</p> Source code in <code>albumentations/augmentations/blur/transforms.py</code> Python<pre><code>def get_params(self) -&gt; dict[str, float]:\n    ksize = random.randrange(self.blur_limit[0], self.blur_limit[1] + 1)\n    if ksize != 0 and ksize % 2 != 1:\n        ksize = (ksize + 1) % (self.blur_limit[1] + 1)\n\n    return {\"ksize\": ksize, \"sigma\": random.uniform(*self.sigma_limit)}\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.blur.transforms.GaussianBlur.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/blur/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, str]:\n    return (\"blur_limit\", \"sigma_limit\")\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.blur.transforms.GlassBlur","title":"<code>class  GlassBlur</code> <code>     (sigma=0.7, max_delta=4, iterations=2, mode='fast', always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Apply glass noise to the input image.</p> <p>Parameters:</p> Name Type Description <code>sigma</code> <code>float</code> <p>standard deviation for Gaussian kernel.</p> <code>max_delta</code> <code>int</code> <p>max distance between pixels which are swapped.</p> <code>iterations</code> <code>int</code> <p>number of repeats. Should be in range [1, inf). Default: (2).</p> <code>mode</code> <code>str</code> <p>mode of computation: fast or exact. Default: \"fast\".</p> <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 0.5.</p> <p>Targets</p> <p>image</p> <p>Image types:     uint8, float32</p> <p>Reference</p> <p>https://arxiv.org/abs/1903.12261 https://github.com/hendrycks/robustness/blob/master/ImageNet-C/create_c/make_imagenet_c.py</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/blur/transforms.py</code> Python<pre><code>class GlassBlur(ImageOnlyTransform):\n    \"\"\"Apply glass noise to the input image.\n\n    Args:\n        sigma (float): standard deviation for Gaussian kernel.\n        max_delta (int): max distance between pixels which are swapped.\n        iterations (int): number of repeats.\n            Should be in range [1, inf). Default: (2).\n        mode (str): mode of computation: fast or exact. Default: \"fast\".\n        p (float): probability of applying the transform. Default: 0.5.\n\n    Targets:\n        image\n\n    Image types:\n        uint8, float32\n\n    Reference:\n        https://arxiv.org/abs/1903.12261\n        https://github.com/hendrycks/robustness/blob/master/ImageNet-C/create_c/make_imagenet_c.py\n\n    \"\"\"\n\n    class InitSchema(BaseTransformInitSchema):\n        sigma: float = Field(default=0.7, ge=0, description=\"Standard deviation for the Gaussian kernel.\")\n        max_delta: int = Field(default=4, ge=1, description=\"Maximum distance between pixels that are swapped.\")\n        iterations: int = Field(default=2, ge=1, description=\"Number of times the glass noise effect is applied.\")\n        mode: Literal[\"fast\", \"exact\"] = \"fast\"\n\n    def __init__(\n        self,\n        sigma: float = 0.7,\n        max_delta: int = 4,\n        iterations: int = 2,\n        mode: Literal[\"fast\", \"exact\"] = \"fast\",\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p=p, always_apply=always_apply)\n        self.sigma = sigma\n        self.max_delta = max_delta\n        self.iterations = iterations\n        self.mode = mode\n\n    def apply(self, img: np.ndarray, *args: Any, dxy: np.ndarray, **params: Any) -&gt; np.ndarray:\n        if dxy is None:\n            msg = \"dxy is None\"\n            raise ValueError(msg)\n\n        return fblur.glass_blur(img, self.sigma, self.max_delta, self.iterations, dxy, self.mode)\n\n    def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, np.ndarray]:\n        height, width = params[\"shape\"][:2]\n\n        # generate array containing all necessary values for transformations\n        width_pixels = height - self.max_delta * 2\n        height_pixels = width - self.max_delta * 2\n        total_pixels = int(width_pixels * height_pixels)\n        dxy = random_utils.randint(-self.max_delta, self.max_delta, size=(total_pixels, self.iterations, 2))\n\n        return {\"dxy\": dxy}\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, str, str, str]:\n        return (\"sigma\", \"max_delta\", \"iterations\", \"mode\")\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.blur.transforms.GlassBlur.apply","title":"<code>apply (self, img, *args, *, dxy, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/blur/transforms.py</code> Python<pre><code>def apply(self, img: np.ndarray, *args: Any, dxy: np.ndarray, **params: Any) -&gt; np.ndarray:\n    if dxy is None:\n        msg = \"dxy is None\"\n        raise ValueError(msg)\n\n    return fblur.glass_blur(img, self.sigma, self.max_delta, self.iterations, dxy, self.mode)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.blur.transforms.GlassBlur.get_params_dependent_on_data","title":"<code>get_params_dependent_on_data (self, params, data)</code>","text":"<p>Returns parameters dependent on input.</p> Source code in <code>albumentations/augmentations/blur/transforms.py</code> Python<pre><code>def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, np.ndarray]:\n    height, width = params[\"shape\"][:2]\n\n    # generate array containing all necessary values for transformations\n    width_pixels = height - self.max_delta * 2\n    height_pixels = width - self.max_delta * 2\n    total_pixels = int(width_pixels * height_pixels)\n    dxy = random_utils.randint(-self.max_delta, self.max_delta, size=(total_pixels, self.iterations, 2))\n\n    return {\"dxy\": dxy}\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.blur.transforms.GlassBlur.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/blur/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, str, str, str]:\n    return (\"sigma\", \"max_delta\", \"iterations\", \"mode\")\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.blur.transforms.MedianBlur","title":"<code>class  MedianBlur</code> <code>     (blur_limit=7, p=0.5, always_apply=None)                 </code>  [view source on GitHub]","text":"<p>Blur the input image using a median filter with a random aperture linear size.</p> <p>Parameters:</p> Name Type Description <code>blur_limit</code> <code>int</code> <p>maximum aperture linear size for blurring the input image. Must be odd and in range [3, inf). Default: (3, 7).</p> <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 0.5.</p> <p>Targets</p> <p>image</p> <p>Image types:     uint8, float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/blur/transforms.py</code> Python<pre><code>class MedianBlur(Blur):\n    \"\"\"Blur the input image using a median filter with a random aperture linear size.\n\n    Args:\n        blur_limit (int): maximum aperture linear size for blurring the input image.\n            Must be odd and in range [3, inf). Default: (3, 7).\n        p (float): probability of applying the transform. Default: 0.5.\n\n    Targets:\n        image\n\n    Image types:\n        uint8, float32\n\n    \"\"\"\n\n    def __init__(self, blur_limit: ScaleIntType = 7, p: float = 0.5, always_apply: bool | None = None):\n        super().__init__(blur_limit, p, always_apply)\n\n    def apply(self, img: np.ndarray, kernel: int, **params: Any) -&gt; np.ndarray:\n        return fblur.median_blur(img, kernel)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.blur.transforms.MedianBlur.__init__","title":"<code>__init__ (self, blur_limit=7, p=0.5, always_apply=None)</code>  <code>special</code>","text":"<p>Initialize self.  See help(type(self)) for accurate signature.</p> Source code in <code>albumentations/augmentations/blur/transforms.py</code> Python<pre><code>def __init__(self, blur_limit: ScaleIntType = 7, p: float = 0.5, always_apply: bool | None = None):\n    super().__init__(blur_limit, p, always_apply)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.blur.transforms.MedianBlur.apply","title":"<code>apply (self, img, kernel, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/blur/transforms.py</code> Python<pre><code>def apply(self, img: np.ndarray, kernel: int, **params: Any) -&gt; np.ndarray:\n    return fblur.median_blur(img, kernel)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.blur.transforms.MotionBlur","title":"<code>class  MotionBlur</code> <code>     (blur_limit=7, allow_shifted=True, always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Apply motion blur to the input image using a random-sized kernel.</p> <p>Parameters:</p> Name Type Description <code>blur_limit</code> <code>int</code> <p>maximum kernel size for blurring the input image. Should be in range [3, inf). Default: (3, 7).</p> <code>allow_shifted</code> <code>bool</code> <p>if set to true creates non shifted kernels only, otherwise creates randomly shifted kernels. Default: True.</p> <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 0.5.</p> <p>Targets</p> <p>image</p> <p>Image types:     uint8, float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/blur/transforms.py</code> Python<pre><code>class MotionBlur(Blur):\n    \"\"\"Apply motion blur to the input image using a random-sized kernel.\n\n    Args:\n        blur_limit (int): maximum kernel size for blurring the input image.\n            Should be in range [3, inf). Default: (3, 7).\n        allow_shifted (bool): if set to true creates non shifted kernels only,\n            otherwise creates randomly shifted kernels. Default: True.\n        p (float): probability of applying the transform. Default: 0.5.\n\n    Targets:\n        image\n\n    Image types:\n        uint8, float32\n\n    \"\"\"\n\n    class InitSchema(BaseTransformInitSchema):\n        allow_shifted: bool = Field(\n            default=True,\n            description=\"If set to true creates non-shifted kernels only, otherwise creates randomly shifted kernels.\",\n        )\n        blur_limit: ScaleIntType = Field(\n            default=(3, 7),\n            description=\"Maximum kernel size for blurring the input image.\",\n        )\n\n        @model_validator(mode=\"after\")\n        def process_blur(self) -&gt; Self:\n            self.blur_limit = cast(Tuple[int, int], to_tuple(self.blur_limit, 3))\n\n            if self.allow_shifted and isinstance(self.blur_limit, tuple) and any(x % 2 != 1 for x in self.blur_limit):\n                raise ValueError(f\"Blur limit must be odd when centered=True. Got: {self.blur_limit}\")\n\n            return self\n\n    def __init__(\n        self,\n        blur_limit: ScaleIntType = 7,\n        allow_shifted: bool = True,\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(blur_limit=blur_limit, p=p, always_apply=always_apply)\n        self.allow_shifted = allow_shifted\n        self.blur_limit = cast(Tuple[int, int], blur_limit)\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return (*super().get_transform_init_args_names(), \"allow_shifted\")\n\n    def apply(self, img: np.ndarray, kernel: np.ndarray, **params: Any) -&gt; np.ndarray:\n        return fmain.convolve(img, kernel=kernel)\n\n    def get_params(self) -&gt; dict[str, Any]:\n        ksize = random.choice(list(range(self.blur_limit[0], self.blur_limit[1] + 1, 2)))\n        if ksize &lt;= TWO:\n            raise ValueError(f\"ksize must be &gt; 2. Got: {ksize}\")\n        kernel = np.zeros((ksize, ksize), dtype=np.uint8)\n        x1, x2 = random.randint(0, ksize - 1), random.randint(0, ksize - 1)\n        if x1 == x2:\n            y1, y2 = random.sample(range(ksize), 2)\n        else:\n            y1, y2 = random.randint(0, ksize - 1), random.randint(0, ksize - 1)\n\n        def make_odd_val(v1: int, v2: int) -&gt; tuple[int, int]:\n            len_v = abs(v1 - v2) + 1\n            if len_v % 2 != 1:\n                if v2 &gt; v1:\n                    v2 -= 1\n                else:\n                    v1 -= 1\n            return v1, v2\n\n        if not self.allow_shifted:\n            x1, x2 = make_odd_val(x1, x2)\n            y1, y2 = make_odd_val(y1, y2)\n\n            xc = (x1 + x2) / 2\n            yc = (y1 + y2) / 2\n\n            center = ksize / 2 - 0.5\n            dx = xc - center\n            dy = yc - center\n            x1, x2 = (int(i - dx) for i in [x1, x2])\n            y1, y2 = (int(i - dy) for i in [y1, y2])\n\n        cv2.line(kernel, (x1, y1), (x2, y2), 1, thickness=1)\n\n        # Normalize kernel\n        return {\"kernel\": kernel.astype(np.float32) / np.sum(kernel)}\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.blur.transforms.MotionBlur.apply","title":"<code>apply (self, img, kernel, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/blur/transforms.py</code> Python<pre><code>def apply(self, img: np.ndarray, kernel: np.ndarray, **params: Any) -&gt; np.ndarray:\n    return fmain.convolve(img, kernel=kernel)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.blur.transforms.MotionBlur.get_params","title":"<code>get_params (self)</code>","text":"<p>Returns parameters independent of input.</p> Source code in <code>albumentations/augmentations/blur/transforms.py</code> Python<pre><code>def get_params(self) -&gt; dict[str, Any]:\n    ksize = random.choice(list(range(self.blur_limit[0], self.blur_limit[1] + 1, 2)))\n    if ksize &lt;= TWO:\n        raise ValueError(f\"ksize must be &gt; 2. Got: {ksize}\")\n    kernel = np.zeros((ksize, ksize), dtype=np.uint8)\n    x1, x2 = random.randint(0, ksize - 1), random.randint(0, ksize - 1)\n    if x1 == x2:\n        y1, y2 = random.sample(range(ksize), 2)\n    else:\n        y1, y2 = random.randint(0, ksize - 1), random.randint(0, ksize - 1)\n\n    def make_odd_val(v1: int, v2: int) -&gt; tuple[int, int]:\n        len_v = abs(v1 - v2) + 1\n        if len_v % 2 != 1:\n            if v2 &gt; v1:\n                v2 -= 1\n            else:\n                v1 -= 1\n        return v1, v2\n\n    if not self.allow_shifted:\n        x1, x2 = make_odd_val(x1, x2)\n        y1, y2 = make_odd_val(y1, y2)\n\n        xc = (x1 + x2) / 2\n        yc = (y1 + y2) / 2\n\n        center = ksize / 2 - 0.5\n        dx = xc - center\n        dy = yc - center\n        x1, x2 = (int(i - dx) for i in [x1, x2])\n        y1, y2 = (int(i - dy) for i in [y1, y2])\n\n    cv2.line(kernel, (x1, y1), (x2, y2), 1, thickness=1)\n\n    # Normalize kernel\n    return {\"kernel\": kernel.astype(np.float32) / np.sum(kernel)}\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.blur.transforms.MotionBlur.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/blur/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return (*super().get_transform_init_args_names(), \"allow_shifted\")\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.blur.transforms.ZoomBlur","title":"<code>class  ZoomBlur</code> <code>     (max_factor=(1, 1.31), step_factor=(0.01, 0.03), always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Apply zoom blur transform.</p> <p>Parameters:</p> Name Type Description <code>max_factor</code> <code>float, float) or float</code> <p>range for max factor for blurring. If max_factor is a single float, the range will be (1, limit). Default: (1, 1.31). All max_factor values should be larger than 1.</p> <code>step_factor</code> <code>float, float) or float</code> <p>If single float will be used as step parameter for np.arange. If tuple of float step_factor will be in range <code>[step_factor[0], step_factor[1])</code>. Default: (0.01, 0.03). All step_factor values should be positive.</p> <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 0.5.</p> <p>Targets</p> <p>image</p> <p>Image types:     unit8, float32</p> <p>Reference</p> <p>https://arxiv.org/abs/1903.12261</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/blur/transforms.py</code> Python<pre><code>class ZoomBlur(ImageOnlyTransform):\n    \"\"\"Apply zoom blur transform.\n\n    Args:\n        max_factor ((float, float) or float): range for max factor for blurring.\n            If max_factor is a single float, the range will be (1, limit). Default: (1, 1.31).\n            All max_factor values should be larger than 1.\n        step_factor ((float, float) or float): If single float will be used as step parameter for np.arange.\n            If tuple of float step_factor will be in range `[step_factor[0], step_factor[1])`. Default: (0.01, 0.03).\n            All step_factor values should be positive.\n        p (float): probability of applying the transform. Default: 0.5.\n\n    Targets:\n        image\n\n    Image types:\n        unit8, float32\n\n    Reference:\n        https://arxiv.org/abs/1903.12261\n    \"\"\"\n\n    class InitSchema(BaseTransformInitSchema):\n        max_factor: OnePlusFloatRangeType = (1, 1.31)\n        step_factor: NonNegativeFloatRangeType = (0.01, 0.03)\n\n    def __init__(\n        self,\n        max_factor: ScaleFloatType = (1, 1.31),\n        step_factor: ScaleFloatType = (0.01, 0.03),\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p, always_apply)\n        self.max_factor = cast(Tuple[float, float], max_factor)\n        self.step_factor = cast(Tuple[float, float], step_factor)\n\n    def apply(self, img: np.ndarray, zoom_factors: np.ndarray, **params: Any) -&gt; np.ndarray:\n        return fblur.zoom_blur(img, zoom_factors)\n\n    def get_params(self) -&gt; dict[str, Any]:\n        max_factor = random.uniform(self.max_factor[0], self.max_factor[1])\n        step_factor = random.uniform(self.step_factor[0], self.step_factor[1])\n        return {\"zoom_factors\": np.arange(1.0, max_factor, step_factor)}\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, str]:\n        return (\"max_factor\", \"step_factor\")\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.blur.transforms.ZoomBlur.apply","title":"<code>apply (self, img, zoom_factors, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/blur/transforms.py</code> Python<pre><code>def apply(self, img: np.ndarray, zoom_factors: np.ndarray, **params: Any) -&gt; np.ndarray:\n    return fblur.zoom_blur(img, zoom_factors)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.blur.transforms.ZoomBlur.get_params","title":"<code>get_params (self)</code>","text":"<p>Returns parameters independent of input.</p> Source code in <code>albumentations/augmentations/blur/transforms.py</code> Python<pre><code>def get_params(self) -&gt; dict[str, Any]:\n    max_factor = random.uniform(self.max_factor[0], self.max_factor[1])\n    step_factor = random.uniform(self.step_factor[0], self.step_factor[1])\n    return {\"zoom_factors\": np.arange(1.0, max_factor, step_factor)}\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.blur.transforms.ZoomBlur.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/blur/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, str]:\n    return (\"max_factor\", \"step_factor\")\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.crops","title":"<code>crops</code>  <code>special</code>","text":""},{"location":"api_reference/full_reference/#albumentations.augmentations.crops.functional","title":"<code>functional</code>","text":""},{"location":"api_reference/full_reference/#albumentations.augmentations.crops.functional.crop_keypoint_by_coords","title":"<code>def crop_keypoint_by_coords    (keypoint, crop_coords)    </code> [view source on GitHub]","text":"<p>Crop a keypoint using the provided coordinates of bottom-left and top-right corners in pixels and the required height and width of the crop.</p> <p>Parameters:</p> Name Type Description <code>keypoint</code> <code>tuple</code> <p>A keypoint <code>(x, y, angle, scale)</code>.</p> <code>crop_coords</code> <code>tuple</code> <p>Crop box coords <code>(x1, x2, y1, y2)</code>.</p> <p>Returns:</p> Type Description <code>KeypointInternalType</code> <p>A keypoint <code>(x, y, angle, scale)</code>.</p> Source code in <code>albumentations/augmentations/crops/functional.py</code> Python<pre><code>def crop_keypoint_by_coords(\n    keypoint: KeypointInternalType,\n    crop_coords: tuple[int, int, int, int],\n) -&gt; KeypointInternalType:\n    \"\"\"Crop a keypoint using the provided coordinates of bottom-left and top-right corners in pixels and the\n    required height and width of the crop.\n\n    Args:\n        keypoint (tuple): A keypoint `(x, y, angle, scale)`.\n        crop_coords (tuple): Crop box coords `(x1, x2, y1, y2)`.\n\n    Returns:\n        A keypoint `(x, y, angle, scale)`.\n\n    \"\"\"\n    x, y, angle, scale = keypoint[:4]\n    x1, y1 = crop_coords[:2]\n    return x - x1, y - y1, angle, scale\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.crops.transforms","title":"<code>transforms</code>","text":""},{"location":"api_reference/full_reference/#albumentations.augmentations.crops.transforms.BBoxSafeRandomCrop","title":"<code>class  BBoxSafeRandomCrop</code> <code>     (erosion_rate=0.0, p=1.0, always_apply=None)                 </code>  [view source on GitHub]","text":"<p>Crop a random part of the input without loss of bboxes.</p> <p>Parameters:</p> Name Type Description <code>erosion_rate</code> <code>float</code> <p>erosion rate applied on input image height before crop.</p> <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 1.</p> <p>Targets</p> <p>image, mask, bboxes, keypoints</p> <p>Image types:     uint8, float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/crops/transforms.py</code> Python<pre><code>class BBoxSafeRandomCrop(_BaseCrop):\n    \"\"\"Crop a random part of the input without loss of bboxes.\n\n    Args:\n        erosion_rate: erosion rate applied on input image height before crop.\n        p: probability of applying the transform. Default: 1.\n\n    Targets:\n        image, mask, bboxes, keypoints\n\n    Image types:\n        uint8, float32\n\n    \"\"\"\n\n    _targets = (Targets.IMAGE, Targets.MASK, Targets.BBOXES, Targets.KEYPOINTS)\n\n    class InitSchema(BaseTransformInitSchema):\n        erosion_rate: float = Field(\n            default=0.0,\n            ge=0.0,\n            le=1.0,\n            description=\"Erosion rate applied on input image height before crop.\",\n        )\n        p: ProbabilityType = 1\n\n    def __init__(self, erosion_rate: float = 0.0, p: float = 1.0, always_apply: bool | None = None):\n        super().__init__(p=p, always_apply=always_apply)\n        self.erosion_rate = erosion_rate\n\n    def _get_coords_no_bbox(self, image_shape: tuple[int, int]) -&gt; tuple[int, int, int, int]:\n        image_height, image_width = image_shape\n\n        erosive_h = int(image_height * (1.0 - self.erosion_rate))\n        crop_height = image_height if erosive_h &gt;= image_height else random.randint(erosive_h, image_height)\n\n        crop_width = int(crop_height * image_width / image_height)\n\n        h_start = random.random()\n        w_start = random.random()\n\n        crop_shape = (crop_height, crop_width)\n\n        return fcrops.get_crop_coords(image_shape, crop_shape, h_start, w_start)\n\n    def get_params_dependent_on_data(\n        self,\n        params: dict[str, Any],\n        data: dict[str, Any],\n    ) -&gt; dict[str, tuple[int, int, int, int]]:\n        image_shape = params[\"shape\"][:2]\n\n        if len(data[\"bboxes\"]) == 0:  # less likely, this class is for use with bboxes.\n            crop_coords = self._get_coords_no_bbox(image_shape)\n            return {\"crop_coords\": crop_coords}\n\n        bbox_union = union_of_bboxes(bboxes=data[\"bboxes\"], erosion_rate=self.erosion_rate)\n\n        if bbox_union is None:\n            crop_coords = self._get_coords_no_bbox(image_shape)\n            return {\"crop_coords\": crop_coords}\n\n        x_min, y_min, x_max, y_max = bbox_union\n\n        x_min = np.clip(x_min, 0, 1)\n        y_min = np.clip(y_min, 0, 1)\n        x_max = np.clip(x_max, x_min, 1)\n        y_max = np.clip(y_max, y_min, 1)\n\n        image_height, image_width = image_shape\n\n        crop_x_min = int(x_min * random.random() * image_width)\n        crop_y_min = int(y_min * random.random() * image_height)\n\n        bbox_xmax = x_max + (1 - x_max) * random.random()\n        bbox_ymax = y_max + (1 - y_max) * random.random()\n        crop_x_max = int(bbox_xmax * image_width)\n        crop_y_max = int(bbox_ymax * image_height)\n\n        return {\"crop_coords\": (crop_x_min, crop_y_min, crop_x_max, crop_y_max)}\n\n    @property\n    def targets_as_params(self) -&gt; list[str]:\n        return [\"bboxes\"]\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return (\"erosion_rate\",)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.crops.transforms.BBoxSafeRandomCrop.targets_as_params","title":"<code>targets_as_params: list[str]</code>  <code>property</code> <code>readonly</code>","text":"<p>Targets used to get params dependent on targets. This is used to check input has all required targets.</p>"},{"location":"api_reference/full_reference/#albumentations.augmentations.crops.transforms.BBoxSafeRandomCrop.get_params_dependent_on_data","title":"<code>get_params_dependent_on_data (self, params, data)</code>","text":"<p>Returns parameters dependent on input.</p> Source code in <code>albumentations/augmentations/crops/transforms.py</code> Python<pre><code>def get_params_dependent_on_data(\n    self,\n    params: dict[str, Any],\n    data: dict[str, Any],\n) -&gt; dict[str, tuple[int, int, int, int]]:\n    image_shape = params[\"shape\"][:2]\n\n    if len(data[\"bboxes\"]) == 0:  # less likely, this class is for use with bboxes.\n        crop_coords = self._get_coords_no_bbox(image_shape)\n        return {\"crop_coords\": crop_coords}\n\n    bbox_union = union_of_bboxes(bboxes=data[\"bboxes\"], erosion_rate=self.erosion_rate)\n\n    if bbox_union is None:\n        crop_coords = self._get_coords_no_bbox(image_shape)\n        return {\"crop_coords\": crop_coords}\n\n    x_min, y_min, x_max, y_max = bbox_union\n\n    x_min = np.clip(x_min, 0, 1)\n    y_min = np.clip(y_min, 0, 1)\n    x_max = np.clip(x_max, x_min, 1)\n    y_max = np.clip(y_max, y_min, 1)\n\n    image_height, image_width = image_shape\n\n    crop_x_min = int(x_min * random.random() * image_width)\n    crop_y_min = int(y_min * random.random() * image_height)\n\n    bbox_xmax = x_max + (1 - x_max) * random.random()\n    bbox_ymax = y_max + (1 - y_max) * random.random()\n    crop_x_max = int(bbox_xmax * image_width)\n    crop_y_max = int(bbox_ymax * image_height)\n\n    return {\"crop_coords\": (crop_x_min, crop_y_min, crop_x_max, crop_y_max)}\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.crops.transforms.BBoxSafeRandomCrop.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/crops/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return (\"erosion_rate\",)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.crops.transforms.CenterCrop","title":"<code>class  CenterCrop</code> <code>     (height, width, p=1.0, always_apply=None)                 </code>  [view source on GitHub]","text":"<p>Crop the central part of the input.</p> <p>Parameters:</p> Name Type Description <code>height</code> <code>int</code> <p>height of the crop.</p> <code>width</code> <code>int</code> <p>width of the crop.</p> <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 1.</p> <p>Targets</p> <p>image, mask, bboxes, keypoints</p> <p>Image types:     uint8, float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/crops/transforms.py</code> Python<pre><code>class CenterCrop(_BaseCrop):\n    \"\"\"Crop the central part of the input.\n\n    Args:\n        height: height of the crop.\n        width: width of the crop.\n        p: probability of applying the transform. Default: 1.\n\n    Targets:\n        image, mask, bboxes, keypoints\n\n    Image types:\n        uint8, float32\n\n    \"\"\"\n\n    class InitSchema(CropInitSchema):\n        pass\n\n    def __init__(self, height: int, width: int, p: float = 1.0, always_apply: bool | None = None):\n        super().__init__(p, always_apply)\n        self.height = height\n        self.width = width\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return \"height\", \"width\"\n\n    def get_params_dependent_on_data(\n        self,\n        params: dict[str, Any],\n        data: dict[str, Any],\n    ) -&gt; dict[str, tuple[int, int, int, int]]:\n        image_shape = params[\"shape\"][:2]\n        crop_coords = fcrops.get_center_crop_coords(image_shape, (self.height, self.width))\n\n        return {\"crop_coords\": crop_coords}\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.crops.transforms.CenterCrop.get_params_dependent_on_data","title":"<code>get_params_dependent_on_data (self, params, data)</code>","text":"<p>Returns parameters dependent on input.</p> Source code in <code>albumentations/augmentations/crops/transforms.py</code> Python<pre><code>def get_params_dependent_on_data(\n    self,\n    params: dict[str, Any],\n    data: dict[str, Any],\n) -&gt; dict[str, tuple[int, int, int, int]]:\n    image_shape = params[\"shape\"][:2]\n    crop_coords = fcrops.get_center_crop_coords(image_shape, (self.height, self.width))\n\n    return {\"crop_coords\": crop_coords}\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.crops.transforms.CenterCrop.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/crops/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return \"height\", \"width\"\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.crops.transforms.Crop","title":"<code>class  Crop</code> <code>     (x_min=0, y_min=0, x_max=1024, y_max=1024, always_apply=None, p=1.0)                 </code>  [view source on GitHub]","text":"<p>Crop region from image.</p> <p>Parameters:</p> Name Type Description <code>x_min</code> <code>int</code> <p>Minimum upper left x coordinate.</p> <code>y_min</code> <code>int</code> <p>Minimum upper left y coordinate.</p> <code>x_max</code> <code>int</code> <p>Maximum lower right x coordinate.</p> <code>y_max</code> <code>int</code> <p>Maximum lower right y coordinate.</p> <p>Targets</p> <p>image, mask, bboxes, keypoints</p> <p>Image types:     uint8, float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/crops/transforms.py</code> Python<pre><code>class Crop(_BaseCrop):\n    \"\"\"Crop region from image.\n\n    Args:\n        x_min: Minimum upper left x coordinate.\n        y_min: Minimum upper left y coordinate.\n        x_max: Maximum lower right x coordinate.\n        y_max: Maximum lower right y coordinate.\n\n    Targets:\n        image, mask, bboxes, keypoints\n\n    Image types:\n        uint8, float32\n\n    \"\"\"\n\n    class InitSchema(BaseTransformInitSchema):\n        x_min: Annotated[int, Field(ge=0, description=\"Minimum upper left x coordinate\")]\n        y_min: Annotated[int, Field(ge=0, description=\"Minimum upper left y coordinate\")]\n        x_max: Annotated[int, Field(gt=0, description=\"Maximum lower right x coordinate\")]\n        y_max: Annotated[int, Field(gt=0, description=\"Maximum lower right y coordinate\")]\n        p: ProbabilityType = 1\n\n        @model_validator(mode=\"after\")\n        def validate_coordinates(self) -&gt; Self:\n            if not self.x_min &lt; self.x_max:\n                msg = \"x_max must be greater than x_min\"\n                raise ValueError(msg)\n            if not self.y_min &lt; self.y_max:\n                msg = \"y_max must be greater than y_min\"\n                raise ValueError(msg)\n            return self\n\n    def __init__(\n        self,\n        x_min: int = 0,\n        y_min: int = 0,\n        x_max: int = 1024,\n        y_max: int = 1024,\n        always_apply: bool | None = None,\n        p: float = 1.0,\n    ):\n        super().__init__(p=p, always_apply=always_apply)\n        self.x_min = x_min\n        self.y_min = y_min\n        self.x_max = x_max\n        self.y_max = y_max\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return \"x_min\", \"y_min\", \"x_max\", \"y_max\"\n\n    def get_params_dependent_on_data(\n        self,\n        params: dict[str, Any],\n        data: dict[str, Any],\n    ) -&gt; dict[str, tuple[int, int, int, int]]:\n        return {\"crop_coords\": (self.x_min, self.y_min, self.x_max, self.y_max)}\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.crops.transforms.Crop.get_params_dependent_on_data","title":"<code>get_params_dependent_on_data (self, params, data)</code>","text":"<p>Returns parameters dependent on input.</p> Source code in <code>albumentations/augmentations/crops/transforms.py</code> Python<pre><code>def get_params_dependent_on_data(\n    self,\n    params: dict[str, Any],\n    data: dict[str, Any],\n) -&gt; dict[str, tuple[int, int, int, int]]:\n    return {\"crop_coords\": (self.x_min, self.y_min, self.x_max, self.y_max)}\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.crops.transforms.Crop.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/crops/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return \"x_min\", \"y_min\", \"x_max\", \"y_max\"\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.crops.transforms.CropAndPad","title":"<code>class  CropAndPad</code> <code>     (px=None, percent=None, pad_mode=0, pad_cval=0, pad_cval_mask=0, keep_size=True, sample_independently=True, interpolation=1, always_apply=None, p=1.0)                 </code>  [view source on GitHub]","text":"<p>Crop and pad images by pixel amounts or fractions of image sizes. Cropping removes pixels at the sides (i.e., extracts a subimage from a given full image). Padding adds pixels to the sides (e.g., black pixels). This transformation will never crop images below a height or width of 1.</p> <p>Note</p> <p>This transformation automatically resizes images back to their original size. To deactivate this, add the parameter <code>keep_size=False</code>.</p> <p>Parameters:</p> Name Type Description <code>px</code> <code>int, tuple[int, int], tuple[int, int, int, int], tuple[Union[int, tuple[int, int], list[int]],       Union[int, tuple[int, int], list[int]],       Union[int, tuple[int, int], list[int]],       Union[int, tuple[int, int], list[int]]]</code> <p>The number of pixels to crop (negative values) or pad (positive values) on each side of the image.     Either this or the parameter <code>percent</code> may be set, not both at the same time.</p> <pre><code>* If `None`, then pixel-based cropping/padding will not be used.\n* If `int`, then that exact number of pixels will always be cropped/padded.\n* If a `tuple` of two `int`s with values `a` and `b`, then each side will be cropped/padded by a\n    random amount sampled uniformly per image and side from the interval `[a, b]`.\n    If `sample_independently` is set to `False`, only one value will be sampled per\n        image and used for all sides.\n* If a `tuple` of four entries, then the entries represent top, right, bottom, and left.\n    Each entry may be:\n    - A single `int` (always crop/pad by exactly that value).\n    - A `tuple` of two `int`s `a` and `b` (crop/pad by an amount within `[a, b]`).\n    - A `list` of `int`s (crop/pad by a random value that is contained in the `list`).\n</code></pre> <code>percent</code> <code>float,      tuple[float, float],      tuple[float, float, float, float],      tuple[Union[float, tuple[float, float], list[float]],            Union[float, tuple[float, float], list[float]],            Union[float, tuple[float, float], list[float]],            Union[float, tuple[float, float], list[float]]]</code> <p>The number of pixels to crop (negative values) or pad (positive values) on each side of the image given     as a fraction of the image height/width. E.g. if this is set to <code>-0.1</code>, the transformation will     always crop away <code>10%</code> of the image's height at both the top and the bottom (both <code>10%</code> each),     as well as <code>10%</code> of the width at the right and left. Expected value range is <code>(-1.0, inf)</code>.     Either this or the parameter <code>px</code> may be set, not both at the same time.</p> <pre><code>* If `None`, then fraction-based cropping/padding will not be used.\n* If `float`, then that fraction will always be cropped/padded.\n* If a `tuple` of two `float`s with values `a` and `b`, then each side will be cropped/padded by a\nrandom fraction sampled uniformly per image and side from the interval `[a, b]`.\nIf `sample_independently` is set to `False`, only one value will be sampled per image and used\nfor all sides.\n* If a `tuple` of four entries, then the entries represent top, right, bottom, and left.\n    Each entry may be:\n    - A single `float` (always crop/pad by exactly that percent value).\n    - A `tuple` of two `float`s `a` and `b` (crop/pad by a fraction from `[a, b]`).\n    - A `list` of `float`s (crop/pad by a random value that is contained in the `list`).\n</code></pre> <code>pad_mode</code> <code>int</code> <p>OpenCV border mode.</p> <code>pad_cval</code> <code>Union[int, float, tuple[Union[int, float], Union[int, float]], list[Union[int, float]]]</code> <p>The constant value to use if the pad mode is <code>BORDER_CONSTANT</code>.     * If <code>number</code>, then that value will be used.     * If a <code>tuple</code> of two numbers and at least one of them is a <code>float</code>, then a random number         will be uniformly sampled per image from the continuous interval <code>[a, b]</code> and used as the value.         If both numbers are <code>int</code>s, the interval is discrete.     * If a <code>list</code> of numbers, then a random value will be chosen from the elements of the <code>list</code> and         used as the value.</p> <code>pad_cval_mask</code> <code>Union[int, float, tuple[Union[int, float], Union[int, float]], list[Union[int, float]]]</code> <p>Same as <code>pad_cval</code> but only for masks.</p> <code>keep_size</code> <code>bool</code> <p>After cropping and padding, the resulting image will usually have a different height/width compared to the original input image. If this parameter is set to <code>True</code>, then the cropped/padded image will be resized to the input image's size, i.e., the output shape is always identical to the input shape.</p> <code>sample_independently</code> <code>bool</code> <p>If <code>False</code> and the values for <code>px</code>/<code>percent</code> result in exactly one probability distribution for all image sides, only one single value will be sampled from that probability distribution and used for all sides. I.e., the crop/pad amount then is the same for all sides. If <code>True</code>, four values will be sampled independently, one per side.</p> <code>interpolation</code> <code>int</code> <p>OpenCV flag that is used to specify the interpolation algorithm for images. Should be one of: <code>cv2.INTER_NEAREST</code>, <code>cv2.INTER_LINEAR</code>, <code>cv2.INTER_CUBIC</code>, <code>cv2.INTER_AREA</code>, <code>cv2.INTER_LANCZOS4</code>. Default: <code>cv2.INTER_LINEAR</code>.</p> <p>Targets</p> <p>image, mask, bboxes, keypoints</p> <p>Image types:     unit8, float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/crops/transforms.py</code> Python<pre><code>class CropAndPad(DualTransform):\n    \"\"\"Crop and pad images by pixel amounts or fractions of image sizes.\n    Cropping removes pixels at the sides (i.e., extracts a subimage from a given full image).\n    Padding adds pixels to the sides (e.g., black pixels).\n    This transformation will never crop images below a height or width of 1.\n\n    Note:\n        This transformation automatically resizes images back to their original size. To deactivate this, add the\n        parameter `keep_size=False`.\n\n    Args:\n        px (int,\n            tuple[int, int],\n            tuple[int, int, int, int],\n            tuple[Union[int, tuple[int, int], list[int]],\n                  Union[int, tuple[int, int], list[int]],\n                  Union[int, tuple[int, int], list[int]],\n                  Union[int, tuple[int, int], list[int]]]):\n            The number of pixels to crop (negative values) or pad (positive values) on each side of the image.\n                Either this or the parameter `percent` may be set, not both at the same time.\n\n                * If `None`, then pixel-based cropping/padding will not be used.\n                * If `int`, then that exact number of pixels will always be cropped/padded.\n                * If a `tuple` of two `int`s with values `a` and `b`, then each side will be cropped/padded by a\n                    random amount sampled uniformly per image and side from the interval `[a, b]`.\n                    If `sample_independently` is set to `False`, only one value will be sampled per\n                        image and used for all sides.\n                * If a `tuple` of four entries, then the entries represent top, right, bottom, and left.\n                    Each entry may be:\n                    - A single `int` (always crop/pad by exactly that value).\n                    - A `tuple` of two `int`s `a` and `b` (crop/pad by an amount within `[a, b]`).\n                    - A `list` of `int`s (crop/pad by a random value that is contained in the `list`).\n\n        percent (float,\n                 tuple[float, float],\n                 tuple[float, float, float, float],\n                 tuple[Union[float, tuple[float, float], list[float]],\n                       Union[float, tuple[float, float], list[float]],\n                       Union[float, tuple[float, float], list[float]],\n                       Union[float, tuple[float, float], list[float]]]):\n            The number of pixels to crop (negative values) or pad (positive values) on each side of the image given\n                as a *fraction* of the image height/width. E.g. if this is set to `-0.1`, the transformation will\n                always crop away `10%` of the image's height at both the top and the bottom (both `10%` each),\n                as well as `10%` of the width at the right and left. Expected value range is `(-1.0, inf)`.\n                Either this or the parameter `px` may be set, not both at the same time.\n\n                * If `None`, then fraction-based cropping/padding will not be used.\n                * If `float`, then that fraction will always be cropped/padded.\n                * If a `tuple` of two `float`s with values `a` and `b`, then each side will be cropped/padded by a\n                random fraction sampled uniformly per image and side from the interval `[a, b]`.\n                If `sample_independently` is set to `False`, only one value will be sampled per image and used\n                for all sides.\n                * If a `tuple` of four entries, then the entries represent top, right, bottom, and left.\n                    Each entry may be:\n                    - A single `float` (always crop/pad by exactly that percent value).\n                    - A `tuple` of two `float`s `a` and `b` (crop/pad by a fraction from `[a, b]`).\n                    - A `list` of `float`s (crop/pad by a random value that is contained in the `list`).\n\n        pad_mode (int): OpenCV border mode.\n        pad_cval (Union[int, float, tuple[Union[int, float], Union[int, float]], list[Union[int, float]]]):\n            The constant value to use if the pad mode is `BORDER_CONSTANT`.\n                * If `number`, then that value will be used.\n                * If a `tuple` of two numbers and at least one of them is a `float`, then a random number\n                    will be uniformly sampled per image from the continuous interval `[a, b]` and used as the value.\n                    If both numbers are `int`s, the interval is discrete.\n                * If a `list` of numbers, then a random value will be chosen from the elements of the `list` and\n                    used as the value.\n\n        pad_cval_mask (Union[int, float, tuple[Union[int, float], Union[int, float]], list[Union[int, float]]]):\n            Same as `pad_cval` but only for masks.\n\n        keep_size (bool):\n            After cropping and padding, the resulting image will usually have a different height/width compared to\n            the original input image. If this parameter is set to `True`, then the cropped/padded image will be\n            resized to the input image's size, i.e., the output shape is always identical to the input shape.\n\n        sample_independently (bool):\n            If `False` and the values for `px`/`percent` result in exactly one probability distribution for all\n            image sides, only one single value will be sampled from that probability distribution and used for\n            all sides. I.e., the crop/pad amount then is the same for all sides. If `True`, four values\n            will be sampled independently, one per side.\n\n        interpolation (int):\n            OpenCV flag that is used to specify the interpolation algorithm for images. Should be one of:\n            `cv2.INTER_NEAREST`, `cv2.INTER_LINEAR`, `cv2.INTER_CUBIC`, `cv2.INTER_AREA`, `cv2.INTER_LANCZOS4`.\n            Default: `cv2.INTER_LINEAR`.\n\n    Targets:\n        image, mask, bboxes, keypoints\n\n    Image types:\n        unit8, float32\n\n    \"\"\"\n\n    _targets = (Targets.IMAGE, Targets.MASK, Targets.BBOXES, Targets.KEYPOINTS)\n\n    class InitSchema(BaseTransformInitSchema):\n        px: PxType | None = Field(\n            default=None,\n            description=\"Number of pixels to crop (negative) or pad (positive).\",\n        )\n        percent: PercentType | None = Field(\n            default=None,\n            description=\"Fraction of image size to crop (negative) or pad (positive).\",\n        )\n        pad_mode: BorderModeType = cv2.BORDER_CONSTANT\n        pad_cval: ScalarType | tuple[ScalarType, ScalarType] | list[ScalarType] = Field(\n            default=0,\n            description=\"Padding value if pad_mode is BORDER_CONSTANT.\",\n        )\n        pad_cval_mask: ScalarType | tuple[ScalarType, ScalarType] | list[ScalarType] = Field(\n            default=0,\n            description=\"Padding value for masks if pad_mode is BORDER_CONSTANT.\",\n        )\n        keep_size: bool = Field(\n            default=True,\n            description=\"Whether to resize the image back to the original size after cropping and padding.\",\n        )\n        sample_independently: bool = Field(\n            default=True,\n            description=\"Whether to sample the crop/pad size independently for each side.\",\n        )\n        interpolation: InterpolationType = cv2.INTER_LINEAR\n        p: ProbabilityType = 1\n\n        @model_validator(mode=\"after\")\n        def check_px_percent(self) -&gt; Self:\n            if self.px is None and self.percent is None:\n                msg = \"Both px and percent parameters cannot be None simultaneously.\"\n                raise ValueError(msg)\n            if self.px is not None and self.percent is not None:\n                msg = \"Only px or percent may be set!\"\n                raise ValueError(msg)\n            return self\n\n    def __init__(\n        self,\n        px: int | list[int] | None = None,\n        percent: float | list[float] | None = None,\n        pad_mode: int = cv2.BORDER_CONSTANT,\n        pad_cval: ScalarType | tuple[ScalarType, ScalarType] | list[ScalarType] = 0,\n        pad_cval_mask: ScalarType | tuple[ScalarType, ScalarType] | list[ScalarType] = 0,\n        keep_size: bool = True,\n        sample_independently: bool = True,\n        interpolation: int = cv2.INTER_LINEAR,\n        always_apply: bool | None = None,\n        p: float = 1.0,\n    ):\n        super().__init__(p=p, always_apply=always_apply)\n\n        self.px = px\n        self.percent = percent\n\n        self.pad_mode = pad_mode\n        self.pad_cval = pad_cval\n        self.pad_cval_mask = pad_cval_mask\n\n        self.keep_size = keep_size\n        self.sample_independently = sample_independently\n\n        self.interpolation = interpolation\n\n    def apply(\n        self,\n        img: np.ndarray,\n        crop_params: Sequence[int],\n        pad_params: Sequence[int],\n        pad_value: ColorType,\n        interpolation: int,\n        **params: Any,\n    ) -&gt; np.ndarray:\n        return fcrops.crop_and_pad(\n            img,\n            crop_params,\n            pad_params,\n            pad_value,\n            params[\"shape\"][:2],\n            interpolation,\n            self.pad_mode,\n            self.keep_size,\n        )\n\n    def apply_to_mask(\n        self,\n        mask: np.ndarray,\n        crop_params: Sequence[int],\n        pad_params: Sequence[int],\n        pad_value_mask: float,\n        interpolation: int,\n        **params: Any,\n    ) -&gt; np.ndarray:\n        return fcrops.crop_and_pad(\n            mask,\n            crop_params,\n            pad_params,\n            pad_value_mask,\n            params[\"shape\"][:2],\n            interpolation,\n            self.pad_mode,\n            self.keep_size,\n        )\n\n    def apply_to_bbox(\n        self,\n        bbox: BoxInternalType,\n        crop_params: Sequence[int],\n        pad_params: Sequence[int],\n        result_shape: tuple[int, int],\n        **params: Any,\n    ) -&gt; BoxInternalType:\n        return fcrops.crop_and_pad_bbox(bbox, crop_params, pad_params, params[\"shape\"][:2], result_shape)\n\n    def apply_to_keypoint(\n        self,\n        keypoint: KeypointInternalType,\n        crop_params: Sequence[int],\n        pad_params: Sequence[int],\n        result_shape: tuple[int, int],\n        **params: Any,\n    ) -&gt; KeypointInternalType:\n        return fcrops.crop_and_pad_keypoint(\n            keypoint,\n            crop_params,\n            pad_params,\n            params[\"shape\"][:2],\n            result_shape,\n            self.keep_size,\n        )\n\n    @staticmethod\n    def __prevent_zero(val1: int, val2: int, max_val: int) -&gt; tuple[int, int]:\n        regain = abs(max_val) + 1\n        regain1 = regain // 2\n        regain2 = regain // 2\n        if regain1 + regain2 &lt; regain:\n            regain1 += 1\n\n        if regain1 &gt; val1:\n            diff = regain1 - val1\n            regain1 = val1\n            regain2 += diff\n        elif regain2 &gt; val2:\n            diff = regain2 - val2\n            regain2 = val2\n            regain1 += diff\n\n        return val1 - regain1, val2 - regain2\n\n    @staticmethod\n    def _prevent_zero(crop_params: list[int], height: int, width: int) -&gt; list[int]:\n        top, right, bottom, left = crop_params\n\n        remaining_height = height - (top + bottom)\n        remaining_width = width - (left + right)\n\n        if remaining_height &lt; 1:\n            top, bottom = CropAndPad.__prevent_zero(top, bottom, height)\n        if remaining_width &lt; 1:\n            left, right = CropAndPad.__prevent_zero(left, right, width)\n\n        return [max(top, 0), max(right, 0), max(bottom, 0), max(left, 0)]\n\n    def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n        height, width = params[\"shape\"][:2]\n\n        if self.px is not None:\n            new_params = self._get_px_params()\n        else:\n            percent_params = self._get_percent_params()\n            new_params = [\n                int(percent_params[0] * height),\n                int(percent_params[1] * width),\n                int(percent_params[2] * height),\n                int(percent_params[3] * width),\n            ]\n\n        pad_params = [max(i, 0) for i in new_params]\n\n        crop_params = self._prevent_zero([-min(i, 0) for i in new_params], height, width)\n\n        top, right, bottom, left = crop_params\n        crop_params = [left, top, width - right, height - bottom]\n        result_rows = crop_params[3] - crop_params[1]\n        result_cols = crop_params[2] - crop_params[0]\n        if result_cols == width and result_rows == height:\n            crop_params = []\n\n        top, right, bottom, left = pad_params\n        pad_params = [top, bottom, left, right]\n        if any(pad_params):\n            result_rows += top + bottom\n            result_cols += left + right\n        else:\n            pad_params = []\n\n        return {\n            \"crop_params\": crop_params or None,\n            \"pad_params\": pad_params or None,\n            \"pad_value\": None if pad_params is None else self._get_pad_value(self.pad_cval),\n            \"pad_value_mask\": None if pad_params is None else self._get_pad_value(self.pad_cval_mask),\n            \"result_shape\": (result_rows, result_cols),\n        }\n\n    def _get_px_params(self) -&gt; list[int]:\n        if self.px is None:\n            msg = \"px is not set\"\n            raise ValueError(msg)\n\n        if isinstance(self.px, int):\n            params = [self.px] * 4\n        elif len(self.px) == PAIR:\n            if self.sample_independently:\n                params = [random.randrange(*self.px) for _ in range(4)]\n            else:\n                px = random.randrange(*self.px)\n                params = [px] * 4\n        elif isinstance(self.px[0], int):\n            params = self.px\n        elif len(self.px[0]) == PAIR:\n            params = [random.randrange(*i) for i in self.px]\n        else:\n            params = [random.choice(i) for i in self.px]\n\n        return params\n\n    def _get_percent_params(self) -&gt; list[float]:\n        if self.percent is None:\n            msg = \"percent is not set\"\n            raise ValueError(msg)\n\n        if isinstance(self.percent, float):\n            params = [self.percent] * 4\n        elif len(self.percent) == PAIR:\n            if self.sample_independently:\n                params = [random.uniform(*self.percent) for _ in range(4)]\n            else:\n                px = random.uniform(*self.percent)\n                params = [px] * 4\n        elif isinstance(self.percent[0], (int, float)):\n            params = self.percent\n        elif len(self.percent[0]) == PAIR:\n            params = [random.uniform(*i) for i in self.percent]\n        else:\n            params = [random.choice(i) for i in self.percent]\n\n        return params  # params = [top, right, bottom, left]\n\n    @staticmethod\n    def _get_pad_value(\n        pad_value: ScalarType | tuple[ScalarType, ScalarType] | list[ScalarType],\n    ) -&gt; ScalarType:\n        if isinstance(pad_value, (int, float)):\n            return pad_value\n\n        if len(pad_value) == PAIR:\n            a, b = pad_value\n            if isinstance(a, int) and isinstance(b, int):\n                return random.randint(a, b)\n\n            return random.uniform(a, b)\n\n        return random.choice(pad_value)\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return (\n            \"px\",\n            \"percent\",\n            \"pad_mode\",\n            \"pad_cval\",\n            \"pad_cval_mask\",\n            \"keep_size\",\n            \"sample_independently\",\n            \"interpolation\",\n        )\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.crops.transforms.CropAndPad.apply","title":"<code>apply (self, img, crop_params, pad_params, pad_value, interpolation, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/crops/transforms.py</code> Python<pre><code>def apply(\n    self,\n    img: np.ndarray,\n    crop_params: Sequence[int],\n    pad_params: Sequence[int],\n    pad_value: ColorType,\n    interpolation: int,\n    **params: Any,\n) -&gt; np.ndarray:\n    return fcrops.crop_and_pad(\n        img,\n        crop_params,\n        pad_params,\n        pad_value,\n        params[\"shape\"][:2],\n        interpolation,\n        self.pad_mode,\n        self.keep_size,\n    )\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.crops.transforms.CropAndPad.get_params_dependent_on_data","title":"<code>get_params_dependent_on_data (self, params, data)</code>","text":"<p>Returns parameters dependent on input.</p> Source code in <code>albumentations/augmentations/crops/transforms.py</code> Python<pre><code>def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n    height, width = params[\"shape\"][:2]\n\n    if self.px is not None:\n        new_params = self._get_px_params()\n    else:\n        percent_params = self._get_percent_params()\n        new_params = [\n            int(percent_params[0] * height),\n            int(percent_params[1] * width),\n            int(percent_params[2] * height),\n            int(percent_params[3] * width),\n        ]\n\n    pad_params = [max(i, 0) for i in new_params]\n\n    crop_params = self._prevent_zero([-min(i, 0) for i in new_params], height, width)\n\n    top, right, bottom, left = crop_params\n    crop_params = [left, top, width - right, height - bottom]\n    result_rows = crop_params[3] - crop_params[1]\n    result_cols = crop_params[2] - crop_params[0]\n    if result_cols == width and result_rows == height:\n        crop_params = []\n\n    top, right, bottom, left = pad_params\n    pad_params = [top, bottom, left, right]\n    if any(pad_params):\n        result_rows += top + bottom\n        result_cols += left + right\n    else:\n        pad_params = []\n\n    return {\n        \"crop_params\": crop_params or None,\n        \"pad_params\": pad_params or None,\n        \"pad_value\": None if pad_params is None else self._get_pad_value(self.pad_cval),\n        \"pad_value_mask\": None if pad_params is None else self._get_pad_value(self.pad_cval_mask),\n        \"result_shape\": (result_rows, result_cols),\n    }\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.crops.transforms.CropAndPad.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/crops/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return (\n        \"px\",\n        \"percent\",\n        \"pad_mode\",\n        \"pad_cval\",\n        \"pad_cval_mask\",\n        \"keep_size\",\n        \"sample_independently\",\n        \"interpolation\",\n    )\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.crops.transforms.CropNonEmptyMaskIfExists","title":"<code>class  CropNonEmptyMaskIfExists</code> <code>     (height, width, ignore_values=None, ignore_channels=None, always_apply=None, p=1.0)                 </code>  [view source on GitHub]","text":"<p>Crop area with mask if mask is non-empty, else make random crop.</p> <p>Parameters:</p> Name Type Description <code>height</code> <code>int</code> <p>vertical size of crop in pixels</p> <code>width</code> <code>int</code> <p>horizontal size of crop in pixels</p> <code>ignore_values</code> <code>list of int</code> <p>values to ignore in mask, <code>0</code> values are always ignored (e.g. if background value is 5 set <code>ignore_values=[5]</code> to ignore)</p> <code>ignore_channels</code> <code>list of int</code> <p>channels to ignore in mask (e.g. if background is a first channel set <code>ignore_channels=[0]</code> to ignore)</p> <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 1.0.</p> <p>Targets</p> <p>image, mask, bboxes, keypoints</p> <p>Image types:     uint8, float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/crops/transforms.py</code> Python<pre><code>class CropNonEmptyMaskIfExists(_BaseCrop):\n    \"\"\"Crop area with mask if mask is non-empty, else make random crop.\n\n    Args:\n        height: vertical size of crop in pixels\n        width: horizontal size of crop in pixels\n        ignore_values (list of int): values to ignore in mask, `0` values are always ignored\n            (e.g. if background value is 5 set `ignore_values=[5]` to ignore)\n        ignore_channels (list of int): channels to ignore in mask\n            (e.g. if background is a first channel set `ignore_channels=[0]` to ignore)\n        p: probability of applying the transform. Default: 1.0.\n\n    Targets:\n        image, mask, bboxes, keypoints\n\n    Image types:\n        uint8, float32\n\n    \"\"\"\n\n    class InitSchema(CropInitSchema):\n        ignore_values: list[int] | None = Field(\n            default=None,\n            description=\"Values to ignore in mask, `0` values are always ignored\",\n        )\n        ignore_channels: list[int] | None = Field(default=None, description=\"Channels to ignore in mask\")\n\n    def __init__(\n        self,\n        height: int,\n        width: int,\n        ignore_values: list[int] | None = None,\n        ignore_channels: list[int] | None = None,\n        always_apply: bool | None = None,\n        p: float = 1.0,\n    ):\n        super().__init__(p, always_apply)\n\n        self.height = height\n        self.width = width\n        self.ignore_values = ignore_values\n        self.ignore_channels = ignore_channels\n\n    def _preprocess_mask(self, mask: np.ndarray) -&gt; np.ndarray:\n        mask_height, mask_width = mask.shape[:2]\n\n        if self.ignore_values is not None:\n            ignore_values_np = np.array(self.ignore_values)\n            mask = np.where(np.isin(mask, ignore_values_np), 0, mask)\n\n        if mask.ndim == NUM_MULTI_CHANNEL_DIMENSIONS and self.ignore_channels is not None:\n            target_channels = np.array([ch for ch in range(mask.shape[-1]) if ch not in self.ignore_channels])\n            mask = np.take(mask, target_channels, axis=-1)\n\n        if self.height &gt; mask_height or self.width &gt; mask_width:\n            raise ValueError(\n                f\"Crop size ({self.height},{self.width}) is larger than image ({mask_height},{mask_width})\",\n            )\n\n        return mask\n\n    def update_params(self, params: dict[str, Any], **kwargs: Any) -&gt; dict[str, Any]:\n        super().update_params(params, **kwargs)\n        if \"mask\" in kwargs:\n            mask = self._preprocess_mask(kwargs[\"mask\"])\n        elif \"masks\" in kwargs and len(kwargs[\"masks\"]):\n            masks = kwargs[\"masks\"]\n            mask = self._preprocess_mask(np.copy(masks[0]))  # need copy as we perform in-place mod afterwards\n            for m in masks[1:]:\n                mask |= self._preprocess_mask(m)\n        else:\n            msg = \"Can not find mask for CropNonEmptyMaskIfExists\"\n            raise RuntimeError(msg)\n\n        mask_height, mask_width = mask.shape[:2]\n\n        if mask.any():\n            mask = mask.sum(axis=-1) if mask.ndim == NUM_MULTI_CHANNEL_DIMENSIONS else mask\n            non_zero_yx = np.argwhere(mask)\n            y, x = random.choice(non_zero_yx)\n            x_min = x - random.randint(0, self.width - 1)\n            y_min = y - random.randint(0, self.height - 1)\n            x_min = np.clip(x_min, 0, mask_width - self.width)\n            y_min = np.clip(y_min, 0, mask_height - self.height)\n        else:\n            x_min = random.randint(0, mask_width - self.width)\n            y_min = random.randint(0, mask_height - self.height)\n\n        x_max = x_min + self.width\n        y_max = y_min + self.height\n\n        crop_coords = x_min, y_min, x_max, y_max\n\n        params[\"crop_coords\"] = crop_coords\n        return params\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return \"height\", \"width\", \"ignore_values\", \"ignore_channels\"\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.crops.transforms.CropNonEmptyMaskIfExists.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/crops/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return \"height\", \"width\", \"ignore_values\", \"ignore_channels\"\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.crops.transforms.CropNonEmptyMaskIfExists.update_params","title":"<code>update_params (self, params, **kwargs)</code>","text":"<p>Update parameters with transform specific params. This method is deprecated, use: - <code>get_params</code> for transform specific params like interpolation and - <code>update_params_shape</code> for data like shape.</p> Source code in <code>albumentations/augmentations/crops/transforms.py</code> Python<pre><code>def update_params(self, params: dict[str, Any], **kwargs: Any) -&gt; dict[str, Any]:\n    super().update_params(params, **kwargs)\n    if \"mask\" in kwargs:\n        mask = self._preprocess_mask(kwargs[\"mask\"])\n    elif \"masks\" in kwargs and len(kwargs[\"masks\"]):\n        masks = kwargs[\"masks\"]\n        mask = self._preprocess_mask(np.copy(masks[0]))  # need copy as we perform in-place mod afterwards\n        for m in masks[1:]:\n            mask |= self._preprocess_mask(m)\n    else:\n        msg = \"Can not find mask for CropNonEmptyMaskIfExists\"\n        raise RuntimeError(msg)\n\n    mask_height, mask_width = mask.shape[:2]\n\n    if mask.any():\n        mask = mask.sum(axis=-1) if mask.ndim == NUM_MULTI_CHANNEL_DIMENSIONS else mask\n        non_zero_yx = np.argwhere(mask)\n        y, x = random.choice(non_zero_yx)\n        x_min = x - random.randint(0, self.width - 1)\n        y_min = y - random.randint(0, self.height - 1)\n        x_min = np.clip(x_min, 0, mask_width - self.width)\n        y_min = np.clip(y_min, 0, mask_height - self.height)\n    else:\n        x_min = random.randint(0, mask_width - self.width)\n        y_min = random.randint(0, mask_height - self.height)\n\n    x_max = x_min + self.width\n    y_max = y_min + self.height\n\n    crop_coords = x_min, y_min, x_max, y_max\n\n    params[\"crop_coords\"] = crop_coords\n    return params\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.crops.transforms.RandomCrop","title":"<code>class  RandomCrop</code> <code>     (height, width, p=1.0, always_apply=None)                 </code>  [view source on GitHub]","text":"<p>Crop a random part of the input.</p> <p>Parameters:</p> Name Type Description <code>height</code> <code>int</code> <p>height of the crop.</p> <code>width</code> <code>int</code> <p>width of the crop.</p> <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 1.</p> <p>Targets</p> <p>image, mask, bboxes, keypoints</p> <p>Image types:     uint8, float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/crops/transforms.py</code> Python<pre><code>class RandomCrop(_BaseCrop):\n    \"\"\"Crop a random part of the input.\n\n    Args:\n        height: height of the crop.\n        width: width of the crop.\n        p: probability of applying the transform. Default: 1.\n\n    Targets:\n        image, mask, bboxes, keypoints\n\n    Image types:\n        uint8, float32\n\n    \"\"\"\n\n    class InitSchema(CropInitSchema):\n        pass\n\n    def __init__(self, height: int, width: int, p: float = 1.0, always_apply: bool | None = None):\n        super().__init__(p=p, always_apply=always_apply)\n        self.height = height\n        self.width = width\n\n    def get_params_dependent_on_data(\n        self,\n        params: dict[str, Any],\n        data: dict[str, Any],\n    ) -&gt; dict[str, tuple[int, int, int, int]]:\n        image_shape = params[\"shape\"][:2]\n\n        image_height, image_width = image_shape\n\n        if self.height &gt; image_height or self.width &gt; image_width:\n            raise CropSizeError(\n                f\"Crop size (height, width) exceeds image dimensions (height, width):\"\n                f\" {(self.height, self.width)} vs {image_shape[:2]}\",\n            )\n\n        h_start = random.random()\n        w_start = random.random()\n        crop_coords = fcrops.get_crop_coords(image_shape, (self.height, self.width), h_start, w_start)\n        return {\"crop_coords\": crop_coords}\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return \"height\", \"width\"\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.crops.transforms.RandomCrop.get_params_dependent_on_data","title":"<code>get_params_dependent_on_data (self, params, data)</code>","text":"<p>Returns parameters dependent on input.</p> Source code in <code>albumentations/augmentations/crops/transforms.py</code> Python<pre><code>def get_params_dependent_on_data(\n    self,\n    params: dict[str, Any],\n    data: dict[str, Any],\n) -&gt; dict[str, tuple[int, int, int, int]]:\n    image_shape = params[\"shape\"][:2]\n\n    image_height, image_width = image_shape\n\n    if self.height &gt; image_height or self.width &gt; image_width:\n        raise CropSizeError(\n            f\"Crop size (height, width) exceeds image dimensions (height, width):\"\n            f\" {(self.height, self.width)} vs {image_shape[:2]}\",\n        )\n\n    h_start = random.random()\n    w_start = random.random()\n    crop_coords = fcrops.get_crop_coords(image_shape, (self.height, self.width), h_start, w_start)\n    return {\"crop_coords\": crop_coords}\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.crops.transforms.RandomCrop.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/crops/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return \"height\", \"width\"\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.crops.transforms.RandomCropFromBorders","title":"<code>class  RandomCropFromBorders</code> <code>     (crop_left=0.1, crop_right=0.1, crop_top=0.1, crop_bottom=0.1, always_apply=None, p=1.0)                 </code>  [view source on GitHub]","text":"<p>Randomly crops parts of the image from the borders without resizing at the end. The cropped regions are defined as fractions of the original image dimensions, specified for each side of the image (left, right, top, bottom).</p> <p>Parameters:</p> Name Type Description <code>crop_left</code> <code>float</code> <p>Fraction of the width to randomly crop from the left side. Must be in the range [0.0, 1.0].                 Default is 0.1.</p> <code>crop_right</code> <code>float</code> <p>Fraction of the width to randomly crop from the right side. Must be in the range [0.0, 1.0].                 Default is 0.1.</p> <code>crop_top</code> <code>float</code> <p>Fraction of the height to randomly crop from the top side. Must be in the range [0.0, 1.0].               Default is 0.1.</p> <code>crop_bottom</code> <code>float</code> <p>Fraction of the height to randomly crop from the bottom side.                  Must be in the range [0.0, 1.0]. Default is 0.1.</p> <code>p</code> <code>float</code> <p>Probability of applying the transform. Default is 1.</p> <p>Targets</p> <p>image, mask, bboxes, keypoints</p> <p>Image types:     uint8, float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/crops/transforms.py</code> Python<pre><code>class RandomCropFromBorders(_BaseCrop):\n    \"\"\"Randomly crops parts of the image from the borders without resizing at the end. The cropped regions are defined\n    as fractions of the original image dimensions, specified for each side of the image (left, right, top, bottom).\n\n    Args:\n        crop_left (float): Fraction of the width to randomly crop from the left side. Must be in the range [0.0, 1.0].\n                            Default is 0.1.\n        crop_right (float): Fraction of the width to randomly crop from the right side. Must be in the range [0.0, 1.0].\n                            Default is 0.1.\n        crop_top (float): Fraction of the height to randomly crop from the top side. Must be in the range [0.0, 1.0].\n                          Default is 0.1.\n        crop_bottom (float): Fraction of the height to randomly crop from the bottom side.\n                             Must be in the range [0.0, 1.0]. Default is 0.1.\n        p (float): Probability of applying the transform. Default is 1.\n\n    Targets:\n        image, mask, bboxes, keypoints\n\n    Image types:\n        uint8, float32\n    \"\"\"\n\n    _targets = (Targets.IMAGE, Targets.MASK, Targets.BBOXES, Targets.KEYPOINTS)\n\n    class InitSchema(BaseTransformInitSchema):\n        crop_left: float = Field(\n            default=0.1,\n            ge=0.0,\n            le=1.0,\n            description=\"Fraction of width to randomly crop from the left side.\",\n        )\n        crop_right: float = Field(\n            default=0.1,\n            ge=0.0,\n            le=1.0,\n            description=\"Fraction of width to randomly crop from the right side.\",\n        )\n        crop_top: float = Field(\n            default=0.1,\n            ge=0.0,\n            le=1.0,\n            description=\"Fraction of height to randomly crop from the top side.\",\n        )\n        crop_bottom: float = Field(\n            default=0.1,\n            ge=0.0,\n            le=1.0,\n            description=\"Fraction of height to randomly crop from the bottom side.\",\n        )\n        p: ProbabilityType = 1\n\n        @model_validator(mode=\"after\")\n        def validate_crop_values(self) -&gt; Self:\n            if self.crop_left + self.crop_right &gt; 1.0:\n                msg = \"The sum of crop_left and crop_right must be &lt;= 1.\"\n                raise ValueError(msg)\n            if self.crop_top + self.crop_bottom &gt; 1.0:\n                msg = \"The sum of crop_top and crop_bottom must be &lt;= 1.\"\n                raise ValueError(msg)\n            return self\n\n    def __init__(\n        self,\n        crop_left: float = 0.1,\n        crop_right: float = 0.1,\n        crop_top: float = 0.1,\n        crop_bottom: float = 0.1,\n        always_apply: bool | None = None,\n        p: float = 1.0,\n    ):\n        super().__init__(p, always_apply)\n        self.crop_left = crop_left\n        self.crop_right = crop_right\n        self.crop_top = crop_top\n        self.crop_bottom = crop_bottom\n\n    def get_params_dependent_on_data(\n        self,\n        params: dict[str, Any],\n        data: dict[str, Any],\n    ) -&gt; dict[str, tuple[int, int, int, int]]:\n        height, width = params[\"shape\"][:2]\n\n        x_min = random.randint(0, int(self.crop_left * width))\n        x_max = random.randint(max(x_min + 1, int((1 - self.crop_right) * width)), width)\n\n        y_min = random.randint(0, int(self.crop_top * height))\n        y_max = random.randint(max(y_min + 1, int((1 - self.crop_bottom) * height)), height)\n\n        crop_coords = x_min, y_min, x_max, y_max\n\n        return {\"crop_coords\": crop_coords}\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return \"crop_left\", \"crop_right\", \"crop_top\", \"crop_bottom\"\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.crops.transforms.RandomCropFromBorders.get_params_dependent_on_data","title":"<code>get_params_dependent_on_data (self, params, data)</code>","text":"<p>Returns parameters dependent on input.</p> Source code in <code>albumentations/augmentations/crops/transforms.py</code> Python<pre><code>def get_params_dependent_on_data(\n    self,\n    params: dict[str, Any],\n    data: dict[str, Any],\n) -&gt; dict[str, tuple[int, int, int, int]]:\n    height, width = params[\"shape\"][:2]\n\n    x_min = random.randint(0, int(self.crop_left * width))\n    x_max = random.randint(max(x_min + 1, int((1 - self.crop_right) * width)), width)\n\n    y_min = random.randint(0, int(self.crop_top * height))\n    y_max = random.randint(max(y_min + 1, int((1 - self.crop_bottom) * height)), height)\n\n    crop_coords = x_min, y_min, x_max, y_max\n\n    return {\"crop_coords\": crop_coords}\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.crops.transforms.RandomCropFromBorders.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/crops/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return \"crop_left\", \"crop_right\", \"crop_top\", \"crop_bottom\"\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.crops.transforms.RandomCropNearBBox","title":"<code>class  RandomCropNearBBox</code> <code>     (max_part_shift=(0, 0.3), cropping_bbox_key='cropping_bbox', cropping_box_key=None, always_apply=None, p=1.0)                 </code>  [view source on GitHub]","text":"<p>Crop bbox from image with random shift by x,y coordinates</p> <p>Parameters:</p> Name Type Description <code>max_part_shift</code> <code>float, (float, float</code> <p>Max shift in <code>height</code> and <code>width</code> dimensions relative to <code>cropping_bbox</code> dimension. If max_part_shift is a single float, the range will be (0, max_part_shift). Default (0, 0.3).</p> <code>cropping_bbox_key</code> <code>str</code> <p>Additional target key for cropping box. Default <code>cropping_bbox</code>.</p> <code>cropping_box_key</code> <code>str</code> <p>[Deprecated] Use <code>cropping_bbox_key</code> instead.</p> <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 1.</p> <p>Targets</p> <p>image, mask, bboxes, keypoints</p> <p>Image types:     uint8, float32</p> <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; aug = Compose([RandomCropNearBBox(max_part_shift=(0.1, 0.5), cropping_bbox_key='test_bbox')],\n&gt;&gt;&gt;              bbox_params=BboxParams(\"pascal_voc\"))\n&gt;&gt;&gt; result = aug(image=image, bboxes=bboxes, test_bbox=[0, 5, 10, 20])\n</code></pre> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/crops/transforms.py</code> Python<pre><code>class RandomCropNearBBox(_BaseCrop):\n    \"\"\"Crop bbox from image with random shift by x,y coordinates\n\n    Args:\n        max_part_shift (float, (float, float)): Max shift in `height` and `width` dimensions relative\n            to `cropping_bbox` dimension.\n            If max_part_shift is a single float, the range will be (0, max_part_shift).\n            Default (0, 0.3).\n        cropping_bbox_key (str): Additional target key for cropping box. Default `cropping_bbox`.\n        cropping_box_key (str): [Deprecated] Use `cropping_bbox_key` instead.\n        p (float): probability of applying the transform. Default: 1.\n\n    Targets:\n        image, mask, bboxes, keypoints\n\n    Image types:\n        uint8, float32\n\n    Examples:\n        &gt;&gt;&gt; aug = Compose([RandomCropNearBBox(max_part_shift=(0.1, 0.5), cropping_bbox_key='test_bbox')],\n        &gt;&gt;&gt;              bbox_params=BboxParams(\"pascal_voc\"))\n        &gt;&gt;&gt; result = aug(image=image, bboxes=bboxes, test_bbox=[0, 5, 10, 20])\n\n    \"\"\"\n\n    _targets = (Targets.IMAGE, Targets.MASK, Targets.BBOXES, Targets.KEYPOINTS)\n\n    class InitSchema(BaseTransformInitSchema):\n        max_part_shift: ZeroOneRangeType = (0, 0.3)\n        cropping_bbox_key: str = Field(default=\"cropping_bbox\", description=\"Additional target key for cropping box.\")\n        p: ProbabilityType = 1\n\n    def __init__(\n        self,\n        max_part_shift: ScaleFloatType = (0, 0.3),\n        cropping_bbox_key: str = \"cropping_bbox\",\n        cropping_box_key: str | None = None,  # Deprecated\n        always_apply: bool | None = None,\n        p: float = 1.0,\n    ):\n        super().__init__(p=p, always_apply=always_apply)\n        # Check for deprecated parameter and issue warning\n        if cropping_box_key is not None:\n            warn(\n                \"The parameter 'cropping_box_key' is deprecated and will be removed in future versions. \"\n                \"Use 'cropping_bbox_key' instead.\",\n                DeprecationWarning,\n                stacklevel=2,\n            )\n            # Ensure the new parameter is used even if the old one is passed\n            cropping_bbox_key = cropping_box_key\n\n        self.max_part_shift = cast(Tuple[float, float], max_part_shift)\n        self.cropping_bbox_key = cropping_bbox_key\n\n    @staticmethod\n    def _clip_bbox(bbox: BoxInternalType, image_shape: tuple[int, int]) -&gt; BoxInternalType:\n        height, width = image_shape[:2]\n        x_min, y_min, x_max, y_max = bbox\n        x_min = np.clip(x_min, 0, width)\n        y_min = np.clip(y_min, 0, height)\n\n        x_max = np.clip(x_max, x_min, width)\n        y_max = np.clip(y_max, y_min, height)\n        return x_min, y_min, x_max, y_max\n\n    def get_params_dependent_on_data(\n        self,\n        params: dict[str, Any],\n        data: dict[str, Any],\n    ) -&gt; dict[str, tuple[float, ...]]:\n        bbox = data[self.cropping_bbox_key]\n\n        image_shape = params[\"shape\"][:2]\n\n        bbox = self._clip_bbox(bbox, image_shape)\n\n        h_max_shift = round((bbox[3] - bbox[1]) * self.max_part_shift[0])\n        w_max_shift = round((bbox[2] - bbox[0]) * self.max_part_shift[1])\n\n        x_min = bbox[0] - random.randint(-w_max_shift, w_max_shift)\n        x_max = bbox[2] + random.randint(-w_max_shift, w_max_shift)\n\n        y_min = bbox[1] - random.randint(-h_max_shift, h_max_shift)\n        y_max = bbox[3] + random.randint(-h_max_shift, h_max_shift)\n\n        crop_coords = self._clip_bbox((x_min, y_min, x_max, y_max), image_shape)\n\n        if crop_coords[0] == crop_coords[2] or crop_coords[1] == crop_coords[3]:\n            crop_shape = (bbox[3] - bbox[1], bbox[2] - bbox[0])\n            crop_coords = fcrops.get_center_crop_coords(image_shape, crop_shape)\n\n        return {\"crop_coords\": crop_coords}\n\n    @property\n    def targets_as_params(self) -&gt; list[str]:\n        return [self.cropping_bbox_key]\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return \"max_part_shift\", \"cropping_bbox_key\"\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.crops.transforms.RandomCropNearBBox.targets_as_params","title":"<code>targets_as_params: list[str]</code>  <code>property</code> <code>readonly</code>","text":"<p>Targets used to get params dependent on targets. This is used to check input has all required targets.</p>"},{"location":"api_reference/full_reference/#albumentations.augmentations.crops.transforms.RandomCropNearBBox.get_params_dependent_on_data","title":"<code>get_params_dependent_on_data (self, params, data)</code>","text":"<p>Returns parameters dependent on input.</p> Source code in <code>albumentations/augmentations/crops/transforms.py</code> Python<pre><code>def get_params_dependent_on_data(\n    self,\n    params: dict[str, Any],\n    data: dict[str, Any],\n) -&gt; dict[str, tuple[float, ...]]:\n    bbox = data[self.cropping_bbox_key]\n\n    image_shape = params[\"shape\"][:2]\n\n    bbox = self._clip_bbox(bbox, image_shape)\n\n    h_max_shift = round((bbox[3] - bbox[1]) * self.max_part_shift[0])\n    w_max_shift = round((bbox[2] - bbox[0]) * self.max_part_shift[1])\n\n    x_min = bbox[0] - random.randint(-w_max_shift, w_max_shift)\n    x_max = bbox[2] + random.randint(-w_max_shift, w_max_shift)\n\n    y_min = bbox[1] - random.randint(-h_max_shift, h_max_shift)\n    y_max = bbox[3] + random.randint(-h_max_shift, h_max_shift)\n\n    crop_coords = self._clip_bbox((x_min, y_min, x_max, y_max), image_shape)\n\n    if crop_coords[0] == crop_coords[2] or crop_coords[1] == crop_coords[3]:\n        crop_shape = (bbox[3] - bbox[1], bbox[2] - bbox[0])\n        crop_coords = fcrops.get_center_crop_coords(image_shape, crop_shape)\n\n    return {\"crop_coords\": crop_coords}\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.crops.transforms.RandomCropNearBBox.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/crops/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return \"max_part_shift\", \"cropping_bbox_key\"\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.crops.transforms.RandomResizedCrop","title":"<code>class  RandomResizedCrop</code> <code>     (size=None, width=None, height=None, *, scale=(0.08, 1.0), ratio=(0.75, 1.3333333333333333), interpolation=1, always_apply=None, p=1.0)                 </code>  [view source on GitHub]","text":"<p>Torchvision's variant of crop a random part of the input and rescale it to some size.</p> <p>Parameters:</p> Name Type Description <code>size</code> <code>int, int</code> <p>expected output size of the crop, for each edge. If size is an int instead of sequence like (height, width), a square output size (size, size) is made. If provided a sequence of length 1, it will be interpreted as (size[0], size[0]).</p> <code>scale</code> <code>float, float</code> <p>Specifies the lower and upper bounds for the random area of the crop, before resizing. The scale is defined with respect to the area of the original image.</p> <code>ratio</code> <code>float, float</code> <p>lower and upper bounds for the random aspect ratio of the crop, before resizing.</p> <code>interpolation</code> <code>OpenCV flag</code> <p>flag that is used to specify the interpolation algorithm. Should be one of: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4. Default: cv2.INTER_LINEAR.</p> <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 1.</p> <p>Targets</p> <p>image, mask, bboxes, keypoints</p> <p>Image types:     uint8, float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/crops/transforms.py</code> Python<pre><code>class RandomResizedCrop(_BaseRandomSizedCrop):\n    \"\"\"Torchvision's variant of crop a random part of the input and rescale it to some size.\n\n    Args:\n        size (int, int): expected output size of the crop, for each edge. If size is an int instead of sequence\n            like (height, width), a square output size (size, size) is made. If provided a sequence of length 1,\n            it will be interpreted as (size[0], size[0]).\n        scale ((float, float)): Specifies the lower and upper bounds for the random area of the crop, before resizing.\n            The scale is defined with respect to the area of the original image.\n        ratio ((float, float)): lower and upper bounds for the random aspect ratio of the crop, before resizing.\n        interpolation (OpenCV flag): flag that is used to specify the interpolation algorithm. Should be one of:\n            cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.\n            Default: cv2.INTER_LINEAR.\n        p (float): probability of applying the transform. Default: 1.\n\n    Targets:\n        image, mask, bboxes, keypoints\n\n    Image types:\n        uint8, float32\n\n    \"\"\"\n\n    _targets = (Targets.IMAGE, Targets.MASK, Targets.BBOXES, Targets.KEYPOINTS)\n\n    class InitSchema(BaseTransformInitSchema):\n        scale: Annotated[tuple[float, float], AfterValidator(check_01)] = (0.08, 1.0)\n        ratio: Annotated[tuple[float, float], AfterValidator(check_0plus)] = (0.75, 1.3333333333333333)\n        width: int | None = Field(\n            None,\n            deprecated=\"Initializing with 'height' and 'width' is deprecated. Use size instead.\",\n        )\n        height: int | None = Field(\n            None,\n            deprecated=\"Initializing with 'height' and 'width' is deprecated. Use size instead.\",\n        )\n        size: ScaleIntType | None = None\n        p: ProbabilityType = 1\n        interpolation: InterpolationType = cv2.INTER_LINEAR\n\n        @model_validator(mode=\"after\")\n        def process(self) -&gt; Self:\n            if isinstance(self.size, int):\n                if isinstance(self.width, int):\n                    self.size = (self.size, self.width)\n                else:\n                    msg = \"If size is an integer, width as integer must be specified.\"\n                    raise TypeError(msg)\n\n            if self.size is None:\n                if self.height is None or self.width is None:\n                    message = \"If 'size' is not provided, both 'height' and 'width' must be specified.\"\n                    raise ValueError(message)\n                self.size = (self.height, self.width)\n\n            return self\n\n    def __init__(\n        self,\n        # NOTE @zetyquickly: when (width, height) are deprecated, make 'size' non optional\n        size: ScaleIntType | None = None,\n        width: int | None = None,\n        height: int | None = None,\n        *,\n        scale: tuple[float, float] = (0.08, 1.0),\n        ratio: tuple[float, float] = (0.75, 1.3333333333333333),\n        interpolation: int = cv2.INTER_LINEAR,\n        always_apply: bool | None = None,\n        p: float = 1.0,\n    ):\n        super().__init__(size=cast(Tuple[int, int], size), interpolation=interpolation, p=p, always_apply=always_apply)\n        self.scale = scale\n        self.ratio = ratio\n\n    def get_params_dependent_on_data(\n        self,\n        params: dict[str, Any],\n        data: dict[str, Any],\n    ) -&gt; dict[str, tuple[int, int, int, int]]:\n        image_shape = params[\"shape\"][:2]\n        image_height, image_width = image_shape\n\n        area = image_height * image_width\n\n        for _ in range(10):\n            target_area = random.uniform(*self.scale) * area\n            log_ratio = (math.log(self.ratio[0]), math.log(self.ratio[1]))\n            aspect_ratio = math.exp(random.uniform(*log_ratio))\n\n            width = int(round(math.sqrt(target_area * aspect_ratio)))\n            height = int(round(math.sqrt(target_area / aspect_ratio)))\n\n            if 0 &lt; width &lt;= image_width and 0 &lt; height &lt;= image_height:\n                i = random.randint(0, image_height - height)\n                j = random.randint(0, image_width - width)\n\n                h_start = i * 1.0 / (image_height - height + 1e-10)\n                w_start = j * 1.0 / (image_width - width + 1e-10)\n\n                crop_shape = (height, width)\n\n                crop_coords = fcrops.get_crop_coords(image_shape, crop_shape, h_start, w_start)\n\n                return {\"crop_coords\": crop_coords}\n\n        # Fallback to central crop\n        in_ratio = image_width / image_height\n        if in_ratio &lt; min(self.ratio):\n            width = image_width\n            height = int(round(image_width / min(self.ratio)))\n        elif in_ratio &gt; max(self.ratio):\n            height = image_height\n            width = int(round(height * max(self.ratio)))\n        else:  # whole image\n            width = image_width\n            height = image_height\n\n        i = (image_height - height) // 2\n        j = (image_width - width) // 2\n\n        h_start = i * 1.0 / (image_height - height + 1e-10)\n        w_start = j * 1.0 / (image_width - width + 1e-10)\n\n        crop_shape = (height, width)\n\n        crop_coords = fcrops.get_crop_coords(image_shape, crop_shape, h_start, w_start)\n\n        return {\"crop_coords\": crop_coords}\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return \"size\", \"scale\", \"ratio\", \"interpolation\"\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.crops.transforms.RandomResizedCrop.get_params_dependent_on_data","title":"<code>get_params_dependent_on_data (self, params, data)</code>","text":"<p>Returns parameters dependent on input.</p> Source code in <code>albumentations/augmentations/crops/transforms.py</code> Python<pre><code>def get_params_dependent_on_data(\n    self,\n    params: dict[str, Any],\n    data: dict[str, Any],\n) -&gt; dict[str, tuple[int, int, int, int]]:\n    image_shape = params[\"shape\"][:2]\n    image_height, image_width = image_shape\n\n    area = image_height * image_width\n\n    for _ in range(10):\n        target_area = random.uniform(*self.scale) * area\n        log_ratio = (math.log(self.ratio[0]), math.log(self.ratio[1]))\n        aspect_ratio = math.exp(random.uniform(*log_ratio))\n\n        width = int(round(math.sqrt(target_area * aspect_ratio)))\n        height = int(round(math.sqrt(target_area / aspect_ratio)))\n\n        if 0 &lt; width &lt;= image_width and 0 &lt; height &lt;= image_height:\n            i = random.randint(0, image_height - height)\n            j = random.randint(0, image_width - width)\n\n            h_start = i * 1.0 / (image_height - height + 1e-10)\n            w_start = j * 1.0 / (image_width - width + 1e-10)\n\n            crop_shape = (height, width)\n\n            crop_coords = fcrops.get_crop_coords(image_shape, crop_shape, h_start, w_start)\n\n            return {\"crop_coords\": crop_coords}\n\n    # Fallback to central crop\n    in_ratio = image_width / image_height\n    if in_ratio &lt; min(self.ratio):\n        width = image_width\n        height = int(round(image_width / min(self.ratio)))\n    elif in_ratio &gt; max(self.ratio):\n        height = image_height\n        width = int(round(height * max(self.ratio)))\n    else:  # whole image\n        width = image_width\n        height = image_height\n\n    i = (image_height - height) // 2\n    j = (image_width - width) // 2\n\n    h_start = i * 1.0 / (image_height - height + 1e-10)\n    w_start = j * 1.0 / (image_width - width + 1e-10)\n\n    crop_shape = (height, width)\n\n    crop_coords = fcrops.get_crop_coords(image_shape, crop_shape, h_start, w_start)\n\n    return {\"crop_coords\": crop_coords}\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.crops.transforms.RandomResizedCrop.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/crops/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return \"size\", \"scale\", \"ratio\", \"interpolation\"\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.crops.transforms.RandomSizedBBoxSafeCrop","title":"<code>class  RandomSizedBBoxSafeCrop</code> <code>     (height, width, erosion_rate=0.0, interpolation=1, always_apply=None, p=1.0)                 </code>  [view source on GitHub]","text":"<p>Crop a random part of the input and rescale it to some size without loss of bboxes.</p> <p>Parameters:</p> Name Type Description <code>height</code> <code>int</code> <p>height after crop and resize.</p> <code>width</code> <code>int</code> <p>width after crop and resize.</p> <code>erosion_rate</code> <code>float</code> <p>erosion rate applied on input image height before crop.</p> <code>interpolation</code> <code>OpenCV flag</code> <p>flag that is used to specify the interpolation algorithm. Should be one of: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4. Default: cv2.INTER_LINEAR.</p> <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 1.</p> <p>Targets</p> <p>image, mask, bboxes, keypoints</p> <p>Image types:     uint8, float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/crops/transforms.py</code> Python<pre><code>class RandomSizedBBoxSafeCrop(BBoxSafeRandomCrop):\n    \"\"\"Crop a random part of the input and rescale it to some size without loss of bboxes.\n\n    Args:\n        height: height after crop and resize.\n        width: width after crop and resize.\n        erosion_rate: erosion rate applied on input image height before crop.\n        interpolation (OpenCV flag): flag that is used to specify the interpolation algorithm. Should be one of:\n            cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.\n            Default: cv2.INTER_LINEAR.\n        p (float): probability of applying the transform. Default: 1.\n\n    Targets:\n        image, mask, bboxes, keypoints\n\n    Image types:\n        uint8, float32\n\n    \"\"\"\n\n    _targets = (Targets.IMAGE, Targets.MASK, Targets.BBOXES, Targets.KEYPOINTS)\n\n    class InitSchema(CropInitSchema):\n        erosion_rate: float = Field(\n            default=0.0,\n            ge=0.0,\n            le=1.0,\n            description=\"Erosion rate applied on input image height before crop.\",\n        )\n        interpolation: InterpolationType = cv2.INTER_LINEAR\n\n    def __init__(\n        self,\n        height: int,\n        width: int,\n        erosion_rate: float = 0.0,\n        interpolation: int = cv2.INTER_LINEAR,\n        always_apply: bool | None = None,\n        p: float = 1.0,\n    ):\n        super().__init__(erosion_rate=erosion_rate, p=p, always_apply=always_apply)\n        self.height = height\n        self.width = width\n        self.interpolation = interpolation\n\n    def apply(\n        self,\n        img: np.ndarray,\n        crop_coords: tuple[int, int, int, int],\n        **params: Any,\n    ) -&gt; np.ndarray:\n        crop = fcrops.crop(img, *crop_coords)\n        return fgeometric.resize(crop, (self.height, self.width), self.interpolation)\n\n    def apply_to_keypoint(\n        self,\n        keypoint: KeypointInternalType,\n        crop_coords: tuple[int, int, int, int],\n        **params: Any,\n    ) -&gt; KeypointInternalType:\n        keypoint = fcrops.crop_keypoint_by_coords(keypoint, crop_coords)\n\n        crop_height = crop_coords[3] - crop_coords[1]\n        crop_width = crop_coords[2] - crop_coords[0]\n\n        scale_y = self.height / crop_height\n        scale_x = self.width / crop_width\n        return fgeometric.keypoint_scale(keypoint, scale_x=scale_x, scale_y=scale_y)\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return (*super().get_transform_init_args_names(), \"height\", \"width\", \"interpolation\")\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.crops.transforms.RandomSizedBBoxSafeCrop.apply","title":"<code>apply (self, img, crop_coords, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/crops/transforms.py</code> Python<pre><code>def apply(\n    self,\n    img: np.ndarray,\n    crop_coords: tuple[int, int, int, int],\n    **params: Any,\n) -&gt; np.ndarray:\n    crop = fcrops.crop(img, *crop_coords)\n    return fgeometric.resize(crop, (self.height, self.width), self.interpolation)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.crops.transforms.RandomSizedBBoxSafeCrop.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/crops/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return (*super().get_transform_init_args_names(), \"height\", \"width\", \"interpolation\")\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.crops.transforms.RandomSizedCrop","title":"<code>class  RandomSizedCrop</code> <code>     (min_max_height, size=None, width=None, height=None, *, w2h_ratio=1.0, interpolation=1, always_apply=None, p=1.0)                 </code>  [view source on GitHub]","text":"<p>Crop a random portion of the input and rescale it to a specific size.</p> <p>Parameters:</p> Name Type Description <code>min_max_height</code> <code>int, int</code> <p>crop size limits.</p> <code>size</code> <code>int, int</code> <p>target size for the output image, i.e. (height, width) after crop and resize</p> <code>w2h_ratio</code> <code>float</code> <p>aspect ratio of crop.</p> <code>interpolation</code> <code>OpenCV flag</code> <p>flag that is used to specify the interpolation algorithm. Should be one of: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4. Default: cv2.INTER_LINEAR.</p> <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 1.</p> <p>Targets</p> <p>image, mask, bboxes, keypoints</p> <p>Image types:     uint8, float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/crops/transforms.py</code> Python<pre><code>class RandomSizedCrop(_BaseRandomSizedCrop):\n    \"\"\"Crop a random portion of the input and rescale it to a specific size.\n\n    Args:\n        min_max_height ((int, int)): crop size limits.\n        size ((int, int)): target size for the output image, i.e. (height, width) after crop and resize\n        w2h_ratio (float): aspect ratio of crop.\n        interpolation (OpenCV flag): flag that is used to specify the interpolation algorithm. Should be one of:\n            cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.\n            Default: cv2.INTER_LINEAR.\n        p (float): probability of applying the transform. Default: 1.\n\n    Targets:\n        image, mask, bboxes, keypoints\n\n    Image types:\n        uint8, float32\n\n    \"\"\"\n\n    _targets = (Targets.IMAGE, Targets.MASK, Targets.BBOXES, Targets.KEYPOINTS)\n\n    class InitSchema(BaseTransformInitSchema):\n        interpolation: InterpolationType = cv2.INTER_LINEAR\n        p: ProbabilityType = 1\n        min_max_height: OnePlusIntRangeType\n        w2h_ratio: Annotated[float, Field(gt=0, description=\"Aspect ratio of crop.\")]\n        width: int | None = Field(\n            None,\n            deprecated=(\n                \"Initializing with 'size' as an integer and a separate 'width' is deprecated. \"\n                \"Please use a tuple (height, width) for the 'size' argument.\"\n            ),\n        )\n        height: int | None = Field(\n            None,\n            deprecated=(\n                \"Initializing with 'height' and 'width' is deprecated. \"\n                \"Please use a tuple (height, width) for the 'size' argument.\"\n            ),\n        )\n        size: ScaleIntType | None = None\n\n        @model_validator(mode=\"after\")\n        def process(self) -&gt; Self:\n            if isinstance(self.size, int):\n                if isinstance(self.width, int):\n                    self.size = (self.size, self.width)\n                else:\n                    msg = \"If size is an integer, width as integer must be specified.\"\n                    raise TypeError(msg)\n\n            if self.size is None:\n                if self.height is None or self.width is None:\n                    message = \"If 'size' is not provided, both 'height' and 'width' must be specified.\"\n                    raise ValueError(message)\n                self.size = (self.height, self.width)\n            return self\n\n    def __init__(\n        self,\n        min_max_height: tuple[int, int],\n        # NOTE @zetyquickly: when (width, height) are deprecated, make 'size' non optional\n        size: ScaleIntType | None = None,\n        width: int | None = None,\n        height: int | None = None,\n        *,\n        w2h_ratio: float = 1.0,\n        interpolation: int = cv2.INTER_LINEAR,\n        always_apply: bool | None = None,\n        p: float = 1.0,\n    ):\n        super().__init__(size=cast(Tuple[int, int], size), interpolation=interpolation, p=p, always_apply=always_apply)\n        self.min_max_height = min_max_height\n        self.w2h_ratio = w2h_ratio\n\n    def get_params_dependent_on_data(\n        self,\n        params: dict[str, Any],\n        data: dict[str, Any],\n    ) -&gt; dict[str, tuple[int, int, int, int]]:\n        image_shape = params[\"shape\"][:2]\n\n        crop_height = random.randint(self.min_max_height[0], self.min_max_height[1])\n        crop_width = int(crop_height * self.w2h_ratio)\n\n        crop_shape = (crop_height, crop_width)\n\n        h_start = random.random()\n        w_start = random.random()\n\n        crop_coords = fcrops.get_crop_coords(image_shape, crop_shape, h_start, w_start)\n\n        return {\"crop_coords\": crop_coords}\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return \"min_max_height\", \"size\", \"w2h_ratio\", \"interpolation\"\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.crops.transforms.RandomSizedCrop.get_params_dependent_on_data","title":"<code>get_params_dependent_on_data (self, params, data)</code>","text":"<p>Returns parameters dependent on input.</p> Source code in <code>albumentations/augmentations/crops/transforms.py</code> Python<pre><code>def get_params_dependent_on_data(\n    self,\n    params: dict[str, Any],\n    data: dict[str, Any],\n) -&gt; dict[str, tuple[int, int, int, int]]:\n    image_shape = params[\"shape\"][:2]\n\n    crop_height = random.randint(self.min_max_height[0], self.min_max_height[1])\n    crop_width = int(crop_height * self.w2h_ratio)\n\n    crop_shape = (crop_height, crop_width)\n\n    h_start = random.random()\n    w_start = random.random()\n\n    crop_coords = fcrops.get_crop_coords(image_shape, crop_shape, h_start, w_start)\n\n    return {\"crop_coords\": crop_coords}\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.crops.transforms.RandomSizedCrop.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/crops/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return \"min_max_height\", \"size\", \"w2h_ratio\", \"interpolation\"\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.domain_adaptation","title":"<code>domain_adaptation</code>","text":""},{"location":"api_reference/full_reference/#albumentations.augmentations.domain_adaptation.FDA","title":"<code>class  FDA</code> <code>     (reference_images, beta_limit=(0, 0.1), read_fn=&lt;function read_rgb_image at 0x7f4afc5b3740&gt;, always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Fourier Domain Adaptation (FDA) for simple \"style transfer\" in the context of unsupervised domain adaptation (UDA). FDA manipulates the frequency components of images to reduce the domain gap between source and target datasets, effectively adapting images from one domain to closely resemble those from another without altering their semantic content.</p> <p>This transform is particularly beneficial in scenarios where the training (source) and testing (target) images come from different distributions, such as synthetic versus real images, or day versus night scenes. Unlike traditional domain adaptation methods that may require complex adversarial training, FDA achieves domain alignment by swapping low-frequency components of the Fourier transform between the source and target images. This technique has shown to improve the performance of models on the target domain, particularly for tasks like semantic segmentation, without additional training for domain invariance.</p> <p>The 'beta_limit' parameter controls the extent of frequency component swapping, with lower values preserving more of the original image's characteristics and higher values leading to more pronounced adaptation effects. It is recommended to use beta values less than 0.3 to avoid introducing artifacts.</p> <p>Parameters:</p> Name Type Description <code>reference_images</code> <code>Sequence[Any]</code> <p>Sequence of objects to be converted into images by <code>read_fn</code>. This typically involves paths to images that serve as target domain examples for adaptation.</p> <code>beta_limit</code> <code>float or tuple of float</code> <p>Coefficient beta from the paper, controlling the swapping extent of frequency components. Values should be less than 0.5.</p> <code>read_fn</code> <code>Callable</code> <p>User-defined function for reading images. It takes an element from <code>reference_images</code> and returns a numpy array of image pixels. By default, it is expected to take a path to an image and return a numpy array.</p> <p>Targets</p> <p>image</p> <p>Image types:     uint8, float32</p> <p>Reference</p> <ul> <li>https://github.com/YanchaoYang/FDA</li> <li>https://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_FDA_Fourier_Domain_Adaptation_for_Semantic_Segmentation_CVPR_2020_paper.pdf</li> </ul> <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import albumentations as A\n&gt;&gt;&gt; image = np.random.randint(0, 256, [100, 100, 3], dtype=np.uint8)\n&gt;&gt;&gt; target_image = np.random.randint(0, 256, [100, 100, 3], dtype=np.uint8)\n&gt;&gt;&gt; aug = A.Compose([A.FDA([target_image], p=1, read_fn=lambda x: x)])\n&gt;&gt;&gt; result = aug(image=image)\n</code></pre> <p>Note</p> <p>FDA is a powerful tool for domain adaptation, particularly in unsupervised settings where annotated target domain samples are unavailable. It enables significant improvements in model generalization by aligning the low-level statistics of source and target images through a simple yet effective Fourier-based method.</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/domain_adaptation.py</code> Python<pre><code>class FDA(ImageOnlyTransform):\n    \"\"\"Fourier Domain Adaptation (FDA) for simple \"style transfer\" in the context of unsupervised domain adaptation\n    (UDA). FDA manipulates the frequency components of images to reduce the domain gap between source\n    and target datasets, effectively adapting images from one domain to closely resemble those from another without\n    altering their semantic content.\n\n    This transform is particularly beneficial in scenarios where the training (source) and testing (target) images\n    come from different distributions, such as synthetic versus real images, or day versus night scenes.\n    Unlike traditional domain adaptation methods that may require complex adversarial training, FDA achieves domain\n    alignment by swapping low-frequency components of the Fourier transform between the source and target images.\n    This technique has shown to improve the performance of models on the target domain, particularly for tasks\n    like semantic segmentation, without additional training for domain invariance.\n\n    The 'beta_limit' parameter controls the extent of frequency component swapping, with lower values preserving more\n    of the original image's characteristics and higher values leading to more pronounced adaptation effects.\n    It is recommended to use beta values less than 0.3 to avoid introducing artifacts.\n\n    Args:\n        reference_images (Sequence[Any]): Sequence of objects to be converted into images by `read_fn`. This typically\n            involves paths to images that serve as target domain examples for adaptation.\n        beta_limit (float or tuple of float): Coefficient beta from the paper, controlling the swapping extent of\n            frequency components. Values should be less than 0.5.\n        read_fn (Callable): User-defined function for reading images. It takes an element from `reference_images` and\n            returns a numpy array of image pixels. By default, it is expected to take a path to an image and return a\n            numpy array.\n\n    Targets:\n        image\n\n    Image types:\n        uint8, float32\n\n    Reference:\n        - https://github.com/YanchaoYang/FDA\n        - https://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_FDA_Fourier_Domain_Adaptation_for_Semantic_Segmentation_CVPR_2020_paper.pdf\n\n    Example:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; import albumentations as A\n        &gt;&gt;&gt; image = np.random.randint(0, 256, [100, 100, 3], dtype=np.uint8)\n        &gt;&gt;&gt; target_image = np.random.randint(0, 256, [100, 100, 3], dtype=np.uint8)\n        &gt;&gt;&gt; aug = A.Compose([A.FDA([target_image], p=1, read_fn=lambda x: x)])\n        &gt;&gt;&gt; result = aug(image=image)\n\n    Note:\n        FDA is a powerful tool for domain adaptation, particularly in unsupervised settings where annotated target\n        domain samples are unavailable. It enables significant improvements in model generalization by aligning\n        the low-level statistics of source and target images through a simple yet effective Fourier-based method.\n    \"\"\"\n\n    class InitSchema(BaseTransformInitSchema):\n        reference_images: Sequence[Any]\n        read_fn: Callable[[Any], np.ndarray]\n        beta_limit: NonNegativeFloatRangeType = (0, 0.1)\n\n        @field_validator(\"beta_limit\")\n        @classmethod\n        def check_ranges(cls, value: tuple[float, float]) -&gt; tuple[float, float]:\n            bounds = 0, MAX_BETA_LIMIT\n            if not bounds[0] &lt;= value[0] &lt;= value[1] &lt;= bounds[1]:\n                raise ValueError(f\"Values should be in the range {bounds} got {value} \")\n            return value\n\n    def __init__(\n        self,\n        reference_images: Sequence[Any],\n        beta_limit: ScaleFloatType = (0, 0.1),\n        read_fn: Callable[[Any], np.ndarray] = read_rgb_image,\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p=p, always_apply=always_apply)\n        self.reference_images = reference_images\n        self.read_fn = read_fn\n        self.beta_limit = cast(Tuple[float, float], beta_limit)\n\n    def apply(\n        self,\n        img: np.ndarray,\n        target_image: np.ndarray,\n        beta: float,\n        **params: Any,\n    ) -&gt; np.ndarray:\n        return fourier_domain_adaptation(img, target_image, beta)\n\n    def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, np.ndarray]:\n        target_img = self.read_fn(random.choice(self.reference_images))\n        target_img = cv2.resize(target_img, dsize=(params[\"cols\"], params[\"rows\"]))\n\n        return {\"target_image\": target_img}\n\n    def get_params(self) -&gt; dict[str, float]:\n        return {\"beta\": random.uniform(self.beta_limit[0], self.beta_limit[1])}\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, str, str]:\n        return \"reference_images\", \"beta_limit\", \"read_fn\"\n\n    def to_dict_private(self) -&gt; dict[str, Any]:\n        msg = \"FDA can not be serialized.\"\n        raise NotImplementedError(msg)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.domain_adaptation.FDA.apply","title":"<code>apply (self, img, target_image, beta, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/domain_adaptation.py</code> Python<pre><code>def apply(\n    self,\n    img: np.ndarray,\n    target_image: np.ndarray,\n    beta: float,\n    **params: Any,\n) -&gt; np.ndarray:\n    return fourier_domain_adaptation(img, target_image, beta)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.domain_adaptation.FDA.get_params","title":"<code>get_params (self)</code>","text":"<p>Returns parameters independent of input.</p> Source code in <code>albumentations/augmentations/domain_adaptation.py</code> Python<pre><code>def get_params(self) -&gt; dict[str, float]:\n    return {\"beta\": random.uniform(self.beta_limit[0], self.beta_limit[1])}\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.domain_adaptation.FDA.get_params_dependent_on_data","title":"<code>get_params_dependent_on_data (self, params, data)</code>","text":"<p>Returns parameters dependent on input.</p> Source code in <code>albumentations/augmentations/domain_adaptation.py</code> Python<pre><code>def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, np.ndarray]:\n    target_img = self.read_fn(random.choice(self.reference_images))\n    target_img = cv2.resize(target_img, dsize=(params[\"cols\"], params[\"rows\"]))\n\n    return {\"target_image\": target_img}\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.domain_adaptation.FDA.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/domain_adaptation.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, str, str]:\n    return \"reference_images\", \"beta_limit\", \"read_fn\"\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.domain_adaptation.HistogramMatching","title":"<code>class  HistogramMatching</code> <code>     (reference_images, blend_ratio=(0.5, 1.0), read_fn=&lt;function read_rgb_image at 0x7f4afc5b3740&gt;, always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Implements histogram matching, a technique that adjusts the pixel values of an input image to match the histogram of a reference image. This adjustment ensures that the output image has a similar tone and contrast to the reference. The process is applied independently to each channel of multi-channel images, provided both the input and reference images have the same number of channels.</p> <p>Histogram matching serves as an effective normalization method in image processing tasks such as feature matching. It is particularly useful when images originate from varied sources or are captured under different lighting conditions, helping to standardize the images' appearance before further processing.</p> <p>Parameters:</p> Name Type Description <code>reference_images</code> <code>Sequence[Any]</code> <p>A sequence of objects to be converted into images by <code>read_fn</code>. Typically, this is a sequence of image paths.</p> <code>blend_ratio</code> <code>tuple[float, float]</code> <p>Specifies the minimum and maximum blend ratio for blending the matched image with the original image. A random blend factor within this range is chosen for each image to increase the diversity of the output images.</p> <code>read_fn</code> <code>Callable[[Any], np.ndarray]</code> <p>A user-defined function for reading images, which accepts an element from <code>reference_images</code> and returns a numpy array of image pixels. By default, this is expected to take a file path and return an image as a numpy array.</p> <code>p</code> <code>float</code> <p>The probability of applying the transform to any given image. Defaults to 0.5.</p> <p>Targets</p> <p>image</p> <p>Image types:     uint8, float32</p> <p>Note</p> <p>This class cannot be serialized directly due to its dynamic nature and dependency on external image data. An attempt to serialize it will raise a NotImplementedError.</p> <p>Reference</p> <p>https://scikit-image.org/docs/dev/auto_examples/color_exposure/plot_histogram_matching.html</p> <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import albumentations as A\n&gt;&gt;&gt; image = np.random.randint(0, 256, [100, 100, 3], dtype=np.uint8)\n&gt;&gt;&gt; target_image = np.random.randint(0, 256, [100, 100, 3], dtype=np.uint8)\n&gt;&gt;&gt; aug = A.Compose([A.HistogramMatching([target_image], p=1, read_fn=lambda x: x)])\n&gt;&gt;&gt; result = aug(image=image)\n</code></pre> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/domain_adaptation.py</code> Python<pre><code>class HistogramMatching(ImageOnlyTransform):\n    \"\"\"Implements histogram matching, a technique that adjusts the pixel values of an input image\n    to match the histogram of a reference image. This adjustment ensures that the output image\n    has a similar tone and contrast to the reference. The process is applied independently to\n    each channel of multi-channel images, provided both the input and reference images have the\n    same number of channels.\n\n    Histogram matching serves as an effective normalization method in image processing tasks such\n    as feature matching. It is particularly useful when images originate from varied sources or are\n    captured under different lighting conditions, helping to standardize the images' appearance\n    before further processing.\n\n    Args:\n        reference_images (Sequence[Any]): A sequence of objects to be converted into images by `read_fn`.\n            Typically, this is a sequence of image paths.\n        blend_ratio (tuple[float, float]): Specifies the minimum and maximum blend ratio for blending the matched\n            image with the original image. A random blend factor within this range is chosen for each image to\n            increase the diversity of the output images.\n        read_fn (Callable[[Any], np.ndarray]): A user-defined function for reading images, which accepts an\n            element from `reference_images` and returns a numpy array of image pixels. By default, this is expected\n            to take a file path and return an image as a numpy array.\n        p (float): The probability of applying the transform to any given image. Defaults to 0.5.\n\n    Targets:\n        image\n\n    Image types:\n        uint8, float32\n\n    Note:\n        This class cannot be serialized directly due to its dynamic nature and dependency on external image data.\n        An attempt to serialize it will raise a NotImplementedError.\n\n    Reference:\n        https://scikit-image.org/docs/dev/auto_examples/color_exposure/plot_histogram_matching.html\n\n    Example:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; import albumentations as A\n        &gt;&gt;&gt; image = np.random.randint(0, 256, [100, 100, 3], dtype=np.uint8)\n        &gt;&gt;&gt; target_image = np.random.randint(0, 256, [100, 100, 3], dtype=np.uint8)\n        &gt;&gt;&gt; aug = A.Compose([A.HistogramMatching([target_image], p=1, read_fn=lambda x: x)])\n        &gt;&gt;&gt; result = aug(image=image)\n    \"\"\"\n\n    class InitSchema(BaseTransformInitSchema):\n        reference_images: Sequence[Any]\n        blend_ratio: Annotated[tuple[float, float], AfterValidator(nondecreasing), AfterValidator(check_01)] = (\n            0.5,\n            1.0,\n        )\n        read_fn: Callable[[Any], np.ndarray]\n\n    def __init__(\n        self,\n        reference_images: Sequence[Any],\n        blend_ratio: tuple[float, float] = (0.5, 1.0),\n        read_fn: Callable[[Any], np.ndarray] = read_rgb_image,\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p=p, always_apply=always_apply)\n        self.reference_images = reference_images\n        self.read_fn = read_fn\n        self.blend_ratio = blend_ratio\n\n    def apply(\n        self: np.ndarray,\n        img: np.ndarray,\n        reference_image: np.ndarray,\n        blend_ratio: float,\n        **params: Any,\n    ) -&gt; np.ndarray:\n        return apply_histogram(img, reference_image, blend_ratio)\n\n    def get_params(self) -&gt; dict[str, np.ndarray]:\n        return {\n            \"reference_image\": self.read_fn(random.choice(self.reference_images)),\n            \"blend_ratio\": random.uniform(self.blend_ratio[0], self.blend_ratio[1]),\n        }\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return \"reference_images\", \"blend_ratio\", \"read_fn\"\n\n    def to_dict_private(self) -&gt; dict[str, Any]:\n        msg = \"HistogramMatching can not be serialized.\"\n        raise NotImplementedError(msg)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.domain_adaptation.HistogramMatching.apply","title":"<code>apply (self, img, reference_image, blend_ratio, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/domain_adaptation.py</code> Python<pre><code>def apply(\n    self: np.ndarray,\n    img: np.ndarray,\n    reference_image: np.ndarray,\n    blend_ratio: float,\n    **params: Any,\n) -&gt; np.ndarray:\n    return apply_histogram(img, reference_image, blend_ratio)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.domain_adaptation.HistogramMatching.get_params","title":"<code>get_params (self)</code>","text":"<p>Returns parameters independent of input.</p> Source code in <code>albumentations/augmentations/domain_adaptation.py</code> Python<pre><code>def get_params(self) -&gt; dict[str, np.ndarray]:\n    return {\n        \"reference_image\": self.read_fn(random.choice(self.reference_images)),\n        \"blend_ratio\": random.uniform(self.blend_ratio[0], self.blend_ratio[1]),\n    }\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.domain_adaptation.HistogramMatching.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/domain_adaptation.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return \"reference_images\", \"blend_ratio\", \"read_fn\"\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.domain_adaptation.PixelDistributionAdaptation","title":"<code>class  PixelDistributionAdaptation</code> <code>     (reference_images, blend_ratio=(0.25, 1.0), read_fn=&lt;function read_rgb_image at 0x7f4afc5b3740&gt;, transform_type='pca', always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Performs pixel-level domain adaptation by aligning the pixel value distribution of an input image with that of a reference image. This process involves fitting a simple statistical transformation (such as PCA, StandardScaler, or MinMaxScaler) to both the original and the reference images, transforming the original image with the transformation trained on it, and then applying the inverse transformation using the transform fitted on the reference image. The result is an adapted image that retains the original content while mimicking the pixel value distribution of the reference domain.</p> <p>The process can be visualized as two main steps: 1. Adjusting the original image to a standard distribution space using a selected transform. 2. Moving the adjusted image into the distribution space of the reference image by applying the inverse    of the transform fitted on the reference image.</p> <p>This technique is especially useful in scenarios where images from different domains (e.g., synthetic vs. real images, day vs. night scenes) need to be harmonized for better consistency or performance in image processing tasks.</p> <p>Parameters:</p> Name Type Description <code>reference_images</code> <code>Sequence[Any]</code> <p>A sequence of objects (typically image paths) that will be converted into images by <code>read_fn</code>. These images serve as references for the domain adaptation.</p> <code>blend_ratio</code> <code>tuple[float, float]</code> <p>Specifies the minimum and maximum blend ratio for mixing the adapted image with the original, enhancing the diversity of the output images.</p> <code>read_fn</code> <code>Callable</code> <p>A user-defined function for reading and converting the objects in <code>reference_images</code> into numpy arrays. By default, it assumes these objects are image paths.</p> <code>transform_type</code> <code>str</code> <p>Specifies the type of statistical transformation to apply. Supported values are \"pca\" for Principal Component Analysis, \"standard\" for StandardScaler, and \"minmax\" for MinMaxScaler.</p> <code>p</code> <code>float</code> <p>The probability of applying the transform to any given image. Default is 1.0.</p> <p>Targets</p> <p>image</p> <p>Image types:     uint8, float32</p> <p>Reference</p> <p>For more information on the underlying approach, see: https://github.com/arsenyinfo/qudida</p> <p>Note</p> <p>The PixelDistributionAdaptation transform is a novel way to perform domain adaptation at the pixel level, suitable for adjusting images across different conditions without complex modeling. It is effective for preparing images before more advanced processing or analysis.</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/domain_adaptation.py</code> Python<pre><code>class PixelDistributionAdaptation(ImageOnlyTransform):\n    \"\"\"Performs pixel-level domain adaptation by aligning the pixel value distribution of an input image\n    with that of a reference image. This process involves fitting a simple statistical transformation\n    (such as PCA, StandardScaler, or MinMaxScaler) to both the original and the reference images,\n    transforming the original image with the transformation trained on it, and then applying the inverse\n    transformation using the transform fitted on the reference image. The result is an adapted image\n    that retains the original content while mimicking the pixel value distribution of the reference domain.\n\n    The process can be visualized as two main steps:\n    1. Adjusting the original image to a standard distribution space using a selected transform.\n    2. Moving the adjusted image into the distribution space of the reference image by applying the inverse\n       of the transform fitted on the reference image.\n\n    This technique is especially useful in scenarios where images from different domains (e.g., synthetic\n    vs. real images, day vs. night scenes) need to be harmonized for better consistency or performance in\n    image processing tasks.\n\n    Args:\n        reference_images (Sequence[Any]): A sequence of objects (typically image paths) that will be\n            converted into images by `read_fn`. These images serve as references for the domain adaptation.\n        blend_ratio (tuple[float, float]): Specifies the minimum and maximum blend ratio for mixing\n            the adapted image with the original, enhancing the diversity of the output images.\n        read_fn (Callable): A user-defined function for reading and converting the objects in\n            `reference_images` into numpy arrays. By default, it assumes these objects are image paths.\n        transform_type (str): Specifies the type of statistical transformation to apply. Supported values\n            are \"pca\" for Principal Component Analysis, \"standard\" for StandardScaler, and \"minmax\" for\n            MinMaxScaler.\n        p (float): The probability of applying the transform to any given image. Default is 1.0.\n\n    Targets:\n        image\n\n    Image types:\n        uint8, float32\n\n    Reference:\n        For more information on the underlying approach, see: https://github.com/arsenyinfo/qudida\n\n    Note:\n        The PixelDistributionAdaptation transform is a novel way to perform domain adaptation at the pixel level,\n        suitable for adjusting images across different conditions without complex modeling. It is effective\n        for preparing images before more advanced processing or analysis.\n    \"\"\"\n\n    class InitSchema(BaseTransformInitSchema):\n        reference_images: Sequence[Any]\n        blend_ratio: Annotated[tuple[float, float], AfterValidator(nondecreasing), AfterValidator(check_01)] = (\n            0.25,\n            1.0,\n        )\n        read_fn: Callable[[Any], np.ndarray]\n        transform_type: Literal[\"pca\", \"standard\", \"minmax\"]\n\n    def __init__(\n        self,\n        reference_images: Sequence[Any],\n        blend_ratio: tuple[float, float] = (0.25, 1.0),\n        read_fn: Callable[[Any], np.ndarray] = read_rgb_image,\n        transform_type: Literal[\"pca\", \"standard\", \"minmax\"] = \"pca\",\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p=p, always_apply=always_apply)\n        self.reference_images = reference_images\n        self.read_fn = read_fn\n        self.blend_ratio = blend_ratio\n        self.transform_type = transform_type\n\n    @staticmethod\n    def _validate_shape(img: np.ndarray) -&gt; None:\n        if is_grayscale_image(img) or is_multispectral_image(img):\n            raise ValueError(\n                f\"Unexpected image shape: expected 3 dimensions, got {len(img.shape)}.\"\n                f\"Is it a grayscale or multispectral image? It's not supported for now.\",\n            )\n\n    def ensure_uint8(self, img: np.ndarray) -&gt; tuple[np.ndarray, bool]:\n        if img.dtype == np.float32:\n            if img.min() &lt; 0 or img.max() &gt; 1:\n                message = (\n                    \"PixelDistributionAdaptation uses uint8 under the hood, so float32 should be converted,\"\n                    \"Can not do it automatically when the image is out of [0..1] range.\"\n                )\n                raise TypeError(message)\n            return clip(img * 255, np.uint8), True\n        return img, False\n\n    def apply(self, img: np.ndarray, reference_image: np.ndarray, blend_ratio: float, **params: Any) -&gt; np.ndarray:\n        self._validate_shape(img)\n        reference_image, _ = self.ensure_uint8(reference_image)\n        img, needs_reconvert = self.ensure_uint8(img)\n\n        adapted = adapt_pixel_distribution(\n            img,\n            ref=reference_image,\n            weight=blend_ratio,\n            transform_type=self.transform_type,\n        )\n\n        return fmain.to_float(adapted) if needs_reconvert else adapted\n\n    def get_params(self) -&gt; dict[str, Any]:\n        return {\n            \"reference_image\": self.read_fn(random.choice(self.reference_images)),\n            \"blend_ratio\": random.uniform(self.blend_ratio[0], self.blend_ratio[1]),\n        }\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, str, str, str]:\n        return \"reference_images\", \"blend_ratio\", \"read_fn\", \"transform_type\"\n\n    def to_dict_private(self) -&gt; dict[str, Any]:\n        msg = \"PixelDistributionAdaptation can not be serialized.\"\n        raise NotImplementedError(msg)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.domain_adaptation.PixelDistributionAdaptation.apply","title":"<code>apply (self, img, reference_image, blend_ratio, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/domain_adaptation.py</code> Python<pre><code>def apply(self, img: np.ndarray, reference_image: np.ndarray, blend_ratio: float, **params: Any) -&gt; np.ndarray:\n    self._validate_shape(img)\n    reference_image, _ = self.ensure_uint8(reference_image)\n    img, needs_reconvert = self.ensure_uint8(img)\n\n    adapted = adapt_pixel_distribution(\n        img,\n        ref=reference_image,\n        weight=blend_ratio,\n        transform_type=self.transform_type,\n    )\n\n    return fmain.to_float(adapted) if needs_reconvert else adapted\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.domain_adaptation.PixelDistributionAdaptation.get_params","title":"<code>get_params (self)</code>","text":"<p>Returns parameters independent of input.</p> Source code in <code>albumentations/augmentations/domain_adaptation.py</code> Python<pre><code>def get_params(self) -&gt; dict[str, Any]:\n    return {\n        \"reference_image\": self.read_fn(random.choice(self.reference_images)),\n        \"blend_ratio\": random.uniform(self.blend_ratio[0], self.blend_ratio[1]),\n    }\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.domain_adaptation.PixelDistributionAdaptation.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/domain_adaptation.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, str, str, str]:\n    return \"reference_images\", \"blend_ratio\", \"read_fn\", \"transform_type\"\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.domain_adaptation_functional","title":"<code>domain_adaptation_functional</code>","text":""},{"location":"api_reference/full_reference/#albumentations.augmentations.domain_adaptation_functional.DomainAdapter","title":"<code>class  DomainAdapter</code> <code>     (transformer, ref_img, color_conversions=(None, None))                 </code>  [view source on GitHub]","text":"<p>Source: https://github.com/arsenyinfo/qudida by Arseny Kravchenko</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/domain_adaptation_functional.py</code> Python<pre><code>class DomainAdapter:\n    \"\"\"Source: https://github.com/arsenyinfo/qudida by Arseny Kravchenko\"\"\"\n\n    def __init__(\n        self,\n        transformer: TransformerInterface,\n        ref_img: np.ndarray,\n        color_conversions: tuple[None, None] = (None, None),\n    ):\n        self.color_in, self.color_out = color_conversions\n        self.source_transformer = deepcopy(transformer)\n        self.target_transformer = transformer\n        self.target_transformer.fit(self.flatten(ref_img))\n\n    def to_colorspace(self, img: np.ndarray) -&gt; np.ndarray:\n        return img if self.color_in is None else cv2.cvtColor(img, self.color_in)\n\n    def from_colorspace(self, img: np.ndarray) -&gt; np.ndarray:\n        if self.color_out is None:\n            return img\n        return cv2.cvtColor(clip(img, np.uint8), self.color_out)\n\n    def flatten(self, img: np.ndarray) -&gt; np.ndarray:\n        img = self.to_colorspace(img)\n        img = fmain.to_float(img)\n        return img.reshape(-1, 3)\n\n    def reconstruct(self, pixels: np.ndarray, height: int, width: int) -&gt; np.ndarray:\n        pixels = (np.clip(pixels, 0, 1) * 255).astype(\"uint8\")\n        return self.from_colorspace(pixels.reshape(height, width, 3))\n\n    @staticmethod\n    def _pca_sign(x: np.ndarray) -&gt; np.ndarray:\n        return np.sign(np.trace(x.components_))\n\n    def __call__(self, image: np.ndarray) -&gt; np.ndarray:\n        height, width = image.shape[:2]\n        pixels = self.flatten(image)\n        self.source_transformer.fit(pixels)\n\n        # dirty hack to make sure colors are not inverted\n        if (\n            hasattr(self.target_transformer, \"components_\")\n            and hasattr(self.source_transformer, \"components_\")\n            and self._pca_sign(self.target_transformer) != self._pca_sign(self.source_transformer)\n        ):\n            self.target_transformer.components_ *= -1\n\n        representation = self.source_transformer.transform(pixels)\n        result = self.target_transformer.inverse_transform(representation)\n        return self.reconstruct(result, height, width)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.domain_adaptation_functional.apply_histogram","title":"<code>def apply_histogram    (img, reference_image, blend_ratio)    </code> [view source on GitHub]","text":"<p>Apply histogram matching to an input image using a reference image and blend the result.</p> <p>This function performs histogram matching between the input image and a reference image, then blends the result with the original input image based on the specified blend ratio.</p> <p>Parameters:</p> Name Type Description <code>img</code> <code>np.ndarray</code> <p>The input image to be transformed. Can be either grayscale or RGB. Supported dtypes: uint8, float32 (values should be in [0, 1] range).</p> <code>reference_image</code> <code>np.ndarray</code> <p>The reference image used for histogram matching. Should have the same number of channels as the input image. Supported dtypes: uint8, float32 (values should be in [0, 1] range).</p> <code>blend_ratio</code> <code>float</code> <p>The ratio for blending the matched image with the original image. Should be in the range [0, 1], where 0 means no change and 1 means full histogram matching.</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>The transformed image after histogram matching and blending.     The output will have the same shape and dtype as the input image.</p> <p>Supported image types:     - Grayscale images: 2D arrays     - RGB images: 3D arrays with 3 channels     - Multispectral images: 3D arrays with more than 3 channels</p> <p>Note</p> <ul> <li>If the input and reference images have different sizes, the reference image   will be resized to match the input image's dimensions.</li> <li>The function uses <code>match_histograms</code> from scikit-image for the core histogram matching.</li> <li>The @clipped and @preserve_channel_dim decorators ensure the output is within   the valid range and maintains the original number of dimensions.</li> </ul> <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from albumentations.augmentations.domain_adaptation_functional import apply_histogram\n&gt;&gt;&gt; input_image = np.random.randint(0, 256, (100, 100, 3), dtype=np.uint8)\n&gt;&gt;&gt; reference_image = np.random.randint(0, 256, (100, 100, 3), dtype=np.uint8)\n&gt;&gt;&gt; result = apply_histogram(input_image, reference_image, blend_ratio=0.7)\n</code></pre> Source code in <code>albumentations/augmentations/domain_adaptation_functional.py</code> Python<pre><code>@clipped\n@preserve_channel_dim\ndef apply_histogram(img: np.ndarray, reference_image: np.ndarray, blend_ratio: float) -&gt; np.ndarray:\n    \"\"\"Apply histogram matching to an input image using a reference image and blend the result.\n\n    This function performs histogram matching between the input image and a reference image,\n    then blends the result with the original input image based on the specified blend ratio.\n\n    Args:\n        img (np.ndarray): The input image to be transformed. Can be either grayscale or RGB.\n            Supported dtypes: uint8, float32 (values should be in [0, 1] range).\n        reference_image (np.ndarray): The reference image used for histogram matching.\n            Should have the same number of channels as the input image.\n            Supported dtypes: uint8, float32 (values should be in [0, 1] range).\n        blend_ratio (float): The ratio for blending the matched image with the original image.\n            Should be in the range [0, 1], where 0 means no change and 1 means full histogram matching.\n\n    Returns:\n        np.ndarray: The transformed image after histogram matching and blending.\n            The output will have the same shape and dtype as the input image.\n\n    Supported image types:\n        - Grayscale images: 2D arrays\n        - RGB images: 3D arrays with 3 channels\n        - Multispectral images: 3D arrays with more than 3 channels\n\n    Note:\n        - If the input and reference images have different sizes, the reference image\n          will be resized to match the input image's dimensions.\n        - The function uses `match_histograms` from scikit-image for the core histogram matching.\n        - The @clipped and @preserve_channel_dim decorators ensure the output is within\n          the valid range and maintains the original number of dimensions.\n\n    Example:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from albumentations.augmentations.domain_adaptation_functional import apply_histogram\n        &gt;&gt;&gt; input_image = np.random.randint(0, 256, (100, 100, 3), dtype=np.uint8)\n        &gt;&gt;&gt; reference_image = np.random.randint(0, 256, (100, 100, 3), dtype=np.uint8)\n        &gt;&gt;&gt; result = apply_histogram(input_image, reference_image, blend_ratio=0.7)\n    \"\"\"\n    # Resize reference image only if necessary\n    if img.shape[:2] != reference_image.shape[:2]:\n        reference_image = cv2.resize(reference_image, dsize=(img.shape[1], img.shape[0]))\n\n    img = np.squeeze(img)\n    reference_image = np.squeeze(reference_image)\n\n    # Match histograms between the images\n    matched = match_histograms(\n        img,\n        reference_image,\n        channel_axis=2 if img.ndim == NUM_MULTI_CHANNEL_DIMENSIONS and img.shape[2] &gt; 1 else None,\n    )\n\n    # Blend the original image and the matched image\n    return add_weighted(matched, blend_ratio, img, 1 - blend_ratio)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.dropout","title":"<code>dropout</code>  <code>special</code>","text":""},{"location":"api_reference/full_reference/#albumentations.augmentations.dropout.channel_dropout","title":"<code>channel_dropout</code>","text":""},{"location":"api_reference/full_reference/#albumentations.augmentations.dropout.channel_dropout.ChannelDropout","title":"<code>class  ChannelDropout</code> <code>     (channel_drop_range=(1, 1), fill_value=0, always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Randomly Drop Channels in the input Image.</p> <p>Parameters:</p> Name Type Description <code>channel_drop_range</code> <code>int, int</code> <p>range from which we choose the number of channels to drop.</p> <code>fill_value</code> <code>int, float</code> <p>pixel value for the dropped channel.</p> <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 0.5.</p> <p>Targets</p> <p>image</p> <p>Image types:     uint8, uint16, unit32, float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/dropout/channel_dropout.py</code> Python<pre><code>class ChannelDropout(ImageOnlyTransform):\n    \"\"\"Randomly Drop Channels in the input Image.\n\n    Args:\n        channel_drop_range (int, int): range from which we choose the number of channels to drop.\n        fill_value (int, float): pixel value for the dropped channel.\n        p (float): probability of applying the transform. Default: 0.5.\n\n    Targets:\n        image\n\n    Image types:\n        uint8, uint16, unit32, float32\n\n    \"\"\"\n\n    class InitSchema(BaseTransformInitSchema):\n        channel_drop_range: OnePlusIntRangeType = (1, 1)\n        fill_value: Annotated[ColorType, Field(description=\"Pixel value for the dropped channel.\")]\n\n    def __init__(\n        self,\n        channel_drop_range: tuple[int, int] = (1, 1),\n        fill_value: float = 0,\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p=p, always_apply=always_apply)\n\n        self.channel_drop_range = channel_drop_range\n        self.fill_value = fill_value\n\n    def apply(self, img: np.ndarray, channels_to_drop: tuple[int, ...], **params: Any) -&gt; np.ndarray:\n        return channel_dropout(img, channels_to_drop, self.fill_value)\n\n    def get_params_dependent_on_data(self, params: Mapping[str, Any], data: Mapping[str, Any]) -&gt; dict[str, Any]:\n        image = data[\"image\"] if \"image\" in data else data[\"images\"][0]\n        num_channels = get_num_channels(image)\n\n        if num_channels == 1:\n            msg = \"Images has one channel. ChannelDropout is not defined.\"\n            raise NotImplementedError(msg)\n\n        if self.channel_drop_range[1] &gt;= num_channels:\n            msg = \"Can not drop all channels in ChannelDropout.\"\n            raise ValueError(msg)\n\n        num_drop_channels = random.randint(self.channel_drop_range[0], self.channel_drop_range[1])\n\n        channels_to_drop = random.sample(range(num_channels), k=num_drop_channels)\n\n        return {\"channels_to_drop\": channels_to_drop}\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return \"channel_drop_range\", \"fill_value\"\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.dropout.channel_dropout.ChannelDropout.apply","title":"<code>apply (self, img, channels_to_drop, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/dropout/channel_dropout.py</code> Python<pre><code>def apply(self, img: np.ndarray, channels_to_drop: tuple[int, ...], **params: Any) -&gt; np.ndarray:\n    return channel_dropout(img, channels_to_drop, self.fill_value)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.dropout.channel_dropout.ChannelDropout.get_params_dependent_on_data","title":"<code>get_params_dependent_on_data (self, params, data)</code>","text":"<p>Returns parameters dependent on input.</p> Source code in <code>albumentations/augmentations/dropout/channel_dropout.py</code> Python<pre><code>def get_params_dependent_on_data(self, params: Mapping[str, Any], data: Mapping[str, Any]) -&gt; dict[str, Any]:\n    image = data[\"image\"] if \"image\" in data else data[\"images\"][0]\n    num_channels = get_num_channels(image)\n\n    if num_channels == 1:\n        msg = \"Images has one channel. ChannelDropout is not defined.\"\n        raise NotImplementedError(msg)\n\n    if self.channel_drop_range[1] &gt;= num_channels:\n        msg = \"Can not drop all channels in ChannelDropout.\"\n        raise ValueError(msg)\n\n    num_drop_channels = random.randint(self.channel_drop_range[0], self.channel_drop_range[1])\n\n    channels_to_drop = random.sample(range(num_channels), k=num_drop_channels)\n\n    return {\"channels_to_drop\": channels_to_drop}\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.dropout.channel_dropout.ChannelDropout.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/dropout/channel_dropout.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return \"channel_drop_range\", \"fill_value\"\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.dropout.coarse_dropout","title":"<code>coarse_dropout</code>","text":""},{"location":"api_reference/full_reference/#albumentations.augmentations.dropout.coarse_dropout.CoarseDropout","title":"<code>class  CoarseDropout</code> <code>     (max_holes=None, max_height=None, max_width=None, min_holes=None, min_height=None, min_width=None, fill_value=0, mask_fill_value=None, num_holes_range=(1, 1), hole_height_range=(8, 8), hole_width_range=(8, 8), always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>CoarseDropout randomly drops out rectangular regions from the image and optionally, the corresponding regions in an associated mask, to simulate the occlusion and varied object sizes found in real-world settings. This transformation is an evolution of CutOut and RandomErasing, offering more flexibility in the size, number of dropout regions, and fill values.</p> <p>Parameters:</p> Name Type Description <code>num_holes_range</code> <code>tuple[int, int]</code> <p>Specifies the range (minimum and maximum) of the number of rectangular regions to zero out. This allows for dynamic variation in the number of regions removed per transformation instance.</p> <code>hole_height_range</code> <code>tuple[ScalarType, ScalarType]</code> <p>Defines the minimum and maximum heights of the dropout regions, providing variability in their vertical dimensions.</p> <code>hole_width_range</code> <code>tuple[ScalarType, ScalarType]</code> <p>Defines the minimum and maximum widths of the dropout regions, providing variability in their horizontal dimensions.</p> <code>fill_value</code> <code>ColorType, Literal[\"random\"]</code> <p>Specifies the value used to fill the dropout regions. This can be a constant value, a tuple specifying pixel intensity across channels, or 'random' which fills the region with random noise.</p> <code>mask_fill_value</code> <code>ColorType | None</code> <p>Specifies the fill value for dropout regions in the mask. If set to <code>None</code>, the mask regions corresponding to the image dropout regions are left unchanged.</p> <p>Targets</p> <p>image, mask, keypoints</p> <p>Image types:     uint8, float32</p> <p>Reference</p> <p>https://arxiv.org/abs/1708.04552 https://github.com/uoguelph-mlrg/Cutout/blob/master/util/cutout.py https://github.com/aleju/imgaug/blob/master/imgaug/augmenters/arithmetic.py</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/dropout/coarse_dropout.py</code> Python<pre><code>class CoarseDropout(DualTransform):\n    \"\"\"CoarseDropout randomly drops out rectangular regions from the image and optionally,\n    the corresponding regions in an associated mask, to simulate the occlusion and\n    varied object sizes found in real-world settings. This transformation is an\n    evolution of CutOut and RandomErasing, offering more flexibility in the size,\n    number of dropout regions, and fill values.\n\n    Args:\n        num_holes_range (tuple[int, int]): Specifies the range (minimum and maximum)\n            of the number of rectangular regions to zero out. This allows for dynamic\n            variation in the number of regions removed per transformation instance.\n        hole_height_range (tuple[ScalarType, ScalarType]): Defines the minimum and\n            maximum heights of the dropout regions, providing variability in their vertical dimensions.\n        hole_width_range (tuple[ScalarType, ScalarType]): Defines the minimum and\n            maximum widths of the dropout regions, providing variability in their horizontal dimensions.\n        fill_value (ColorType, Literal[\"random\"]): Specifies the value used to fill the dropout regions.\n            This can be a constant value, a tuple specifying pixel intensity across channels, or 'random'\n            which fills the region with random noise.\n        mask_fill_value (ColorType | None): Specifies the fill value for dropout regions in the mask.\n            If set to `None`, the mask regions corresponding to the image dropout regions are left unchanged.\n\n\n    Targets:\n        image, mask, keypoints\n\n    Image types:\n        uint8, float32\n\n    Reference:\n        https://arxiv.org/abs/1708.04552\n        https://github.com/uoguelph-mlrg/Cutout/blob/master/util/cutout.py\n        https://github.com/aleju/imgaug/blob/master/imgaug/augmenters/arithmetic.py\n\n    \"\"\"\n\n    _targets = (Targets.IMAGE, Targets.MASK, Targets.KEYPOINTS)\n\n    class InitSchema(BaseTransformInitSchema):\n        min_holes: int | None = Field(\n            default=None,\n            ge=0,\n            description=\"Minimum number of regions to zero out.\",\n        )\n        max_holes: int | None = Field(\n            default=8,\n            ge=0,\n            description=\"Maximum number of regions to zero out.\",\n        )\n        num_holes_range: Annotated[tuple[int, int], AfterValidator(check_1plus), AfterValidator(nondecreasing)] = (1, 1)\n\n        min_height: ScalarType | None = Field(\n            default=None,\n            ge=0,\n            description=\"Minimum height of the hole.\",\n        )\n        max_height: ScalarType | None = Field(\n            default=8,\n            ge=0,\n            description=\"Maximum height of the hole.\",\n        )\n        hole_height_range: tuple[ScalarType, ScalarType] = (8, 8)\n\n        min_width: ScalarType | None = Field(\n            default=None,\n            ge=0,\n            description=\"Minimum width of the hole.\",\n        )\n        max_width: ScalarType | None = Field(\n            default=8,\n            ge=0,\n            description=\"Maximum width of the hole.\",\n        )\n        hole_width_range: tuple[ScalarType, ScalarType] = (8, 8)\n\n        fill_value: ColorType | Literal[\"random\"] = Field(default=0, description=\"Value for dropped pixels.\")\n        mask_fill_value: ColorType | None = Field(default=None, description=\"Fill value for dropped pixels in mask.\")\n\n        @staticmethod\n        def update_range(\n            min_value: NumericType | None,\n            max_value: NumericType | None,\n            default_range: tuple[NumericType, NumericType],\n        ) -&gt; tuple[NumericType, NumericType]:\n            if max_value is not None:\n                return (min_value or max_value, max_value)\n\n            return default_range\n\n        @staticmethod\n        # Validation for hole dimensions ranges\n        def validate_range(range_value: tuple[ScalarType, ScalarType], range_name: str, minimum: float = 0) -&gt; None:\n            if not minimum &lt;= range_value[0] &lt;= range_value[1]:\n                raise ValueError(\n                    f\"First value in {range_name} should be less or equal than the second value \"\n                    f\"and at least {minimum}. Got: {range_value}\",\n                )\n            if isinstance(range_value[0], float) and not all(0 &lt;= x &lt;= 1 for x in range_value):\n                raise ValueError(f\"All values in {range_name} should be in [0, 1] range. Got: {range_value}\")\n\n        @model_validator(mode=\"after\")\n        def check_num_holes_and_dimensions(self) -&gt; Self:\n            if self.min_holes is not None:\n                warn(\"`min_holes` is deprecated. Use num_holes_range instead.\", DeprecationWarning, stacklevel=2)\n\n            if self.max_holes is not None:\n                warn(\"`max_holes` is deprecated. Use num_holes_range instead.\", DeprecationWarning, stacklevel=2)\n\n            if self.min_height is not None:\n                warn(\"`min_height` is deprecated. Use hole_height_range instead.\", DeprecationWarning, stacklevel=2)\n\n            if self.max_height is not None:\n                warn(\"`max_height` is deprecated. Use hole_height_range instead.\", DeprecationWarning, stacklevel=2)\n\n            if self.min_width is not None:\n                warn(\"`min_width` is deprecated. Use hole_width_range instead.\", DeprecationWarning, stacklevel=2)\n\n            if self.max_width is not None:\n                warn(\"`max_width` is deprecated. Use hole_width_range instead.\", DeprecationWarning, stacklevel=2)\n\n            if self.max_holes is not None:\n                # Update ranges for holes, heights, and widths\n                self.num_holes_range = self.update_range(self.min_holes, self.max_holes, self.num_holes_range)\n\n            self.validate_range(self.num_holes_range, \"num_holes_range\", minimum=1)\n\n            if self.max_height is not None:\n                self.hole_height_range = self.update_range(self.min_height, self.max_height, self.hole_height_range)\n            self.validate_range(self.hole_height_range, \"hole_height_range\")\n\n            if self.max_width is not None:\n                self.hole_width_range = self.update_range(self.min_width, self.max_width, self.hole_width_range)\n            self.validate_range(self.hole_width_range, \"hole_width_range\")\n\n            return self\n\n    def __init__(\n        self,\n        max_holes: int | None = None,\n        max_height: ScalarType | None = None,\n        max_width: ScalarType | None = None,\n        min_holes: int | None = None,\n        min_height: ScalarType | None = None,\n        min_width: ScalarType | None = None,\n        fill_value: ColorType | Literal[\"random\"] = 0,\n        mask_fill_value: ColorType | None = None,\n        num_holes_range: tuple[int, int] = (1, 1),\n        hole_height_range: tuple[ScalarType, ScalarType] = (8, 8),\n        hole_width_range: tuple[ScalarType, ScalarType] = (8, 8),\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p, always_apply)\n        self.num_holes_range = num_holes_range\n        self.hole_height_range = hole_height_range\n        self.hole_width_range = hole_width_range\n\n        self.fill_value = fill_value  # type: ignore[assignment]\n        self.mask_fill_value = mask_fill_value\n\n    def apply(\n        self,\n        img: np.ndarray,\n        fill_value: ColorType | Literal[\"random\"],\n        holes: Iterable[tuple[int, int, int, int]],\n        **params: Any,\n    ) -&gt; np.ndarray:\n        return cutout(img, holes, fill_value)\n\n    def apply_to_mask(\n        self,\n        mask: np.ndarray,\n        mask_fill_value: ScalarType,\n        holes: Iterable[tuple[int, int, int, int]],\n        **params: Any,\n    ) -&gt; np.ndarray:\n        if mask_fill_value is None:\n            return mask\n        return cutout(mask, holes, mask_fill_value)\n\n    @staticmethod\n    def calculate_hole_dimensions(\n        height: int,\n        width: int,\n        height_range: tuple[ScalarType, ScalarType],\n        width_range: tuple[ScalarType, ScalarType],\n        size: int,\n    ) -&gt; tuple[np.ndarray, np.ndarray]:\n        \"\"\"Calculate random hole dimensions based on the provided ranges.\"\"\"\n        if isinstance(height_range[0], int):\n            min_height = height_range[0]\n            max_height = min(height_range[1], height)\n\n            min_width = width_range[0]\n            max_width = min(width_range[1], width)\n\n            hole_heights = randint(np.int64(min_height), np.int64(max_height + 1), size=size)\n            hole_widths = randint(np.int64(min_width), np.int64(max_width + 1), size=size)\n\n        else:  # Assume float\n            hole_heights = (height * uniform(height_range[0], height_range[1], size=size)).astype(int)\n            hole_widths = (width * uniform(width_range[0], width_range[1], size=size)).astype(int)\n\n        return hole_heights, hole_widths\n\n    def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n        height, width = params[\"shape\"][:2]\n\n        num_holes = randint(self.num_holes_range[0], self.num_holes_range[1] + 1)\n\n        hole_heights, hole_widths = self.calculate_hole_dimensions(\n            height,\n            width,\n            self.hole_height_range,\n            self.hole_width_range,\n            size=num_holes,\n        )\n\n        y1 = randint(np.int8(0), height - hole_heights + 1, size=num_holes)\n        x1 = randint(np.int8(0), width - hole_widths + 1, size=num_holes)\n        y2 = y1 + hole_heights\n        x2 = x1 + hole_widths\n\n        holes = np.stack([x1, y1, x2, y2], axis=-1)\n\n        return {\"holes\": holes.tolist()}\n\n    def apply_to_keypoints(\n        self,\n        keypoints: Sequence[KeypointType],\n        holes: Iterable[tuple[int, int, int, int]],\n        **params: Any,\n    ) -&gt; list[KeypointType]:\n        return [keypoint for keypoint in keypoints if not any(keypoint_in_hole(keypoint, hole) for hole in holes)]\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return (\n            \"num_holes_range\",\n            \"hole_height_range\",\n            \"hole_width_range\",\n            \"fill_value\",\n            \"mask_fill_value\",\n        )\n\n    @property\n    def targets(self) -&gt; dict[str, Callable[..., Any]]:\n        return {\n            \"image\": self.apply,\n            \"mask\": self.apply_to_mask,\n            \"masks\": self.apply_to_masks,\n            \"keypoints\": self.apply_to_keypoints,\n        }\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.dropout.coarse_dropout.CoarseDropout.apply","title":"<code>apply (self, img, fill_value, holes, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/dropout/coarse_dropout.py</code> Python<pre><code>def apply(\n    self,\n    img: np.ndarray,\n    fill_value: ColorType | Literal[\"random\"],\n    holes: Iterable[tuple[int, int, int, int]],\n    **params: Any,\n) -&gt; np.ndarray:\n    return cutout(img, holes, fill_value)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.dropout.coarse_dropout.CoarseDropout.calculate_hole_dimensions","title":"<code>calculate_hole_dimensions (height, width, height_range, width_range, size)</code>  <code>staticmethod</code>","text":"<p>Calculate random hole dimensions based on the provided ranges.</p> Source code in <code>albumentations/augmentations/dropout/coarse_dropout.py</code> Python<pre><code>@staticmethod\ndef calculate_hole_dimensions(\n    height: int,\n    width: int,\n    height_range: tuple[ScalarType, ScalarType],\n    width_range: tuple[ScalarType, ScalarType],\n    size: int,\n) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"Calculate random hole dimensions based on the provided ranges.\"\"\"\n    if isinstance(height_range[0], int):\n        min_height = height_range[0]\n        max_height = min(height_range[1], height)\n\n        min_width = width_range[0]\n        max_width = min(width_range[1], width)\n\n        hole_heights = randint(np.int64(min_height), np.int64(max_height + 1), size=size)\n        hole_widths = randint(np.int64(min_width), np.int64(max_width + 1), size=size)\n\n    else:  # Assume float\n        hole_heights = (height * uniform(height_range[0], height_range[1], size=size)).astype(int)\n        hole_widths = (width * uniform(width_range[0], width_range[1], size=size)).astype(int)\n\n    return hole_heights, hole_widths\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.dropout.coarse_dropout.CoarseDropout.get_params_dependent_on_data","title":"<code>get_params_dependent_on_data (self, params, data)</code>","text":"<p>Returns parameters dependent on input.</p> Source code in <code>albumentations/augmentations/dropout/coarse_dropout.py</code> Python<pre><code>def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n    height, width = params[\"shape\"][:2]\n\n    num_holes = randint(self.num_holes_range[0], self.num_holes_range[1] + 1)\n\n    hole_heights, hole_widths = self.calculate_hole_dimensions(\n        height,\n        width,\n        self.hole_height_range,\n        self.hole_width_range,\n        size=num_holes,\n    )\n\n    y1 = randint(np.int8(0), height - hole_heights + 1, size=num_holes)\n    x1 = randint(np.int8(0), width - hole_widths + 1, size=num_holes)\n    y2 = y1 + hole_heights\n    x2 = x1 + hole_widths\n\n    holes = np.stack([x1, y1, x2, y2], axis=-1)\n\n    return {\"holes\": holes.tolist()}\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.dropout.coarse_dropout.CoarseDropout.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/dropout/coarse_dropout.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return (\n        \"num_holes_range\",\n        \"hole_height_range\",\n        \"hole_width_range\",\n        \"fill_value\",\n        \"mask_fill_value\",\n    )\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.dropout.functional","title":"<code>functional</code>","text":""},{"location":"api_reference/full_reference/#albumentations.augmentations.dropout.functional.cutout","title":"<code>def cutout    (img, holes, fill_value=0)    </code> [view source on GitHub]","text":"<p>Apply cutout augmentation to the image by cutting out holes and filling them with either a given value or random noise.</p> <p>Parameters:</p> Name Type Description <code>img</code> <code>np.ndarray</code> <p>The image to augment.</p> <code>holes</code> <code>Iterable[tuple[int, int, int, int]]</code> <p>An iterable of tuples where each tuple contains the coordinates of the top-left and bottom-right corners of the rectangular hole (x1, y1, x2, y2).</p> <code>fill_value</code> <code>Union[ColorType, Literal[\"random\"]]</code> <p>The fill value to use for the hole. Can be a single integer, a tuple or list of numbers for multichannel, or the string \"random\" to fill with random noise.</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>The augmented image.</p> Source code in <code>albumentations/augmentations/dropout/functional.py</code> Python<pre><code>def cutout(\n    img: np.ndarray,\n    holes: Iterable[tuple[int, int, int, int]],\n    fill_value: ColorType | Literal[\"random\"] = 0,\n) -&gt; np.ndarray:\n    \"\"\"Apply cutout augmentation to the image by cutting out holes and filling them\n    with either a given value or random noise.\n\n    Args:\n        img (np.ndarray): The image to augment.\n        holes (Iterable[tuple[int, int, int, int]]): An iterable of tuples where each\n            tuple contains the coordinates of the top-left and bottom-right corners of\n            the rectangular hole (x1, y1, x2, y2).\n        fill_value (Union[ColorType, Literal[\"random\"]]): The fill value to use for the hole. Can be\n            a single integer, a tuple or list of numbers for multichannel,\n            or the string \"random\" to fill with random noise.\n\n    Returns:\n        np.ndarray: The augmented image.\n    \"\"\"\n    img = img.copy()\n\n    if isinstance(fill_value, (int, float, tuple, list)):\n        fill_value = np.array(fill_value, dtype=img.dtype)\n\n    for x1, y1, x2, y2 in holes:\n        if isinstance(fill_value, str) and fill_value == \"random\":\n            shape = (y2 - y1, x2 - x1) if img.ndim == MONO_CHANNEL_DIMENSIONS else (y2 - y1, x2 - x1, img.shape[2])\n            random_fill = generate_random_fill(img.dtype, shape)\n            img[y1:y2, x1:x2] = random_fill\n        else:\n            img[y1:y2, x1:x2] = fill_value\n\n    return img\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.dropout.functional.generate_random_fill","title":"<code>def generate_random_fill    (dtype, shape)    </code> [view source on GitHub]","text":"<p>Generate a random fill based on dtype and target shape.</p> Source code in <code>albumentations/augmentations/dropout/functional.py</code> Python<pre><code>def generate_random_fill(dtype: np.dtype, shape: tuple[int, ...]) -&gt; np.ndarray:\n    \"\"\"Generate a random fill based on dtype and target shape.\"\"\"\n    max_value = MAX_VALUES_BY_DTYPE[dtype]\n    if np.issubdtype(dtype, np.integer):\n        return random_utils.randint(0, max_value + 1, size=shape, dtype=dtype)\n    if np.issubdtype(dtype, np.floating):\n        return random_utils.uniform(0, max_value, size=shape).astype(dtype)\n    raise ValueError(f\"Unsupported dtype: {dtype}\")\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.dropout.grid_dropout","title":"<code>grid_dropout</code>","text":""},{"location":"api_reference/full_reference/#albumentations.augmentations.dropout.grid_dropout.GridDropout","title":"<code>class  GridDropout</code> <code>     (ratio=0.5, unit_size_min=None, unit_size_max=None, holes_number_x=None, holes_number_y=None, shift_x=None, shift_y=None, random_offset=False, fill_value=0, mask_fill_value=None, unit_size_range=None, holes_number_xy=None, shift_xy=(0, 0), always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>GridDropout, drops out rectangular regions of an image and the corresponding mask in a grid fashion.</p> <p>Parameters:</p> Name Type Description <code>ratio</code> <code>float</code> <p>The ratio of the mask holes to the unit_size (same for horizontal and vertical directions). Must be between 0 and 1. Default: 0.5.</p> <code>random_offset</code> <code>bool</code> <p>Whether to offset the grid randomly between 0 and grid unit size - hole size. If True, entered shift_x and shift_y are ignored and set randomly. Default: False.</p> <code>fill_value</code> <code>Optional[ColorType]</code> <p>Value for the dropped pixels. Default: 0.</p> <code>mask_fill_value</code> <code>Optional[ColorType]</code> <p>Value for the dropped pixels in mask. If None, transformation is not applied to the mask. Default: None.</p> <code>unit_size_range</code> <code>Optional[tuple[int, int]]</code> <p>Range from which to sample grid size. Default: None.  Must be between 2 and the image shorter edge.</p> <code>holes_number_xy</code> <code>Optional[tuple[int, int]]</code> <p>The number of grid units in x and y directions. First value should be between 1 and image width//2, Second value should be between 1 and image height//2. Default: None.</p> <code>shift_xy</code> <code>tuple[int, int]</code> <p>Offsets of the grid start in x and y directions. Offsets of the grid start in x and y directions from (0,0) coordinate. Default: (0, 0).</p> <code>p</code> <code>float</code> <p>Probability of applying the transform. Default: 0.5.</p> <p>Targets</p> <p>image, mask</p> <p>Image types:     uint8, float32</p> <p>Reference</p> <p>https://arxiv.org/abs/2001.04086</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/dropout/grid_dropout.py</code> Python<pre><code>class GridDropout(DualTransform):\n    \"\"\"GridDropout, drops out rectangular regions of an image and the corresponding mask in a grid fashion.\n\n    Args:\n        ratio (float): The ratio of the mask holes to the unit_size (same for horizontal and vertical directions).\n            Must be between 0 and 1. Default: 0.5.\n        random_offset (bool): Whether to offset the grid randomly between 0 and grid unit size - hole size.\n            If True, entered shift_x and shift_y are ignored and set randomly. Default: False.\n        fill_value (Optional[ColorType]): Value for the dropped pixels. Default: 0.\n        mask_fill_value (Optional[ColorType]): Value for the dropped pixels in mask.\n            If None, transformation is not applied to the mask. Default: None.\n        unit_size_range (Optional[tuple[int, int]]): Range from which to sample grid size. Default: None.\n             Must be between 2 and the image shorter edge.\n        holes_number_xy (Optional[tuple[int, int]]): The number of grid units in x and y directions.\n            First value should be between 1 and image width//2,\n            Second value should be between 1 and image height//2.\n            Default: None.\n        shift_xy (tuple[int, int]): Offsets of the grid start in x and y directions.\n            Offsets of the grid start in x and y directions from (0,0) coordinate.\n            Default: (0, 0).\n\n        p (float): Probability of applying the transform. Default: 0.5.\n\n    Targets:\n        image, mask\n\n    Image types:\n        uint8, float32\n\n    Reference:\n        https://arxiv.org/abs/2001.04086\n\n    \"\"\"\n\n    _targets = (Targets.IMAGE, Targets.MASK)\n\n    class InitSchema(BaseTransformInitSchema):\n        ratio: float = Field(description=\"The ratio of the mask holes to the unit_size.\", gt=0, le=1)\n\n        unit_size_min: int | None = Field(None, description=\"Minimum size of the grid unit.\", ge=2)\n        unit_size_max: int | None = Field(None, description=\"Maximum size of the grid unit.\", ge=2)\n\n        holes_number_x: int | None = Field(None, description=\"The number of grid units in x direction.\", ge=1)\n        holes_number_y: int | None = Field(None, description=\"The number of grid units in y direction.\", ge=1)\n\n        shift_x: int | None = Field(0, description=\"Offsets of the grid start in x direction.\", ge=0)\n        shift_y: int | None = Field(0, description=\"Offsets of the grid start in y direction.\", ge=0)\n\n        random_offset: bool = Field(False, description=\"Whether to offset the grid randomly.\")\n        fill_value: ColorType | None = Field(0, description=\"Value for the dropped pixels.\")\n        mask_fill_value: ColorType | None = Field(None, description=\"Value for the dropped pixels in mask.\")\n        unit_size_range: (\n            Annotated[tuple[int, int], AfterValidator(check_1plus), AfterValidator(nondecreasing)] | None\n        ) = None\n        shift_xy: Annotated[tuple[int, int], AfterValidator(check_0plus)] = Field(\n            (0, 0),\n            description=\"Offsets of the grid start in x and y directions.\",\n        )\n        holes_number_xy: Annotated[tuple[int, int], AfterValidator(check_1plus)] | None = Field(\n            None,\n            description=\"The number of grid units in x and y directions.\",\n        )\n\n        @model_validator(mode=\"after\")\n        def validate_normalization(self) -&gt; Self:\n            if self.unit_size_min is not None and self.unit_size_max is not None:\n                self.unit_size_range = self.unit_size_min, self.unit_size_max\n                warn(\n                    \"unit_size_min and unit_size_max are deprecated. Use unit_size_range instead.\",\n                    DeprecationWarning,\n                    stacklevel=2,\n                )\n\n            if self.shift_x is not None and self.shift_y is not None:\n                self.shift_xy = self.shift_x, self.shift_y\n                warn(\"shift_x and shift_y are deprecated. Use shift_xy instead.\", DeprecationWarning, stacklevel=2)\n\n            if self.holes_number_x is not None and self.holes_number_y is not None:\n                self.holes_number_xy = self.holes_number_x, self.holes_number_y\n                warn(\n                    \"holes_number_x and holes_number_y are deprecated. Use holes_number_xy instead.\",\n                    DeprecationWarning,\n                    stacklevel=2,\n                )\n\n            if self.unit_size_range and not MIN_UNIT_SIZE &lt;= self.unit_size_range[0] &lt;= self.unit_size_range[1]:\n                raise ValueError(\"Max unit size should be &gt;= min size, both at least 2 pixels.\")\n\n            return self\n\n    def __init__(\n        self,\n        ratio: float = 0.5,\n        unit_size_min: int | None = None,\n        unit_size_max: int | None = None,\n        holes_number_x: int | None = None,\n        holes_number_y: int | None = None,\n        shift_x: int | None = None,\n        shift_y: int | None = None,\n        random_offset: bool = False,\n        fill_value: ColorType = 0,\n        mask_fill_value: ColorType | None = None,\n        unit_size_range: tuple[int, int] | None = None,\n        holes_number_xy: tuple[int, int] | None = None,\n        shift_xy: tuple[int, int] = (0, 0),\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p, always_apply)\n        self.ratio = ratio\n        self.unit_size_range = unit_size_range\n        self.holes_number_xy = holes_number_xy\n        self.random_offset = random_offset\n        self.fill_value = fill_value\n        self.mask_fill_value = mask_fill_value\n        self.shift_xy = shift_xy\n\n    def apply(self, img: np.ndarray, holes: Iterable[tuple[int, int, int, int]], **params: Any) -&gt; np.ndarray:\n        return fdropout.cutout(img, holes, self.fill_value)\n\n    def apply_to_mask(\n        self,\n        mask: np.ndarray,\n        holes: Iterable[tuple[int, int, int, int]],\n        **params: Any,\n    ) -&gt; np.ndarray:\n        if self.mask_fill_value is None:\n            return mask\n\n        return fdropout.cutout(mask, holes, self.mask_fill_value)\n\n    def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n        height, width = params[\"shape\"][:2]\n        unit_width, unit_height = self._calculate_unit_dimensions(width, height)\n        hole_width, hole_height = self._calculate_hole_dimensions(unit_width, unit_height)\n        shift_x, shift_y = self._calculate_shifts(unit_width, unit_height, hole_width, hole_height)\n        holes = self._generate_holes(width, height, unit_width, unit_height, hole_width, hole_height, shift_x, shift_y)\n        return {\"holes\": holes}\n\n    def _calculate_unit_dimensions(self, width: int, height: int) -&gt; tuple[int, int]:\n        \"\"\"Calculates the dimensions of the grid units.\"\"\"\n        if self.unit_size_range is not None:\n            self._validate_unit_sizes(height, width)\n            unit_size = random.randint(*self.unit_size_range)\n            return unit_size, unit_size\n\n        return self._calculate_dimensions_based_on_holes(width, height)\n\n    def _validate_unit_sizes(self, height: int, width: int) -&gt; None:\n        \"\"\"Validates the minimum and maximum unit sizes.\"\"\"\n        if self.unit_size_range is None:\n            raise ValueError(\"unit_size_range must not be None.\")\n        if self.unit_size_range[1] &gt; min(height, width):\n            msg = \"Grid size limits must be within the shortest image edge.\"\n            raise ValueError(msg)\n\n    def _calculate_dimensions_based_on_holes(self, width: int, height: int) -&gt; tuple[int, int]:\n        \"\"\"Calculates dimensions based on the number of holes specified.\"\"\"\n        holes_number_x, holes_number_y = self.holes_number_xy or (None, None)\n        unit_width = self._calculate_dimension(width, holes_number_x, 10)\n        unit_height = self._calculate_dimension(height, holes_number_y, unit_width)\n        return unit_width, unit_height\n\n    @staticmethod\n    def _calculate_dimension(dimension: int, holes_number: int | None, fallback: int) -&gt; int:\n        \"\"\"Helper function to calculate unit width or height.\"\"\"\n        if holes_number is None:\n            return max(2, dimension // fallback)\n\n        if not 1 &lt;= holes_number &lt;= dimension // 2:\n            raise ValueError(f\"The number of holes must be between 1 and {dimension // 2}.\")\n        return dimension // holes_number\n\n    def _calculate_hole_dimensions(self, unit_width: int, unit_height: int) -&gt; tuple[int, int]:\n        \"\"\"Calculates the dimensions of the holes to be dropped out.\"\"\"\n        hole_width = int(unit_width * self.ratio)\n        hole_height = int(unit_height * self.ratio)\n        hole_width = min(max(hole_width, 1), unit_width - 1)\n        hole_height = min(max(hole_height, 1), unit_height - 1)\n        return hole_width, hole_height\n\n    def _calculate_shifts(\n        self,\n        unit_width: int,\n        unit_height: int,\n        hole_width: int,\n        hole_height: int,\n    ) -&gt; tuple[int, int]:\n        \"\"\"Calculates the shifts for the grid start.\"\"\"\n        if self.random_offset:\n            shift_x = random.randint(0, unit_width - hole_width)\n            shift_y = random.randint(0, unit_height - hole_height)\n            return shift_x, shift_y\n\n        if isinstance(self.shift_xy, Sequence) and len(self.shift_xy) == PAIR:\n            shift_x = min(max(0, self.shift_xy[0]), unit_width - hole_width)\n            shift_y = min(max(0, self.shift_xy[1]), unit_height - hole_height)\n            return shift_x, shift_y\n\n        return 0, 0\n\n    def _generate_holes(\n        self,\n        width: int,\n        height: int,\n        unit_width: int,\n        unit_height: int,\n        hole_width: int,\n        hole_height: int,\n        shift_x: int,\n        shift_y: int,\n    ) -&gt; list[tuple[int, int, int, int]]:\n        \"\"\"Generates the list of holes to be dropped out.\"\"\"\n        holes = []\n        for i in range(width // unit_width + 1):\n            for j in range(height // unit_height + 1):\n                x1 = min(shift_x + unit_width * i, width)\n                y1 = min(shift_y + unit_height * j, height)\n                x2 = min(x1 + hole_width, width)\n                y2 = min(y1 + hole_height, height)\n                holes.append((x1, y1, x2, y2))\n        return holes\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return (\n            \"ratio\",\n            \"unit_size_range\",\n            \"holes_number_xy\",\n            \"shift_xy\",\n            \"random_offset\",\n            \"fill_value\",\n            \"mask_fill_value\",\n        )\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.dropout.grid_dropout.GridDropout.apply","title":"<code>apply (self, img, holes, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/dropout/grid_dropout.py</code> Python<pre><code>def apply(self, img: np.ndarray, holes: Iterable[tuple[int, int, int, int]], **params: Any) -&gt; np.ndarray:\n    return fdropout.cutout(img, holes, self.fill_value)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.dropout.grid_dropout.GridDropout.get_params_dependent_on_data","title":"<code>get_params_dependent_on_data (self, params, data)</code>","text":"<p>Returns parameters dependent on input.</p> Source code in <code>albumentations/augmentations/dropout/grid_dropout.py</code> Python<pre><code>def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n    height, width = params[\"shape\"][:2]\n    unit_width, unit_height = self._calculate_unit_dimensions(width, height)\n    hole_width, hole_height = self._calculate_hole_dimensions(unit_width, unit_height)\n    shift_x, shift_y = self._calculate_shifts(unit_width, unit_height, hole_width, hole_height)\n    holes = self._generate_holes(width, height, unit_width, unit_height, hole_width, hole_height, shift_x, shift_y)\n    return {\"holes\": holes}\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.dropout.grid_dropout.GridDropout.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/dropout/grid_dropout.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return (\n        \"ratio\",\n        \"unit_size_range\",\n        \"holes_number_xy\",\n        \"shift_xy\",\n        \"random_offset\",\n        \"fill_value\",\n        \"mask_fill_value\",\n    )\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.dropout.mask_dropout","title":"<code>mask_dropout</code>","text":""},{"location":"api_reference/full_reference/#albumentations.augmentations.dropout.mask_dropout.MaskDropout","title":"<code>class  MaskDropout</code> <code>     (max_objects=(1, 1), image_fill_value=0, mask_fill_value=0, always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Image &amp; mask augmentation that zero out mask and image regions corresponding to randomly chosen object instance from mask.</p> <p>Mask must be single-channel image, zero values treated as background. Image can be any number of channels.</p> <p>Parameters:</p> Name Type Description <code>max_objects</code> <code>ScaleIntType</code> <p>Maximum number of labels that can be zeroed out. Can be tuple, in this case it's [min, max]</p> <code>image_fill_value</code> <code>float | Literal['inpaint']</code> <p>Fill value to use when filling image. Can be 'inpaint' to apply inpainting (works only  for 3-channel images)</p> <code>mask_fill_value</code> <code>ScalarType</code> <p>Fill value to use when filling mask.</p> <p>Targets</p> <p>image, mask</p> <p>Image types:     uint8, float32</p> <p>Reference</p> <p>https://www.kaggle.com/c/severstal-steel-defect-detection/discussion/114254</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/dropout/mask_dropout.py</code> Python<pre><code>class MaskDropout(DualTransform):\n    \"\"\"Image &amp; mask augmentation that zero out mask and image regions corresponding\n    to randomly chosen object instance from mask.\n\n    Mask must be single-channel image, zero values treated as background.\n    Image can be any number of channels.\n\n    Args:\n        max_objects: Maximum number of labels that can be zeroed out. Can be tuple, in this case it's [min, max]\n        image_fill_value: Fill value to use when filling image.\n            Can be 'inpaint' to apply inpainting (works only  for 3-channel images)\n        mask_fill_value: Fill value to use when filling mask.\n\n    Targets:\n        image, mask\n\n    Image types:\n        uint8, float32\n\n    Reference:\n        https://www.kaggle.com/c/severstal-steel-defect-detection/discussion/114254\n\n    \"\"\"\n\n    _targets = (Targets.IMAGE, Targets.MASK)\n\n    class InitSchema(BaseTransformInitSchema):\n        max_objects: OnePlusIntRangeType = (1, 1)\n\n        image_fill_value: float | Literal[\"inpaint\"] = Field(\n            default=0,\n            description=(\n                \"Fill value to use when filling image. \"\n                \"Can be 'inpaint' to apply inpainting (works only for 3-channel images).\"\n            ),\n        )\n        mask_fill_value: float = Field(default=0, description=\"Fill value to use when filling mask.\")\n\n    def __init__(\n        self,\n        max_objects: ScaleIntType = (1, 1),\n        image_fill_value: float | Literal[\"inpaint\"] = 0,\n        mask_fill_value: ScalarType = 0,\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p=p, always_apply=always_apply)\n        self.max_objects = cast(Tuple[int, int], max_objects)\n        self.image_fill_value = image_fill_value\n        self.mask_fill_value = mask_fill_value\n\n    @property\n    def targets_as_params(self) -&gt; list[str]:\n        return [\"mask\"]\n\n    def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n        mask = data[\"mask\"]\n\n        label_image, num_labels = label(mask, return_num=True)\n\n        if num_labels == 0:\n            dropout_mask = None\n        else:\n            objects_to_drop = random.randint(self.max_objects[0], self.max_objects[1])\n            objects_to_drop = min(num_labels, objects_to_drop)\n\n            if objects_to_drop == num_labels:\n                dropout_mask = mask &gt; 0\n            else:\n                labels_index = random.sample(range(1, num_labels + 1), objects_to_drop)\n                dropout_mask = np.zeros((mask.shape[0], mask.shape[1]), dtype=bool)\n                for label_index in labels_index:\n                    dropout_mask |= label_image == label_index\n\n        params.update({\"dropout_mask\": dropout_mask})\n        return params\n\n    def apply(self, img: np.ndarray, dropout_mask: np.ndarray, **params: Any) -&gt; np.ndarray:\n        if dropout_mask is None:\n            return img\n\n        if self.image_fill_value == \"inpaint\":\n            dropout_mask = dropout_mask.astype(np.uint8)\n            _, _, width, height = cv2.boundingRect(dropout_mask)\n            radius = min(3, max(width, height) // 2)\n            return cv2.inpaint(img, dropout_mask, radius, cv2.INPAINT_NS)\n\n        img = img.copy()\n        img[dropout_mask] = self.image_fill_value\n\n        return img\n\n    def apply_to_mask(self, mask: np.ndarray, dropout_mask: np.ndarray, **params: Any) -&gt; np.ndarray:\n        if dropout_mask is None:\n            return mask\n\n        mask = mask.copy()\n        mask[dropout_mask] = self.mask_fill_value\n        return mask\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return \"max_objects\", \"image_fill_value\", \"mask_fill_value\"\n\n    @property\n    def targets(self) -&gt; dict[str, Callable[..., Any]]:\n        return {\n            \"image\": self.apply,\n            \"mask\": self.apply_to_mask,\n            \"masks\": self.apply_to_masks,\n        }\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.dropout.mask_dropout.MaskDropout.targets_as_params","title":"<code>targets_as_params: list[str]</code>  <code>property</code> <code>readonly</code>","text":"<p>Targets used to get params dependent on targets. This is used to check input has all required targets.</p>"},{"location":"api_reference/full_reference/#albumentations.augmentations.dropout.mask_dropout.MaskDropout.apply","title":"<code>apply (self, img, dropout_mask, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/dropout/mask_dropout.py</code> Python<pre><code>def apply(self, img: np.ndarray, dropout_mask: np.ndarray, **params: Any) -&gt; np.ndarray:\n    if dropout_mask is None:\n        return img\n\n    if self.image_fill_value == \"inpaint\":\n        dropout_mask = dropout_mask.astype(np.uint8)\n        _, _, width, height = cv2.boundingRect(dropout_mask)\n        radius = min(3, max(width, height) // 2)\n        return cv2.inpaint(img, dropout_mask, radius, cv2.INPAINT_NS)\n\n    img = img.copy()\n    img[dropout_mask] = self.image_fill_value\n\n    return img\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.dropout.mask_dropout.MaskDropout.get_params_dependent_on_data","title":"<code>get_params_dependent_on_data (self, params, data)</code>","text":"<p>Returns parameters dependent on input.</p> Source code in <code>albumentations/augmentations/dropout/mask_dropout.py</code> Python<pre><code>def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n    mask = data[\"mask\"]\n\n    label_image, num_labels = label(mask, return_num=True)\n\n    if num_labels == 0:\n        dropout_mask = None\n    else:\n        objects_to_drop = random.randint(self.max_objects[0], self.max_objects[1])\n        objects_to_drop = min(num_labels, objects_to_drop)\n\n        if objects_to_drop == num_labels:\n            dropout_mask = mask &gt; 0\n        else:\n            labels_index = random.sample(range(1, num_labels + 1), objects_to_drop)\n            dropout_mask = np.zeros((mask.shape[0], mask.shape[1]), dtype=bool)\n            for label_index in labels_index:\n                dropout_mask |= label_image == label_index\n\n    params.update({\"dropout_mask\": dropout_mask})\n    return params\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.dropout.mask_dropout.MaskDropout.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/dropout/mask_dropout.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return \"max_objects\", \"image_fill_value\", \"mask_fill_value\"\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.dropout.xy_masking","title":"<code>xy_masking</code>","text":""},{"location":"api_reference/full_reference/#albumentations.augmentations.dropout.xy_masking.XYMasking","title":"<code>class  XYMasking</code> <code>     (num_masks_x=0, num_masks_y=0, mask_x_length=0, mask_y_length=0, fill_value=0, mask_fill_value=0, always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Applies masking strips to an image, either horizontally (X axis) or vertically (Y axis), simulating occlusions. This transform is useful for training models to recognize images with varied visibility conditions. It's particularly effective for spectrogram images, allowing spectral and frequency masking to improve model robustness.</p> <p>At least one of <code>max_x_length</code> or <code>max_y_length</code> must be specified, dictating the mask's maximum size along each axis.</p> <p>Parameters:</p> Name Type Description <code>num_masks_x</code> <code>Union[int, tuple[int, int]]</code> <p>Number or range of horizontal regions to mask. Defaults to 0.</p> <code>num_masks_y</code> <code>Union[int, tuple[int, int]]</code> <p>Number or range of vertical regions to mask. Defaults to 0.</p> <code>mask_x_length</code> <code>[Union[int, tuple[int, int]]</code> <p>Specifies the length of the masks along the X (horizontal) axis. If an integer is provided, it sets a fixed mask length. If a tuple of two integers (min, max) is provided, the mask length is randomly chosen within this range for each mask. This allows for variable-length masks in the horizontal direction.</p> <code>mask_y_length</code> <code>Union[int, tuple[int, int]]</code> <p>Specifies the height of the masks along the Y (vertical) axis. Similar to <code>mask_x_length</code>, an integer sets a fixed mask height, while a tuple (min, max) allows for variable-height masks, chosen randomly within the specified range for each mask. This flexibility facilitates creating masks of various sizes in the vertical direction.</p> <code>fill_value</code> <code>Union[int, float, list[int], list[float]]</code> <p>Value to fill image masks. Defaults to 0.</p> <code>mask_fill_value</code> <code>Optional[Union[int, float, list[int], list[float]]]</code> <p>Value to fill masks in the mask. If <code>None</code>, uses mask is not affected. Default: <code>None</code>.</p> <code>p</code> <code>float</code> <p>Probability of applying the transform. Defaults to 0.5.</p> <p>Targets</p> <p>image, mask, keypoints</p> <p>Image types:     uint8, float32</p> <p>Note: Either <code>max_x_length</code> or <code>max_y_length</code> or both must be defined.</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/dropout/xy_masking.py</code> Python<pre><code>class XYMasking(DualTransform):\n    \"\"\"Applies masking strips to an image, either horizontally (X axis) or vertically (Y axis),\n    simulating occlusions. This transform is useful for training models to recognize images\n    with varied visibility conditions. It's particularly effective for spectrogram images,\n    allowing spectral and frequency masking to improve model robustness.\n\n    At least one of `max_x_length` or `max_y_length` must be specified, dictating the mask's\n    maximum size along each axis.\n\n    Args:\n        num_masks_x (Union[int, tuple[int, int]]): Number or range of horizontal regions to mask. Defaults to 0.\n        num_masks_y (Union[int, tuple[int, int]]): Number or range of vertical regions to mask. Defaults to 0.\n        mask_x_length ([Union[int, tuple[int, int]]): Specifies the length of the masks along\n            the X (horizontal) axis. If an integer is provided, it sets a fixed mask length.\n            If a tuple of two integers (min, max) is provided,\n            the mask length is randomly chosen within this range for each mask.\n            This allows for variable-length masks in the horizontal direction.\n        mask_y_length (Union[int, tuple[int, int]]): Specifies the height of the masks along\n            the Y (vertical) axis. Similar to `mask_x_length`, an integer sets a fixed mask height,\n            while a tuple (min, max) allows for variable-height masks, chosen randomly\n            within the specified range for each mask. This flexibility facilitates creating masks of various\n            sizes in the vertical direction.\n        fill_value (Union[int, float, list[int], list[float]]): Value to fill image masks. Defaults to 0.\n        mask_fill_value (Optional[Union[int, float, list[int], list[float]]]): Value to fill masks in the mask.\n            If `None`, uses mask is not affected. Default: `None`.\n        p (float): Probability of applying the transform. Defaults to 0.5.\n\n    Targets:\n        image, mask, keypoints\n\n    Image types:\n        uint8, float32\n\n    Note: Either `max_x_length` or `max_y_length` or both must be defined.\n\n    \"\"\"\n\n    _targets = (Targets.IMAGE, Targets.MASK, Targets.KEYPOINTS)\n\n    class InitSchema(BaseTransformInitSchema):\n        num_masks_x: NonNegativeIntRangeType = 0\n        num_masks_y: NonNegativeIntRangeType = 0\n        mask_x_length: NonNegativeIntRangeType = 0\n        mask_y_length: NonNegativeIntRangeType = 0\n\n        fill_value: ColorType = Field(default=0, description=\"Value to fill image masks.\")\n        mask_fill_value: ColorType = Field(default=0, description=\"Value to fill masks in the mask.\")\n\n        @model_validator(mode=\"after\")\n        def check_mask_length(self) -&gt; Self:\n            if (\n                isinstance(self.mask_x_length, int)\n                and self.mask_x_length &lt;= 0\n                and isinstance(self.mask_y_length, int)\n                and self.mask_y_length &lt;= 0\n            ):\n                msg = \"At least one of `mask_x_length` or `mask_y_length` Should be a positive number.\"\n                raise ValueError(msg)\n            return self\n\n    def __init__(\n        self,\n        num_masks_x: ScaleIntType = 0,\n        num_masks_y: ScaleIntType = 0,\n        mask_x_length: ScaleIntType = 0,\n        mask_y_length: ScaleIntType = 0,\n        fill_value: ColorType = 0,\n        mask_fill_value: ColorType = 0,\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p, always_apply)\n        self.num_masks_x = cast(Tuple[int, int], num_masks_x)\n        self.num_masks_y = cast(Tuple[int, int], num_masks_y)\n\n        self.mask_x_length = cast(Tuple[int, int], mask_x_length)\n        self.mask_y_length = cast(Tuple[int, int], mask_y_length)\n        self.fill_value = fill_value\n        self.mask_fill_value = mask_fill_value\n\n    def apply(\n        self,\n        img: np.ndarray,\n        masks_x: list[tuple[int, int, int, int]],\n        masks_y: list[tuple[int, int, int, int]],\n        **params: Any,\n    ) -&gt; np.ndarray:\n        return cutout(img, masks_x + masks_y, self.fill_value)\n\n    def apply_to_mask(\n        self,\n        mask: np.ndarray,\n        masks_x: list[tuple[int, int, int, int]],\n        masks_y: list[tuple[int, int, int, int]],\n        **params: Any,\n    ) -&gt; np.ndarray:\n        if self.mask_fill_value is None:\n            return mask\n        return cutout(mask, masks_x + masks_y, self.mask_fill_value)\n\n    def validate_mask_length(\n        self,\n        mask_length: tuple[int, int] | None,\n        dimension_size: int,\n        dimension_name: str,\n    ) -&gt; None:\n        \"\"\"Validate the mask length against the corresponding image dimension size.\n\n        Args:\n            mask_length (Optional[tuple[int, int]]): The length of the mask to be validated.\n            dimension_size (int): The size of the image dimension (width or height)\n                against which to validate the mask length.\n            dimension_name (str): The name of the dimension ('width' or 'height') for error messaging.\n\n        \"\"\"\n        if mask_length is not None:\n            if isinstance(mask_length, (tuple, list)):\n                if mask_length[0] &lt; 0 or mask_length[1] &gt; dimension_size:\n                    raise ValueError(\n                        f\"{dimension_name} range {mask_length} is out of valid range [0, {dimension_size}]\",\n                    )\n            elif mask_length &lt; 0 or mask_length &gt; dimension_size:\n                raise ValueError(f\"{dimension_name} {mask_length} exceeds image {dimension_name} {dimension_size}\")\n\n    def get_params_dependent_on_data(\n        self,\n        params: dict[str, Any],\n        data: dict[str, Any],\n    ) -&gt; dict[str, list[tuple[int, int, int, int]]]:\n        height, width = params[\"shape\"][:2]\n\n        # Use the helper method to validate mask lengths against image dimensions\n        self.validate_mask_length(self.mask_x_length, width, \"mask_x_length\")\n        self.validate_mask_length(self.mask_y_length, height, \"mask_y_length\")\n\n        masks_x = self.generate_masks(self.num_masks_x, width, height, self.mask_x_length, axis=\"x\")\n        masks_y = self.generate_masks(self.num_masks_y, width, height, self.mask_y_length, axis=\"y\")\n\n        return {\"masks_x\": masks_x, \"masks_y\": masks_y}\n\n    @staticmethod\n    def generate_mask_size(mask_length: tuple[int, int]) -&gt; int:\n        return random.randint(mask_length[0], mask_length[1])\n\n    def generate_masks(\n        self,\n        num_masks: tuple[int, int],\n        width: int,\n        height: int,\n        max_length: tuple[int, int] | None,\n        axis: str,\n    ) -&gt; list[tuple[int, int, int, int]]:\n        if max_length is None or max_length == 0 or isinstance(num_masks, (int, float)) and num_masks == 0:\n            return []\n\n        masks = []\n\n        num_masks_integer = num_masks if isinstance(num_masks, int) else random.randint(num_masks[0], num_masks[1])\n\n        for _ in range(num_masks_integer):\n            length = self.generate_mask_size(max_length)\n\n            if axis == \"x\":\n                x1 = random.randint(0, width - length)\n                y1 = 0\n                x2, y2 = x1 + length, height\n            else:  # axis == 'y'\n                y1 = random.randint(0, height - length)\n                x1 = 0\n                x2, y2 = width, y1 + length\n\n            masks.append((x1, y1, x2, y2))\n        return masks\n\n    def apply_to_keypoints(\n        self,\n        keypoints: Sequence[KeypointType],\n        masks_x: list[tuple[int, int, int, int]],\n        masks_y: list[tuple[int, int, int, int]],\n        **params: Any,\n    ) -&gt; list[KeypointType]:\n        return [\n            keypoint\n            for keypoint in keypoints\n            if not any(keypoint_in_hole(keypoint, hole) for hole in masks_x + masks_y)\n        ]\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return (\n            \"num_masks_x\",\n            \"num_masks_y\",\n            \"mask_x_length\",\n            \"mask_y_length\",\n            \"fill_value\",\n            \"mask_fill_value\",\n        )\n\n    @property\n    def targets(self) -&gt; dict[str, Callable[..., Any]]:\n        return {\n            \"image\": self.apply,\n            \"mask\": self.apply_to_mask,\n            \"masks\": self.apply_to_masks,\n            \"keypoints\": self.apply_to_keypoints,\n        }\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.dropout.xy_masking.XYMasking.apply","title":"<code>apply (self, img, masks_x, masks_y, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/dropout/xy_masking.py</code> Python<pre><code>def apply(\n    self,\n    img: np.ndarray,\n    masks_x: list[tuple[int, int, int, int]],\n    masks_y: list[tuple[int, int, int, int]],\n    **params: Any,\n) -&gt; np.ndarray:\n    return cutout(img, masks_x + masks_y, self.fill_value)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.dropout.xy_masking.XYMasking.get_params_dependent_on_data","title":"<code>get_params_dependent_on_data (self, params, data)</code>","text":"<p>Returns parameters dependent on input.</p> Source code in <code>albumentations/augmentations/dropout/xy_masking.py</code> Python<pre><code>def get_params_dependent_on_data(\n    self,\n    params: dict[str, Any],\n    data: dict[str, Any],\n) -&gt; dict[str, list[tuple[int, int, int, int]]]:\n    height, width = params[\"shape\"][:2]\n\n    # Use the helper method to validate mask lengths against image dimensions\n    self.validate_mask_length(self.mask_x_length, width, \"mask_x_length\")\n    self.validate_mask_length(self.mask_y_length, height, \"mask_y_length\")\n\n    masks_x = self.generate_masks(self.num_masks_x, width, height, self.mask_x_length, axis=\"x\")\n    masks_y = self.generate_masks(self.num_masks_y, width, height, self.mask_y_length, axis=\"y\")\n\n    return {\"masks_x\": masks_x, \"masks_y\": masks_y}\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.dropout.xy_masking.XYMasking.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/dropout/xy_masking.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return (\n        \"num_masks_x\",\n        \"num_masks_y\",\n        \"mask_x_length\",\n        \"mask_y_length\",\n        \"fill_value\",\n        \"mask_fill_value\",\n    )\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.dropout.xy_masking.XYMasking.validate_mask_length","title":"<code>validate_mask_length (self, mask_length, dimension_size, dimension_name)</code>","text":"<p>Validate the mask length against the corresponding image dimension size.</p> <p>Parameters:</p> Name Type Description <code>mask_length</code> <code>Optional[tuple[int, int]]</code> <p>The length of the mask to be validated.</p> <code>dimension_size</code> <code>int</code> <p>The size of the image dimension (width or height) against which to validate the mask length.</p> <code>dimension_name</code> <code>str</code> <p>The name of the dimension ('width' or 'height') for error messaging.</p> Source code in <code>albumentations/augmentations/dropout/xy_masking.py</code> Python<pre><code>def validate_mask_length(\n    self,\n    mask_length: tuple[int, int] | None,\n    dimension_size: int,\n    dimension_name: str,\n) -&gt; None:\n    \"\"\"Validate the mask length against the corresponding image dimension size.\n\n    Args:\n        mask_length (Optional[tuple[int, int]]): The length of the mask to be validated.\n        dimension_size (int): The size of the image dimension (width or height)\n            against which to validate the mask length.\n        dimension_name (str): The name of the dimension ('width' or 'height') for error messaging.\n\n    \"\"\"\n    if mask_length is not None:\n        if isinstance(mask_length, (tuple, list)):\n            if mask_length[0] &lt; 0 or mask_length[1] &gt; dimension_size:\n                raise ValueError(\n                    f\"{dimension_name} range {mask_length} is out of valid range [0, {dimension_size}]\",\n                )\n        elif mask_length &lt; 0 or mask_length &gt; dimension_size:\n            raise ValueError(f\"{dimension_name} {mask_length} exceeds image {dimension_name} {dimension_size}\")\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.functional","title":"<code>functional</code>","text":""},{"location":"api_reference/full_reference/#albumentations.augmentations.functional.add_fog","title":"<code>def add_fog    (img, fog_coef, alpha_coef, haze_list)    </code> [view source on GitHub]","text":"<p>Add fog to an image using the provided coefficients and haze points.</p> <p>Parameters:</p> Name Type Description <code>img</code> <code>np.ndarray</code> <p>The input image, expected to be a numpy array.</p> <code>fog_coef</code> <code>float</code> <p>The fog coefficient, used to determine the intensity of the fog.</p> <code>alpha_coef</code> <code>float</code> <p>The alpha coefficient, used to determine the transparency of the fog.</p> <code>haze_list</code> <code>list[tuple[int, int]]</code> <p>A list of tuples, where each tuple represents the x and y coordinates of a haze point.</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>The output image with added fog, as a numpy array.</p> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If the input image's dtype is not uint8 or float32.</p> <p>Reference</p> <p>https://github.com/UjjwalSaxena/Automold--Road-Augmentation-Library</p> Source code in <code>albumentations/augmentations/functional.py</code> Python<pre><code>@preserve_channel_dim\ndef add_fog(img: np.ndarray, fog_coef: float, alpha_coef: float, haze_list: list[tuple[int, int]]) -&gt; np.ndarray:\n    \"\"\"Add fog to an image using the provided coefficients and haze points.\n\n    Args:\n        img (np.ndarray): The input image, expected to be a numpy array.\n        fog_coef (float): The fog coefficient, used to determine the intensity of the fog.\n        alpha_coef (float): The alpha coefficient, used to determine the transparency of the fog.\n        haze_list (list[tuple[int, int]]): A list of tuples, where each tuple represents the x and y\n            coordinates of a haze point.\n\n    Returns:\n        np.ndarray: The output image with added fog, as a numpy array.\n\n    Raises:\n        ValueError: If the input image's dtype is not uint8 or float32.\n\n    Reference:\n        https://github.com/UjjwalSaxena/Automold--Road-Augmentation-Library\n    \"\"\"\n    non_rgb_warning(img)\n\n    input_dtype = img.dtype\n    needs_float = False\n\n    if input_dtype == np.float32:\n        img = from_float(img, dtype=np.dtype(\"uint8\"))\n        needs_float = True\n    elif input_dtype not in (np.uint8, np.float32):\n        raise ValueError(f\"Unexpected dtype {input_dtype} for RandomFog augmentation\")\n\n    width = img.shape[1]\n\n    hw = max(int(width // 3 * fog_coef), 10)\n\n    for haze_points in haze_list:\n        x, y = haze_points\n        overlay = img.copy()\n        output = img.copy()\n        alpha = alpha_coef * fog_coef\n        rad = hw // 2\n        point = (x + hw // 2, y + hw // 2)\n        cv2.circle(overlay, point, int(rad), (255, 255, 255), -1)\n        output = add_weighted(overlay, alpha, output, 1 - alpha)\n\n        img = output.copy()\n\n    image_rgb = cv2.blur(img, (hw // 10, hw // 10))\n\n    return to_float(image_rgb, max_value=255) if needs_float else image_rgb\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.functional.add_gravel","title":"<code>def add_gravel    (img, gravels)    </code> [view source on GitHub]","text":"<p>Add gravel to the image.</p> <p>Parameters:</p> Name Type Description <code>img</code> <code>numpy.ndarray</code> <p>image to add gravel to</p> <code>gravels</code> <code>list</code> <p>list of gravel parameters. (float, float, float, float): (top-left x, top-left y, bottom-right x, bottom right y)</p> <p>Returns:</p> Type Description <code>numpy.ndarray</code> <p>Reference</p> <p>https://github.com/UjjwalSaxena/Automold--Road-Augmentation-Library</p> Source code in <code>albumentations/augmentations/functional.py</code> Python<pre><code>@preserve_channel_dim\ndef add_gravel(img: np.ndarray, gravels: list[Any]) -&gt; np.ndarray:\n    \"\"\"Add gravel to the image.\n\n    Args:\n        img (numpy.ndarray): image to add gravel to\n        gravels (list): list of gravel parameters. (float, float, float, float):\n            (top-left x, top-left y, bottom-right x, bottom right y)\n\n    Returns:\n        numpy.ndarray:\n\n    Reference:\n        https://github.com/UjjwalSaxena/Automold--Road-Augmentation-Library\n    \"\"\"\n    non_rgb_warning(img)\n    input_dtype = img.dtype\n    needs_float = False\n\n    if input_dtype == np.float32:\n        img = from_float(img, dtype=np.dtype(\"uint8\"))\n        needs_float = True\n    elif input_dtype not in (np.uint8, np.float32):\n        raise ValueError(f\"Unexpected dtype {input_dtype} for AddGravel augmentation\")\n\n    image_hls = cv2.cvtColor(img, cv2.COLOR_RGB2HLS)\n\n    for gravel in gravels:\n        y1, y2, x1, x2, sat = gravel\n        image_hls[x1:x2, y1:y2, 1] = sat\n\n    image_rgb = cv2.cvtColor(image_hls, cv2.COLOR_HLS2RGB)\n\n    return to_float(image_rgb, max_value=255) if needs_float else image_rgb\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.functional.add_rain","title":"<code>def add_rain    (img, slant, drop_length, drop_width, drop_color, blur_value, brightness_coefficient, rain_drops)    </code> [view source on GitHub]","text":"<p>Adds rain drops to the image.</p> <p>Parameters:</p> Name Type Description <code>img</code> <code>np.ndarray</code> <p>Input image.</p> <code>slant</code> <code>int</code> <p>The angle of the rain drops.</p> <code>drop_length</code> <code>int</code> <p>The length of each rain drop.</p> <code>drop_width</code> <code>int</code> <p>The width of each rain drop.</p> <code>drop_color</code> <code>tuple[int, int, int]</code> <p>The color of the rain drops in RGB format.</p> <code>blur_value</code> <code>int</code> <p>The size of the kernel used to blur the image. Rainy views are blurry.</p> <code>brightness_coefficient</code> <code>float</code> <p>Coefficient to adjust the brightness of the image. Rainy days are usually shady.</p> <code>rain_drops</code> <code>list[tuple[int, int]]</code> <p>A list of tuples where each tuple represents the (x, y) coordinates of the starting point of a rain drop.</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Image with rain effect added.</p> <p>Reference</p> <p>https://github.com/UjjwalSaxena/Automold--Road-Augmentation-Library</p> Source code in <code>albumentations/augmentations/functional.py</code> Python<pre><code>@preserve_channel_dim\ndef add_rain(\n    img: np.ndarray,\n    slant: int,\n    drop_length: int,\n    drop_width: int,\n    drop_color: tuple[int, int, int],\n    blur_value: int,\n    brightness_coefficient: float,\n    rain_drops: list[tuple[int, int]],\n) -&gt; np.ndarray:\n    \"\"\"Adds rain drops to the image.\n\n    Args:\n        img (np.ndarray): Input image.\n        slant (int): The angle of the rain drops.\n        drop_length (int): The length of each rain drop.\n        drop_width (int): The width of each rain drop.\n        drop_color (tuple[int, int, int]): The color of the rain drops in RGB format.\n        blur_value (int): The size of the kernel used to blur the image. Rainy views are blurry.\n        brightness_coefficient (float): Coefficient to adjust the brightness of the image. Rainy days are usually shady.\n        rain_drops (list[tuple[int, int]]): A list of tuples where each tuple represents the (x, y)\n            coordinates of the starting point of a rain drop.\n\n    Returns:\n        np.ndarray: Image with rain effect added.\n\n    Reference:\n        https://github.com/UjjwalSaxena/Automold--Road-Augmentation-Library\n    \"\"\"\n    non_rgb_warning(img)\n\n    input_dtype = img.dtype\n    needs_float = False\n\n    if input_dtype == np.float32:\n        img = from_float(img, dtype=np.dtype(\"uint8\"))\n        needs_float = True\n\n    image = img.copy()\n\n    for rain_drop_x0, rain_drop_y0 in rain_drops:\n        rain_drop_x1 = rain_drop_x0 + slant\n        rain_drop_y1 = rain_drop_y0 + drop_length\n\n        cv2.line(\n            image,\n            (rain_drop_x0, rain_drop_y0),\n            (rain_drop_x1, rain_drop_y1),\n            drop_color,\n            drop_width,\n        )\n\n    image = cv2.blur(image, (blur_value, blur_value))  # rainy view are blurry\n    image_hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV).astype(np.float32)\n    image_hsv[:, :, 2] *= brightness_coefficient\n\n    image_rgb = cv2.cvtColor(image_hsv.astype(np.uint8), cv2.COLOR_HSV2RGB)\n\n    return to_float(image_rgb, max_value=255) if needs_float else image_rgb\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.functional.add_shadow","title":"<code>def add_shadow    (img, vertices_list, intensities)    </code> [view source on GitHub]","text":"<p>Add shadows to the image by reducing the intensity of the pixel values in specified regions.</p> <p>Parameters:</p> Name Type Description <code>img</code> <code>np.ndarray</code> <p>Input image. Multichannel images are supported.</p> <code>vertices_list</code> <code>list[np.ndarray]</code> <p>List of vertices for shadow polygons.</p> <code>intensities</code> <code>np.ndarray</code> <p>Array of shadow intensities. Range is [0, 1].</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Image with shadows added.</p> <p>Reference</p> <p>https://github.com/UjjwalSaxena/Automold--Road-Augmentation-Library</p> Source code in <code>albumentations/augmentations/functional.py</code> Python<pre><code>@preserve_channel_dim\ndef add_shadow(img: np.ndarray, vertices_list: list[np.ndarray], intensities: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Add shadows to the image by reducing the intensity of the pixel values in specified regions.\n\n    Args:\n        img (np.ndarray): Input image. Multichannel images are supported.\n        vertices_list (list[np.ndarray]): List of vertices for shadow polygons.\n        intensities (np.ndarray): Array of shadow intensities. Range is [0, 1].\n\n    Returns:\n        np.ndarray: Image with shadows added.\n\n    Reference:\n        https://github.com/UjjwalSaxena/Automold--Road-Augmentation-Library\n    \"\"\"\n    input_dtype = img.dtype\n    needs_float = False\n    num_channels = get_num_channels(img)\n    max_value = MAX_VALUES_BY_DTYPE[np.uint8]\n\n    if input_dtype == np.float32:\n        img = from_float(img, dtype=np.dtype(\"uint8\"))\n        needs_float = True\n\n    img_shadowed = img.copy()\n\n    # Iterate over the vertices and intensity list\n    for vertices, shadow_intensity in zip(vertices_list, intensities):\n        # Create mask for the current shadow polygon\n        mask = np.zeros((img.shape[0], img.shape[1], 1), dtype=np.uint8)\n        cv2.fillPoly(mask, [vertices], (max_value,))\n\n        # Duplicate the mask to have the same number of channels as the image\n        mask = np.repeat(mask, num_channels, axis=2)\n\n        # Apply shadow to the channels directly\n        # It could be tempting to convert to HLS and apply the shadow to the L channel, but it creates artifacts\n        shadowed_indices = mask[:, :, 0] == max_value\n        img_shadowed[shadowed_indices] = clip(\n            img_shadowed[shadowed_indices] * shadow_intensity,\n            np.uint8,\n        )\n\n    if needs_float:\n        return to_float(img_shadowed, max_value=max_value)\n\n    return img_shadowed\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.functional.add_snow","title":"<code>def add_snow    (img, snow_point, brightness_coeff)    </code> [view source on GitHub]","text":"<p>Bleaches out pixels, imitating snow.</p> <p>Parameters:</p> Name Type Description <code>img</code> <code>np.ndarray</code> <p>Input image.</p> <code>snow_point</code> <code>float</code> <p>A float in the range [0, 1], scaled and adjusted to determine the threshold for pixel modification.</p> <code>brightness_coeff</code> <code>float</code> <p>Coefficient applied to increase the brightness of pixels below the snow_point threshold. Larger values lead to more pronounced snow effects.</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Image with simulated snow effect.</p> <p>Reference</p> <p>https://github.com/UjjwalSaxena/Automold--Road-Augmentation-Library</p> Source code in <code>albumentations/augmentations/functional.py</code> Python<pre><code>@preserve_channel_dim\ndef add_snow(img: np.ndarray, snow_point: float, brightness_coeff: float) -&gt; np.ndarray:\n    \"\"\"Bleaches out pixels, imitating snow.\n\n    Args:\n        img (np.ndarray): Input image.\n        snow_point (float): A float in the range [0, 1], scaled and adjusted to determine\n            the threshold for pixel modification.\n        brightness_coeff (float): Coefficient applied to increase the brightness of pixels below the snow_point\n            threshold. Larger values lead to more pronounced snow effects.\n\n    Returns:\n        np.ndarray: Image with simulated snow effect.\n\n    Reference:\n        https://github.com/UjjwalSaxena/Automold--Road-Augmentation-Library\n\n    \"\"\"\n    non_rgb_warning(img)\n\n    input_dtype = img.dtype\n    needs_float = False\n\n    snow_point *= 127.5  # = 255 / 2\n    snow_point += 85  # = 255 / 3\n\n    if input_dtype == np.float32:\n        img = from_float(img, dtype=np.dtype(\"uint8\"))\n        needs_float = True\n\n    image_hls = cv2.cvtColor(img, cv2.COLOR_RGB2HLS)\n    image_hls = np.array(image_hls, dtype=np.float32)\n\n    image_hls[:, :, 1][image_hls[:, :, 1] &lt; snow_point] *= brightness_coeff\n\n    image_hls[:, :, 1] = clip(image_hls[:, :, 1], np.uint8)\n\n    image_hls = np.array(image_hls, dtype=np.uint8)\n\n    image_rgb = cv2.cvtColor(image_hls, cv2.COLOR_HLS2RGB)\n\n    return to_float(image_rgb, max_value=255) if needs_float else image_rgb\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.functional.add_sun_flare","title":"<code>def add_sun_flare    (img, flare_center, src_radius, src_color, circles)    </code> [view source on GitHub]","text":"<p>Add a sun flare effect to an image.</p> <p>Parameters:</p> Name Type Description <code>img</code> <code>np.ndarray</code> <p>The input image.</p> <code>flare_center</code> <code>tuple[float, float]</code> <p>(x, y) coordinates of the flare center</p> <code>src_radius</code> <code>int</code> <p>The radius of the source of the flare.</p> <code>src_color</code> <code>ColorType</code> <p>The color of the flare, represented as a tuple of RGB values.</p> <code>circles</code> <code>list[Any]</code> <p>A list of tuples, each representing a circle that contributes to the flare effect. Each tuple contains the alpha value, the center coordinates, the radius, and the color of the circle.</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>The output image with the sun flare effect added.</p> <p>Reference</p> <p>https://github.com/UjjwalSaxena/Automold--Road-Augmentation-Library</p> Source code in <code>albumentations/augmentations/functional.py</code> Python<pre><code>@preserve_channel_dim\ndef add_sun_flare(\n    img: np.ndarray,\n    flare_center: tuple[float, float],\n    src_radius: int,\n    src_color: ColorType,\n    circles: list[Any],\n) -&gt; np.ndarray:\n    \"\"\"Add a sun flare effect to an image.\n\n    Args:\n        img (np.ndarray): The input image.\n        flare_center (tuple[float, float]): (x, y) coordinates of the flare center\n        src_radius (int): The radius of the source of the flare.\n        src_color (ColorType): The color of the flare, represented as a tuple of RGB values.\n        circles (list[Any]): A list of tuples, each representing a circle that contributes to the flare effect.\n            Each tuple contains the alpha value, the center coordinates, the radius, and the color of the circle.\n\n    Returns:\n        np.ndarray: The output image with the sun flare effect added.\n\n    Reference:\n        https://github.com/UjjwalSaxena/Automold--Road-Augmentation-Library\n    \"\"\"\n    non_rgb_warning(img)\n\n    input_dtype = img.dtype\n    needs_float = False\n\n    if input_dtype == np.float32:\n        img = from_float(img, dtype=np.dtype(\"uint8\"))\n        needs_float = True\n\n    overlay = img.copy()\n    output = img.copy()\n\n    for alpha, (x, y), rad3, (r_color, g_color, b_color) in circles:\n        cv2.circle(overlay, (x, y), rad3, (r_color, g_color, b_color), -1)\n        output = add_weighted(overlay, alpha, output, 1 - alpha)\n\n    point = [int(x) for x in flare_center]\n\n    overlay = output.copy()\n    num_times = src_radius // 10\n    alpha = np.linspace(0.0, 1, num=num_times)\n    rad = np.linspace(1, src_radius, num=num_times)\n    for i in range(num_times):\n        cv2.circle(overlay, point, int(rad[i]), src_color, -1)\n        alp = alpha[num_times - i - 1] * alpha[num_times - i - 1] * alpha[num_times - i - 1]\n        output = add_weighted(overlay, alp, output, 1 - alp)\n\n    return to_float(output, max_value=255) if needs_float else output\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.functional.almost_equal_intervals","title":"<code>def almost_equal_intervals    (n, parts)    </code> [view source on GitHub]","text":"<p>Generates an array of nearly equal integer intervals that sum up to <code>n</code>.</p> <p>This function divides the number <code>n</code> into <code>parts</code> nearly equal parts. It ensures that the sum of all parts equals <code>n</code>, and the difference between any two parts is at most one. This is useful for distributing a total amount into nearly equal discrete parts.</p> <p>Parameters:</p> Name Type Description <code>n</code> <code>int</code> <p>The total value to be split.</p> <code>parts</code> <code>int</code> <p>The number of parts to split into.</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>An array of integers where each integer represents the size of a part.</p> <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; almost_equal_intervals(20, 3)\narray([7, 7, 6])  # Splits 20 into three parts: 7, 7, and 6\n&gt;&gt;&gt; almost_equal_intervals(16, 4)\narray([4, 4, 4, 4])  # Splits 16 into four equal parts\n</code></pre> Source code in <code>albumentations/augmentations/functional.py</code> Python<pre><code>def almost_equal_intervals(n: int, parts: int) -&gt; np.ndarray:\n    \"\"\"Generates an array of nearly equal integer intervals that sum up to `n`.\n\n    This function divides the number `n` into `parts` nearly equal parts. It ensures that\n    the sum of all parts equals `n`, and the difference between any two parts is at most one.\n    This is useful for distributing a total amount into nearly equal discrete parts.\n\n    Args:\n        n (int): The total value to be split.\n        parts (int): The number of parts to split into.\n\n    Returns:\n        np.ndarray: An array of integers where each integer represents the size of a part.\n\n    Example:\n        &gt;&gt;&gt; almost_equal_intervals(20, 3)\n        array([7, 7, 6])  # Splits 20 into three parts: 7, 7, and 6\n        &gt;&gt;&gt; almost_equal_intervals(16, 4)\n        array([4, 4, 4, 4])  # Splits 16 into four equal parts\n    \"\"\"\n    part_size, remainder = divmod(n, parts)\n    # Create an array with the base part size and adjust the first `remainder` parts by adding 1\n    return np.array([part_size + 1 if i &lt; remainder else part_size for i in range(parts)])\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.functional.bbox_from_mask","title":"<code>def bbox_from_mask    (mask)    </code> [view source on GitHub]","text":"<p>Create bounding box from binary mask (fast version)</p> <p>Parameters:</p> Name Type Description <code>mask</code> <code>numpy.ndarray</code> <p>binary mask.</p> <p>Returns:</p> Type Description <code>tuple</code> <p>A bounding box tuple <code>(x_min, y_min, x_max, y_max)</code>.</p> Source code in <code>albumentations/augmentations/functional.py</code> Python<pre><code>def bbox_from_mask(mask: np.ndarray) -&gt; tuple[int, int, int, int]:\n    \"\"\"Create bounding box from binary mask (fast version)\n\n    Args:\n        mask (numpy.ndarray): binary mask.\n\n    Returns:\n        tuple: A bounding box tuple `(x_min, y_min, x_max, y_max)`.\n\n    \"\"\"\n    rows = np.any(mask, axis=1)\n    if not rows.any():\n        return -1, -1, -1, -1\n    cols = np.any(mask, axis=0)\n    y_min, y_max = np.where(rows)[0][[0, -1]]\n    x_min, x_max = np.where(cols)[0][[0, -1]]\n    return x_min, y_min, x_max + 1, y_max + 1\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.functional.center","title":"<code>def center    (image_shape)    </code> [view source on GitHub]","text":"<p>Calculate the center coordinates if image. Used by images, masks and keypoints.</p> <p>Parameters:</p> Name Type Description <code>image_shape</code> <code>tuple[int, int]</code> <p>The shape of the image.</p> <p>Returns:</p> Type Description <code>tuple[float, float]</code> <p>The center coordinates.</p> Source code in <code>albumentations/augmentations/functional.py</code> Python<pre><code>def center(image_shape: tuple[int, int]) -&gt; tuple[float, float]:\n    \"\"\"Calculate the center coordinates if image. Used by images, masks and keypoints.\n\n    Args:\n        image_shape (tuple[int, int]): The shape of the image.\n\n    Returns:\n        tuple[float, float]: The center coordinates.\n    \"\"\"\n    height, width = image_shape[:2]\n    return width / 2 - 0.5, height / 2 - 0.5\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.functional.center_bbox","title":"<code>def center_bbox    (image_shape)    </code> [view source on GitHub]","text":"<p>Calculate the center coordinates for of image for bounding boxes.</p> <p>Parameters:</p> Name Type Description <code>image_shape</code> <code>tuple[int, int]</code> <p>The shape of the image.</p> <p>Returns:</p> Type Description <code>tuple[float, float]</code> <p>The center coordinates.</p> Source code in <code>albumentations/augmentations/functional.py</code> Python<pre><code>def center_bbox(image_shape: tuple[int, int]) -&gt; tuple[float, float]:\n    \"\"\"Calculate the center coordinates for of image for bounding boxes.\n\n    Args:\n        image_shape (tuple[int, int]): The shape of the image.\n\n    Returns:\n        tuple[float, float]: The center coordinates.\n    \"\"\"\n    height, width = image_shape[:2]\n    return width / 2, height / 2\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.functional.create_shape_groups","title":"<code>def create_shape_groups    (tiles)    </code> [view source on GitHub]","text":"<p>Groups tiles by their shape and stores the indices for each shape.</p> Source code in <code>albumentations/augmentations/functional.py</code> Python<pre><code>def create_shape_groups(tiles: np.ndarray) -&gt; dict[tuple[int, int], list[int]]:\n    \"\"\"Groups tiles by their shape and stores the indices for each shape.\"\"\"\n    shape_groups = defaultdict(list)\n    for index, (start_y, start_x, end_y, end_x) in enumerate(tiles):\n        shape = (end_y - start_y, end_x - start_x)\n        shape_groups[shape].append(index)\n    return shape_groups\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.functional.fancy_pca","title":"<code>def fancy_pca    (img, alpha=0.1)    </code> [view source on GitHub]","text":"<p>Perform 'Fancy PCA' augmentation</p> <p>Parameters:</p> Name Type Description <code>img</code> <code>np.ndarray</code> <p>numpy array with (h, w, rgb) shape, as ints between 0-255</p> <code>alpha</code> <code>float</code> <p>how much to perturb/scale the eigen vectors and values     the paper used std=0.1</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>numpy image-like array as uint8 range(0, 255)</p> <p>Reference</p> <p>http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf</p> Source code in <code>albumentations/augmentations/functional.py</code> Python<pre><code>@clipped\ndef fancy_pca(img: np.ndarray, alpha: float = 0.1) -&gt; np.ndarray:\n    \"\"\"Perform 'Fancy PCA' augmentation\n\n    Args:\n        img: numpy array with (h, w, rgb) shape, as ints between 0-255\n        alpha: how much to perturb/scale the eigen vectors and values\n                the paper used std=0.1\n\n    Returns:\n        numpy image-like array as uint8 range(0, 255)\n\n    Reference:\n        http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf\n    \"\"\"\n    if not is_rgb_image(img) or img.dtype != np.uint8:\n        msg = \"Image must be RGB image in uint8 format.\"\n        raise TypeError(msg)\n\n    orig_img = img.astype(float).copy()\n\n    img = to_float(img)  # rescale to 0 to 1 range\n\n    # flatten image to columns of RGB\n    img_rs = img.reshape(-1, 3)\n    # img_rs shape (640000, 3)\n\n    # center mean\n    img_centered = img_rs - np.mean(img_rs, axis=0)\n\n    # paper says 3x3 covariance matrix\n    img_cov = np.cov(img_centered, rowvar=False)\n\n    # eigen values and eigen vectors\n    eig_vals, eig_vecs = np.linalg.eigh(img_cov)\n\n    # sort values and vector\n    sort_perm = eig_vals[::-1].argsort()\n    eig_vals[::-1].sort()\n    eig_vecs = eig_vecs[:, sort_perm]\n\n    # &gt; get [p1, p2, p3]\n    m1 = np.column_stack(eig_vecs)\n\n    # get 3x1 matrix of eigen values multiplied by random variable draw from normal\n    # distribution with mean of 0 and standard deviation of 0.1\n    m2 = np.zeros((3, 1))\n    # according to the paper alpha should only be draw once per augmentation (not once per channel)\n    # &gt; alpha = np.random.normal(0, alpha_std)\n\n    # broad cast to speed things up\n    m2[:, 0] = alpha * eig_vals[:]\n\n    # this is the vector that we're going to add to each pixel in a moment\n    add_vect = np.array(m1) @ np.array(m2)\n\n    for idx in range(3):  # RGB\n        orig_img[..., idx] += add_vect[idx] * 255\n\n    # for image processing it was found that working with float 0.0 to 1.0\n    # was easier than integers between 0-255\n    return orig_img\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.functional.generate_shuffled_splits","title":"<code>def generate_shuffled_splits    (size, divisions, random_state=None)    </code> [view source on GitHub]","text":"<p>Generate shuffled splits for a given dimension size and number of divisions.</p> <p>Parameters:</p> Name Type Description <code>size</code> <code>int</code> <p>Total size of the dimension (height or width).</p> <code>divisions</code> <code>int</code> <p>Number of divisions (rows or columns).</p> <code>random_state</code> <code>Optional[np.random.RandomState]</code> <p>Seed for the random number generator for reproducibility.</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Cumulative edges of the shuffled intervals.</p> Source code in <code>albumentations/augmentations/functional.py</code> Python<pre><code>def generate_shuffled_splits(\n    size: int,\n    divisions: int,\n    random_state: np.random.RandomState | None = None,\n) -&gt; np.ndarray:\n    \"\"\"Generate shuffled splits for a given dimension size and number of divisions.\n\n    Args:\n        size (int): Total size of the dimension (height or width).\n        divisions (int): Number of divisions (rows or columns).\n        random_state (Optional[np.random.RandomState]): Seed for the random number generator for reproducibility.\n\n    Returns:\n        np.ndarray: Cumulative edges of the shuffled intervals.\n    \"\"\"\n    intervals = almost_equal_intervals(size, divisions)\n    intervals = random_utils.shuffle(intervals, random_state=random_state)\n    return np.insert(np.cumsum(intervals), 0, 0)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.functional.grayscale_to_multichannel","title":"<code>def grayscale_to_multichannel    (grayscale_image, num_output_channels=3)    </code> [view source on GitHub]","text":"<p>Convert a grayscale image to a multi-channel image.</p> <p>This function takes a 2D grayscale image or a 3D image with a single channel and converts it to a multi-channel image by repeating the grayscale data across the specified number of channels.</p> <p>Parameters:</p> Name Type Description <code>grayscale_image</code> <code>np.ndarray</code> <p>Input grayscale image. Can be 2D (height, width)                           or 3D (height, width, 1).</p> <code>num_output_channels</code> <code>int</code> <p>Number of channels in the output image. Defaults to 3.</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Multi-channel image with shape (height, width, num_channels).</p> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If the input is not a 2D grayscale image or 3D with shape (height, width, 1).</p> <p>Note</p> <p>If the input is already a multi-channel image with the desired number of channels, it will be returned unchanged.</p> Source code in <code>albumentations/augmentations/functional.py</code> Python<pre><code>def grayscale_to_multichannel(grayscale_image: np.ndarray, num_output_channels: int = 3) -&gt; np.ndarray:\n    \"\"\"Convert a grayscale image to a multi-channel image.\n\n    This function takes a 2D grayscale image or a 3D image with a single channel\n    and converts it to a multi-channel image by repeating the grayscale data\n    across the specified number of channels.\n\n    Args:\n        grayscale_image (np.ndarray): Input grayscale image. Can be 2D (height, width)\n                                      or 3D (height, width, 1).\n        num_output_channels (int, optional): Number of channels in the output image. Defaults to 3.\n\n    Returns:\n        np.ndarray: Multi-channel image with shape (height, width, num_channels).\n\n    Raises:\n        ValueError: If the input is not a 2D grayscale image or 3D with shape (height, width, 1).\n\n    Note:\n        If the input is already a multi-channel image with the desired number of channels,\n        it will be returned unchanged.\n    \"\"\"\n    grayscale_image = grayscale_image.copy().squeeze()\n    return np.stack([grayscale_image] * num_output_channels, axis=-1)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.functional.iso_noise","title":"<code>def iso_noise    (image, color_shift=0.05, intensity=0.5, random_state=None)    </code> [view source on GitHub]","text":"<p>Apply poisson noise to an image to simulate camera sensor noise.</p> <p>Parameters:</p> Name Type Description <code>image</code> <code>np.ndarray</code> <p>Input image. Currently, only RGB images are supported.</p> <code>color_shift</code> <code>float</code> <p>The amount of color shift to apply. Default is 0.05.</p> <code>intensity</code> <code>float</code> <p>Multiplication factor for noise values. Values of ~0.5 produce a noticeable,                yet acceptable level of noise. Default is 0.5.</p> <code>random_state</code> <code>Optional[np.random.RandomState]</code> <p>If specified, this will be random state used for noise generation.</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>The noised image.</p> <p>Exceptions:</p> Type Description <code>TypeError</code> <p>If the input image's dtype is not RGB.</p> Source code in <code>albumentations/augmentations/functional.py</code> Python<pre><code>@clipped\ndef iso_noise(\n    image: np.ndarray,\n    color_shift: float = 0.05,\n    intensity: float = 0.5,\n    random_state: np.random.RandomState | None = None,\n) -&gt; np.ndarray:\n    \"\"\"Apply poisson noise to an image to simulate camera sensor noise.\n\n    Args:\n        image (np.ndarray): Input image. Currently, only RGB images are supported.\n        color_shift (float): The amount of color shift to apply. Default is 0.05.\n        intensity (float): Multiplication factor for noise values. Values of ~0.5 produce a noticeable,\n                           yet acceptable level of noise. Default is 0.5.\n        random_state (Optional[np.random.RandomState]): If specified, this will be random state used\n            for noise generation.\n\n    Returns:\n        np.ndarray: The noised image.\n\n    Raises:\n        TypeError: If the input image's dtype is not RGB.\n    \"\"\"\n    if not is_rgb_image(image):\n        msg = \"Image must be RGB\"\n        raise TypeError(msg)\n\n    input_dtype = image.dtype\n    factor = 1\n\n    if input_dtype == np.uint8:\n        image = to_float(image)\n        factor = MAX_VALUES_BY_DTYPE[input_dtype]\n\n    hls = cv2.cvtColor(image, cv2.COLOR_RGB2HLS)\n    _, stddev = cv2.meanStdDev(hls)\n\n    luminance_noise = random_utils.poisson(stddev[1] * intensity * 255, size=hls.shape[:2], random_state=random_state)\n    color_noise = random_utils.normal(0, color_shift * 360 * intensity, size=hls.shape[:2], random_state=random_state)\n\n    hue = hls[..., 0]\n    hue += color_noise\n    hue %= 360\n\n    luminance = hls[..., 1]\n    luminance += (luminance_noise / 255) * (1.0 - luminance)\n\n    return cv2.cvtColor(hls, cv2.COLOR_HLS2RGB) * factor\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.functional.mask_from_bbox","title":"<code>def mask_from_bbox    (img, bbox)    </code> [view source on GitHub]","text":"<p>Create binary mask from bounding box</p> <p>Parameters:</p> Name Type Description <code>img</code> <code>np.ndarray</code> <p>input image</p> <code>bbox</code> <code>tuple[int, int, int, int]</code> <p>A bounding box tuple <code>(x_min, y_min, x_max, y_max)</code></p> <p>Returns:</p> Type Description <code>mask</code> <p>binary mask</p> Source code in <code>albumentations/augmentations/functional.py</code> Python<pre><code>def mask_from_bbox(img: np.ndarray, bbox: tuple[int, int, int, int]) -&gt; np.ndarray:\n    \"\"\"Create binary mask from bounding box\n\n    Args:\n        img: input image\n        bbox: A bounding box tuple `(x_min, y_min, x_max, y_max)`\n\n    Returns:\n        mask: binary mask\n\n    \"\"\"\n    mask = np.zeros(img.shape[:2], dtype=np.uint8)\n    x_min, y_min, x_max, y_max = bbox\n    mask[y_min:y_max, x_min:x_max] = 1\n    return mask\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.functional.move_tone_curve","title":"<code>def move_tone_curve    (img, low_y, high_y)    </code> [view source on GitHub]","text":"<p>Rescales the relationship between bright and dark areas of the image by manipulating its tone curve.</p> <p>Parameters:</p> Name Type Description <code>img</code> <code>np.ndarray</code> <p>np.ndarray. Any number of channels</p> <code>low_y</code> <code>float | np.ndarray</code> <p>per-channel or single y-position of a Bezier control point used to adjust the tone curve, must be in range [0, 1]</p> <code>high_y</code> <code>float | np.ndarray</code> <p>per-channel or single y-position of a Bezier control point used to adjust image tone curve, must be in range [0, 1]</p> Source code in <code>albumentations/augmentations/functional.py</code> Python<pre><code>@preserve_channel_dim\ndef move_tone_curve(\n    img: np.ndarray,\n    low_y: float | np.ndarray,\n    high_y: float | np.ndarray,\n) -&gt; np.ndarray:\n    \"\"\"Rescales the relationship between bright and dark areas of the image by manipulating its tone curve.\n\n    Args:\n        img: np.ndarray. Any number of channels\n        low_y: per-channel or single y-position of a Bezier control point used\n            to adjust the tone curve, must be in range [0, 1]\n        high_y: per-channel or single y-position of a Bezier control point used\n            to adjust image tone curve, must be in range [0, 1]\n\n    \"\"\"\n    input_dtype = img.dtype\n    needs_float = False\n\n    if input_dtype in [np.float32, np.float64, np.float16]:\n        img = from_float(img, dtype=np.uint8)\n        needs_float = True\n\n    t = np.linspace(0.0, 1.0, 256)\n\n    def evaluate_bez(t: np.ndarray, low_y: float | np.ndarray, high_y: float | np.ndarray) -&gt; np.ndarray:\n        one_minus_t = 1 - t\n        return (3 * one_minus_t**2 * t * low_y + 3 * one_minus_t * t**2 * high_y + t**3) * 255\n\n    num_channels = get_num_channels(img)\n\n    if np.isscalar(low_y) and np.isscalar(high_y):\n        lut = clip(np.rint(evaluate_bez(t, low_y, high_y)), np.uint8)\n        output = cv2.LUT(img, lut)\n    elif isinstance(low_y, np.ndarray) and isinstance(high_y, np.ndarray):\n        luts = clip(np.rint(evaluate_bez(t[:, np.newaxis], low_y, high_y).T), np.uint8)\n        output = cv2.merge([cv2.LUT(img[:, :, i], luts[i]) for i in range(num_channels)])\n    else:\n        raise TypeError(\n            f\"low_y and high_y must both be of type float or np.ndarray. Got {type(low_y)} and {type(high_y)}\",\n        )\n\n    return to_float(output, max_value=255) if needs_float else output\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.functional.posterize","title":"<code>def posterize    (img, bits)    </code> [view source on GitHub]","text":"<p>Reduce the number of bits for each color channel.</p> <p>Parameters:</p> Name Type Description <code>img</code> <code>np.ndarray</code> <p>image to posterize.</p> <code>bits</code> <code>int</code> <p>number of high bits. Must be in range [0, 8]</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Image with reduced color channels.</p> Source code in <code>albumentations/augmentations/functional.py</code> Python<pre><code>@preserve_channel_dim\ndef posterize(img: np.ndarray, bits: int) -&gt; np.ndarray:\n    \"\"\"Reduce the number of bits for each color channel.\n\n    Args:\n        img: image to posterize.\n        bits: number of high bits. Must be in range [0, 8]\n\n    Returns:\n        Image with reduced color channels.\n\n    \"\"\"\n    bits_array = np.uint8(bits)\n\n    if img.dtype != np.uint8:\n        msg = \"Image must have uint8 channel type\"\n        raise TypeError(msg)\n    if np.any((bits_array &lt; 0) | (bits_array &gt; EIGHT)):\n        msg = \"bits must be in range [0, 8]\"\n        raise ValueError(msg)\n\n    if not bits_array.shape or len(bits_array) == 1:\n        if bits_array == 0:\n            return np.zeros_like(img)\n        if bits_array == EIGHT:\n            return img.copy()\n\n        lut = np.arange(0, 256, dtype=np.uint8)\n        mask = ~np.uint8(2 ** (8 - bits_array) - 1)\n        lut &amp;= mask\n\n        return cv2.LUT(img, lut)\n\n    if not is_rgb_image(img):\n        msg = \"If bits is iterable image must be RGB\"\n        raise TypeError(msg)\n\n    result_img = np.empty_like(img)\n    for i, channel_bits in enumerate(bits_array):\n        if channel_bits == 0:\n            result_img[..., i] = np.zeros_like(img[..., i])\n        elif channel_bits == EIGHT:\n            result_img[..., i] = img[..., i].copy()\n        else:\n            lut = np.arange(0, 256, dtype=np.uint8)\n            mask = ~np.uint8(2 ** (8 - channel_bits) - 1)\n            lut &amp;= mask\n\n            result_img[..., i] = cv2.LUT(img[..., i], lut)\n\n    return result_img\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.functional.shuffle_tiles_within_shape_groups","title":"<code>def shuffle_tiles_within_shape_groups    (shape_groups, random_state=None)    </code> [view source on GitHub]","text":"<p>Shuffles indices within each group of similar shapes and creates a list where each index points to the index of the tile it should be mapped to.</p> <p>Parameters:</p> Name Type Description <code>shape_groups</code> <code>dict[tuple[int, int], list[int]]</code> <p>Groups of tile indices categorized by shape.</p> <code>random_state</code> <code>Optional[np.random.RandomState]</code> <p>Seed for the random number generator for reproducibility.</p> <p>Returns:</p> Type Description <code>list[int]</code> <p>A list where each index is mapped to the new index of the tile after shuffling.</p> Source code in <code>albumentations/augmentations/functional.py</code> Python<pre><code>def shuffle_tiles_within_shape_groups(\n    shape_groups: dict[tuple[int, int], list[int]],\n    random_state: np.random.RandomState | None = None,\n) -&gt; list[int]:\n    \"\"\"Shuffles indices within each group of similar shapes and creates a list where each\n    index points to the index of the tile it should be mapped to.\n\n    Args:\n        shape_groups (dict[tuple[int, int], list[int]]): Groups of tile indices categorized by shape.\n        random_state (Optional[np.random.RandomState]): Seed for the random number generator for reproducibility.\n\n    Returns:\n        list[int]: A list where each index is mapped to the new index of the tile after shuffling.\n    \"\"\"\n    # Initialize the output list with the same size as the total number of tiles, filled with -1\n    num_tiles = sum(len(indices) for indices in shape_groups.values())\n    mapping = [-1] * num_tiles\n\n    # Prepare the random number generator\n\n    for indices in shape_groups.values():\n        shuffled_indices = random_utils.shuffle(indices.copy(), random_state=random_state)\n        for old, new in zip(indices, shuffled_indices):\n            mapping[old] = new\n\n    return mapping\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.functional.solarize","title":"<code>def solarize    (img, threshold=128)    </code> [view source on GitHub]","text":"<p>Invert all pixel values above a threshold.</p> <p>Parameters:</p> Name Type Description <code>img</code> <code>np.ndarray</code> <p>The image to solarize.</p> <code>threshold</code> <code>int</code> <p>All pixels above this grayscale level are inverted.</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Solarized image.</p> Source code in <code>albumentations/augmentations/functional.py</code> Python<pre><code>def solarize(img: np.ndarray, threshold: int = 128) -&gt; np.ndarray:\n    \"\"\"Invert all pixel values above a threshold.\n\n    Args:\n        img: The image to solarize.\n        threshold: All pixels above this grayscale level are inverted.\n\n    Returns:\n        Solarized image.\n\n    \"\"\"\n    dtype = img.dtype\n    max_val = MAX_VALUES_BY_DTYPE[dtype]\n\n    if dtype == np.uint8:\n        lut = [(i if i &lt; threshold else max_val - i) for i in range(int(max_val) + 1)]\n\n        prev_shape = img.shape\n        img = cv2.LUT(img, np.array(lut, dtype=dtype))\n\n        if len(prev_shape) != len(img.shape):\n            img = np.expand_dims(img, -1)\n        return img\n\n    result_img = img.copy()\n    cond = img &gt;= threshold\n    result_img[cond] = max_val - result_img[cond]\n    return result_img\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.functional.split_uniform_grid","title":"<code>def split_uniform_grid    (image_shape, grid, random_state=None)    </code> [view source on GitHub]","text":"<p>Splits an image shape into a uniform grid specified by the grid dimensions.</p> <p>Parameters:</p> Name Type Description <code>image_shape</code> <code>tuple[int, int]</code> <p>The shape of the image as (height, width).</p> <code>grid</code> <code>tuple[int, int]</code> <p>The grid size as (rows, columns).</p> <code>random_state</code> <code>Optional[np.random.RandomState]</code> <p>The random state to use for shuffling the splits. If None, the splits are not shuffled.</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>An array containing the tiles' coordinates in the format (start_y, start_x, end_y, end_x).</p> <p>Note</p> <p>The function uses <code>generate_shuffled_splits</code> to generate the splits for the height and width of the image. The splits are then used to calculate the coordinates of the tiles.</p> Source code in <code>albumentations/augmentations/functional.py</code> Python<pre><code>def split_uniform_grid(\n    image_shape: tuple[int, int],\n    grid: tuple[int, int],\n    random_state: np.random.RandomState | None = None,\n) -&gt; np.ndarray:\n    \"\"\"Splits an image shape into a uniform grid specified by the grid dimensions.\n\n    Args:\n        image_shape (tuple[int, int]): The shape of the image as (height, width).\n        grid (tuple[int, int]): The grid size as (rows, columns).\n        random_state (Optional[np.random.RandomState]): The random state to use for shuffling the splits.\n            If None, the splits are not shuffled.\n\n    Returns:\n        np.ndarray: An array containing the tiles' coordinates in the format (start_y, start_x, end_y, end_x).\n\n    Note:\n        The function uses `generate_shuffled_splits` to generate the splits for the height and width of the image.\n        The splits are then used to calculate the coordinates of the tiles.\n    \"\"\"\n    n_rows, n_cols = grid\n\n    height_splits = generate_shuffled_splits(image_shape[0], grid[0], random_state)\n    width_splits = generate_shuffled_splits(image_shape[1], grid[1], random_state)\n\n    # Calculate tiles coordinates\n    tiles = [\n        (height_splits[i], width_splits[j], height_splits[i + 1], width_splits[j + 1])\n        for i in range(n_rows)\n        for j in range(n_cols)\n    ]\n\n    return np.array(tiles)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.functional.swap_tiles_on_image","title":"<code>def swap_tiles_on_image    (image, tiles, mapping=None)    </code> [view source on GitHub]","text":"<p>Swap tiles on the image according to the new format.</p> <p>Parameters:</p> Name Type Description <code>image</code> <code>np.ndarray</code> <p>Input image.</p> <code>tiles</code> <code>np.ndarray</code> <p>Array of tiles with each tile as [start_y, start_x, end_y, end_x].</p> <code>mapping</code> <code>list[int] | None</code> <p>list of new tile indices.</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Output image with tiles swapped according to the random shuffle.</p> Source code in <code>albumentations/augmentations/functional.py</code> Python<pre><code>def swap_tiles_on_image(image: np.ndarray, tiles: np.ndarray, mapping: list[int] | None = None) -&gt; np.ndarray:\n    \"\"\"Swap tiles on the image according to the new format.\n\n    Args:\n        image: Input image.\n        tiles: Array of tiles with each tile as [start_y, start_x, end_y, end_x].\n        mapping: list of new tile indices.\n\n    Returns:\n        np.ndarray: Output image with tiles swapped according to the random shuffle.\n    \"\"\"\n    # If no tiles are provided, return a copy of the original image\n    if tiles.size == 0 or mapping is None:\n        return image.copy()\n\n    # Create a copy of the image to retain original for reference\n    new_image = np.empty_like(image)\n    for num, new_index in enumerate(mapping):\n        start_y, start_x, end_y, end_x = tiles[new_index]\n        start_y_orig, start_x_orig, end_y_orig, end_x_orig = tiles[num]\n        # Assign the corresponding tile from the original image to the new image\n        new_image[start_y:end_y, start_x:end_x] = image[start_y_orig:end_y_orig, start_x_orig:end_x_orig]\n\n    return new_image\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.functional.to_gray_average","title":"<code>def to_gray_average    (img)    </code> [view source on GitHub]","text":"<p>Convert an image to grayscale using the average method.</p> <p>This function computes the arithmetic mean across all channels for each pixel, resulting in a grayscale representation of the image.</p> <p>Key aspects of this method: 1. It treats all channels equally, regardless of their perceptual importance. 2. Works with any number of channels, making it versatile for various image types. 3. Simple and fast to compute, but may not accurately represent perceived brightness. 4. For RGB images, the formula is: Gray = (R + G + B) / 3</p> <p>Note: This method may produce different results compared to weighted methods (like RGB weighted average) which account for human perception of color brightness. It may also produce unexpected results for images with alpha channels or non-color data in additional channels.</p> <p>Parameters:</p> Name Type Description <code>img</code> <code>np.ndarray</code> <p>Input image as a numpy array. Can be any number of channels.</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Grayscale image as a 2D numpy array. The output data type             matches the input data type.</p> <p>Image types:     uint8, float32</p> <p>Number of channels:     any</p> Source code in <code>albumentations/augmentations/functional.py</code> Python<pre><code>def to_gray_average(img: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Convert an image to grayscale using the average method.\n\n    This function computes the arithmetic mean across all channels for each pixel,\n    resulting in a grayscale representation of the image.\n\n    Key aspects of this method:\n    1. It treats all channels equally, regardless of their perceptual importance.\n    2. Works with any number of channels, making it versatile for various image types.\n    3. Simple and fast to compute, but may not accurately represent perceived brightness.\n    4. For RGB images, the formula is: Gray = (R + G + B) / 3\n\n    Note: This method may produce different results compared to weighted methods\n    (like RGB weighted average) which account for human perception of color brightness.\n    It may also produce unexpected results for images with alpha channels or\n    non-color data in additional channels.\n\n    Args:\n        img (np.ndarray): Input image as a numpy array. Can be any number of channels.\n\n    Returns:\n        np.ndarray: Grayscale image as a 2D numpy array. The output data type\n                    matches the input data type.\n\n    Image types:\n        uint8, float32\n\n    Number of channels:\n        any\n    \"\"\"\n    return np.mean(img, axis=-1).astype(img.dtype)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.functional.to_gray_desaturation","title":"<code>def to_gray_desaturation    (img)    </code> [view source on GitHub]","text":"<p>Convert an image to grayscale using the desaturation method.</p> <p>Parameters:</p> Name Type Description <code>img</code> <code>np.ndarray</code> <p>Input image as a numpy array.</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Grayscale image as a 2D numpy array.</p> <p>Image types:     uint8, float32</p> <p>Number of channels:     any</p> Source code in <code>albumentations/augmentations/functional.py</code> Python<pre><code>@clipped\ndef to_gray_desaturation(img: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Convert an image to grayscale using the desaturation method.\n\n    Args:\n        img (np.ndarray): Input image as a numpy array.\n\n    Returns:\n        np.ndarray: Grayscale image as a 2D numpy array.\n\n    Image types:\n        uint8, float32\n\n    Number of channels:\n        any\n    \"\"\"\n    float_image = img.astype(np.float32)\n    return (np.max(float_image, axis=-1) + np.min(float_image, axis=-1)) / 2\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.functional.to_gray_from_lab","title":"<code>def to_gray_from_lab    (img)    </code> [view source on GitHub]","text":"<p>Convert an RGB image to grayscale using the L channel from the LAB color space.</p> <p>This function converts the RGB image to the LAB color space and extracts the L channel. The LAB color space is designed to approximate human vision, where L represents lightness.</p> <p>Key aspects of this method: 1. The L channel represents the lightness of each pixel, ranging from 0 (black) to 100 (white). 2. It's more perceptually uniform than RGB, meaning equal changes in L values correspond to    roughly equal changes in perceived lightness. 3. The L channel is independent of the color information (A and B channels), making it    suitable for grayscale conversion.</p> <p>This method can be particularly useful when you want a grayscale image that closely matches human perception of lightness, potentially preserving more perceived contrast than simple RGB-based methods.</p> <p>Parameters:</p> Name Type Description <code>img</code> <code>np.ndarray</code> <p>Input RGB image as a numpy array.</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Grayscale image as a 2D numpy array, representing the L (lightness) channel.             Values are scaled to match the input image's data type range.</p> <p>Image types:     uint8, float32</p> <p>Number of channels:     3</p> Source code in <code>albumentations/augmentations/functional.py</code> Python<pre><code>@clipped\ndef to_gray_from_lab(img: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Convert an RGB image to grayscale using the L channel from the LAB color space.\n\n    This function converts the RGB image to the LAB color space and extracts the L channel.\n    The LAB color space is designed to approximate human vision, where L represents lightness.\n\n    Key aspects of this method:\n    1. The L channel represents the lightness of each pixel, ranging from 0 (black) to 100 (white).\n    2. It's more perceptually uniform than RGB, meaning equal changes in L values correspond to\n       roughly equal changes in perceived lightness.\n    3. The L channel is independent of the color information (A and B channels), making it\n       suitable for grayscale conversion.\n\n    This method can be particularly useful when you want a grayscale image that closely\n    matches human perception of lightness, potentially preserving more perceived contrast\n    than simple RGB-based methods.\n\n    Args:\n        img (np.ndarray): Input RGB image as a numpy array.\n\n    Returns:\n        np.ndarray: Grayscale image as a 2D numpy array, representing the L (lightness) channel.\n                    Values are scaled to match the input image's data type range.\n\n    Image types:\n        uint8, float32\n\n    Number of channels:\n        3\n    \"\"\"\n    dtype = img.dtype\n    img_uint8 = from_float(img, dtype=np.uint8) if dtype == np.float32 else img\n    result = cv2.cvtColor(img_uint8, cv2.COLOR_RGB2LAB)[..., 0]\n\n    return to_float(result) if dtype == np.float32 else result\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.functional.to_gray_max","title":"<code>def to_gray_max    (img)    </code> [view source on GitHub]","text":"<p>Convert an image to grayscale using the maximum channel value method.</p> <p>This function takes the maximum value across all channels for each pixel, resulting in a grayscale image that preserves the brightest parts of the original image.</p> <p>Key aspects of this method: 1. Works with any number of channels, making it versatile for various image types. 2. For 3-channel (e.g., RGB) images, this method is equivalent to extracting the V (Value)    channel from the HSV color space. 3. Preserves the brightest parts of the image but may lose some color contrast information. 4. Simple and fast to compute.</p> <p>Note: - This method tends to produce brighter grayscale images compared to other conversion methods,   as it always selects the highest intensity value from the channels. - For RGB images, it may not accurately represent perceived brightness as it doesn't   account for human color perception.</p> <p>Parameters:</p> Name Type Description <code>img</code> <code>np.ndarray</code> <p>Input image as a numpy array. Can be any number of channels.</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Grayscale image as a 2D numpy array. The output data type             matches the input data type.</p> <p>Image types:     uint8, float32</p> <p>Number of channels:     any</p> Source code in <code>albumentations/augmentations/functional.py</code> Python<pre><code>def to_gray_max(img: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Convert an image to grayscale using the maximum channel value method.\n\n    This function takes the maximum value across all channels for each pixel,\n    resulting in a grayscale image that preserves the brightest parts of the original image.\n\n    Key aspects of this method:\n    1. Works with any number of channels, making it versatile for various image types.\n    2. For 3-channel (e.g., RGB) images, this method is equivalent to extracting the V (Value)\n       channel from the HSV color space.\n    3. Preserves the brightest parts of the image but may lose some color contrast information.\n    4. Simple and fast to compute.\n\n    Note:\n    - This method tends to produce brighter grayscale images compared to other conversion methods,\n      as it always selects the highest intensity value from the channels.\n    - For RGB images, it may not accurately represent perceived brightness as it doesn't\n      account for human color perception.\n\n    Args:\n        img (np.ndarray): Input image as a numpy array. Can be any number of channels.\n\n    Returns:\n        np.ndarray: Grayscale image as a 2D numpy array. The output data type\n                    matches the input data type.\n\n    Image types:\n        uint8, float32\n\n    Number of channels:\n        any\n    \"\"\"\n    return np.max(img, axis=-1)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.functional.to_gray_pca","title":"<code>def to_gray_pca    (img)    </code> [view source on GitHub]","text":"<p>Convert an image to grayscale using Principal Component Analysis (PCA).</p> <p>This function applies PCA to reduce a multi-channel image to a single channel, effectively creating a grayscale representation that captures the maximum variance in the color data.</p> <p>Parameters:</p> Name Type Description <code>img</code> <code>np.ndarray</code> <p>Input image as a numpy array with shape (height, width, channels).</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Grayscale image as a 2D numpy array with shape (height, width).             If input is uint8, output is uint8 in range [0, 255].             If input is float32, output is float32 in range [0, 1].</p> <p>Note</p> <p>This method can potentially preserve more information from the original image compared to standard weighted average methods, as it accounts for the correlations between color channels.</p> <p>Image types:     uint8, float32</p> <p>Number of channels:     any</p> Source code in <code>albumentations/augmentations/functional.py</code> Python<pre><code>@clipped\ndef to_gray_pca(img: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Convert an image to grayscale using Principal Component Analysis (PCA).\n\n    This function applies PCA to reduce a multi-channel image to a single channel,\n    effectively creating a grayscale representation that captures the maximum variance\n    in the color data.\n\n    Args:\n        img (np.ndarray): Input image as a numpy array with shape (height, width, channels).\n\n    Returns:\n        np.ndarray: Grayscale image as a 2D numpy array with shape (height, width).\n                    If input is uint8, output is uint8 in range [0, 255].\n                    If input is float32, output is float32 in range [0, 1].\n\n    Note:\n        This method can potentially preserve more information from the original image\n        compared to standard weighted average methods, as it accounts for the\n        correlations between color channels.\n\n    Image types:\n        uint8, float32\n\n    Number of channels:\n        any\n    \"\"\"\n    dtype = img.dtype\n    # Reshape the image to a 2D array of pixels\n    pixels = img.reshape(-1, img.shape[2])\n\n    # Perform PCA\n    pca = PCA(n_components=1)\n    pca_result = pca.fit_transform(pixels)\n\n    # Reshape back to image dimensions and scale to 0-255\n    grayscale = pca_result.reshape(img.shape[:2])\n    grayscale = normalize_per_image(grayscale, \"min_max\")\n\n    return from_float(grayscale, dtype=np.uint8) if dtype == np.uint8 else grayscale\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.functional.to_gray_weighted_average","title":"<code>def to_gray_weighted_average    (img)    </code> [view source on GitHub]","text":"<p>Convert an RGB image to grayscale using the weighted average method.</p> <p>This function uses OpenCV's cvtColor function with COLOR_RGB2GRAY conversion, which applies the following formula: Y = 0.299R + 0.587G + 0.114*B</p> <p>Parameters:</p> Name Type Description <code>img</code> <code>np.ndarray</code> <p>Input RGB image as a numpy array.</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Grayscale image as a 2D numpy array.</p> <p>Image types:     uint8, float32</p> <p>Number of channels:     3</p> Source code in <code>albumentations/augmentations/functional.py</code> Python<pre><code>def to_gray_weighted_average(img: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Convert an RGB image to grayscale using the weighted average method.\n\n    This function uses OpenCV's cvtColor function with COLOR_RGB2GRAY conversion,\n    which applies the following formula:\n    Y = 0.299*R + 0.587*G + 0.114*B\n\n    Args:\n        img (np.ndarray): Input RGB image as a numpy array.\n\n    Returns:\n        np.ndarray: Grayscale image as a 2D numpy array.\n\n    Image types:\n        uint8, float32\n\n    Number of channels:\n        3\n    \"\"\"\n    return cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric","title":"<code>geometric</code>  <code>special</code>","text":""},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.functional","title":"<code>functional</code>","text":""},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.functional.bbox_d4","title":"<code>def bbox_d4    (bbox, group_member)    </code> [view source on GitHub]","text":"<p>Applies a <code>D_4</code> symmetry group transformation to a bounding box.</p> <p>The function transforms a bounding box according to the specified group member from the <code>D_4</code> group. These transformations include rotations and reflections, specified to work on an image's bounding box given its dimensions.</p> <ul> <li>bbox (BoxInternalType): The bounding box to transform. This should be a structure specifying coordinates     like (xmin, ymin, xmax, ymax).</li> <li>group_member (D4Type): A string identifier for the <code>D_4</code> group transformation to apply.     Valid values are 'e', 'r90', 'r180', 'r270', 'v', 'hvt', 'h', 't'.</li> </ul> <ul> <li>BoxInternalType: The transformed bounding box.</li> </ul> <ul> <li>ValueError: If an invalid group member is specified.</li> </ul> <p>Examples:</p> <ul> <li>Applying a 90-degree rotation:   <code>bbox_d4((10, 20, 110, 120), 'r90')</code>   This would rotate the bounding box 90 degrees within a 100x100 image.</li> </ul> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>def bbox_d4(\n    bbox: BoxInternalType,\n    group_member: D4Type,\n) -&gt; BoxInternalType:\n    \"\"\"Applies a `D_4` symmetry group transformation to a bounding box.\n\n    The function transforms a bounding box according to the specified group member from the `D_4` group.\n    These transformations include rotations and reflections, specified to work on an image's bounding box given\n    its dimensions.\n\n    Parameters:\n    - bbox (BoxInternalType): The bounding box to transform. This should be a structure specifying coordinates\n        like (xmin, ymin, xmax, ymax).\n    - group_member (D4Type): A string identifier for the `D_4` group transformation to apply.\n        Valid values are 'e', 'r90', 'r180', 'r270', 'v', 'hvt', 'h', 't'.\n\n    Returns:\n    - BoxInternalType: The transformed bounding box.\n\n    Raises:\n    - ValueError: If an invalid group member is specified.\n\n    Examples:\n    - Applying a 90-degree rotation:\n      `bbox_d4((10, 20, 110, 120), 'r90')`\n      This would rotate the bounding box 90 degrees within a 100x100 image.\n    \"\"\"\n    transformations = {\n        \"e\": lambda x: x,  # Identity transformation\n        \"r90\": lambda x: bbox_rot90(x, 1),  # Rotate 90 degrees\n        \"r180\": lambda x: bbox_rot90(x, 2),  # Rotate 180 degrees\n        \"r270\": lambda x: bbox_rot90(x, 3),  # Rotate 270 degrees\n        \"v\": lambda x: bbox_vflip(x),  # Vertical flip\n        \"hvt\": lambda x: bbox_transpose(bbox_rot90(x, 2)),  # Reflect over anti-diagonal\n        \"h\": lambda x: bbox_hflip(x),  # Horizontal flip\n        \"t\": lambda x: bbox_transpose(x),  # Transpose (reflect over main diagonal)\n    }\n\n    # Execute the appropriate transformation\n    if group_member in transformations:\n        return transformations[group_member](bbox)\n\n    raise ValueError(f\"Invalid group member: {group_member}\")\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.functional.bbox_flip","title":"<code>def bbox_flip    (bbox, d)    </code> [view source on GitHub]","text":"<p>Flip a bounding box either vertically, horizontally or both depending on the value of <code>d</code>.</p> <p>Parameters:</p> Name Type Description <code>bbox</code> <code>BoxInternalType</code> <p>A bounding box <code>(x_min, y_min, x_max, y_max)</code>.</p> <code>d</code> <code>int</code> <p>dimension. 0 for vertical flip, 1 for horizontal, -1 for transpose</p> <p>Returns:</p> Type Description <code>BoxInternalType</code> <p>A bounding box <code>(x_min, y_min, x_max, y_max)</code>.</p> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>if value of <code>d</code> is not -1, 0 or 1.</p> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>def bbox_flip(bbox: BoxInternalType, d: int) -&gt; BoxInternalType:\n    \"\"\"Flip a bounding box either vertically, horizontally or both depending on the value of `d`.\n\n    Args:\n        bbox: A bounding box `(x_min, y_min, x_max, y_max)`.\n        d: dimension. 0 for vertical flip, 1 for horizontal, -1 for transpose\n\n    Returns:\n        A bounding box `(x_min, y_min, x_max, y_max)`.\n\n    Raises:\n        ValueError: if value of `d` is not -1, 0 or 1.\n\n    \"\"\"\n    if d == 0:\n        bbox = bbox_vflip(bbox)\n    elif d == 1:\n        bbox = bbox_hflip(bbox)\n    elif d == -1:\n        bbox = bbox_hflip(bbox)\n        bbox = bbox_vflip(bbox)\n    else:\n        raise ValueError(f\"Invalid d value {d}. Valid values are -1, 0 and 1\")\n    return bbox\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.functional.bbox_hflip","title":"<code>def bbox_hflip    (bbox)    </code> [view source on GitHub]","text":"<p>Flip a bounding box horizontally around the y-axis.</p> <p>Parameters:</p> Name Type Description <code>bbox</code> <code>BoxInternalType</code> <p>A bounding box <code>(x_min, y_min, x_max, y_max)</code>.</p> <p>Returns:</p> Type Description <code>BoxInternalType</code> <p>A bounding box <code>(x_min, y_min, x_max, y_max)</code>.</p> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>def bbox_hflip(bbox: BoxInternalType) -&gt; BoxInternalType:\n    \"\"\"Flip a bounding box horizontally around the y-axis.\n\n    Args:\n        bbox: A bounding box `(x_min, y_min, x_max, y_max)`.\n\n    Returns:\n        A bounding box `(x_min, y_min, x_max, y_max)`.\n\n    \"\"\"\n    x_min, y_min, x_max, y_max = bbox[:4]\n    return 1 - x_max, y_min, 1 - x_min, y_max\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.functional.bbox_rot90","title":"<code>def bbox_rot90    (bbox, factor)    </code> [view source on GitHub]","text":"<p>Rotates a bounding box by 90 degrees CCW (see np.rot90)</p> <p>Parameters:</p> Name Type Description <code>bbox</code> <code>BoxInternalType</code> <p>A bounding box tuple (x_min, y_min, x_max, y_max).</p> <code>factor</code> <code>int</code> <p>Number of CCW rotations. Must be in set {0, 1, 2, 3} See np.rot90.</p> <p>Returns:</p> Type Description <code>tuple</code> <p>A bounding box tuple (x_min, y_min, x_max, y_max).</p> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>def bbox_rot90(bbox: BoxInternalType, factor: int) -&gt; BoxInternalType:\n    \"\"\"Rotates a bounding box by 90 degrees CCW (see np.rot90)\n\n    Args:\n        bbox: A bounding box tuple (x_min, y_min, x_max, y_max).\n        factor: Number of CCW rotations. Must be in set {0, 1, 2, 3} See np.rot90.\n\n    Returns:\n        tuple: A bounding box tuple (x_min, y_min, x_max, y_max).\n\n    \"\"\"\n    if factor not in {0, 1, 2, 3}:\n        msg = \"Parameter n must be in set {0, 1, 2, 3}\"\n        raise ValueError(msg)\n    x_min, y_min, x_max, y_max = bbox[:4]\n    if factor == 1:\n        bbox = y_min, 1 - x_max, y_max, 1 - x_min\n    elif factor == ROT90_180_FACTOR:\n        bbox = 1 - x_max, 1 - y_max, 1 - x_min, 1 - y_min\n    elif factor == ROT90_270_FACTOR:\n        bbox = 1 - y_max, x_min, 1 - y_min, x_max\n    return bbox\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.functional.bbox_rotate","title":"<code>def bbox_rotate    (bbox, angle, method, image_shape)    </code> [view source on GitHub]","text":"<p>Rotates a bounding box by angle degrees.</p> <p>Parameters:</p> Name Type Description <code>bbox</code> <code>BoxInternalType</code> <p>A bounding box <code>(x_min, y_min, x_max, y_max)</code>.</p> <code>angle</code> <code>float</code> <p>Angle of rotation in degrees.</p> <code>method</code> <code>str</code> <p>Rotation method used. Should be one of: \"largest_box\", \"ellipse\". Default: \"largest_box\".</p> <code>image_shape</code> <code>tuple[int, int]</code> <p>Image shape <code>(height, width)</code>.</p> <p>Returns:</p> Type Description <code>BoxInternalType</code> <p>A bounding box <code>(x_min, y_min, x_max, y_max)</code>.</p> <p>Reference</p> <p>https://arxiv.org/abs/2109.13488</p> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>def bbox_rotate(bbox: BoxInternalType, angle: float, method: str, image_shape: tuple[int, int]) -&gt; BoxInternalType:\n    \"\"\"Rotates a bounding box by angle degrees.\n\n    Args:\n        bbox: A bounding box `(x_min, y_min, x_max, y_max)`.\n        angle: Angle of rotation in degrees.\n        method: Rotation method used. Should be one of: \"largest_box\", \"ellipse\". Default: \"largest_box\".\n        image_shape: Image shape `(height, width)`.\n\n    Returns:\n        A bounding box `(x_min, y_min, x_max, y_max)`.\n\n    Reference:\n        https://arxiv.org/abs/2109.13488\n\n    \"\"\"\n    rows, cols = image_shape\n    x_min, y_min, x_max, y_max = bbox[:4]\n    scale = cols / float(rows)\n    if method == \"largest_box\":\n        x = np.array([x_min, x_max, x_max, x_min]) - 0.5\n        y = np.array([y_min, y_min, y_max, y_max]) - 0.5\n    elif method == \"ellipse\":\n        w = (x_max - x_min) / 2\n        h = (y_max - y_min) / 2\n        data = np.arange(0, 360, dtype=np.float32)\n        x = w * np.sin(np.radians(data)) + (w + x_min - 0.5)\n        y = h * np.cos(np.radians(data)) + (h + y_min - 0.5)\n    else:\n        raise ValueError(f\"Method {method} is not a valid rotation method.\")\n    angle = np.deg2rad(angle)\n    x_t = (np.cos(angle) * x * scale + np.sin(angle) * y) / scale\n    y_t = -np.sin(angle) * x * scale + np.cos(angle) * y\n    x_t = x_t + 0.5\n    y_t = y_t + 0.5\n\n    x_min, x_max = min(x_t), max(x_t)\n    y_min, y_max = min(y_t), max(y_t)\n\n    return x_min, y_min, x_max, y_max\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.functional.bbox_transpose","title":"<code>def bbox_transpose    (bbox)    </code> [view source on GitHub]","text":"<p>Transposes a bounding box along given axis.</p> <p>Parameters:</p> Name Type Description <code>bbox</code> <code>KeypointInternalType</code> <p>A bounding box <code>(x_min, y_min, x_max, y_max)</code>.</p> <p>Returns:</p> Type Description <code>KeypointInternalType</code> <p>A bounding box tuple <code>(x_min, y_min, x_max, y_max)</code>.</p> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If axis not equal to 0 or 1.</p> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>def bbox_transpose(\n    bbox: KeypointInternalType,\n) -&gt; KeypointInternalType:\n    \"\"\"Transposes a bounding box along given axis.\n\n    Args:\n        bbox: A bounding box `(x_min, y_min, x_max, y_max)`.\n\n    Returns:\n        A bounding box tuple `(x_min, y_min, x_max, y_max)`.\n\n    Raises:\n        ValueError: If axis not equal to 0 or 1.\n\n    \"\"\"\n    x_min, y_min, x_max, y_max = bbox[:4]\n    return (y_min, x_min, y_max, x_max)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.functional.bbox_vflip","title":"<code>def bbox_vflip    (bbox)    </code> [view source on GitHub]","text":"<p>Flip a bounding box vertically around the x-axis.</p> <p>Parameters:</p> Name Type Description <code>bbox</code> <code>BoxInternalType</code> <p>A bounding box <code>(x_min, y_min, x_max, y_max)</code>.</p> <p>Returns:</p> Type Description <code>tuple</code> <p>A bounding box <code>(x_min, y_min, x_max, y_max)</code>.</p> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>def bbox_vflip(bbox: BoxInternalType) -&gt; BoxInternalType:\n    \"\"\"Flip a bounding box vertically around the x-axis.\n\n    Args:\n        bbox: A bounding box `(x_min, y_min, x_max, y_max)`.\n\n    Returns:\n        tuple: A bounding box `(x_min, y_min, x_max, y_max)`.\n\n    \"\"\"\n    x_min, y_min, x_max, y_max = bbox[:4]\n    return x_min, 1 - y_max, x_max, 1 - y_min\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.functional.bboxes_affine","title":"<code>def bboxes_affine    (bboxes, matrix, rotate_method, image_shape, border_mode, output_shape)    </code> [view source on GitHub]","text":"<p>Apply an affine transformation to bounding boxes.</p> <p>For reflection border modes (cv2.BORDER_REFLECT_101, cv2.BORDER_REFLECT), this function: 1. Calculates necessary padding to avoid information loss 2. Applies padding to the bounding boxes 3. Adjusts the transformation matrix to account for padding 4. Applies the affine transformation 5. Validates the transformed bounding boxes</p> <p>For other border modes, it directly applies the affine transformation without padding.</p> <p>Parameters:</p> Name Type Description <code>bboxes</code> <code>np.ndarray</code> <p>Input bounding boxes</p> <code>matrix</code> <code>skimage.transform.ProjectiveTransform</code> <p>Affine transformation matrix</p> <code>rotate_method</code> <code>str</code> <p>Method for rotating bounding boxes ('largest_box' or 'ellipse')</p> <code>image_shape</code> <code>Sequence[int]</code> <p>Shape of the input image</p> <code>border_mode</code> <code>int</code> <p>OpenCV border mode</p> <code>output_shape</code> <code>Sequence[int]</code> <p>Shape of the output image</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Transformed and normalized bounding boxes</p> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>def bboxes_affine(\n    bboxes: np.ndarray,\n    matrix: skimage.transform.ProjectiveTransform,\n    rotate_method: Literal[\"largest_box\", \"ellipse\"],\n    image_shape: tuple[int, int],\n    border_mode: int,\n    output_shape: Sequence[int],\n) -&gt; np.ndarray:\n    \"\"\"Apply an affine transformation to bounding boxes.\n\n    For reflection border modes (cv2.BORDER_REFLECT_101, cv2.BORDER_REFLECT), this function:\n    1. Calculates necessary padding to avoid information loss\n    2. Applies padding to the bounding boxes\n    3. Adjusts the transformation matrix to account for padding\n    4. Applies the affine transformation\n    5. Validates the transformed bounding boxes\n\n    For other border modes, it directly applies the affine transformation without padding.\n\n    Args:\n        bboxes (np.ndarray): Input bounding boxes\n        matrix (skimage.transform.ProjectiveTransform): Affine transformation matrix\n        rotate_method (str): Method for rotating bounding boxes ('largest_box' or 'ellipse')\n        image_shape (Sequence[int]): Shape of the input image\n        border_mode (int): OpenCV border mode\n        output_shape (Sequence[int]): Shape of the output image\n\n    Returns:\n        np.ndarray: Transformed and normalized bounding boxes\n    \"\"\"\n    if is_identity_matrix(matrix):\n        return bboxes\n\n    bboxes = denormalize_bboxes(bboxes, image_shape)\n\n    if border_mode in REFLECT_BORDER_MODES:\n        # Step 1: Compute affine transform padding\n        pad_left, pad_right, pad_top, pad_bottom = calculate_affine_transform_padding(matrix, image_shape)\n        grid_dimensions = get_pad_grid_dimensions(pad_top, pad_bottom, pad_left, pad_right, image_shape)\n        bboxes = generate_reflected_bboxes(bboxes, grid_dimensions, image_shape, center_in_origin=True)\n\n    # Apply affine transform\n    if rotate_method == \"largest_box\":\n        transformed_bboxes = bboxes_affine_largest_box(bboxes, matrix)\n    elif rotate_method == \"ellipse\":\n        transformed_bboxes = bboxes_affine_ellipse(bboxes, matrix)\n    else:\n        raise ValueError(f\"Method {rotate_method} is not a valid rotation method.\")\n\n    # Validate and normalize bboxes\n    validated_bboxes = validate_bboxes(transformed_bboxes, output_shape)\n\n    return normalize_bboxes(validated_bboxes, output_shape)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.functional.bboxes_affine_ellipse","title":"<code>def bboxes_affine_ellipse    (bboxes, matrix)    </code> [view source on GitHub]","text":"<p>Apply an affine transformation to bounding boxes using an ellipse approximation method.</p> <p>This function transforms bounding boxes by approximating each box with an ellipse, transforming points along the ellipse's circumference, and then computing the new bounding box that encloses the transformed ellipse.</p> <p>Parameters:</p> Name Type Description <code>bboxes</code> <code>np.ndarray</code> <p>An array of bounding boxes with shape (N, 4+) where N is the number of                  bounding boxes. Each row should contain [x_min, y_min, x_max, y_max]                  followed by any additional attributes (e.g., class labels).</p> <code>matrix</code> <code>skimage.transform.ProjectiveTransform</code> <p>The affine transformation matrix to apply.</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>An array of transformed bounding boxes with the same shape as the input.             Each row contains [new_x_min, new_y_min, new_x_max, new_y_max] followed by             any additional attributes from the input bounding boxes.</p> <p>Note</p> <ul> <li>This function assumes that the input bounding boxes are in the format [x_min, y_min, x_max, y_max].</li> <li>The ellipse approximation method can provide a tighter bounding box compared to the   largest box method, especially for rotations.</li> <li>360 points are used to approximate each ellipse, which provides a good balance between   accuracy and computational efficiency.</li> <li>Any additional attributes beyond the first 4 coordinates are preserved unchanged.</li> <li>This method may be more suitable for objects that are roughly elliptical in shape.</li> </ul> <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; bboxes = np.array([[10, 10, 30, 20, 1], [40, 40, 60, 60, 2]])  # Two boxes with class labels\n&gt;&gt;&gt; matrix = skimage.transform.AffineTransform(rotation=np.pi/4)  # 45-degree rotation\n&gt;&gt;&gt; transformed_bboxes = bboxes_affine_ellipse(bboxes, matrix)\n&gt;&gt;&gt; print(transformed_bboxes)\n[[ 5.86  5.86 34.14 24.14  1.  ]\n [30.   30.   70.   70.    2.  ]]\n</code></pre> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>def bboxes_affine_ellipse(bboxes: np.ndarray, matrix: skimage.transform.ProjectiveTransform) -&gt; np.ndarray:\n    \"\"\"Apply an affine transformation to bounding boxes using an ellipse approximation method.\n\n    This function transforms bounding boxes by approximating each box with an ellipse,\n    transforming points along the ellipse's circumference, and then computing the\n    new bounding box that encloses the transformed ellipse.\n\n    Args:\n        bboxes (np.ndarray): An array of bounding boxes with shape (N, 4+) where N is the number of\n                             bounding boxes. Each row should contain [x_min, y_min, x_max, y_max]\n                             followed by any additional attributes (e.g., class labels).\n        matrix (skimage.transform.ProjectiveTransform): The affine transformation matrix to apply.\n\n    Returns:\n        np.ndarray: An array of transformed bounding boxes with the same shape as the input.\n                    Each row contains [new_x_min, new_y_min, new_x_max, new_y_max] followed by\n                    any additional attributes from the input bounding boxes.\n\n    Note:\n        - This function assumes that the input bounding boxes are in the format [x_min, y_min, x_max, y_max].\n        - The ellipse approximation method can provide a tighter bounding box compared to the\n          largest box method, especially for rotations.\n        - 360 points are used to approximate each ellipse, which provides a good balance between\n          accuracy and computational efficiency.\n        - Any additional attributes beyond the first 4 coordinates are preserved unchanged.\n        - This method may be more suitable for objects that are roughly elliptical in shape.\n\n    Example:\n        &gt;&gt;&gt; bboxes = np.array([[10, 10, 30, 20, 1], [40, 40, 60, 60, 2]])  # Two boxes with class labels\n        &gt;&gt;&gt; matrix = skimage.transform.AffineTransform(rotation=np.pi/4)  # 45-degree rotation\n        &gt;&gt;&gt; transformed_bboxes = bboxes_affine_ellipse(bboxes, matrix)\n        &gt;&gt;&gt; print(transformed_bboxes)\n        [[ 5.86  5.86 34.14 24.14  1.  ]\n         [30.   30.   70.   70.    2.  ]]\n    \"\"\"\n    x_min, y_min, x_max, y_max = bboxes[:, 0], bboxes[:, 1], bboxes[:, 2], bboxes[:, 3]\n    bbox_width = (x_max - x_min) / 2\n    bbox_height = (y_max - y_min) / 2\n    center_x = x_min + bbox_width\n    center_y = y_min + bbox_height\n\n    angles = np.arange(0, 360, dtype=np.float32)\n    cos_angles = np.cos(np.radians(angles))\n    sin_angles = np.sin(np.radians(angles))\n\n    # Generate points for all ellipses at once\n    x = bbox_width[:, np.newaxis] * sin_angles + center_x[:, np.newaxis]\n    y = bbox_height[:, np.newaxis] * cos_angles + center_y[:, np.newaxis]\n    points = np.stack([x, y], axis=-1).reshape(-1, 2)\n\n    # Transform all points at once\n    transformed_points = skimage.transform.matrix_transform(points, matrix.params)\n    transformed_points = transformed_points.reshape(len(bboxes), -1, 2)\n\n    # Compute new bounding boxes\n    new_x_min = np.min(transformed_points[:, :, 0], axis=1)\n    new_x_max = np.max(transformed_points[:, :, 0], axis=1)\n    new_y_min = np.min(transformed_points[:, :, 1], axis=1)\n    new_y_max = np.max(transformed_points[:, :, 1], axis=1)\n\n    return np.column_stack([new_x_min, new_y_min, new_x_max, new_y_max, bboxes[:, 4:]])\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.functional.bboxes_affine_largest_box","title":"<code>def bboxes_affine_largest_box    (bboxes, matrix)    </code> [view source on GitHub]","text":"<p>Apply an affine transformation to bounding boxes and return the largest enclosing boxes.</p> <p>This function transforms each corner of every bounding box using the given affine transformation matrix, then computes the new bounding boxes that fully enclose the transformed corners.</p> <p>Parameters:</p> Name Type Description <code>bboxes</code> <code>np.ndarray</code> <p>An array of bounding boxes with shape (N, 4+) where N is the number of                  bounding boxes. Each row should contain [x_min, y_min, x_max, y_max]                  followed by any additional attributes (e.g., class labels).</p> <code>matrix</code> <code>skimage.transform.ProjectiveTransform</code> <p>The affine transformation matrix to apply.</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>An array of transformed bounding boxes with the same shape as the input.             Each row contains [new_x_min, new_y_min, new_x_max, new_y_max] followed by             any additional attributes from the input bounding boxes.</p> <p>Note</p> <ul> <li>This function assumes that the input bounding boxes are in the format [x_min, y_min, x_max, y_max].</li> <li>The resulting bounding boxes are the smallest axis-aligned boxes that completely   enclose the transformed original boxes. They may be larger than the minimal possible   bounding box if the original box becomes rotated.</li> <li>Any additional attributes beyond the first 4 coordinates are preserved unchanged.</li> <li>This method is called \"largest box\" because it returns the largest axis-aligned box   that encloses all corners of the transformed bounding box.</li> </ul> <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; bboxes = np.array([[10, 10, 20, 20, 1], [30, 30, 40, 40, 2]])  # Two boxes with class labels\n&gt;&gt;&gt; matrix = skimage.transform.AffineTransform(scale=(2, 2), translation=(5, 5))\n&gt;&gt;&gt; transformed_bboxes = bboxes_affine_largest_box(bboxes, matrix)\n&gt;&gt;&gt; print(transformed_bboxes)\n[[ 25.  25.  45.  45.   1.]\n [ 65.  65.  85.  85.   2.]]\n</code></pre> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>def bboxes_affine_largest_box(bboxes: np.ndarray, matrix: skimage.transform.ProjectiveTransform) -&gt; np.ndarray:\n    \"\"\"Apply an affine transformation to bounding boxes and return the largest enclosing boxes.\n\n    This function transforms each corner of every bounding box using the given affine transformation\n    matrix, then computes the new bounding boxes that fully enclose the transformed corners.\n\n    Args:\n        bboxes (np.ndarray): An array of bounding boxes with shape (N, 4+) where N is the number of\n                             bounding boxes. Each row should contain [x_min, y_min, x_max, y_max]\n                             followed by any additional attributes (e.g., class labels).\n        matrix (skimage.transform.ProjectiveTransform): The affine transformation matrix to apply.\n\n    Returns:\n        np.ndarray: An array of transformed bounding boxes with the same shape as the input.\n                    Each row contains [new_x_min, new_y_min, new_x_max, new_y_max] followed by\n                    any additional attributes from the input bounding boxes.\n\n    Note:\n        - This function assumes that the input bounding boxes are in the format [x_min, y_min, x_max, y_max].\n        - The resulting bounding boxes are the smallest axis-aligned boxes that completely\n          enclose the transformed original boxes. They may be larger than the minimal possible\n          bounding box if the original box becomes rotated.\n        - Any additional attributes beyond the first 4 coordinates are preserved unchanged.\n        - This method is called \"largest box\" because it returns the largest axis-aligned box\n          that encloses all corners of the transformed bounding box.\n\n    Example:\n        &gt;&gt;&gt; bboxes = np.array([[10, 10, 20, 20, 1], [30, 30, 40, 40, 2]])  # Two boxes with class labels\n        &gt;&gt;&gt; matrix = skimage.transform.AffineTransform(scale=(2, 2), translation=(5, 5))\n        &gt;&gt;&gt; transformed_bboxes = bboxes_affine_largest_box(bboxes, matrix)\n        &gt;&gt;&gt; print(transformed_bboxes)\n        [[ 25.  25.  45.  45.   1.]\n         [ 65.  65.  85.  85.   2.]]\n    \"\"\"\n    # Extract corners of all bboxes\n    x_min, y_min, x_max, y_max = bboxes[:, 0], bboxes[:, 1], bboxes[:, 2], bboxes[:, 3]\n    corners = np.array([[x_min, y_min], [x_max, y_min], [x_max, y_max], [x_min, y_max]]).transpose(\n        2,\n        0,\n        1,\n    )  # Shape: (num_bboxes, 4, 2)\n\n    # Transform all corners at once\n    transformed_corners = skimage.transform.matrix_transform(corners.reshape(-1, 2), matrix.params)\n    transformed_corners = transformed_corners.reshape(-1, 4, 2)\n\n    # Compute new bounding boxes\n    new_x_min = np.min(transformed_corners[:, :, 0], axis=1)\n    new_x_max = np.max(transformed_corners[:, :, 0], axis=1)\n    new_y_min = np.min(transformed_corners[:, :, 1], axis=1)\n    new_y_max = np.max(transformed_corners[:, :, 1], axis=1)\n\n    return np.column_stack([new_x_min, new_y_min, new_x_max, new_y_max, bboxes[:, 4:]])\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.functional.calculate_affine_transform_padding","title":"<code>def calculate_affine_transform_padding    (matrix, image_shape)    </code> [view source on GitHub]","text":"<p>Calculate the necessary padding for an affine transformation to avoid empty spaces.</p> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>def calculate_affine_transform_padding(\n    matrix: skimage.transform.ProjectiveTransform,\n    image_shape: Sequence[int],\n) -&gt; tuple[int, int, int, int]:\n    \"\"\"Calculate the necessary padding for an affine transformation to avoid empty spaces.\"\"\"\n    height, width = image_shape[:2]\n\n    # Check for identity transform\n    if is_identity_matrix(matrix):\n        return (0, 0, 0, 0)\n\n    # Original corners\n    corners = np.array([[0, 0], [width, 0], [width, height], [0, height]])\n\n    # Transform corners\n    transformed_corners = matrix(corners)\n\n    # Find box that includes both original and transformed corners\n    all_corners = np.vstack((corners, transformed_corners))\n    min_x, min_y = all_corners.min(axis=0)\n    max_x, max_y = all_corners.max(axis=0)\n    # Compute the inverse transform\n    inverse_matrix = matrix.inverse\n\n    # Apply inverse transform to all corners of the bounding box\n    bbox_corners = np.array([[min_x, min_y], [max_x, min_y], [max_x, max_y], [min_x, max_y]])\n\n    inverse_corners = inverse_matrix(bbox_corners)\n\n    min_x, min_y = inverse_corners.min(axis=0)\n    max_x, max_y = inverse_corners.max(axis=0)\n\n    pad_left = max(0, math.ceil(0 - min_x))\n    pad_right = max(0, math.ceil(max_x - width))\n    pad_top = max(0, math.ceil(0 - min_y))\n    pad_bottom = max(0, math.ceil(max_y - height))\n\n    return pad_left, pad_right, pad_top, pad_bottom\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.functional.calculate_grid_dimensions","title":"<code>def calculate_grid_dimensions    (image_shape, num_grid_xy)    </code> [view source on GitHub]","text":"<p>Calculate the dimensions of a grid overlay on an image using vectorized operations.</p> <p>This function divides an image into a grid and calculates the dimensions (x_min, y_min, x_max, y_max) for each cell in the grid without using loops.</p> <p>Parameters:</p> Name Type Description <code>image_shape</code> <code>tuple[int, int]</code> <p>The shape of the image (height, width).</p> <code>num_grid_xy</code> <code>tuple[int, int]</code> <p>The number of grid cells in (x, y) directions.</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>A 3D array of shape (grid_height, grid_width, 4) where each element             is [x_min, y_min, x_max, y_max] for a grid cell.</p> <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; image_shape = (100, 150)\n&gt;&gt;&gt; num_grid_xy = (3, 2)\n&gt;&gt;&gt; dimensions = calculate_grid_dimensions(image_shape, num_grid_xy)\n&gt;&gt;&gt; print(dimensions.shape)\n(2, 3, 4)\n&gt;&gt;&gt; print(dimensions[0, 0])  # First cell\n[  0   0  50  50]\n</code></pre> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>def calculate_grid_dimensions(\n    image_shape: tuple[int, int],\n    num_grid_xy: tuple[int, int],\n) -&gt; np.ndarray:\n    \"\"\"Calculate the dimensions of a grid overlay on an image using vectorized operations.\n\n    This function divides an image into a grid and calculates the dimensions\n    (x_min, y_min, x_max, y_max) for each cell in the grid without using loops.\n\n    Args:\n        image_shape (tuple[int, int]): The shape of the image (height, width).\n        num_grid_xy (tuple[int, int]): The number of grid cells in (x, y) directions.\n\n    Returns:\n        np.ndarray: A 3D array of shape (grid_height, grid_width, 4) where each element\n                    is [x_min, y_min, x_max, y_max] for a grid cell.\n\n    Example:\n        &gt;&gt;&gt; image_shape = (100, 150)\n        &gt;&gt;&gt; num_grid_xy = (3, 2)\n        &gt;&gt;&gt; dimensions = calculate_grid_dimensions(image_shape, num_grid_xy)\n        &gt;&gt;&gt; print(dimensions.shape)\n        (2, 3, 4)\n        &gt;&gt;&gt; print(dimensions[0, 0])  # First cell\n        [  0   0  50  50]\n    \"\"\"\n    num_grid_yx = np.array(num_grid_xy[::-1])  # Reverse to match image_shape order\n    image_shape = np.array(image_shape)\n\n    square_shape = image_shape // num_grid_yx\n    last_square_shape = image_shape - (square_shape * (num_grid_yx - 1))\n\n    grid_width, grid_height = num_grid_xy\n\n    # Create meshgrid for row and column indices\n    col_indices, row_indices = np.meshgrid(np.arange(grid_width), np.arange(grid_height))\n\n    # Calculate x_min and y_min\n    x_min = col_indices * square_shape[1]\n    y_min = row_indices * square_shape[0]\n\n    # Calculate x_max and y_max\n    x_max = np.where(col_indices == grid_width - 1, x_min + last_square_shape[1], x_min + square_shape[1])\n    y_max = np.where(row_indices == grid_height - 1, y_min + last_square_shape[0], y_min + square_shape[0])\n\n    # Stack the dimensions\n    return np.stack([x_min, y_min, x_max, y_max], axis=-1).astype(np.int16)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.functional.compute_transformed_image_bounds","title":"<code>def compute_transformed_image_bounds    (matrix, image_shape)    </code> [view source on GitHub]","text":"<p>Compute the bounds of an image after applying an affine transformation.</p> <p>Parameters:</p> Name Type Description <code>matrix</code> <code>skimage.transform.ProjectiveTransform</code> <p>The affine transformation matrix.</p> <code>image_shape</code> <code>tuple[int, int]</code> <p>The shape of the image as (height, width).</p> <p>Returns:</p> Type Description <code>tuple[np.ndarray, np.ndarray]</code> <p>A tuple containing:     - min_coords: An array with the minimum x and y coordinates.     - max_coords: An array with the maximum x and y coordinates.</p> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>def compute_transformed_image_bounds(\n    matrix: skimage.transform.ProjectiveTransform,\n    image_shape: tuple[int, int],\n) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"Compute the bounds of an image after applying an affine transformation.\n\n    Args:\n        matrix (skimage.transform.ProjectiveTransform): The affine transformation matrix.\n        image_shape (tuple[int, int]): The shape of the image as (height, width).\n\n    Returns:\n        tuple[np.ndarray, np.ndarray]: A tuple containing:\n            - min_coords: An array with the minimum x and y coordinates.\n            - max_coords: An array with the maximum x and y coordinates.\n    \"\"\"\n    height, width = image_shape[:2]\n\n    # Define the corners of the image\n    corners = np.array([[0, 0], [width, 0], [width, height], [0, height]])\n\n    # Transform the corners\n    transformed_corners = matrix(corners)\n\n    # Calculate the bounding box of the transformed corners\n    min_coords = np.floor(transformed_corners.min(axis=0)).astype(int)\n    max_coords = np.ceil(transformed_corners.max(axis=0)).astype(int)\n\n    return min_coords, max_coords\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.functional.create_affine_transformation_matrix","title":"<code>def create_affine_transformation_matrix    (translate, shear, scale, rotate, shift)    </code> [view source on GitHub]","text":"<p>Create an affine transformation matrix combining translation, shear, scale, and rotation.</p> <p>This function creates a complex affine transformation by combining multiple transformations in a specific order. The transformations are applied as follows: 1. Shift to top-left: Moves the center of transformation to (0, 0) 2. Apply main transformations: scale, rotation, shear, and translation 3. Shift back to center: Moves the center of transformation back to its original position</p> <p>The order of these transformations is crucial as matrix multiplications are not commutative.</p> <p>Parameters:</p> Name Type Description <code>translate</code> <code>TranslateDict</code> <p>Translation in x and y directions.                        Keys: 'x', 'y'. Values: translation amounts in pixels.</p> <code>shear</code> <code>ShearDict</code> <p>Shear in x and y directions.                Keys: 'x', 'y'. Values: shear angles in degrees.</p> <code>scale</code> <code>ScaleDict</code> <p>Scale factors for x and y directions.                Keys: 'x', 'y'. Values: scale factors (1.0 means no scaling).</p> <code>rotate</code> <code>float</code> <p>Rotation angle in degrees. Positive values rotate counter-clockwise.</p> <code>shift</code> <code>tuple[float, float]</code> <p>Shift to apply before and after transformations.                          Typically the image center (width/2, height/2).</p> <p>Returns:</p> Type Description <code>skimage.transform.ProjectiveTransform</code> <p>The resulting affine transformation matrix.</p> <p>Note</p> <ul> <li>All angle inputs (rotate, shear) are in degrees and are converted to radians internally.</li> <li>The order of transformations in the AffineTransform is: scale, rotation, shear, translation.</li> <li>The resulting transformation can be applied to coordinates using the call method.</li> </ul> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>def create_affine_transformation_matrix(\n    translate: TranslateDict,\n    shear: ShearDict,\n    scale: ScaleDict,\n    rotate: float,\n    shift: tuple[float, float],\n) -&gt; skimage.transform.ProjectiveTransform:\n    \"\"\"Create an affine transformation matrix combining translation, shear, scale, and rotation.\n\n    This function creates a complex affine transformation by combining multiple transformations\n    in a specific order. The transformations are applied as follows:\n    1. Shift to top-left: Moves the center of transformation to (0, 0)\n    2. Apply main transformations: scale, rotation, shear, and translation\n    3. Shift back to center: Moves the center of transformation back to its original position\n\n    The order of these transformations is crucial as matrix multiplications are not commutative.\n\n    Args:\n        translate (TranslateDict): Translation in x and y directions.\n                                   Keys: 'x', 'y'. Values: translation amounts in pixels.\n        shear (ShearDict): Shear in x and y directions.\n                           Keys: 'x', 'y'. Values: shear angles in degrees.\n        scale (ScaleDict): Scale factors for x and y directions.\n                           Keys: 'x', 'y'. Values: scale factors (1.0 means no scaling).\n        rotate (float): Rotation angle in degrees. Positive values rotate counter-clockwise.\n        shift (tuple[float, float]): Shift to apply before and after transformations.\n                                     Typically the image center (width/2, height/2).\n\n    Returns:\n        skimage.transform.ProjectiveTransform: The resulting affine transformation matrix.\n\n    Note:\n        - All angle inputs (rotate, shear) are in degrees and are converted to radians internally.\n        - The order of transformations in the AffineTransform is: scale, rotation, shear, translation.\n        - The resulting transformation can be applied to coordinates using the __call__ method.\n    \"\"\"\n    # Step 1: Create matrix to shift to top-left\n    # This moves the center of transformation to (0, 0)\n    matrix_to_topleft = skimage.transform.SimilarityTransform(translation=[shift[0], shift[1]])\n\n    # Step 2: Create matrix for main transformations\n    # This includes scaling, translation, rotation, and x-shear\n    matrix_transforms = skimage.transform.AffineTransform(\n        scale=(scale[\"x\"], scale[\"y\"]),\n        rotation=np.deg2rad(rotate),\n        shear=(np.deg2rad(shear[\"x\"]), np.deg2rad(shear[\"y\"])),  # Both x and y shear\n        translation=(translate[\"x\"], translate[\"y\"]),\n    )\n\n    # Step 3: Create matrix to shift back to center\n    # This is the inverse of the top-left shift\n    matrix_to_center = matrix_to_topleft.inverse\n\n    # Combine all transformations\n    # The order is important: transformations are applied from right to left\n    return (\n        matrix_to_center  # 3. Shift back to original center\n        + matrix_transforms  # 2. Apply main transformations\n        + matrix_to_topleft  # 1. Shift to top-left\n    )\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.functional.d4","title":"<code>def d4    (img, group_member)    </code> [view source on GitHub]","text":"<p>Applies a <code>D_4</code> symmetry group transformation to an image array.</p> <p>This function manipulates an image using transformations such as rotations and flips, corresponding to the <code>D_4</code> dihedral group symmetry operations. Each transformation is identified by a unique group member code.</p> <ul> <li>img (np.ndarray): The input image array to transform.</li> <li>group_member (D4Type): A string identifier indicating the specific transformation to apply. Valid codes include:</li> <li>'e': Identity (no transformation).</li> <li>'r90': Rotate 90 degrees counterclockwise.</li> <li>'r180': Rotate 180 degrees.</li> <li>'r270': Rotate 270 degrees counterclockwise.</li> <li>'v': Vertical flip.</li> <li>'hvt': Transpose over second diagonal</li> <li>'h': Horizontal flip.</li> <li>'t': Transpose (reflect over the main diagonal).</li> </ul> <ul> <li>np.ndarray: The transformed image array.</li> </ul> <ul> <li>ValueError: If an invalid group member is specified.</li> </ul> <p>Examples:</p> <ul> <li>Rotating an image by 90 degrees:   <code>transformed_image = d4(original_image, 'r90')</code></li> <li>Applying a horizontal flip to an image:   <code>transformed_image = d4(original_image, 'h')</code></li> </ul> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>def d4(img: np.ndarray, group_member: D4Type) -&gt; np.ndarray:\n    \"\"\"Applies a `D_4` symmetry group transformation to an image array.\n\n    This function manipulates an image using transformations such as rotations and flips,\n    corresponding to the `D_4` dihedral group symmetry operations.\n    Each transformation is identified by a unique group member code.\n\n    Parameters:\n    - img (np.ndarray): The input image array to transform.\n    - group_member (D4Type): A string identifier indicating the specific transformation to apply. Valid codes include:\n      - 'e': Identity (no transformation).\n      - 'r90': Rotate 90 degrees counterclockwise.\n      - 'r180': Rotate 180 degrees.\n      - 'r270': Rotate 270 degrees counterclockwise.\n      - 'v': Vertical flip.\n      - 'hvt': Transpose over second diagonal\n      - 'h': Horizontal flip.\n      - 't': Transpose (reflect over the main diagonal).\n\n    Returns:\n    - np.ndarray: The transformed image array.\n\n    Raises:\n    - ValueError: If an invalid group member is specified.\n\n    Examples:\n    - Rotating an image by 90 degrees:\n      `transformed_image = d4(original_image, 'r90')`\n    - Applying a horizontal flip to an image:\n      `transformed_image = d4(original_image, 'h')`\n    \"\"\"\n    transformations = {\n        \"e\": lambda x: x,  # Identity transformation\n        \"r90\": lambda x: rot90(x, 1),  # Rotate 90 degrees\n        \"r180\": lambda x: rot90(x, 2),  # Rotate 180 degrees\n        \"r270\": lambda x: rot90(x, 3),  # Rotate 270 degrees\n        \"v\": vflip,  # Vertical flip\n        \"hvt\": lambda x: transpose(rot90(x, 2)),  # Reflect over anti-diagonal\n        \"h\": hflip,  # Horizontal flip\n        \"t\": transpose,  # Transpose (reflect over main diagonal)\n    }\n\n    # Execute the appropriate transformation\n    if group_member in transformations:\n        return transformations[group_member](img)\n\n    raise ValueError(f\"Invalid group member: {group_member}\")\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.functional.distort_image","title":"<code>def distort_image    (image, generated_mesh, interpolation)    </code> [view source on GitHub]","text":"<p>Apply perspective distortion to an image based on a generated mesh.</p> <p>This function applies a perspective transformation to each cell of the image defined by the generated mesh. The distortion is applied using OpenCV's perspective transformation and blending techniques.</p> <p>Parameters:</p> Name Type Description <code>image</code> <code>np.ndarray</code> <p>The input image to be distorted. Can be a 2D grayscale image or a                 3D color image.</p> <code>generated_mesh</code> <code>np.ndarray</code> <p>A 2D array where each row represents a quadrilateral cell                         as [x1, y1, x2, y2, dst_x1, dst_y1, dst_x2, dst_y2, dst_x3, dst_y3, dst_x4, dst_y4].                         The first four values define the source rectangle, and the last eight values                         define the destination quadrilateral.</p> <code>interpolation</code> <code>int</code> <p>Interpolation method to be used in the perspective transformation.                  Should be one of the OpenCV interpolation flags (e.g., cv2.INTER_LINEAR).</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>The distorted image with the same shape and dtype as the input image.</p> <p>Note</p> <ul> <li>The function preserves the channel dimension of the input image.</li> <li>Each cell of the generated mesh is transformed independently and then blended into the output image.</li> <li>The distortion is applied using perspective transformation, which allows for more complex   distortions compared to affine transformations.</li> </ul> <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; image = np.random.randint(0, 255, (100, 100, 3), dtype=np.uint8)\n&gt;&gt;&gt; mesh = np.array([[0, 0, 50, 50, 5, 5, 45, 5, 45, 45, 5, 45]])\n&gt;&gt;&gt; distorted = distort_image(image, mesh, cv2.INTER_LINEAR)\n&gt;&gt;&gt; distorted.shape\n(100, 100, 3)\n</code></pre> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>@preserve_channel_dim\ndef distort_image(image: np.ndarray, generated_mesh: np.ndarray, interpolation: int) -&gt; np.ndarray:\n    \"\"\"Apply perspective distortion to an image based on a generated mesh.\n\n    This function applies a perspective transformation to each cell of the image defined by the\n    generated mesh. The distortion is applied using OpenCV's perspective transformation and\n    blending techniques.\n\n    Args:\n        image (np.ndarray): The input image to be distorted. Can be a 2D grayscale image or a\n                            3D color image.\n        generated_mesh (np.ndarray): A 2D array where each row represents a quadrilateral cell\n                                    as [x1, y1, x2, y2, dst_x1, dst_y1, dst_x2, dst_y2, dst_x3, dst_y3, dst_x4, dst_y4].\n                                    The first four values define the source rectangle, and the last eight values\n                                    define the destination quadrilateral.\n        interpolation (int): Interpolation method to be used in the perspective transformation.\n                             Should be one of the OpenCV interpolation flags (e.g., cv2.INTER_LINEAR).\n\n    Returns:\n        np.ndarray: The distorted image with the same shape and dtype as the input image.\n\n    Note:\n        - The function preserves the channel dimension of the input image.\n        - Each cell of the generated mesh is transformed independently and then blended into the output image.\n        - The distortion is applied using perspective transformation, which allows for more complex\n          distortions compared to affine transformations.\n\n    Example:\n        &gt;&gt;&gt; image = np.random.randint(0, 255, (100, 100, 3), dtype=np.uint8)\n        &gt;&gt;&gt; mesh = np.array([[0, 0, 50, 50, 5, 5, 45, 5, 45, 45, 5, 45]])\n        &gt;&gt;&gt; distorted = distort_image(image, mesh, cv2.INTER_LINEAR)\n        &gt;&gt;&gt; distorted.shape\n        (100, 100, 3)\n    \"\"\"\n    distorted_image = np.zeros_like(image)\n\n    for mesh in generated_mesh:\n        # Extract source rectangle and destination quadrilateral\n        x1, y1, x2, y2 = mesh[:4]  # Source rectangle\n        dst_quad = mesh[4:].reshape(4, 2)  # Destination quadrilateral\n\n        # Convert source rectangle to quadrilateral\n        src_quad = np.array(\n            [\n                [x1, y1],  # Top-left\n                [x2, y1],  # Top-right\n                [x2, y2],  # Bottom-right\n                [x1, y2],  # Bottom-left\n            ],\n            dtype=np.float32,\n        )\n\n        # Calculate Perspective transformation matrix\n        perspective_mat = cv2.getPerspectiveTransform(src_quad, dst_quad)\n\n        # Apply Perspective transformation\n        warped = cv2.warpPerspective(image, perspective_mat, (image.shape[1], image.shape[0]), flags=interpolation)\n\n        # Create mask for the transformed region\n        mask = np.zeros(image.shape[:2], dtype=np.uint8)\n        cv2.fillConvexPoly(mask, np.int32(dst_quad), 255)\n\n        # Copy only the warped quadrilateral area to the output image\n        distorted_image = cv2.copyTo(warped, mask, distorted_image)\n\n    return distorted_image\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.functional.elastic_transform","title":"<code>def elastic_transform    (img, alpha, sigma, interpolation, border_mode, value=None, random_state=None, approximate=False, same_dxdy=False)    </code> [view source on GitHub]","text":"<p>Apply an elastic transformation to an image.</p> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>@preserve_channel_dim\ndef elastic_transform(\n    img: np.ndarray,\n    alpha: float,\n    sigma: float,\n    interpolation: int,\n    border_mode: int,\n    value: ColorType | None = None,\n    random_state: np.random.RandomState | None = None,\n    approximate: bool = False,\n    same_dxdy: bool = False,\n) -&gt; np.ndarray:\n    \"\"\"Apply an elastic transformation to an image.\"\"\"\n    if approximate:\n        return elastic_transform_approximate(\n            img,\n            alpha,\n            sigma,\n            interpolation,\n            border_mode,\n            value,\n            random_state,\n            same_dxdy,\n        )\n    return elastic_transform_precise(\n        img,\n        alpha,\n        sigma,\n        interpolation,\n        border_mode,\n        value,\n        random_state,\n        same_dxdy,\n    )\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.functional.elastic_transform_approximate","title":"<code>def elastic_transform_approximate    (img, alpha, sigma, interpolation, border_mode, value, random_state, same_dxdy=False)    </code> [view source on GitHub]","text":"<p>Apply an approximate elastic transformation to an image.</p> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>def elastic_transform_approximate(\n    img: np.ndarray,\n    alpha: float,\n    sigma: float,\n    interpolation: int,\n    border_mode: int,\n    value: ColorType | None,\n    random_state: np.random.RandomState | None,\n    same_dxdy: bool = False,\n) -&gt; np.ndarray:\n    \"\"\"Apply an approximate elastic transformation to an image.\"\"\"\n    return elastic_transform_helper(\n        img,\n        alpha,\n        sigma,\n        interpolation,\n        border_mode,\n        value,\n        random_state,\n        same_dxdy,\n        kernel_size=(17, 17),\n    )\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.functional.elastic_transform_precise","title":"<code>def elastic_transform_precise    (img, alpha, sigma, interpolation, border_mode, value, random_state, same_dxdy=False)    </code> [view source on GitHub]","text":"<p>Apply a precise elastic transformation to an image.</p> <p>This function applies an elastic deformation to the input image using a precise method. The transformation involves creating random displacement fields, smoothing them using Gaussian blur with adaptive kernel size, and then remapping the image according to the smoothed displacement fields.</p> <p>Parameters:</p> Name Type Description <code>img</code> <code>np.ndarray</code> <p>Input image.</p> <code>alpha</code> <code>float</code> <p>Scaling factor for the random displacement fields.</p> <code>sigma</code> <code>float</code> <p>Standard deviation for Gaussian blur applied to the displacement fields.</p> <code>interpolation</code> <code>int</code> <p>Interpolation method to be used (e.g., cv2.INTER_LINEAR).</p> <code>border_mode</code> <code>int</code> <p>Pixel extrapolation method (e.g., cv2.BORDER_CONSTANT).</p> <code>value</code> <code>ColorType | None</code> <p>Border value if border_mode is cv2.BORDER_CONSTANT.</p> <code>random_state</code> <code>np.random.RandomState | None</code> <p>Random state for reproducibility.</p> <code>same_dxdy</code> <code>bool</code> <p>If True, use the same displacement field for both x and y directions.</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Transformed image with precise elastic deformation applied.</p> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>def elastic_transform_precise(\n    img: np.ndarray,\n    alpha: float,\n    sigma: float,\n    interpolation: int,\n    border_mode: int,\n    value: ColorType | None,\n    random_state: np.random.RandomState | None,\n    same_dxdy: bool = False,\n) -&gt; np.ndarray:\n    \"\"\"Apply a precise elastic transformation to an image.\n\n    This function applies an elastic deformation to the input image using a precise method.\n    The transformation involves creating random displacement fields, smoothing them using Gaussian\n    blur with adaptive kernel size, and then remapping the image according to the smoothed displacement fields.\n\n    Args:\n        img (np.ndarray): Input image.\n        alpha (float): Scaling factor for the random displacement fields.\n        sigma (float): Standard deviation for Gaussian blur applied to the displacement fields.\n        interpolation (int): Interpolation method to be used (e.g., cv2.INTER_LINEAR).\n        border_mode (int): Pixel extrapolation method (e.g., cv2.BORDER_CONSTANT).\n        value (ColorType | None): Border value if border_mode is cv2.BORDER_CONSTANT.\n        random_state (np.random.RandomState | None): Random state for reproducibility.\n        same_dxdy (bool, optional): If True, use the same displacement field for both x and y directions.\n\n    Returns:\n        np.ndarray: Transformed image with precise elastic deformation applied.\n    \"\"\"\n    return elastic_transform_helper(\n        img,\n        alpha,\n        sigma,\n        interpolation,\n        border_mode,\n        value,\n        random_state,\n        same_dxdy,\n        kernel_size=(0, 0),\n    )\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.functional.find_keypoint","title":"<code>def find_keypoint    (position, distance_map, threshold, inverted)    </code> [view source on GitHub]","text":"<p>Determine if a valid keypoint can be found at the given position.</p> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>def find_keypoint(\n    position: tuple[int, int],\n    distance_map: np.ndarray,\n    threshold: float | None,\n    inverted: bool,\n) -&gt; tuple[float, float] | None:\n    \"\"\"Determine if a valid keypoint can be found at the given position.\"\"\"\n    y, x = position\n    value = distance_map[y, x]\n    if not inverted and threshold is not None and value &gt;= threshold:\n        return None\n    if inverted and threshold is not None and value &lt; threshold:\n        return None\n    return float(x), float(y)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.functional.flip_bboxes","title":"<code>def flip_bboxes    (bboxes, flip_horizontal=False, flip_vertical=False, image_shape=(0, 0))    </code> [view source on GitHub]","text":"<p>Flip bounding boxes horizontally and/or vertically.</p> <p>Parameters:</p> Name Type Description <code>bboxes</code> <code>np.ndarray</code> <p>Array of bounding boxes with shape (n, m) where each row is [x_min, y_min, x_max, y_max, ...].</p> <code>flip_horizontal</code> <code>bool</code> <p>Whether to flip horizontally.</p> <code>flip_vertical</code> <code>bool</code> <p>Whether to flip vertically.</p> <code>image_shape</code> <code>tuple[int, int]</code> <p>Shape of the image as (height, width).</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Flipped bounding boxes.</p> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>def flip_bboxes(\n    bboxes: np.ndarray,\n    flip_horizontal: bool = False,\n    flip_vertical: bool = False,\n    image_shape: tuple[int, int] = (0, 0),\n) -&gt; np.ndarray:\n    \"\"\"Flip bounding boxes horizontally and/or vertically.\n\n    Args:\n        bboxes (np.ndarray): Array of bounding boxes with shape (n, m) where each row is\n            [x_min, y_min, x_max, y_max, ...].\n        flip_horizontal (bool): Whether to flip horizontally.\n        flip_vertical (bool): Whether to flip vertically.\n        image_shape (tuple[int, int]): Shape of the image as (height, width).\n\n    Returns:\n        np.ndarray: Flipped bounding boxes.\n    \"\"\"\n    rows, cols = image_shape[:2]\n    flipped_bboxes = bboxes.copy()\n    if flip_horizontal:\n        flipped_bboxes[:, [0, 2]] = cols - flipped_bboxes[:, [2, 0]]\n    if flip_vertical:\n        flipped_bboxes[:, [1, 3]] = rows - flipped_bboxes[:, [3, 1]]\n    return flipped_bboxes\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.functional.from_distance_maps","title":"<code>def from_distance_maps    (distance_maps, inverted, if_not_found_coords, threshold)    </code> [view source on GitHub]","text":"<p>Convert outputs of <code>to_distance_maps</code> to <code>KeypointsOnImage</code>. This is the inverse of <code>to_distance_maps</code>.</p> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>def from_distance_maps(\n    distance_maps: np.ndarray,\n    inverted: bool,\n    if_not_found_coords: Sequence[int] | dict[str, Any] | None,\n    threshold: float | None,\n) -&gt; list[tuple[float, float]]:\n    \"\"\"Convert outputs of `to_distance_maps` to `KeypointsOnImage`.\n    This is the inverse of `to_distance_maps`.\n    \"\"\"\n    if distance_maps.ndim != NUM_MULTI_CHANNEL_DIMENSIONS:\n        msg = f\"Expected three-dimensional input, got {distance_maps.ndim} dimensions and shape {distance_maps.shape}.\"\n        raise ValueError(msg)\n    height, width, nb_keypoints = distance_maps.shape\n\n    drop_if_not_found, if_not_found_x, if_not_found_y = validate_if_not_found_coords(if_not_found_coords)\n\n    keypoints = []\n    for i in range(nb_keypoints):\n        hitidx_flat = np.argmax(distance_maps[..., i]) if inverted else np.argmin(distance_maps[..., i])\n        hitidx_ndim = np.unravel_index(hitidx_flat, (height, width))\n        keypoint = find_keypoint(hitidx_ndim, distance_maps[:, :, i], threshold, inverted)\n        if keypoint:\n            keypoints.append(keypoint)\n        elif not drop_if_not_found:\n            keypoints.append((if_not_found_x, if_not_found_y))\n\n    return keypoints\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.functional.generate_distorted_grid_polygons","title":"<code>def generate_distorted_grid_polygons    (dimensions, magnitude)    </code> [view source on GitHub]","text":"<p>Generate distorted grid polygons based on input dimensions and magnitude.</p> <p>This function creates a grid of polygons and applies random distortions to the internal vertices, while keeping the boundary vertices fixed. The distortion is applied consistently across shared vertices to avoid gaps or overlaps in the resulting grid.</p> <p>Parameters:</p> Name Type Description <code>dimensions</code> <code>np.ndarray</code> <p>A 3D array of shape (grid_height, grid_width, 4) where each element                      is [x_min, y_min, x_max, y_max] representing the dimensions of a grid cell.</p> <code>magnitude</code> <code>int</code> <p>Maximum pixel-wise displacement for distortion. The actual displacement              will be randomly chosen in the range [-magnitude, magnitude].</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>A 2D array of shape (total_cells, 8) where each row represents a distorted polygon             as [x1, y1, x2, y1, x2, y2, x1, y2]. The total_cells is equal to grid_height * grid_width.</p> <p>Note</p> <ul> <li>Only internal grid points are distorted; boundary points remain fixed.</li> <li>The function ensures consistent distortion across shared vertices of adjacent cells.</li> <li>The distortion is applied to the following points of each internal cell:<ul> <li>Bottom-right of the cell above and to the left</li> <li>Bottom-left of the cell above</li> <li>Top-right of the cell to the left</li> <li>Top-left of the current cell</li> </ul> </li> <li>Each square represents a cell, and the X marks indicate the coordinates where displacement occurs.     +--+--+--+--+     |  |  |  |  |     +--X--X--X--+     |  |  |  |  |     +--X--X--X--+     |  |  |  |  |     +--X--X--X--+     |  |  |  |  |     +--+--+--+--+</li> <li>For each X, the coordinates of the left, right, top, and bottom edges   in the four adjacent cells are displaced.</li> </ul> <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; dimensions = np.array([[[0, 0, 50, 50], [50, 0, 100, 50]],\n...                        [[0, 50, 50, 100], [50, 50, 100, 100]]])\n&gt;&gt;&gt; distorted = generate_distorted_grid_polygons(dimensions, magnitude=10)\n&gt;&gt;&gt; distorted.shape\n(4, 8)\n</code></pre> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>def generate_distorted_grid_polygons(\n    dimensions: np.ndarray,\n    magnitude: int,\n) -&gt; np.ndarray:\n    \"\"\"Generate distorted grid polygons based on input dimensions and magnitude.\n\n    This function creates a grid of polygons and applies random distortions to the internal vertices,\n    while keeping the boundary vertices fixed. The distortion is applied consistently across shared\n    vertices to avoid gaps or overlaps in the resulting grid.\n\n    Args:\n        dimensions (np.ndarray): A 3D array of shape (grid_height, grid_width, 4) where each element\n                                 is [x_min, y_min, x_max, y_max] representing the dimensions of a grid cell.\n        magnitude (int): Maximum pixel-wise displacement for distortion. The actual displacement\n                         will be randomly chosen in the range [-magnitude, magnitude].\n\n    Returns:\n        np.ndarray: A 2D array of shape (total_cells, 8) where each row represents a distorted polygon\n                    as [x1, y1, x2, y1, x2, y2, x1, y2]. The total_cells is equal to grid_height * grid_width.\n\n    Note:\n        - Only internal grid points are distorted; boundary points remain fixed.\n        - The function ensures consistent distortion across shared vertices of adjacent cells.\n        - The distortion is applied to the following points of each internal cell:\n            * Bottom-right of the cell above and to the left\n            * Bottom-left of the cell above\n            * Top-right of the cell to the left\n            * Top-left of the current cell\n        - Each square represents a cell, and the X marks indicate the coordinates where displacement occurs.\n            +--+--+--+--+\n            |  |  |  |  |\n            +--X--X--X--+\n            |  |  |  |  |\n            +--X--X--X--+\n            |  |  |  |  |\n            +--X--X--X--+\n            |  |  |  |  |\n            +--+--+--+--+\n        - For each X, the coordinates of the left, right, top, and bottom edges\n          in the four adjacent cells are displaced.\n\n    Example:\n        &gt;&gt;&gt; dimensions = np.array([[[0, 0, 50, 50], [50, 0, 100, 50]],\n        ...                        [[0, 50, 50, 100], [50, 50, 100, 100]]])\n        &gt;&gt;&gt; distorted = generate_distorted_grid_polygons(dimensions, magnitude=10)\n        &gt;&gt;&gt; distorted.shape\n        (4, 8)\n    \"\"\"\n    grid_height, grid_width = dimensions.shape[:2]\n    total_cells = grid_height * grid_width\n\n    # Initialize polygons\n    polygons = np.zeros((total_cells, 8), dtype=np.float32)\n    polygons[:, 0:2] = dimensions.reshape(-1, 4)[:, [0, 1]]  # x1, y1\n    polygons[:, 2:4] = dimensions.reshape(-1, 4)[:, [2, 1]]  # x2, y1\n    polygons[:, 4:6] = dimensions.reshape(-1, 4)[:, [2, 3]]  # x2, y2\n    polygons[:, 6:8] = dimensions.reshape(-1, 4)[:, [0, 3]]  # x1, y2\n\n    # Generate displacements for internal grid points only\n    internal_points_height, internal_points_width = grid_height - 1, grid_width - 1\n    displacements = random_utils.randint(\n        -magnitude,\n        magnitude + 1,\n        size=(internal_points_height, internal_points_width, 2),\n    ).astype(np.float32)\n\n    # Apply displacements to internal polygon vertices\n    for i in range(1, grid_height):\n        for j in range(1, grid_width):\n            dx, dy = displacements[i - 1, j - 1]\n\n            # Bottom-right of cell (i-1, j-1)\n            polygons[(i - 1) * grid_width + (j - 1), 4:6] += [dx, dy]\n\n            # Bottom-left of cell (i-1, j)\n            polygons[(i - 1) * grid_width + j, 6:8] += [dx, dy]\n\n            # Top-right of cell (i, j-1)\n            polygons[i * grid_width + (j - 1), 2:4] += [dx, dy]\n\n            # Top-left of cell (i, j)\n            polygons[i * grid_width + j, 0:2] += [dx, dy]\n\n    return polygons\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.functional.generate_reflected_bboxes","title":"<code>def generate_reflected_bboxes    (bboxes, grid_dims, image_shape, center_in_origin=False)    </code> [view source on GitHub]","text":"<p>Generate reflected bounding boxes for the entire reflection grid.</p> <p>Parameters:</p> Name Type Description <code>bboxes</code> <code>np.ndarray</code> <p>Original bounding boxes.</p> <code>grid_dims</code> <code>dict[str, tuple[int, int]]</code> <p>Grid dimensions and original position.</p> <code>image_shape</code> <code>tuple[int, int]</code> <p>Shape of the original image as (height, width).</p> <code>center_in_origin</code> <code>bool</code> <p>If True, center the grid at the origin. Default is False.</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Array of reflected and shifted bounding boxes for the entire grid.</p> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>def generate_reflected_bboxes(\n    bboxes: np.ndarray,\n    grid_dims: dict[str, tuple[int, int]],\n    image_shape: tuple[int, int],\n    center_in_origin: bool = False,\n) -&gt; np.ndarray:\n    \"\"\"Generate reflected bounding boxes for the entire reflection grid.\n\n    Args:\n        bboxes (np.ndarray): Original bounding boxes.\n        grid_dims (dict[str, tuple[int, int]]): Grid dimensions and original position.\n        image_shape (tuple[int, int]): Shape of the original image as (height, width).\n        center_in_origin (bool): If True, center the grid at the origin. Default is False.\n\n    Returns:\n        np.ndarray: Array of reflected and shifted bounding boxes for the entire grid.\n    \"\"\"\n    rows, cols = image_shape[:2]\n    grid_rows, grid_cols = grid_dims[\"grid_shape\"]\n    original_row, original_col = grid_dims[\"original_position\"]\n\n    # Prepare flipped versions of bboxes\n    bboxes_hflipped = flip_bboxes(bboxes, flip_horizontal=True, image_shape=image_shape)\n    bboxes_vflipped = flip_bboxes(bboxes, flip_vertical=True, image_shape=image_shape)\n    bboxes_hvflipped = flip_bboxes(bboxes, flip_horizontal=True, flip_vertical=True, image_shape=image_shape)\n\n    # Shift all versions to the original position\n    shift_vector = np.array([original_col * cols, original_row * rows, original_col * cols, original_row * rows])\n    bboxes = shift_bboxes(bboxes, shift_vector)\n    bboxes_hflipped = shift_bboxes(bboxes_hflipped, shift_vector)\n    bboxes_vflipped = shift_bboxes(bboxes_vflipped, shift_vector)\n    bboxes_hvflipped = shift_bboxes(bboxes_hvflipped, shift_vector)\n\n    new_bboxes = []\n\n    for grid_row in range(grid_rows):\n        for grid_col in range(grid_cols):\n            # Determine which version of bboxes to use based on grid position\n            if (grid_row - original_row) % 2 == 0 and (grid_col - original_col) % 2 == 0:\n                current_bboxes = bboxes\n            elif (grid_row - original_row) % 2 == 0:\n                current_bboxes = bboxes_hflipped\n            elif (grid_col - original_col) % 2 == 0:\n                current_bboxes = bboxes_vflipped\n            else:\n                current_bboxes = bboxes_hvflipped\n\n            # Shift to the current grid cell\n            cell_shift = np.array(\n                [\n                    (grid_col - original_col) * cols,\n                    (grid_row - original_row) * rows,\n                    (grid_col - original_col) * cols,\n                    (grid_row - original_row) * rows,\n                ],\n            )\n            shifted_bboxes = shift_bboxes(current_bboxes, cell_shift)\n\n            new_bboxes.append(shifted_bboxes)\n\n    result = np.vstack(new_bboxes)\n\n    return shift_bboxes(result, -shift_vector) if center_in_origin else result\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.functional.generate_reflected_keypoints","title":"<code>def generate_reflected_keypoints    (keypoints, grid_dims, image_shape, center_in_origin=False)    </code> [view source on GitHub]","text":"<p>Generate reflected keypoints for the entire reflection grid.</p> <p>This function creates a grid of keypoints by reflecting and shifting the original keypoints. It handles both centered and non-centered grids based on the <code>center_in_origin</code> parameter.</p> <p>Parameters:</p> Name Type Description <code>keypoints</code> <code>np.ndarray</code> <p>Original keypoints array of shape (N, 4+), where N is the number of keypoints,                     and each keypoint is represented by at least 4 values (x, y, angle, scale, ...).</p> <code>grid_dims</code> <code>dict[str, tuple[int, int]]</code> <p>A dictionary containing grid dimensions and original position. It should have the following keys: - \"grid_shape\": tuple[int, int] representing (grid_rows, grid_cols) - \"original_position\": tuple[int, int] representing (original_row, original_col)</p> <code>image_shape</code> <code>tuple[int, int]</code> <p>Shape of the original image as (height, width).</p> <code>center_in_origin</code> <code>bool</code> <p>If True, center the grid at the origin. Default is False.</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Array of reflected and shifted keypoints for the entire grid. The shape is             (N * grid_rows * grid_cols, 4+), where N is the number of original keypoints.</p> <p>Note</p> <ul> <li>The function handles keypoint flipping and shifting to create a grid of reflected keypoints.</li> <li>It preserves the angle and scale information of the keypoints during transformations.</li> <li>The resulting grid can be either centered at the origin or positioned based on the original grid.</li> </ul> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>def generate_reflected_keypoints(\n    keypoints: np.ndarray,\n    grid_dims: dict[str, tuple[int, int]],\n    image_shape: tuple[int, int],\n    center_in_origin: bool = False,\n) -&gt; np.ndarray:\n    \"\"\"Generate reflected keypoints for the entire reflection grid.\n\n    This function creates a grid of keypoints by reflecting and shifting the original keypoints.\n    It handles both centered and non-centered grids based on the `center_in_origin` parameter.\n\n    Args:\n        keypoints (np.ndarray): Original keypoints array of shape (N, 4+), where N is the number of keypoints,\n                                and each keypoint is represented by at least 4 values (x, y, angle, scale, ...).\n        grid_dims (dict[str, tuple[int, int]]): A dictionary containing grid dimensions and original position.\n            It should have the following keys:\n            - \"grid_shape\": tuple[int, int] representing (grid_rows, grid_cols)\n            - \"original_position\": tuple[int, int] representing (original_row, original_col)\n        image_shape (tuple[int, int]): Shape of the original image as (height, width).\n        center_in_origin (bool, optional): If True, center the grid at the origin. Default is False.\n\n    Returns:\n        np.ndarray: Array of reflected and shifted keypoints for the entire grid. The shape is\n                    (N * grid_rows * grid_cols, 4+), where N is the number of original keypoints.\n\n    Note:\n        - The function handles keypoint flipping and shifting to create a grid of reflected keypoints.\n        - It preserves the angle and scale information of the keypoints during transformations.\n        - The resulting grid can be either centered at the origin or positioned based on the original grid.\n    \"\"\"\n    grid_rows, grid_cols = grid_dims[\"grid_shape\"]\n    original_row, original_col = grid_dims[\"original_position\"]\n\n    # Prepare flipped versions of keypoints\n    keypoints_hflipped = flip_keypoints(keypoints, flip_horizontal=True, image_shape=image_shape)\n    keypoints_vflipped = flip_keypoints(keypoints, flip_vertical=True, image_shape=image_shape)\n    keypoints_hvflipped = flip_keypoints(keypoints, flip_horizontal=True, flip_vertical=True, image_shape=image_shape)\n\n    rows, cols = image_shape[:2]\n\n    # Shift all versions to the original position\n    shift_vector = np.array([original_col * cols, original_row * rows, 0, 0])  # Only shift x and y\n    keypoints = shift_keypoints(keypoints, shift_vector)\n    keypoints_hflipped = shift_keypoints(keypoints_hflipped, shift_vector)\n    keypoints_vflipped = shift_keypoints(keypoints_vflipped, shift_vector)\n    keypoints_hvflipped = shift_keypoints(keypoints_hvflipped, shift_vector)\n\n    new_keypoints = []\n\n    for grid_row in range(grid_rows):\n        for grid_col in range(grid_cols):\n            # Determine which version of keypoints to use based on grid position\n            if (grid_row - original_row) % 2 == 0 and (grid_col - original_col) % 2 == 0:\n                current_keypoints = keypoints\n            elif (grid_row - original_row) % 2 == 0:\n                current_keypoints = keypoints_hflipped\n            elif (grid_col - original_col) % 2 == 0:\n                current_keypoints = keypoints_vflipped\n            else:\n                current_keypoints = keypoints_hvflipped\n\n            # Shift to the current grid cell\n            cell_shift = np.array([(grid_col - original_col) * cols, (grid_row - original_row) * rows, 0, 0])\n            shifted_keypoints = shift_keypoints(current_keypoints, cell_shift)\n\n            new_keypoints.append(shifted_keypoints)\n\n    result = np.vstack(new_keypoints)\n\n    return shift_keypoints(result, -shift_vector) if center_in_origin else result\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.functional.get_pad_grid_dimensions","title":"<code>def get_pad_grid_dimensions    (pad_top, pad_bottom, pad_left, pad_right, image_shape)    </code> [view source on GitHub]","text":"<p>Calculate the dimensions of the grid needed for reflection padding and the position of the original image.</p> <p>Parameters:</p> Name Type Description <code>pad_top</code> <code>int</code> <p>Number of pixels to pad above the image.</p> <code>pad_bottom</code> <code>int</code> <p>Number of pixels to pad below the image.</p> <code>pad_left</code> <code>int</code> <p>Number of pixels to pad to the left of the image.</p> <code>pad_right</code> <code>int</code> <p>Number of pixels to pad to the right of the image.</p> <code>image_shape</code> <code>tuple[int, int]</code> <p>Shape of the original image as (height, width).</p> <p>Returns:</p> Type Description <code>dict[str, tuple[int, int]]</code> <p>A dictionary containing:     - 'grid_shape': A tuple (grid_rows, grid_cols) where:         - grid_rows (int): Number of times the image needs to be repeated vertically.         - grid_cols (int): Number of times the image needs to be repeated horizontally.     - 'original_position': A tuple (original_row, original_col) where:         - original_row (int): Row index of the original image in the grid.         - original_col (int): Column index of the original image in the grid.</p> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>def get_pad_grid_dimensions(\n    pad_top: int,\n    pad_bottom: int,\n    pad_left: int,\n    pad_right: int,\n    image_shape: tuple[int, int],\n) -&gt; dict[str, tuple[int, int]]:\n    \"\"\"Calculate the dimensions of the grid needed for reflection padding and the position of the original image.\n\n    Args:\n        pad_top (int): Number of pixels to pad above the image.\n        pad_bottom (int): Number of pixels to pad below the image.\n        pad_left (int): Number of pixels to pad to the left of the image.\n        pad_right (int): Number of pixels to pad to the right of the image.\n        image_shape (tuple[int, int]): Shape of the original image as (height, width).\n\n    Returns:\n        dict[str, tuple[int, int]]: A dictionary containing:\n            - 'grid_shape': A tuple (grid_rows, grid_cols) where:\n                - grid_rows (int): Number of times the image needs to be repeated vertically.\n                - grid_cols (int): Number of times the image needs to be repeated horizontally.\n            - 'original_position': A tuple (original_row, original_col) where:\n                - original_row (int): Row index of the original image in the grid.\n                - original_col (int): Column index of the original image in the grid.\n    \"\"\"\n    rows, cols = image_shape[:2]\n\n    grid_rows = 1 + math.ceil(pad_top / rows) + math.ceil(pad_bottom / rows)\n    grid_cols = 1 + math.ceil(pad_left / cols) + math.ceil(pad_right / cols)\n    original_row = math.ceil(pad_top / rows)\n    original_col = math.ceil(pad_left / cols)\n\n    return {\"grid_shape\": (grid_rows, grid_cols), \"original_position\": (original_row, original_col)}\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.functional.keypoint_d4","title":"<code>def keypoint_d4    (keypoint, group_member, image_shape, ** params)    </code> [view source on GitHub]","text":"<p>Applies a <code>D_4</code> symmetry group transformation to a keypoint.</p> <p>This function adjusts a keypoint's coordinates according to the specified <code>D_4</code> group transformation, which includes rotations and reflections suitable for image processing tasks. These transformations account for the dimensions of the image to ensure the keypoint remains within its boundaries.</p> <ul> <li>keypoint (KeypointInternalType): The keypoint to transform. T     his should be a structure or tuple specifying coordinates     like (x, y, [additional parameters]).</li> <li>group_member (D4Type): A string identifier for the <code>D_4</code> group transformation to apply.     Valid values are 'e', 'r90', 'r180', 'r270', 'v', 'hv', 'h', 't'.</li> <li>image_shape (tuple[int, int]): The shape of the image.</li> <li>params (Any): Not used</li> </ul> <ul> <li>KeypointInternalType: The transformed keypoint.</li> </ul> <ul> <li>ValueError: If an invalid group member is specified, indicating that the specified transformation does not exist.</li> </ul> <p>Examples:</p> <ul> <li>Rotating a keypoint by 90 degrees in a 100x100 image:   <code>keypoint_d4((50, 30), 'r90', 100, 100)</code>   This would move the keypoint from (50, 30) to (70, 50) assuming standard coordinate transformations.</li> </ul> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>def keypoint_d4(\n    keypoint: KeypointInternalType,\n    group_member: D4Type,\n    image_shape: tuple[int, int],\n    **params: Any,\n) -&gt; KeypointInternalType:\n    \"\"\"Applies a `D_4` symmetry group transformation to a keypoint.\n\n    This function adjusts a keypoint's coordinates according to the specified `D_4` group transformation,\n    which includes rotations and reflections suitable for image processing tasks. These transformations account\n    for the dimensions of the image to ensure the keypoint remains within its boundaries.\n\n    Parameters:\n    - keypoint (KeypointInternalType): The keypoint to transform. T\n        his should be a structure or tuple specifying coordinates\n        like (x, y, [additional parameters]).\n    - group_member (D4Type): A string identifier for the `D_4` group transformation to apply.\n        Valid values are 'e', 'r90', 'r180', 'r270', 'v', 'hv', 'h', 't'.\n    - image_shape (tuple[int, int]): The shape of the image.\n    - params (Any): Not used\n\n    Returns:\n    - KeypointInternalType: The transformed keypoint.\n\n    Raises:\n    - ValueError: If an invalid group member is specified, indicating that the specified transformation does not exist.\n\n    Examples:\n    - Rotating a keypoint by 90 degrees in a 100x100 image:\n      `keypoint_d4((50, 30), 'r90', 100, 100)`\n      This would move the keypoint from (50, 30) to (70, 50) assuming standard coordinate transformations.\n    \"\"\"\n    rows, cols = image_shape[:2]\n    transformations = {\n        \"e\": lambda x: x,  # Identity transformation\n        \"r90\": lambda x: keypoint_rot90(x, 1, image_shape),  # Rotate 90 degrees\n        \"r180\": lambda x: keypoint_rot90(x, 2, image_shape),  # Rotate 180 degrees\n        \"r270\": lambda x: keypoint_rot90(x, 3, image_shape),  # Rotate 270 degrees\n        \"v\": lambda x: keypoint_vflip(x, rows),  # Vertical flip\n        \"hvt\": lambda x: keypoint_transpose(keypoint_rot90(x, 2, image_shape)),  # Reflect over anti diagonal\n        \"h\": lambda x: keypoint_hflip(x, cols),  # Horizontal flip\n        \"t\": lambda x: keypoint_transpose(x),  # Transpose (reflect over main diagonal)\n    }\n    # Execute the appropriate transformation\n    if group_member in transformations:\n        return transformations[group_member](keypoint)\n\n    raise ValueError(f\"Invalid group member: {group_member}\")\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.functional.keypoint_flip","title":"<code>def keypoint_flip    (keypoint, d, image_shape)    </code> [view source on GitHub]","text":"<p>Flip a keypoint either vertically, horizontally or both depending on the value of <code>d</code>.</p> <p>Parameters:</p> Name Type Description <code>keypoint</code> <code>KeypointInternalType</code> <p>A keypoint <code>(x, y, angle, scale)</code>.</p> <code>d</code> <code>int</code> <p>Number of flip. Must be -1, 0 or 1: * 0 - vertical flip, * 1 - horizontal flip, * -1 - vertical and horizontal flip.</p> <code>image_shape</code> <code>tuple[int, int]</code> <p>A tuple of image shape <code>(height, width, channels)</code>.</p> <p>Returns:</p> Type Description <code>KeypointInternalType</code> <p>A keypoint <code>(x, y, angle, scale)</code>.</p> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>if value of <code>d</code> is not -1, 0 or 1.</p> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>@angle_2pi_range\ndef keypoint_flip(keypoint: KeypointInternalType, d: int, image_shape: tuple[int, int]) -&gt; KeypointInternalType:\n    \"\"\"Flip a keypoint either vertically, horizontally or both depending on the value of `d`.\n\n    Args:\n        keypoint: A keypoint `(x, y, angle, scale)`.\n        d: Number of flip. Must be -1, 0 or 1:\n            * 0 - vertical flip,\n            * 1 - horizontal flip,\n            * -1 - vertical and horizontal flip.\n        image_shape: A tuple of image shape `(height, width, channels)`.\n\n    Returns:\n        A keypoint `(x, y, angle, scale)`.\n\n    Raises:\n        ValueError: if value of `d` is not -1, 0 or 1.\n\n    \"\"\"\n    rows, cols = image_shape[:2]\n\n    if d == 0:\n        keypoint = keypoint_vflip(keypoint, rows)\n    elif d == 1:\n        keypoint = keypoint_hflip(keypoint, cols)\n    elif d == -1:\n        keypoint = keypoint_hflip(keypoint, cols)\n        keypoint = keypoint_vflip(keypoint, rows)\n    else:\n        raise ValueError(f\"Invalid d value {d}. Valid values are -1, 0 and 1\")\n    return keypoint\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.functional.keypoint_hflip","title":"<code>def keypoint_hflip    (keypoint, cols)    </code> [view source on GitHub]","text":"<p>Flip a keypoint horizontally around the y-axis.</p> <p>Parameters:</p> Name Type Description <code>keypoint</code> <code>KeypointInternalType</code> <p>A keypoint <code>(x, y, angle, scale)</code>.</p> <code>cols</code> <code>int</code> <p>Image width.</p> <p>Returns:</p> Type Description <code>KeypointInternalType</code> <p>A keypoint <code>(x, y, angle, scale)</code>.</p> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>@angle_2pi_range\ndef keypoint_hflip(keypoint: KeypointInternalType, cols: int) -&gt; KeypointInternalType:\n    \"\"\"Flip a keypoint horizontally around the y-axis.\n\n    Args:\n        keypoint: A keypoint `(x, y, angle, scale)`.\n        cols: Image width.\n\n    Returns:\n        A keypoint `(x, y, angle, scale)`.\n\n    \"\"\"\n    x, y, angle, scale = keypoint[:4]\n    angle = math.pi - angle\n    return (cols - 1) - x, y, angle, scale\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.functional.keypoint_rot90","title":"<code>def keypoint_rot90    (keypoint, factor, image_shape, ** params)    </code> [view source on GitHub]","text":"<p>Rotate a keypoint by 90 degrees counter-clockwise (CCW) a specified number of times.</p> <p>Parameters:</p> Name Type Description <code>keypoint</code> <code>KeypointInternalType</code> <p>A keypoint in the format <code>(x, y, angle, scale)</code>.</p> <code>factor</code> <code>int</code> <p>The number of 90 degree CCW rotations to apply. Must be in the range [0, 3].</p> <code>image_shape</code> <code>tuple[int, int]</code> <p>The shape of the image.</p> <code>**params</code> <code>Any</code> <p>Additional parameters.</p> <p>Returns:</p> Type Description <code>KeypointInternalType</code> <p>The rotated keypoint in the format <code>(x, y, angle, scale)</code>.</p> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If the factor is not in the set {0, 1, 2, 3}.</p> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>@angle_2pi_range\ndef keypoint_rot90(\n    keypoint: KeypointInternalType,\n    factor: int,\n    image_shape: tuple[int, int],\n    **params: Any,\n) -&gt; KeypointInternalType:\n    \"\"\"Rotate a keypoint by 90 degrees counter-clockwise (CCW) a specified number of times.\n\n    Args:\n        keypoint (KeypointInternalType): A keypoint in the format `(x, y, angle, scale)`.\n        factor (int): The number of 90 degree CCW rotations to apply. Must be in the range [0, 3].\n        image_shape (tuple[int, int]): The shape of the image.\n        **params: Additional parameters.\n\n    Returns:\n        KeypointInternalType: The rotated keypoint in the format `(x, y, angle, scale)`.\n\n    Raises:\n        ValueError: If the factor is not in the set {0, 1, 2, 3}.\n    \"\"\"\n    x, y, angle, scale = keypoint\n\n    if factor not in {0, 1, 2, 3}:\n        raise ValueError(\"Parameter factor must be in set {0, 1, 2, 3}\")\n\n    rows, cols = image_shape[:2]\n\n    if factor == 1:\n        x, y, angle = y, (cols - 1) - x, angle - math.pi / 2\n    elif factor == ROT90_180_FACTOR:\n        x, y, angle = (cols - 1) - x, (rows - 1) - y, angle - math.pi\n    elif factor == ROT90_270_FACTOR:\n        x, y, angle = (rows - 1) - y, x, angle + math.pi / 2\n\n    return x, y, angle, scale\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.functional.keypoint_rotate","title":"<code>def keypoint_rotate    (keypoint, angle, image_shape, ** params)    </code> [view source on GitHub]","text":"<p>Rotate a keypoint by a specified angle.</p> <p>Parameters:</p> Name Type Description <code>keypoint</code> <code>KeypointInternalType</code> <p>A keypoint in the format <code>(x, y, angle, scale)</code>.</p> <code>angle</code> <code>float</code> <p>The angle by which to rotate the keypoint, in degrees.</p> <code>image_shape</code> <code>tuple[int, int]</code> <p>The shape of the image the keypoint belongs to.</p> <code>**params</code> <code>Any</code> <p>Additional parameters.</p> <p>Returns:</p> Type Description <code>KeypointInternalType</code> <p>The rotated keypoint in the format <code>(x, y, angle, scale)</code>.</p> <p>Note</p> <p>The rotation is performed around the center of the image.</p> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>@angle_2pi_range\ndef keypoint_rotate(\n    keypoint: KeypointInternalType,\n    angle: float,\n    image_shape: tuple[int, int],\n    **params: Any,\n) -&gt; KeypointInternalType:\n    \"\"\"Rotate a keypoint by a specified angle.\n\n    Args:\n        keypoint (KeypointInternalType): A keypoint in the format `(x, y, angle, scale)`.\n        angle (float): The angle by which to rotate the keypoint, in degrees.\n        image_shape (tuple[int, int]): The shape of the image the keypoint belongs to.\n        **params: Additional parameters.\n\n    Returns:\n        KeypointInternalType: The rotated keypoint in the format `(x, y, angle, scale)`.\n\n    Note:\n        The rotation is performed around the center of the image.\n    \"\"\"\n    image_center = center(image_shape)\n    matrix = cv2.getRotationMatrix2D(image_center, angle, 1.0)\n    x, y, a, s = keypoint[:4]\n    x, y = cv2.transform(np.array([[[x, y]]]), matrix).squeeze()\n    return x, y, a + math.radians(angle), s\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.functional.keypoint_scale","title":"<code>def keypoint_scale    (keypoint, scale_x, scale_y)    </code> [view source on GitHub]","text":"<p>Scales a keypoint by scale_x and scale_y.</p> <p>Parameters:</p> Name Type Description <code>keypoint</code> <code>KeypointInternalType</code> <p>A keypoint <code>(x, y, angle, scale)</code>.</p> <code>scale_x</code> <code>float</code> <p>Scale coefficient x-axis.</p> <code>scale_y</code> <code>float</code> <p>Scale coefficient y-axis.</p> <p>Returns:</p> Type Description <code>KeypointInternalType</code> <p>A keypoint <code>(x, y, angle, scale)</code>.</p> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>def keypoint_scale(keypoint: KeypointInternalType, scale_x: float, scale_y: float) -&gt; KeypointInternalType:\n    \"\"\"Scales a keypoint by scale_x and scale_y.\n\n    Args:\n        keypoint: A keypoint `(x, y, angle, scale)`.\n        scale_x: Scale coefficient x-axis.\n        scale_y: Scale coefficient y-axis.\n\n    Returns:\n        A keypoint `(x, y, angle, scale)`.\n\n    \"\"\"\n    x, y, angle, scale = keypoint[:4]\n    return x * scale_x, y * scale_y, angle, scale * max(scale_x, scale_y)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.functional.keypoint_transpose","title":"<code>def keypoint_transpose    (keypoint)    </code> [view source on GitHub]","text":"<p>Transposes a keypoint along a specified axis: main diagonal</p> <p>Parameters:</p> Name Type Description <code>keypoint</code> <code>KeypointInternalType</code> <p>A keypoint <code>(x, y, angle, scale)</code>.</p> <p>Returns:</p> Type Description <code>KeypointInternalType</code> <p>A transformed keypoint <code>(x, y, angle, scale)</code>.</p> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If axis is not 0 or 1.</p> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>@angle_2pi_range\ndef keypoint_transpose(keypoint: KeypointInternalType) -&gt; KeypointInternalType:\n    \"\"\"Transposes a keypoint along a specified axis: main diagonal\n\n    Args:\n        keypoint: A keypoint `(x, y, angle, scale)`.\n\n    Returns:\n        A transformed keypoint `(x, y, angle, scale)`.\n\n    Raises:\n        ValueError: If axis is not 0 or 1.\n\n    \"\"\"\n    x, y, angle, scale = keypoint[:4]\n\n    # Transpose over the main diagonal: swap x and y.\n    new_x, new_y = y, x\n    # Adjust angle to reflect the coordinate swap.\n    angle = np.pi / 2 - angle if angle &lt;= np.pi else 3 * np.pi / 2 - angle\n\n    return new_x, new_y, angle, scale\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.functional.keypoint_vflip","title":"<code>def keypoint_vflip    (keypoint, rows)    </code> [view source on GitHub]","text":"<p>Flip a keypoint vertically around the x-axis.</p> <p>Parameters:</p> Name Type Description <code>keypoint</code> <code>KeypointInternalType</code> <p>A keypoint <code>(x, y, angle, scale)</code>.</p> <code>rows</code> <code>int</code> <p>Image height.</p> <p>Returns:</p> Type Description <code>tuple</code> <p>A keypoint <code>(x, y, angle, scale)</code>.</p> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>@angle_2pi_range\ndef keypoint_vflip(keypoint: KeypointInternalType, rows: int) -&gt; KeypointInternalType:\n    \"\"\"Flip a keypoint vertically around the x-axis.\n\n    Args:\n        keypoint: A keypoint `(x, y, angle, scale)`.\n        rows: Image height.\n\n\n    Returns:\n        tuple: A keypoint `(x, y, angle, scale)`.\n\n    \"\"\"\n    x, y, angle, scale = keypoint[:4]\n    angle = -angle\n    return x, (rows - 1) - y, angle, scale\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.functional.keypoints_affine","title":"<code>def keypoints_affine    (keypoints, matrix, image_shape, scale, mode)    </code> [view source on GitHub]","text":"<p>Apply an affine transformation to keypoints.</p> <p>This function transforms keypoints using the given affine transformation matrix. It handles reflection padding if necessary, updates coordinates, angles, and scales.</p> <p>Parameters:</p> Name Type Description <code>keypoints</code> <code>np.ndarray</code> <p>Array of keypoints with shape (N, 4+) where N is the number of keypoints.                     Each keypoint is represented as [x, y, angle, scale, ...].</p> <code>matrix</code> <code>skimage.transform.ProjectiveTransform</code> <p>The affine transformation matrix.</p> <code>image_shape</code> <code>tuple[int, int]</code> <p>Shape of the image (height, width).</p> <code>scale</code> <code>dict[str, Any]</code> <p>Dictionary containing scale factors for x and y directions.                     Expected keys are 'x' and 'y'.</p> <code>mode</code> <code>int</code> <p>Border mode for handling keypoints near image edges.         Use cv2.BORDER_REFLECT_101, cv2.BORDER_REFLECT, etc.</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Transformed keypoints array with the same shape as input.</p> <p>Notes</p> <ul> <li>The function applies reflection padding if the mode is in REFLECT_BORDER_MODES.</li> <li>Coordinates (x, y) are transformed using the affine matrix.</li> <li>Angles are adjusted based on the rotation component of the affine transformation.</li> <li>Scales are multiplied by the maximum of x and y scale factors.</li> <li>The @angle_2pi_range decorator ensures angles remain in the [0, 2\u03c0] range.</li> </ul> <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; keypoints = np.array([[100, 100, 0, 1]])\n&gt;&gt;&gt; matrix = skimage.transform.ProjectiveTransform(...)\n&gt;&gt;&gt; scale = {'x': 1.5, 'y': 1.2}\n&gt;&gt;&gt; transformed_keypoints = keypoints_affine(keypoints, matrix, (480, 640), scale, cv2.BORDER_REFLECT_101)\n</code></pre> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>@angle_2pi_range\ndef keypoints_affine(\n    keypoints: np.ndarray,\n    matrix: skimage.transform.ProjectiveTransform,\n    image_shape: tuple[int, int],\n    scale: dict[str, Any],\n    mode: int,\n) -&gt; np.ndarray:\n    \"\"\"Apply an affine transformation to keypoints.\n\n    This function transforms keypoints using the given affine transformation matrix.\n    It handles reflection padding if necessary, updates coordinates, angles, and scales.\n\n    Args:\n        keypoints (np.ndarray): Array of keypoints with shape (N, 4+) where N is the number of keypoints.\n                                Each keypoint is represented as [x, y, angle, scale, ...].\n        matrix (skimage.transform.ProjectiveTransform): The affine transformation matrix.\n        image_shape (tuple[int, int]): Shape of the image (height, width).\n        scale (dict[str, Any]): Dictionary containing scale factors for x and y directions.\n                                Expected keys are 'x' and 'y'.\n        mode (int): Border mode for handling keypoints near image edges.\n                    Use cv2.BORDER_REFLECT_101, cv2.BORDER_REFLECT, etc.\n\n    Returns:\n        np.ndarray: Transformed keypoints array with the same shape as input.\n\n    Notes:\n        - The function applies reflection padding if the mode is in REFLECT_BORDER_MODES.\n        - Coordinates (x, y) are transformed using the affine matrix.\n        - Angles are adjusted based on the rotation component of the affine transformation.\n        - Scales are multiplied by the maximum of x and y scale factors.\n        - The @angle_2pi_range decorator ensures angles remain in the [0, 2\u03c0] range.\n\n    Example:\n        &gt;&gt;&gt; keypoints = np.array([[100, 100, 0, 1]])\n        &gt;&gt;&gt; matrix = skimage.transform.ProjectiveTransform(...)\n        &gt;&gt;&gt; scale = {'x': 1.5, 'y': 1.2}\n        &gt;&gt;&gt; transformed_keypoints = keypoints_affine(keypoints, matrix, (480, 640), scale, cv2.BORDER_REFLECT_101)\n    \"\"\"\n    keypoints = keypoints.copy().astype(np.float32)\n\n    if is_identity_matrix(matrix):\n        return keypoints\n\n    if mode in REFLECT_BORDER_MODES:\n        # Step 1: Compute affine transform padding\n        pad_left, pad_right, pad_top, pad_bottom = calculate_affine_transform_padding(matrix, image_shape)\n        grid_dimensions = get_pad_grid_dimensions(pad_top, pad_bottom, pad_left, pad_right, image_shape)\n        keypoints = generate_reflected_keypoints(keypoints, grid_dimensions, image_shape, center_in_origin=True)\n\n    # Extract x, y coordinates\n    xy = keypoints[:, :2]\n\n    # Transform x, y coordinates\n    xy_transformed = cv2.transform(xy.reshape(-1, 1, 2), matrix.params[:2]).squeeze()\n\n    # Calculate angle adjustment\n    angle_adjustment = rotation2d_matrix_to_euler_angles(matrix.params[:2], y_up=False)\n\n    # Update angles\n    keypoints[:, 2] = keypoints[:, 2] + angle_adjustment\n\n    # Update scales\n    max_scale = max(scale[\"x\"], scale[\"y\"])\n\n    keypoints[:, 3] *= max_scale\n\n    # Update x, y coordinates\n    keypoints[:, :2] = xy_transformed\n\n    return keypoints\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.functional.optical_distortion","title":"<code>def optical_distortion    (img, k, dx, dy, interpolation, border_mode, value=None)    </code> [view source on GitHub]","text":"<p>Barrel / pincushion distortion. Unconventional augment.</p> <p>Reference</p> <p>|  https://stackoverflow.com/questions/6199636/formulas-for-barrel-pincushion-distortion |  https://stackoverflow.com/questions/10364201/image-transformation-in-opencv |  https://stackoverflow.com/questions/2477774/correcting-fisheye-distortion-programmatically |  http://www.coldvision.io/2017/03/02/advanced-lane-finding-using-opencv/</p> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>@preserve_channel_dim\ndef optical_distortion(\n    img: np.ndarray,\n    k: int,\n    dx: int,\n    dy: int,\n    interpolation: int,\n    border_mode: int,\n    value: ColorType | None = None,\n) -&gt; np.ndarray:\n    \"\"\"Barrel / pincushion distortion. Unconventional augment.\n\n    Reference:\n        |  https://stackoverflow.com/questions/6199636/formulas-for-barrel-pincushion-distortion\n        |  https://stackoverflow.com/questions/10364201/image-transformation-in-opencv\n        |  https://stackoverflow.com/questions/2477774/correcting-fisheye-distortion-programmatically\n        |  http://www.coldvision.io/2017/03/02/advanced-lane-finding-using-opencv/\n    \"\"\"\n    height, width = img.shape[:2]\n\n    fx = width\n    fy = height\n\n    cx = width * 0.5 + dx\n    cy = height * 0.5 + dy\n\n    camera_matrix = np.array([[fx, 0, cx], [0, fy, cy], [0, 0, 1]], dtype=np.float32)\n\n    distortion = np.array([k, k, 0, 0, 0], dtype=np.float32)\n    map1, map2 = cv2.initUndistortRectifyMap(camera_matrix, distortion, None, None, (width, height), cv2.CV_32FC1)\n    return cv2.remap(img, map1, map2, interpolation=interpolation, borderMode=border_mode, borderValue=value)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.functional.rotation2d_matrix_to_euler_angles","title":"<code>def rotation2d_matrix_to_euler_angles    (matrix, y_up)    </code> [view source on GitHub]","text":"<p>matrix (np.ndarray): Rotation matrix y_up (bool): is Y axis looks up or down</p> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>def rotation2d_matrix_to_euler_angles(matrix: np.ndarray, y_up: bool) -&gt; float:\n    \"\"\"Args:\n    matrix (np.ndarray): Rotation matrix\n    y_up (bool): is Y axis looks up or down\n\n    \"\"\"\n    if y_up:\n        return np.arctan2(matrix[1, 0], matrix[0, 0])\n    return np.arctan2(-matrix[1, 0], matrix[0, 0])\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.functional.shift_bboxes","title":"<code>def shift_bboxes    (bboxes, shift_vector)    </code> [view source on GitHub]","text":"<p>Shift bounding boxes by a given vector.</p> <p>Parameters:</p> Name Type Description <code>bboxes</code> <code>np.ndarray</code> <p>Array of bounding boxes with shape (n, m) where n is the number of bboxes                  and m &gt;= 4. The first 4 columns are [x_min, y_min, x_max, y_max].</p> <code>shift_vector</code> <code>np.ndarray</code> <p>Vector to shift the bounding boxes by, with shape (4,) for                        [shift_x, shift_y, shift_x, shift_y].</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Shifted bounding boxes with the same shape as input.</p> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>def shift_bboxes(bboxes: np.ndarray, shift_vector: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Shift bounding boxes by a given vector.\n\n    Args:\n        bboxes (np.ndarray): Array of bounding boxes with shape (n, m) where n is the number of bboxes\n                             and m &gt;= 4. The first 4 columns are [x_min, y_min, x_max, y_max].\n        shift_vector (np.ndarray): Vector to shift the bounding boxes by, with shape (4,) for\n                                   [shift_x, shift_y, shift_x, shift_y].\n\n    Returns:\n        np.ndarray: Shifted bounding boxes with the same shape as input.\n    \"\"\"\n    # Create a copy of the input array to avoid modifying it in-place\n    shifted_bboxes = bboxes.copy()\n\n    # Add the shift vector to the first 4 columns\n    shifted_bboxes[:, :4] += shift_vector\n\n    return shifted_bboxes\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.functional.to_distance_maps","title":"<code>def to_distance_maps    (keypoints, image_shape, inverted=False)    </code> [view source on GitHub]","text":"<p>Generate a <code>(H,W,N)</code> array of distance maps for <code>N</code> keypoints.</p> <p>The <code>n</code>-th distance map contains at every location <code>(y, x)</code> the euclidean distance to the <code>n</code>-th keypoint.</p> <p>This function can be used as a helper when augmenting keypoints with a method that only supports the augmentation of images.</p> <p>Parameters:</p> Name Type Description <code>keypoints</code> <code>Sequence[tuple[float, float]]</code> <p>keypoint coordinates</p> <code>image_shape</code> <code>tuple[int, int]</code> <p>tuple[int, int] shape of the image</p> <code>inverted</code> <code>bool</code> <p>If <code>True</code>, inverted distance maps are returned where each distance value d is replaced by <code>d/(d+1)</code>, i.e. the distance maps have values in the range <code>(0.0, 1.0]</code> with <code>1.0</code> denoting exactly the position of the respective keypoint.</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>(H, W, N) ndarray     A <code>float32</code> array containing <code>N</code> distance maps for <code>N</code>     keypoints. Each location <code>(y, x, n)</code> in the array denotes the     euclidean distance at <code>(y, x)</code> to the <code>n</code>-th keypoint.     If <code>inverted</code> is <code>True</code>, the distance <code>d</code> is replaced     by <code>d/(d+1)</code>. The height and width of the array match the     height and width in <code>KeypointsOnImage.shape</code>.</p> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>def to_distance_maps(\n    keypoints: Sequence[tuple[float, float]],\n    image_shape: tuple[int, int],\n    inverted: bool = False,\n) -&gt; np.ndarray:\n    \"\"\"Generate a ``(H,W,N)`` array of distance maps for ``N`` keypoints.\n\n    The ``n``-th distance map contains at every location ``(y, x)`` the\n    euclidean distance to the ``n``-th keypoint.\n\n    This function can be used as a helper when augmenting keypoints with a\n    method that only supports the augmentation of images.\n\n    Args:\n        keypoints: keypoint coordinates\n        image_shape: tuple[int, int] shape of the image\n        inverted (bool): If ``True``, inverted distance maps are returned where each\n            distance value d is replaced by ``d/(d+1)``, i.e. the distance\n            maps have values in the range ``(0.0, 1.0]`` with ``1.0`` denoting\n            exactly the position of the respective keypoint.\n\n    Returns:\n        (H, W, N) ndarray\n            A ``float32`` array containing ``N`` distance maps for ``N``\n            keypoints. Each location ``(y, x, n)`` in the array denotes the\n            euclidean distance at ``(y, x)`` to the ``n``-th keypoint.\n            If `inverted` is ``True``, the distance ``d`` is replaced\n            by ``d/(d+1)``. The height and width of the array match the\n            height and width in ``KeypointsOnImage.shape``.\n\n    \"\"\"\n    height, width = image_shape[:2]\n    distance_maps = np.zeros((height, width, len(keypoints)), dtype=np.float32)\n\n    yy = np.arange(0, height)\n    xx = np.arange(0, width)\n    grid_xx, grid_yy = np.meshgrid(xx, yy)\n\n    for i, (x, y) in enumerate(keypoints):\n        distance_maps[:, :, i] = (grid_xx - x) ** 2 + (grid_yy - y) ** 2\n\n    distance_maps = np.sqrt(distance_maps)\n    if inverted:\n        return 1 / (distance_maps + 1)\n    return distance_maps\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.functional.transpose","title":"<code>def transpose    (img)    </code> [view source on GitHub]","text":"<p>Transposes the first two dimensions of an array of any dimensionality. Retains the order of any additional dimensions.</p> <p>Parameters:</p> Name Type Description <code>img</code> <code>np.ndarray</code> <p>Input array.</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Transposed array.</p> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>def transpose(img: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Transposes the first two dimensions of an array of any dimensionality.\n    Retains the order of any additional dimensions.\n\n    Args:\n        img (np.ndarray): Input array.\n\n    Returns:\n        np.ndarray: Transposed array.\n    \"\"\"\n    # Generate the new axes order\n    new_axes = list(range(img.ndim))\n    new_axes[0], new_axes[1] = 1, 0  # Swap the first two dimensions\n\n    # Transpose the array using the new axes order\n    return img.transpose(new_axes)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.functional.validate_bboxes","title":"<code>def validate_bboxes    (bboxes, image_shape)    </code> [view source on GitHub]","text":"<p>Validate bounding boxes and remove invalid ones.</p> <p>Parameters:</p> Name Type Description <code>bboxes</code> <code>np.ndarray</code> <p>Array of bounding boxes with shape (n, 4) where each row is [x_min, y_min, x_max, y_max].</p> <code>image_shape</code> <code>tuple[int, int]</code> <p>Shape of the image as (height, width).</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Array of valid bounding boxes, potentially with fewer boxes than the input.</p> <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; bboxes = np.array([[10, 20, 30, 40], [-10, -10, 5, 5], [100, 100, 120, 120]])\n&gt;&gt;&gt; valid_bboxes = validate_bboxes(bboxes, (100, 100))\n&gt;&gt;&gt; print(valid_bboxes)\n[[10 20 30 40]]\n</code></pre> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>def validate_bboxes(bboxes: np.ndarray, image_shape: Sequence[int]) -&gt; np.ndarray:\n    \"\"\"Validate bounding boxes and remove invalid ones.\n\n    Args:\n        bboxes (np.ndarray): Array of bounding boxes with shape (n, 4) where each row is [x_min, y_min, x_max, y_max].\n        image_shape (tuple[int, int]): Shape of the image as (height, width).\n\n    Returns:\n        np.ndarray: Array of valid bounding boxes, potentially with fewer boxes than the input.\n\n    Example:\n        &gt;&gt;&gt; bboxes = np.array([[10, 20, 30, 40], [-10, -10, 5, 5], [100, 100, 120, 120]])\n        &gt;&gt;&gt; valid_bboxes = validate_bboxes(bboxes, (100, 100))\n        &gt;&gt;&gt; print(valid_bboxes)\n        [[10 20 30 40]]\n    \"\"\"\n    rows, cols = image_shape[:2]\n\n    x_min, y_min, x_max, y_max = bboxes[:, 0], bboxes[:, 1], bboxes[:, 2], bboxes[:, 3]\n\n    valid_indices = (x_max &gt; 0) &amp; (y_max &gt; 0) &amp; (x_min &lt; cols) &amp; (y_min &lt; rows)\n\n    return bboxes[valid_indices]\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.functional.validate_if_not_found_coords","title":"<code>def validate_if_not_found_coords    (if_not_found_coords)    </code> [view source on GitHub]","text":"<p>Validate and process <code>if_not_found_coords</code> parameter.</p> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>def validate_if_not_found_coords(\n    if_not_found_coords: Sequence[int] | dict[str, Any] | None,\n) -&gt; tuple[bool, int, int]:\n    \"\"\"Validate and process `if_not_found_coords` parameter.\"\"\"\n    if if_not_found_coords is None:\n        return True, -1, -1\n    if isinstance(if_not_found_coords, (tuple, list)):\n        if len(if_not_found_coords) != PAIR:\n            msg = \"Expected tuple/list 'if_not_found_coords' to contain exactly two entries.\"\n            raise ValueError(msg)\n        return False, if_not_found_coords[0], if_not_found_coords[1]\n    if isinstance(if_not_found_coords, dict):\n        return False, if_not_found_coords[\"x\"], if_not_found_coords[\"y\"]\n\n    msg = \"Expected if_not_found_coords to be None, tuple, list, or dict.\"\n    raise ValueError(msg)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.functional.validate_keypoints","title":"<code>def validate_keypoints    (keypoints, image_shape)    </code> [view source on GitHub]","text":"<p>Validate keypoints and remove those that fall outside the image boundaries.</p> <p>Parameters:</p> Name Type Description <code>keypoints</code> <code>np.ndarray</code> <p>Array of keypoints with shape (N, M) where N is the number of keypoints                     and M &gt;= 2. The first two columns represent x and y coordinates.</p> <code>image_shape</code> <code>tuple[int, int]</code> <p>Shape of the image as (height, width).</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Array of valid keypoints that fall within the image boundaries.</p> <p>Note</p> <p>This function only checks the x and y coordinates (first two columns) of the keypoints. Any additional columns (e.g., angle, scale) are preserved for valid keypoints.</p> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>def validate_keypoints(keypoints: np.ndarray, image_shape: tuple[int, int]) -&gt; np.ndarray:\n    \"\"\"Validate keypoints and remove those that fall outside the image boundaries.\n\n    Args:\n        keypoints (np.ndarray): Array of keypoints with shape (N, M) where N is the number of keypoints\n                                and M &gt;= 2. The first two columns represent x and y coordinates.\n        image_shape (tuple[int, int]): Shape of the image as (height, width).\n\n    Returns:\n        np.ndarray: Array of valid keypoints that fall within the image boundaries.\n\n    Note:\n        This function only checks the x and y coordinates (first two columns) of the keypoints.\n        Any additional columns (e.g., angle, scale) are preserved for valid keypoints.\n    \"\"\"\n    rows, cols = image_shape[:2]\n\n    x, y = keypoints[:, 0], keypoints[:, 1]\n\n    valid_indices = (x &gt;= 0) &amp; (x &lt; cols) &amp; (y &gt;= 0) &amp; (y &lt; rows)\n\n    return keypoints[valid_indices]\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.resize","title":"<code>resize</code>","text":""},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.resize.LongestMaxSize","title":"<code>class  LongestMaxSize</code> <code>     (max_size=1024, interpolation=1, always_apply=None, p=1)                 </code>  [view source on GitHub]","text":"<p>Rescale an image so that maximum side is equal to max_size, keeping the aspect ratio of the initial image.</p> <p>Parameters:</p> Name Type Description <code>max_size</code> <code>int, list of int</code> <p>maximum size of the image after the transformation. When using a list, max size will be randomly selected from the values in the list.</p> <code>interpolation</code> <code>OpenCV flag</code> <p>interpolation method. Default: cv2.INTER_LINEAR.</p> <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 1.</p> <p>Targets</p> <p>image, mask, bboxes, keypoints</p> <p>Image types:     uint8, float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/geometric/resize.py</code> Python<pre><code>class LongestMaxSize(DualTransform):\n    \"\"\"Rescale an image so that maximum side is equal to max_size, keeping the aspect ratio of the initial image.\n\n    Args:\n        max_size (int, list of int): maximum size of the image after the transformation. When using a list, max size\n            will be randomly selected from the values in the list.\n        interpolation (OpenCV flag): interpolation method. Default: cv2.INTER_LINEAR.\n        p (float): probability of applying the transform. Default: 1.\n\n    Targets:\n        image, mask, bboxes, keypoints\n\n    Image types:\n        uint8, float32\n\n    \"\"\"\n\n    _targets = (Targets.IMAGE, Targets.MASK, Targets.BBOXES, Targets.KEYPOINTS)\n\n    class InitSchema(MaxSizeInitSchema):\n        pass\n\n    def __init__(\n        self,\n        max_size: int | Sequence[int] = 1024,\n        interpolation: int = cv2.INTER_LINEAR,\n        always_apply: bool | None = None,\n        p: float = 1,\n    ):\n        super().__init__(p, always_apply)\n        self.interpolation = interpolation\n        self.max_size = max_size\n\n    def apply(\n        self,\n        img: np.ndarray,\n        max_size: int,\n        interpolation: int,\n        **params: Any,\n    ) -&gt; np.ndarray:\n        return fgeometric.longest_max_size(img, max_size=max_size, interpolation=interpolation)\n\n    def apply_to_bbox(self, bbox: BoxInternalType, **params: Any) -&gt; BoxInternalType:\n        # Bounding box coordinates are scale invariant\n        return bbox\n\n    def apply_to_keypoint(\n        self,\n        keypoint: KeypointInternalType,\n        max_size: int,\n        **params: Any,\n    ) -&gt; KeypointInternalType:\n        image_shape = params[\"shape\"][:2]\n\n        scale = max_size / max(image_shape)\n        return fgeometric.keypoint_scale(keypoint, scale, scale)\n\n    def get_params(self) -&gt; dict[str, int]:\n        return {\"max_size\": self.max_size if isinstance(self.max_size, int) else random.choice(self.max_size)}\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return \"max_size\", \"interpolation\"\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.resize.LongestMaxSize.apply","title":"<code>apply (self, img, max_size, interpolation, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/geometric/resize.py</code> Python<pre><code>def apply(\n    self,\n    img: np.ndarray,\n    max_size: int,\n    interpolation: int,\n    **params: Any,\n) -&gt; np.ndarray:\n    return fgeometric.longest_max_size(img, max_size=max_size, interpolation=interpolation)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.resize.LongestMaxSize.get_params","title":"<code>get_params (self)</code>","text":"<p>Returns parameters independent of input.</p> Source code in <code>albumentations/augmentations/geometric/resize.py</code> Python<pre><code>def get_params(self) -&gt; dict[str, int]:\n    return {\"max_size\": self.max_size if isinstance(self.max_size, int) else random.choice(self.max_size)}\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.resize.LongestMaxSize.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/geometric/resize.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return \"max_size\", \"interpolation\"\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.resize.RandomScale","title":"<code>class  RandomScale</code> <code>     (scale_limit=0.1, interpolation=1, always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Randomly resize the input. Output image size is different from the input image size.</p> <p>Parameters:</p> Name Type Description <code>scale_limit</code> <code>float, float) or float</code> <p>scaling factor range. If scale_limit is a single float value, the range will be (-scale_limit, scale_limit). Note that the scale_limit will be biased by 1. If scale_limit is a tuple, like (low, high), sampling will be done from the range (1 + low, 1 + high). Default: (-0.1, 0.1).</p> <code>interpolation</code> <code>OpenCV flag</code> <p>flag that is used to specify the interpolation algorithm. Should be one of: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4. Default: cv2.INTER_LINEAR.</p> <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 0.5.</p> <p>Targets</p> <p>image, mask, bboxes, keypoints</p> <p>Image types:     uint8, float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/geometric/resize.py</code> Python<pre><code>class RandomScale(DualTransform):\n    \"\"\"Randomly resize the input. Output image size is different from the input image size.\n\n    Args:\n        scale_limit ((float, float) or float): scaling factor range. If scale_limit is a single float value, the\n            range will be (-scale_limit, scale_limit). Note that the scale_limit will be biased by 1.\n            If scale_limit is a tuple, like (low, high), sampling will be done from the range (1 + low, 1 + high).\n            Default: (-0.1, 0.1).\n        interpolation (OpenCV flag): flag that is used to specify the interpolation algorithm. Should be one of:\n            cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.\n            Default: cv2.INTER_LINEAR.\n        p (float): probability of applying the transform. Default: 0.5.\n\n    Targets:\n        image, mask, bboxes, keypoints\n\n    Image types:\n        uint8, float32\n\n    \"\"\"\n\n    _targets = (Targets.IMAGE, Targets.MASK, Targets.BBOXES, Targets.KEYPOINTS)\n\n    class InitSchema(BaseTransformInitSchema):\n        scale_limit: ScaleFloatType = Field(\n            default=0.1,\n            description=\"Scaling factor range. If a single float value =&gt; (1-scale_limit, 1 + scale_limit).\",\n        )\n        interpolation: InterpolationType = cv2.INTER_LINEAR\n\n        @field_validator(\"scale_limit\")\n        @classmethod\n        def check_scale_limit(cls, v: ScaleFloatType) -&gt; tuple[float, float]:\n            return to_tuple(v, bias=1.0)\n\n    def __init__(\n        self,\n        scale_limit: ScaleFloatType = 0.1,\n        interpolation: int = cv2.INTER_LINEAR,\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p, always_apply)\n        self.scale_limit = cast(Tuple[float, float], scale_limit)\n        self.interpolation = interpolation\n\n    def get_params(self) -&gt; dict[str, float]:\n        return {\"scale\": random.uniform(self.scale_limit[0], self.scale_limit[1])}\n\n    def apply(\n        self,\n        img: np.ndarray,\n        scale: float,\n        interpolation: int,\n        **params: Any,\n    ) -&gt; np.ndarray:\n        return fgeometric.scale(img, scale, interpolation)\n\n    def apply_to_bbox(self, bbox: BoxInternalType, **params: Any) -&gt; BoxInternalType:\n        # Bounding box coordinates are scale invariant\n        return bbox\n\n    def apply_to_keypoint(\n        self,\n        keypoint: KeypointInternalType,\n        scale: float,\n        **params: Any,\n    ) -&gt; KeypointInternalType:\n        return fgeometric.keypoint_scale(keypoint, scale, scale)\n\n    def get_transform_init_args(self) -&gt; dict[str, Any]:\n        return {\"interpolation\": self.interpolation, \"scale_limit\": to_tuple(self.scale_limit, bias=-1.0)}\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.resize.RandomScale.apply","title":"<code>apply (self, img, scale, interpolation, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/geometric/resize.py</code> Python<pre><code>def apply(\n    self,\n    img: np.ndarray,\n    scale: float,\n    interpolation: int,\n    **params: Any,\n) -&gt; np.ndarray:\n    return fgeometric.scale(img, scale, interpolation)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.resize.RandomScale.get_params","title":"<code>get_params (self)</code>","text":"<p>Returns parameters independent of input.</p> Source code in <code>albumentations/augmentations/geometric/resize.py</code> Python<pre><code>def get_params(self) -&gt; dict[str, float]:\n    return {\"scale\": random.uniform(self.scale_limit[0], self.scale_limit[1])}\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.resize.Resize","title":"<code>class  Resize</code> <code>     (height, width, interpolation=1, always_apply=None, p=1)                 </code>  [view source on GitHub]","text":"<p>Resize the input to the given height and width.</p> <p>Parameters:</p> Name Type Description <code>height</code> <code>int</code> <p>desired height of the output.</p> <code>width</code> <code>int</code> <p>desired width of the output.</p> <code>interpolation</code> <code>OpenCV flag</code> <p>flag that is used to specify the interpolation algorithm. Should be one of: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4. Default: cv2.INTER_LINEAR.</p> <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 1.</p> <p>Targets</p> <p>image, mask, bboxes, keypoints</p> <p>Image types:     uint8, float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/geometric/resize.py</code> Python<pre><code>class Resize(DualTransform):\n    \"\"\"Resize the input to the given height and width.\n\n    Args:\n        height (int): desired height of the output.\n        width (int): desired width of the output.\n        interpolation (OpenCV flag): flag that is used to specify the interpolation algorithm. Should be one of:\n            cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.\n            Default: cv2.INTER_LINEAR.\n        p (float): probability of applying the transform. Default: 1.\n\n    Targets:\n        image, mask, bboxes, keypoints\n\n    Image types:\n        uint8, float32\n\n    \"\"\"\n\n    _targets = (Targets.IMAGE, Targets.MASK, Targets.KEYPOINTS, Targets.BBOXES)\n\n    class InitSchema(BaseTransformInitSchema):\n        height: int = Field(ge=1, description=\"Desired height of the output.\")\n        width: int = Field(ge=1, description=\"Desired width of the output.\")\n        interpolation: InterpolationType = cv2.INTER_LINEAR\n        p: ProbabilityType = 1\n\n    def __init__(\n        self,\n        height: int,\n        width: int,\n        interpolation: int = cv2.INTER_LINEAR,\n        always_apply: bool | None = None,\n        p: float = 1,\n    ):\n        super().__init__(p, always_apply)\n        self.height = height\n        self.width = width\n        self.interpolation = interpolation\n\n    def apply(self, img: np.ndarray, interpolation: int, **params: Any) -&gt; np.ndarray:\n        return fgeometric.resize(img, (self.height, self.width), interpolation=interpolation)\n\n    def apply_to_bbox(self, bbox: BoxInternalType, **params: Any) -&gt; BoxInternalType:\n        # Bounding box coordinates are scale invariant\n        return bbox\n\n    def apply_to_keypoint(self, keypoint: KeypointInternalType, **params: Any) -&gt; KeypointInternalType:\n        height, width = params[\"shape\"][:2]\n        scale_x = self.width / width\n        scale_y = self.height / height\n        return fgeometric.keypoint_scale(keypoint, scale_x, scale_y)\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return \"height\", \"width\", \"interpolation\"\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.resize.Resize.apply","title":"<code>apply (self, img, interpolation, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/geometric/resize.py</code> Python<pre><code>def apply(self, img: np.ndarray, interpolation: int, **params: Any) -&gt; np.ndarray:\n    return fgeometric.resize(img, (self.height, self.width), interpolation=interpolation)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.resize.Resize.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/geometric/resize.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return \"height\", \"width\", \"interpolation\"\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.resize.SmallestMaxSize","title":"<code>class  SmallestMaxSize</code> <code>     (max_size=1024, interpolation=1, always_apply=None, p=1)                 </code>  [view source on GitHub]","text":"<p>Rescale an image so that minimum side is equal to max_size, keeping the aspect ratio of the initial image.</p> <p>Parameters:</p> Name Type Description <code>max_size</code> <code>int, list of int</code> <p>maximum size of smallest side of the image after the transformation. When using a list, max size will be randomly selected from the values in the list.</p> <code>interpolation</code> <code>OpenCV flag</code> <p>interpolation method. Default: cv2.INTER_LINEAR.</p> <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 1.</p> <p>Targets</p> <p>image, mask, bboxes, keypoints</p> <p>Image types:     uint8, float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/geometric/resize.py</code> Python<pre><code>class SmallestMaxSize(DualTransform):\n    \"\"\"Rescale an image so that minimum side is equal to max_size, keeping the aspect ratio of the initial image.\n\n    Args:\n        max_size (int, list of int): maximum size of smallest side of the image after the transformation. When using a\n            list, max size will be randomly selected from the values in the list.\n        interpolation (OpenCV flag): interpolation method. Default: cv2.INTER_LINEAR.\n        p (float): probability of applying the transform. Default: 1.\n\n    Targets:\n        image, mask, bboxes, keypoints\n\n    Image types:\n        uint8, float32\n\n    \"\"\"\n\n    _targets = (Targets.IMAGE, Targets.MASK, Targets.KEYPOINTS, Targets.BBOXES)\n\n    class InitSchema(MaxSizeInitSchema):\n        pass\n\n    def __init__(\n        self,\n        max_size: int | Sequence[int] = 1024,\n        interpolation: int = cv2.INTER_LINEAR,\n        always_apply: bool | None = None,\n        p: float = 1,\n    ):\n        super().__init__(p, always_apply)\n        self.interpolation = interpolation\n        self.max_size = max_size\n\n    def apply(\n        self,\n        img: np.ndarray,\n        max_size: int,\n        interpolation: int,\n        **params: Any,\n    ) -&gt; np.ndarray:\n        return fgeometric.smallest_max_size(img, max_size=max_size, interpolation=interpolation)\n\n    def apply_to_bbox(self, bbox: BoxInternalType, **params: Any) -&gt; BoxInternalType:\n        return bbox\n\n    def apply_to_keypoint(\n        self,\n        keypoint: KeypointInternalType,\n        max_size: int,\n        **params: Any,\n    ) -&gt; KeypointInternalType:\n        image_shape = params[\"shape\"][:2]\n        height, width = image_shape\n\n        scale = max_size / min(image_shape)\n        return fgeometric.keypoint_scale(keypoint, scale, scale)\n\n    def get_params(self) -&gt; dict[str, int]:\n        return {\"max_size\": self.max_size if isinstance(self.max_size, int) else random.choice(self.max_size)}\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return (\"max_size\", \"interpolation\")\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.resize.SmallestMaxSize.apply","title":"<code>apply (self, img, max_size, interpolation, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/geometric/resize.py</code> Python<pre><code>def apply(\n    self,\n    img: np.ndarray,\n    max_size: int,\n    interpolation: int,\n    **params: Any,\n) -&gt; np.ndarray:\n    return fgeometric.smallest_max_size(img, max_size=max_size, interpolation=interpolation)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.resize.SmallestMaxSize.get_params","title":"<code>get_params (self)</code>","text":"<p>Returns parameters independent of input.</p> Source code in <code>albumentations/augmentations/geometric/resize.py</code> Python<pre><code>def get_params(self) -&gt; dict[str, int]:\n    return {\"max_size\": self.max_size if isinstance(self.max_size, int) else random.choice(self.max_size)}\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.resize.SmallestMaxSize.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/geometric/resize.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return (\"max_size\", \"interpolation\")\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.rotate","title":"<code>rotate</code>","text":""},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.rotate.RandomRotate90","title":"<code>class  RandomRotate90</code> <code> </code>  [view source on GitHub]","text":"<p>Randomly rotate the input by 90 degrees zero or more times.</p> <p>Parameters:</p> Name Type Description <code>p</code> <p>probability of applying the transform. Default: 0.5.</p> <p>Targets</p> <p>image, mask, bboxes, keypoints</p> <p>Image types:     uint8, float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/geometric/rotate.py</code> Python<pre><code>class RandomRotate90(DualTransform):\n    \"\"\"Randomly rotate the input by 90 degrees zero or more times.\n\n    Args:\n        p: probability of applying the transform. Default: 0.5.\n\n    Targets:\n        image, mask, bboxes, keypoints\n\n    Image types:\n        uint8, float32\n\n    \"\"\"\n\n    _targets = (Targets.IMAGE, Targets.MASK, Targets.BBOXES, Targets.KEYPOINTS)\n\n    def apply(self, img: np.ndarray, factor: int, **params: Any) -&gt; np.ndarray:\n        return fgeometric.rot90(img, factor)\n\n    def get_params(self) -&gt; dict[str, int]:\n        # Random int in the range [0, 3]\n        return {\"factor\": random.randint(0, 3)}\n\n    def apply_to_bbox(self, bbox: BoxInternalType, factor: int, **params: Any) -&gt; BoxInternalType:\n        return fgeometric.bbox_rot90(bbox, factor)\n\n    def apply_to_keypoint(self, keypoint: KeypointInternalType, factor: int, **params: Any) -&gt; BoxInternalType:\n        return fgeometric.keypoint_rot90(keypoint, factor, params[\"shape\"])\n\n    def get_transform_init_args_names(self) -&gt; tuple[()]:\n        return ()\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.rotate.RandomRotate90.apply","title":"<code>apply (self, img, factor, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/geometric/rotate.py</code> Python<pre><code>def apply(self, img: np.ndarray, factor: int, **params: Any) -&gt; np.ndarray:\n    return fgeometric.rot90(img, factor)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.rotate.RandomRotate90.get_params","title":"<code>get_params (self)</code>","text":"<p>Returns parameters independent of input.</p> Source code in <code>albumentations/augmentations/geometric/rotate.py</code> Python<pre><code>def get_params(self) -&gt; dict[str, int]:\n    # Random int in the range [0, 3]\n    return {\"factor\": random.randint(0, 3)}\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.rotate.RandomRotate90.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/geometric/rotate.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[()]:\n    return ()\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.rotate.Rotate","title":"<code>class  Rotate</code> <code>     (limit=(-90, 90), interpolation=1, border_mode=4, value=None, mask_value=None, rotate_method='largest_box', crop_border=False, always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Rotate the input by an angle selected randomly from the uniform distribution.</p> <p>Parameters:</p> Name Type Description <code>limit</code> <code>ScaleFloatType</code> <p>range from which a random angle is picked. If limit is a single int an angle is picked from (-limit, limit). Default: (-90, 90)</p> <code>interpolation</code> <code>OpenCV flag</code> <p>flag that is used to specify the interpolation algorithm. Should be one of: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4. Default: cv2.INTER_LINEAR.</p> <code>border_mode</code> <code>OpenCV flag</code> <p>flag that is used to specify the pixel extrapolation method. Should be one of: cv2.BORDER_CONSTANT, cv2.BORDER_REPLICATE, cv2.BORDER_REFLECT, cv2.BORDER_WRAP, cv2.BORDER_REFLECT_101. Default: cv2.BORDER_REFLECT_101</p> <code>value</code> <code>int, float, list of ints, list of float</code> <p>padding value if border_mode is cv2.BORDER_CONSTANT.</p> <code>mask_value</code> <code>int, float,         list of ints,         list of float</code> <p>padding value if border_mode is cv2.BORDER_CONSTANT applied for masks.</p> <code>rotate_method</code> <code>str</code> <p>rotation method used for the bounding boxes. Should be one of \"largest_box\" or \"ellipse\". Default: \"largest_box\"</p> <code>crop_border</code> <code>bool</code> <p>If True would make a largest possible crop within rotated image</p> <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 0.5.</p> <p>Targets</p> <p>image, mask, bboxes, keypoints</p> <p>Image types:     uint8, float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/geometric/rotate.py</code> Python<pre><code>class Rotate(DualTransform):\n    \"\"\"Rotate the input by an angle selected randomly from the uniform distribution.\n\n    Args:\n        limit: range from which a random angle is picked. If limit is a single int\n            an angle is picked from (-limit, limit). Default: (-90, 90)\n        interpolation (OpenCV flag): flag that is used to specify the interpolation algorithm. Should be one of:\n            cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.\n            Default: cv2.INTER_LINEAR.\n        border_mode (OpenCV flag): flag that is used to specify the pixel extrapolation method. Should be one of:\n            cv2.BORDER_CONSTANT, cv2.BORDER_REPLICATE, cv2.BORDER_REFLECT, cv2.BORDER_WRAP, cv2.BORDER_REFLECT_101.\n            Default: cv2.BORDER_REFLECT_101\n        value (int, float, list of ints, list of float): padding value if border_mode is cv2.BORDER_CONSTANT.\n        mask_value (int, float,\n                    list of ints,\n                    list of float): padding value if border_mode is cv2.BORDER_CONSTANT applied for masks.\n        rotate_method (str): rotation method used for the bounding boxes. Should be one of \"largest_box\" or \"ellipse\".\n            Default: \"largest_box\"\n        crop_border (bool): If True would make a largest possible crop within rotated image\n        p (float): probability of applying the transform. Default: 0.5.\n\n    Targets:\n        image, mask, bboxes, keypoints\n\n    Image types:\n        uint8, float32\n\n    \"\"\"\n\n    _targets = (Targets.IMAGE, Targets.MASK, Targets.BBOXES, Targets.KEYPOINTS)\n\n    class InitSchema(RotateInitSchema):\n        rotate_method: Literal[\"largest_box\", \"ellipse\"] = \"largest_box\"\n        crop_border: bool = Field(\n            default=False,\n            description=\"If True, makes a largest possible crop within the rotated image.\",\n        )\n\n    def __init__(\n        self,\n        limit: ScaleFloatType = (-90, 90),\n        interpolation: int = cv2.INTER_LINEAR,\n        border_mode: int = cv2.BORDER_REFLECT_101,\n        value: ColorType | None = None,\n        mask_value: ColorType | None = None,\n        rotate_method: Literal[\"largest_box\", \"ellipse\"] = \"largest_box\",\n        crop_border: bool = False,\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p=p, always_apply=always_apply)\n        self.limit = cast(Tuple[float, float], limit)\n        self.interpolation = interpolation\n        self.border_mode = border_mode\n        self.value = value\n        self.mask_value = mask_value\n        self.rotate_method = rotate_method\n        self.crop_border = crop_border\n\n    def apply(\n        self,\n        img: np.ndarray,\n        angle: float,\n        interpolation: int,\n        x_min: int,\n        x_max: int,\n        y_min: int,\n        y_max: int,\n        **params: Any,\n    ) -&gt; np.ndarray:\n        img_out = fgeometric.rotate(img, angle, interpolation, self.border_mode, self.value)\n        if self.crop_border:\n            return fcrops.crop(img_out, x_min, y_min, x_max, y_max)\n        return img_out\n\n    def apply_to_mask(\n        self,\n        mask: np.ndarray,\n        angle: float,\n        x_min: int,\n        x_max: int,\n        y_min: int,\n        y_max: int,\n        **params: Any,\n    ) -&gt; np.ndarray:\n        img_out = fgeometric.rotate(mask, angle, cv2.INTER_NEAREST, self.border_mode, self.mask_value)\n        if self.crop_border:\n            return fcrops.crop(img_out, x_min, y_min, x_max, y_max)\n        return img_out\n\n    def apply_to_bbox(\n        self,\n        bbox: BoxInternalType,\n        angle: float,\n        x_min: int,\n        x_max: int,\n        y_min: int,\n        y_max: int,\n        **params: Any,\n    ) -&gt; np.ndarray:\n        image_shape = params[\"shape\"][:2]\n        bbox_out = fgeometric.bbox_rotate(bbox, angle, self.rotate_method, image_shape)\n        if self.crop_border:\n            return fcrops.crop_bbox_by_coords(bbox_out, (x_min, y_min, x_max, y_max), image_shape)\n        return bbox_out\n\n    def apply_to_keypoint(\n        self,\n        keypoint: KeypointInternalType,\n        angle: float,\n        x_min: int,\n        x_max: int,\n        y_min: int,\n        y_max: int,\n        **params: Any,\n    ) -&gt; KeypointInternalType:\n        keypoint_out = fgeometric.keypoint_rotate(keypoint, angle, params[\"shape\"][:2], **params)\n        if self.crop_border:\n            return fcrops.crop_keypoint_by_coords(keypoint_out, (x_min, y_min, x_max, y_max))\n        return keypoint_out\n\n    @staticmethod\n    def _rotated_rect_with_max_area(height: int, width: int, angle: float) -&gt; dict[str, int]:\n        \"\"\"Given a rectangle of size wxh that has been rotated by 'angle' (in\n        degrees), computes the width and height of the largest possible\n        axis-aligned rectangle (maximal area) within the rotated rectangle.\n\n        Reference:\n            https://stackoverflow.com/questions/16702966/rotate-image-and-crop-out-black-borders\n        \"\"\"\n        angle = math.radians(angle)\n        width_is_longer = width &gt;= height\n        side_long, side_short = (width, height) if width_is_longer else (height, width)\n\n        # since the solutions for angle, -angle and 180-angle are all the same,\n        # it is sufficient to look at the first quadrant and the absolute values of sin,cos:\n        sin_a, cos_a = abs(math.sin(angle)), abs(math.cos(angle))\n        if side_short &lt;= 2.0 * sin_a * cos_a * side_long or abs(sin_a - cos_a) &lt; SMALL_NUMBER:\n            # half constrained case: two crop corners touch the longer side,\n            # the other two corners are on the mid-line parallel to the longer line\n            x = 0.5 * side_short\n            wr, hr = (x / sin_a, x / cos_a) if width_is_longer else (x / cos_a, x / sin_a)\n        else:\n            # fully constrained case: crop touches all 4 sides\n            cos_2a = cos_a * cos_a - sin_a * sin_a\n            wr, hr = (width * cos_a - height * sin_a) / cos_2a, (height * cos_a - width * sin_a) / cos_2a\n\n        return {\n            \"x_min\": max(0, int(width / 2 - wr / 2)),\n            \"x_max\": min(width, int(width / 2 + wr / 2)),\n            \"y_min\": max(0, int(height / 2 - hr / 2)),\n            \"y_max\": min(height, int(height / 2 + hr / 2)),\n        }\n\n    def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n        out_params = {\"angle\": random.uniform(self.limit[0], self.limit[1])}\n        if self.crop_border:\n            height, width = params[\"shape\"][:2]\n            out_params.update(self._rotated_rect_with_max_area(height, width, out_params[\"angle\"]))\n        else:\n            out_params.update({\"x_min\": -1, \"x_max\": -1, \"y_min\": -1, \"y_max\": -1})\n\n        return out_params\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return \"limit\", \"interpolation\", \"border_mode\", \"value\", \"mask_value\", \"rotate_method\", \"crop_border\"\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.rotate.Rotate.apply","title":"<code>apply (self, img, angle, interpolation, x_min, x_max, y_min, y_max, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/geometric/rotate.py</code> Python<pre><code>def apply(\n    self,\n    img: np.ndarray,\n    angle: float,\n    interpolation: int,\n    x_min: int,\n    x_max: int,\n    y_min: int,\n    y_max: int,\n    **params: Any,\n) -&gt; np.ndarray:\n    img_out = fgeometric.rotate(img, angle, interpolation, self.border_mode, self.value)\n    if self.crop_border:\n        return fcrops.crop(img_out, x_min, y_min, x_max, y_max)\n    return img_out\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.rotate.Rotate.get_params_dependent_on_data","title":"<code>get_params_dependent_on_data (self, params, data)</code>","text":"<p>Returns parameters dependent on input.</p> Source code in <code>albumentations/augmentations/geometric/rotate.py</code> Python<pre><code>def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n    out_params = {\"angle\": random.uniform(self.limit[0], self.limit[1])}\n    if self.crop_border:\n        height, width = params[\"shape\"][:2]\n        out_params.update(self._rotated_rect_with_max_area(height, width, out_params[\"angle\"]))\n    else:\n        out_params.update({\"x_min\": -1, \"x_max\": -1, \"y_min\": -1, \"y_max\": -1})\n\n    return out_params\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.rotate.Rotate.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/geometric/rotate.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return \"limit\", \"interpolation\", \"border_mode\", \"value\", \"mask_value\", \"rotate_method\", \"crop_border\"\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.rotate.SafeRotate","title":"<code>class  SafeRotate</code> <code>     (limit=(-90, 90), interpolation=1, border_mode=4, value=None, mask_value=None, always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Rotate the input inside the input's frame by an angle selected randomly from the uniform distribution.</p> <p>The resulting image may have artifacts in it. After rotation, the image may have a different aspect ratio, and after resizing, it returns to its original shape with the original aspect ratio of the image. For these reason we may see some artifacts.</p> <p>Parameters:</p> Name Type Description <code>limit</code> <code>int, int) or int</code> <p>range from which a random angle is picked. If limit is a single int an angle is picked from (-limit, limit). Default: (-90, 90)</p> <code>interpolation</code> <code>OpenCV flag</code> <p>flag that is used to specify the interpolation algorithm. Should be one of: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4. Default: cv2.INTER_LINEAR.</p> <code>border_mode</code> <code>OpenCV flag</code> <p>flag that is used to specify the pixel extrapolation method. Should be one of: cv2.BORDER_CONSTANT, cv2.BORDER_REPLICATE, cv2.BORDER_REFLECT, cv2.BORDER_WRAP, cv2.BORDER_REFLECT_101. Default: cv2.BORDER_REFLECT_101</p> <code>value</code> <code>int, float, list of ints, list of float</code> <p>padding value if border_mode is cv2.BORDER_CONSTANT.</p> <code>mask_value</code> <code>int, float,         list of ints,         list of float</code> <p>padding value if border_mode is cv2.BORDER_CONSTANT applied for masks.</p> <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 0.5.</p> <p>Targets</p> <p>image, mask, bboxes, keypoints</p> <p>Image types:     uint8, float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/geometric/rotate.py</code> Python<pre><code>class SafeRotate(DualTransform):\n    \"\"\"Rotate the input inside the input's frame by an angle selected randomly from the uniform distribution.\n\n    The resulting image may have artifacts in it. After rotation, the image may have a different aspect ratio, and\n    after resizing, it returns to its original shape with the original aspect ratio of the image. For these reason we\n    may see some artifacts.\n\n    Args:\n        limit ((int, int) or int): range from which a random angle is picked. If limit is a single int\n            an angle is picked from (-limit, limit). Default: (-90, 90)\n        interpolation (OpenCV flag): flag that is used to specify the interpolation algorithm. Should be one of:\n            cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.\n            Default: cv2.INTER_LINEAR.\n        border_mode (OpenCV flag): flag that is used to specify the pixel extrapolation method. Should be one of:\n            cv2.BORDER_CONSTANT, cv2.BORDER_REPLICATE, cv2.BORDER_REFLECT, cv2.BORDER_WRAP, cv2.BORDER_REFLECT_101.\n            Default: cv2.BORDER_REFLECT_101\n        value (int, float, list of ints, list of float): padding value if border_mode is cv2.BORDER_CONSTANT.\n        mask_value (int, float,\n                    list of ints,\n                    list of float): padding value if border_mode is cv2.BORDER_CONSTANT applied for masks.\n        p (float): probability of applying the transform. Default: 0.5.\n\n    Targets:\n        image, mask, bboxes, keypoints\n\n    Image types:\n        uint8, float32\n\n    \"\"\"\n\n    _targets = (Targets.IMAGE, Targets.MASK, Targets.BBOXES, Targets.KEYPOINTS)\n\n    class InitSchema(RotateInitSchema):\n        pass\n\n    def __init__(\n        self,\n        limit: ScaleFloatType = (-90, 90),\n        interpolation: int = cv2.INTER_LINEAR,\n        border_mode: int = cv2.BORDER_REFLECT_101,\n        value: ColorType | None = None,\n        mask_value: ColorType | None = None,\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p=p, always_apply=always_apply)\n        self.limit = cast(Tuple[float, float], limit)\n        self.interpolation = interpolation\n        self.border_mode = border_mode\n        self.value = value\n        self.mask_value = mask_value\n\n    def apply(self, img: np.ndarray, matrix: np.ndarray, **params: Any) -&gt; np.ndarray:\n        return fgeometric.safe_rotate(img, matrix, self.interpolation, self.value, self.border_mode)\n\n    def apply_to_mask(self, mask: np.ndarray, matrix: np.ndarray, **params: Any) -&gt; np.ndarray:\n        return fgeometric.safe_rotate(mask, matrix, cv2.INTER_NEAREST, self.mask_value, self.border_mode)\n\n    def apply_to_bbox(self, bbox: BoxInternalType, **params: Any) -&gt; BoxInternalType:\n        return fgeometric.bbox_safe_rotate(bbox, params[\"matrix\"], params[\"shape\"])\n\n    def apply_to_keypoint(\n        self,\n        keypoint: KeypointInternalType,\n        angle: float,\n        scale_x: float,\n        scale_y: float,\n        **params: Any,\n    ) -&gt; KeypointInternalType:\n        return fgeometric.keypoint_safe_rotate(keypoint, params[\"matrix\"], angle, scale_x, scale_y, params[\"shape\"])\n\n    def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n        image_shape = params[\"shape\"]\n\n        angle = random.uniform(*self.limit)\n\n        # https://stackoverflow.com/questions/43892506/opencv-python-rotate-image-without-cropping-sides\n        image_center = center(image_shape)\n\n        # Rotation Matrix\n        rotation_mat = cv2.getRotationMatrix2D(image_center, angle, 1.0)\n\n        # rotation calculates the cos and sin, taking absolutes of those.\n        abs_cos = abs(rotation_mat[0, 0])\n        abs_sin = abs(rotation_mat[0, 1])\n\n        height, width = image_shape[:2]\n\n        # find the new width and height bounds\n        new_w = math.ceil(height * abs_sin + width * abs_cos)\n        new_h = math.ceil(height * abs_cos + width * abs_sin)\n\n        scale_x = width / new_w\n        scale_y = height / new_h\n\n        # Shift the image to create padding\n        rotation_mat[0, 2] += new_w / 2 - image_center[0]\n        rotation_mat[1, 2] += new_h / 2 - image_center[1]\n\n        # Rescale to original size\n        scale_mat = np.diag(np.ones(3))\n        scale_mat[0, 0] *= scale_x\n        scale_mat[1, 1] *= scale_y\n        _tmp = np.diag(np.ones(3))\n        _tmp[:2] = rotation_mat\n        _tmp = scale_mat @ _tmp\n        rotation_mat = _tmp[:2]\n\n        return {\"matrix\": rotation_mat, \"angle\": angle, \"scale_x\": scale_x, \"scale_y\": scale_y}\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return \"limit\", \"interpolation\", \"border_mode\", \"value\", \"mask_value\"\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.rotate.SafeRotate.apply","title":"<code>apply (self, img, matrix, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/geometric/rotate.py</code> Python<pre><code>def apply(self, img: np.ndarray, matrix: np.ndarray, **params: Any) -&gt; np.ndarray:\n    return fgeometric.safe_rotate(img, matrix, self.interpolation, self.value, self.border_mode)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.rotate.SafeRotate.get_params_dependent_on_data","title":"<code>get_params_dependent_on_data (self, params, data)</code>","text":"<p>Returns parameters dependent on input.</p> Source code in <code>albumentations/augmentations/geometric/rotate.py</code> Python<pre><code>def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n    image_shape = params[\"shape\"]\n\n    angle = random.uniform(*self.limit)\n\n    # https://stackoverflow.com/questions/43892506/opencv-python-rotate-image-without-cropping-sides\n    image_center = center(image_shape)\n\n    # Rotation Matrix\n    rotation_mat = cv2.getRotationMatrix2D(image_center, angle, 1.0)\n\n    # rotation calculates the cos and sin, taking absolutes of those.\n    abs_cos = abs(rotation_mat[0, 0])\n    abs_sin = abs(rotation_mat[0, 1])\n\n    height, width = image_shape[:2]\n\n    # find the new width and height bounds\n    new_w = math.ceil(height * abs_sin + width * abs_cos)\n    new_h = math.ceil(height * abs_cos + width * abs_sin)\n\n    scale_x = width / new_w\n    scale_y = height / new_h\n\n    # Shift the image to create padding\n    rotation_mat[0, 2] += new_w / 2 - image_center[0]\n    rotation_mat[1, 2] += new_h / 2 - image_center[1]\n\n    # Rescale to original size\n    scale_mat = np.diag(np.ones(3))\n    scale_mat[0, 0] *= scale_x\n    scale_mat[1, 1] *= scale_y\n    _tmp = np.diag(np.ones(3))\n    _tmp[:2] = rotation_mat\n    _tmp = scale_mat @ _tmp\n    rotation_mat = _tmp[:2]\n\n    return {\"matrix\": rotation_mat, \"angle\": angle, \"scale_x\": scale_x, \"scale_y\": scale_y}\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.rotate.SafeRotate.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/geometric/rotate.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return \"limit\", \"interpolation\", \"border_mode\", \"value\", \"mask_value\"\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.transforms","title":"<code>transforms</code>","text":""},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.transforms.Affine","title":"<code>class  Affine</code> <code>     (scale=None, translate_percent=None, translate_px=None, rotate=None, shear=None, interpolation=1, mask_interpolation=0, cval=0, cval_mask=0, mode=0, fit_output=False, keep_ratio=False, rotate_method='largest_box', balanced_scale=False, always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Augmentation to apply affine transformations to images.</p> <p>Affine transformations involve:</p> <pre><code>- Translation (\"move\" image on the x-/y-axis)\n- Rotation\n- Scaling (\"zoom\" in/out)\n- Shear (move one side of the image, turning a square into a trapezoid)\n</code></pre> <p>All such transformations can create \"new\" pixels in the image without a defined content, e.g. if the image is translated to the left, pixels are created on the right. A method has to be defined to deal with these pixel values. The parameters <code>cval</code> and <code>mode</code> of this class deal with this.</p> <p>Some transformations involve interpolations between several pixels of the input image to generate output pixel values. The parameters <code>interpolation</code> and <code>mask_interpolation</code> deals with the method of interpolation used for this.</p> <p>Parameters:</p> Name Type Description <code>scale</code> <code>number, tuple of number or dict</code> <p>Scaling factor to use, where <code>1.0</code> denotes \"no change\" and <code>0.5</code> is zoomed out to <code>50</code> percent of the original size.     * If a single number, then that value will be used for all images.     * If a tuple <code>(a, b)</code>, then a value will be uniformly sampled per image from the interval <code>[a, b]</code>.       That the same range will be used for both x- and y-axis. To keep the aspect ratio, set       <code>keep_ratio=True</code>, then the same value will be used for both x- and y-axis.     * If a dictionary, then it is expected to have the keys <code>x</code> and/or <code>y</code>.       Each of these keys can have the same values as described above.       Using a dictionary allows to set different values for the two axis and sampling will then happen       independently per axis, resulting in samples that differ between the axes. Note that when       the <code>keep_ratio=True</code>, the x- and y-axis ranges should be the same.</p> <code>translate_percent</code> <code>None, number, tuple of number or dict</code> <p>Translation as a fraction of the image height/width (x-translation, y-translation), where <code>0</code> denotes \"no change\" and <code>0.5</code> denotes \"half of the axis size\".     * If <code>None</code> then equivalent to <code>0.0</code> unless <code>translate_px</code> has a value other than <code>None</code>.     * If a single number, then that value will be used for all images.     * If a tuple <code>(a, b)</code>, then a value will be uniformly sampled per image from the interval <code>[a, b]</code>.       That sampled fraction value will be used identically for both x- and y-axis.     * If a dictionary, then it is expected to have the keys <code>x</code> and/or <code>y</code>.       Each of these keys can have the same values as described above.       Using a dictionary allows to set different values for the two axis and sampling will then happen       independently per axis, resulting in samples that differ between the axes.</p> <code>translate_px</code> <code>None, int, tuple of int or dict</code> <p>Translation in pixels.     * If <code>None</code> then equivalent to <code>0</code> unless <code>translate_percent</code> has a value other than <code>None</code>.     * If a single int, then that value will be used for all images.     * If a tuple <code>(a, b)</code>, then a value will be uniformly sampled per image from       the discrete interval <code>[a..b]</code>. That number will be used identically for both x- and y-axis.     * If a dictionary, then it is expected to have the keys <code>x</code> and/or <code>y</code>.       Each of these keys can have the same values as described above.       Using a dictionary allows to set different values for the two axis and sampling will then happen       independently per axis, resulting in samples that differ between the axes.</p> <code>rotate</code> <code>number or tuple of number</code> <p>Rotation in degrees (NOT radians), i.e. expected value range is around <code>[-360, 360]</code>. Rotation happens around the center of the image, not the top left corner as in some other frameworks.     * If a number, then that value will be used for all images.     * If a tuple <code>(a, b)</code>, then a value will be uniformly sampled per image from the interval <code>[a, b]</code>       and used as the rotation value.</p> <code>shear</code> <code>number, tuple of number or dict</code> <p>Shear in degrees (NOT radians), i.e. expected value range is around <code>[-360, 360]</code>, with reasonable values being in the range of <code>[-45, 45]</code>.     * If a number, then that value will be used for all images as       the shear on the x-axis (no shear on the y-axis will be done).     * If a tuple <code>(a, b)</code>, then two value will be uniformly sampled per image       from the interval <code>[a, b]</code> and be used as the x- and y-shear value.     * If a dictionary, then it is expected to have the keys <code>x</code> and/or <code>y</code>.       Each of these keys can have the same values as described above.       Using a dictionary allows to set different values for the two axis and sampling will then happen       independently per axis, resulting in samples that differ between the axes.</p> <code>interpolation</code> <code>int</code> <p>OpenCV interpolation flag.</p> <code>mask_interpolation</code> <code>int</code> <p>OpenCV interpolation flag.</p> <code>cval</code> <code>number or sequence of number</code> <p>The constant value to use when filling in newly created pixels. (E.g. translating by 1px to the right will create a new 1px-wide column of pixels on the left of the image). The value is only used when <code>mode=constant</code>. The expected value range is <code>[0, 255]</code> for <code>uint8</code> images.</p> <code>cval_mask</code> <code>number or tuple of number</code> <p>Same as cval but only for masks.</p> <code>mode</code> <code>int</code> <p>OpenCV border flag.</p> <code>fit_output</code> <code>bool</code> <p>If True, the image plane size and position will be adjusted to tightly capture the whole image after affine transformation (<code>translate_percent</code> and <code>translate_px</code> are ignored). Otherwise (<code>False</code>),  parts of the transformed image may end up outside the image plane. Fitting the output shape can be useful to avoid corners of the image being outside the image plane after applying rotations. Default: False</p> <code>keep_ratio</code> <code>bool</code> <p>When True, the original aspect ratio will be kept when the random scale is applied. Default: False.</p> <code>rotate_method</code> <code>Literal[\"largest_box\", \"ellipse\"]</code> <p>rotation method used for the bounding boxes. Should be one of \"largest_box\" or \"ellipse\"[1]. Default: \"largest_box\"</p> <code>balanced_scale</code> <code>bool</code> <p>When True, scaling factors are chosen to be either entirely below or above 1, ensuring balanced scaling. Default: False.</p> <p>This is important because without it, scaling tends to lean towards upscaling. For example, if we want the image to zoom in and out by 2x, we may pick an interval [0.5, 2]. Since the interval [0.5, 1] is three times smaller than [1, 2], values above 1 are picked three times more often if sampled directly from [0.5, 2]. With <code>balanced_scale</code>, the  function ensures that half the time, the scaling factor is picked from below 1 (zooming out), and the other half from above 1 (zooming in). This makes the zooming in and out process more balanced.</p> <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 0.5.</p> <p>Targets</p> <p>image, mask, keypoints, bboxes</p> <p>Image types:     uint8, float32</p> <p>Reference</p> <p>[1] https://arxiv.org/abs/2109.13488</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>class Affine(DualTransform):\n    \"\"\"Augmentation to apply affine transformations to images.\n\n    Affine transformations involve:\n\n        - Translation (\"move\" image on the x-/y-axis)\n        - Rotation\n        - Scaling (\"zoom\" in/out)\n        - Shear (move one side of the image, turning a square into a trapezoid)\n\n    All such transformations can create \"new\" pixels in the image without a defined content, e.g.\n    if the image is translated to the left, pixels are created on the right.\n    A method has to be defined to deal with these pixel values.\n    The parameters `cval` and `mode` of this class deal with this.\n\n    Some transformations involve interpolations between several pixels\n    of the input image to generate output pixel values. The parameters `interpolation` and\n    `mask_interpolation` deals with the method of interpolation used for this.\n\n    Args:\n        scale (number, tuple of number or dict): Scaling factor to use, where ``1.0`` denotes \"no change\" and\n            ``0.5`` is zoomed out to ``50`` percent of the original size.\n                * If a single number, then that value will be used for all images.\n                * If a tuple ``(a, b)``, then a value will be uniformly sampled per image from the interval ``[a, b]``.\n                  That the same range will be used for both x- and y-axis. To keep the aspect ratio, set\n                  ``keep_ratio=True``, then the same value will be used for both x- and y-axis.\n                * If a dictionary, then it is expected to have the keys ``x`` and/or ``y``.\n                  Each of these keys can have the same values as described above.\n                  Using a dictionary allows to set different values for the two axis and sampling will then happen\n                  *independently* per axis, resulting in samples that differ between the axes. Note that when\n                  the ``keep_ratio=True``, the x- and y-axis ranges should be the same.\n        translate_percent (None, number, tuple of number or dict): Translation as a fraction of the image height/width\n            (x-translation, y-translation), where ``0`` denotes \"no change\"\n            and ``0.5`` denotes \"half of the axis size\".\n                * If ``None`` then equivalent to ``0.0`` unless `translate_px` has a value other than ``None``.\n                * If a single number, then that value will be used for all images.\n                * If a tuple ``(a, b)``, then a value will be uniformly sampled per image from the interval ``[a, b]``.\n                  That sampled fraction value will be used identically for both x- and y-axis.\n                * If a dictionary, then it is expected to have the keys ``x`` and/or ``y``.\n                  Each of these keys can have the same values as described above.\n                  Using a dictionary allows to set different values for the two axis and sampling will then happen\n                  *independently* per axis, resulting in samples that differ between the axes.\n        translate_px (None, int, tuple of int or dict): Translation in pixels.\n                * If ``None`` then equivalent to ``0`` unless `translate_percent` has a value other than ``None``.\n                * If a single int, then that value will be used for all images.\n                * If a tuple ``(a, b)``, then a value will be uniformly sampled per image from\n                  the discrete interval ``[a..b]``. That number will be used identically for both x- and y-axis.\n                * If a dictionary, then it is expected to have the keys ``x`` and/or ``y``.\n                  Each of these keys can have the same values as described above.\n                  Using a dictionary allows to set different values for the two axis and sampling will then happen\n                  *independently* per axis, resulting in samples that differ between the axes.\n        rotate (number or tuple of number): Rotation in degrees (**NOT** radians), i.e. expected value range is\n            around ``[-360, 360]``. Rotation happens around the *center* of the image,\n            not the top left corner as in some other frameworks.\n                * If a number, then that value will be used for all images.\n                * If a tuple ``(a, b)``, then a value will be uniformly sampled per image from the interval ``[a, b]``\n                  and used as the rotation value.\n        shear (number, tuple of number or dict): Shear in degrees (**NOT** radians), i.e. expected value range is\n            around ``[-360, 360]``, with reasonable values being in the range of ``[-45, 45]``.\n                * If a number, then that value will be used for all images as\n                  the shear on the x-axis (no shear on the y-axis will be done).\n                * If a tuple ``(a, b)``, then two value will be uniformly sampled per image\n                  from the interval ``[a, b]`` and be used as the x- and y-shear value.\n                * If a dictionary, then it is expected to have the keys ``x`` and/or ``y``.\n                  Each of these keys can have the same values as described above.\n                  Using a dictionary allows to set different values for the two axis and sampling will then happen\n                  *independently* per axis, resulting in samples that differ between the axes.\n        interpolation (int): OpenCV interpolation flag.\n        mask_interpolation (int): OpenCV interpolation flag.\n        cval (number or sequence of number): The constant value to use when filling in newly created pixels.\n            (E.g. translating by 1px to the right will create a new 1px-wide column of pixels\n            on the left of the image).\n            The value is only used when `mode=constant`. The expected value range is ``[0, 255]`` for ``uint8`` images.\n        cval_mask (number or tuple of number): Same as cval but only for masks.\n        mode (int): OpenCV border flag.\n        fit_output (bool): If True, the image plane size and position will be adjusted to tightly capture\n            the whole image after affine transformation (`translate_percent` and `translate_px` are ignored).\n            Otherwise (``False``),  parts of the transformed image may end up outside the image plane.\n            Fitting the output shape can be useful to avoid corners of the image being outside the image plane\n            after applying rotations. Default: False\n        keep_ratio (bool): When True, the original aspect ratio will be kept when the random scale is applied.\n            Default: False.\n        rotate_method (Literal[\"largest_box\", \"ellipse\"]): rotation method used for the bounding boxes.\n            Should be one of \"largest_box\" or \"ellipse\"[1]. Default: \"largest_box\"\n        balanced_scale (bool): When True, scaling factors are chosen to be either entirely below or above 1,\n            ensuring balanced scaling. Default: False.\n\n            This is important because without it, scaling tends to lean towards upscaling. For example, if we want\n            the image to zoom in and out by 2x, we may pick an interval [0.5, 2]. Since the interval [0.5, 1] is\n            three times smaller than [1, 2], values above 1 are picked three times more often if sampled directly\n            from [0.5, 2]. With `balanced_scale`, the  function ensures that half the time, the scaling\n            factor is picked from below 1 (zooming out), and the other half from above 1 (zooming in).\n            This makes the zooming in and out process more balanced.\n        p (float): probability of applying the transform. Default: 0.5.\n\n    Targets:\n        image, mask, keypoints, bboxes\n\n    Image types:\n        uint8, float32\n\n    Reference:\n        [1] https://arxiv.org/abs/2109.13488\n\n    \"\"\"\n\n    _targets = (Targets.IMAGE, Targets.MASK, Targets.BBOXES, Targets.KEYPOINTS)\n\n    class InitSchema(BaseTransformInitSchema):\n        scale: ScaleFloatType | dict[str, Any] | None = Field(\n            default=None,\n            description=\"Scaling factor or dictionary for independent axis scaling.\",\n        )\n        translate_percent: ScaleFloatType | dict[str, Any] | None = Field(\n            default=None,\n            description=\"Translation as a fraction of the image dimension.\",\n        )\n        translate_px: ScaleIntType | dict[str, Any] | None = Field(\n            default=None,\n            description=\"Translation in pixels.\",\n        )\n        rotate: ScaleFloatType | None = Field(default=None, description=\"Rotation angle in degrees.\")\n        shear: ScaleFloatType | dict[str, Any] | None = Field(\n            default=None,\n            description=\"Shear angle in degrees.\",\n        )\n        interpolation: InterpolationType = cv2.INTER_LINEAR\n        mask_interpolation: InterpolationType = cv2.INTER_NEAREST\n\n        cval: ColorType = Field(default=0, description=\"Value used for constant padding.\")\n        cval_mask: ColorType = Field(default=0, description=\"Value used for mask constant padding.\")\n        mode: BorderModeType = cv2.BORDER_CONSTANT\n        fit_output: Annotated[bool, Field(default=False, description=\"Adjust output to capture whole image.\")]\n        keep_ratio: Annotated[bool, Field(default=False, description=\"Maintain aspect ratio when scaling.\")]\n        rotate_method: Literal[\"largest_box\", \"ellipse\"] = \"largest_box\"\n        balanced_scale: Annotated[bool, Field(default=False, description=\"Use balanced scaling.\")]\n\n    def __init__(\n        self,\n        scale: ScaleFloatType | dict[str, Any] | None = None,\n        translate_percent: ScaleFloatType | dict[str, Any] | None = None,\n        translate_px: ScaleIntType | dict[str, Any] | None = None,\n        rotate: ScaleFloatType | None = None,\n        shear: ScaleFloatType | dict[str, Any] | None = None,\n        interpolation: int = cv2.INTER_LINEAR,\n        mask_interpolation: int = cv2.INTER_NEAREST,\n        cval: ColorType = 0,\n        cval_mask: ColorType = 0,\n        mode: int = cv2.BORDER_CONSTANT,\n        fit_output: bool = False,\n        keep_ratio: bool = False,\n        rotate_method: Literal[\"largest_box\", \"ellipse\"] = \"largest_box\",\n        balanced_scale: bool = False,\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p=p, always_apply=always_apply)\n\n        params = [scale, translate_percent, translate_px, rotate, shear]\n        if all(p is None for p in params):\n            scale = {\"x\": (0.9, 1.1), \"y\": (0.9, 1.1)}\n            translate_percent = {\"x\": (-0.1, 0.1), \"y\": (-0.1, 0.1)}\n            rotate = (-15, 15)\n            shear = {\"x\": (-10, 10), \"y\": (-10, 10)}\n        else:\n            scale = scale if scale is not None else 1.0\n            rotate = rotate if rotate is not None else 0.0\n            shear = shear if shear is not None else 0.0\n\n        self.interpolation = interpolation\n        self.mask_interpolation = mask_interpolation\n        self.cval = cval\n        self.cval_mask = cval_mask\n        self.mode = mode\n        self.scale = self._handle_dict_arg(scale, \"scale\")\n        self.translate_percent, self.translate_px = self._handle_translate_arg(translate_px, translate_percent)\n        self.rotate = to_tuple(rotate, rotate)\n        self.fit_output = fit_output\n        self.shear = self._handle_dict_arg(shear, \"shear\")\n        self.keep_ratio = keep_ratio\n        self.rotate_method = rotate_method\n        self.balanced_scale = balanced_scale\n\n        if self.keep_ratio and self.scale[\"x\"] != self.scale[\"y\"]:\n            raise ValueError(f\"When keep_ratio is True, the x and y scale range should be identical. got {self.scale}\")\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return (\n            \"interpolation\",\n            \"mask_interpolation\",\n            \"cval\",\n            \"mode\",\n            \"scale\",\n            \"translate_percent\",\n            \"translate_px\",\n            \"rotate\",\n            \"fit_output\",\n            \"shear\",\n            \"cval_mask\",\n            \"keep_ratio\",\n            \"rotate_method\",\n            \"balanced_scale\",\n        )\n\n    @staticmethod\n    def _handle_dict_arg(\n        val: float | tuple[float, float] | dict[str, Any],\n        name: str,\n        default: float = 1.0,\n    ) -&gt; dict[str, Any]:\n        if isinstance(val, dict):\n            if \"x\" not in val and \"y\" not in val:\n                raise ValueError(\n                    f'Expected {name} dictionary to contain at least key \"x\" or key \"y\". Found neither of them.',\n                )\n            x = val.get(\"x\", default)\n            y = val.get(\"y\", default)\n            return {\"x\": to_tuple(x, x), \"y\": to_tuple(y, y)}\n        return {\"x\": to_tuple(val, val), \"y\": to_tuple(val, val)}\n\n    @classmethod\n    def _handle_translate_arg(\n        cls,\n        translate_px: ScaleFloatType | dict[str, Any] | None,\n        translate_percent: ScaleFloatType | dict[str, Any] | None,\n    ) -&gt; Any:\n        if translate_percent is None and translate_px is None:\n            translate_px = 0\n\n        if translate_percent is not None and translate_px is not None:\n            msg = \"Expected either translate_percent or translate_px to be provided, but both were provided.\"\n            raise ValueError(msg)\n\n        if translate_percent is not None:\n            # translate by percent\n            return cls._handle_dict_arg(translate_percent, \"translate_percent\", default=0.0), translate_px\n\n        if translate_px is None:\n            msg = \"translate_px is None.\"\n            raise ValueError(msg)\n        # translate by pixels\n        return translate_percent, cls._handle_dict_arg(translate_px, \"translate_px\")\n\n    def apply(\n        self,\n        img: np.ndarray,\n        matrix: skimage.transform.ProjectiveTransform,\n        output_shape: SizeType,\n        **params: Any,\n    ) -&gt; np.ndarray:\n        return fgeometric.warp_affine(\n            img,\n            matrix,\n            interpolation=self.interpolation,\n            cval=self.cval,\n            mode=self.mode,\n            output_shape=output_shape,\n        )\n\n    def apply_to_mask(\n        self,\n        mask: np.ndarray,\n        matrix: skimage.transform.ProjectiveTransform,\n        output_shape: SizeType,\n        **params: Any,\n    ) -&gt; np.ndarray:\n        return fgeometric.warp_affine(\n            mask,\n            matrix,\n            interpolation=self.mask_interpolation,\n            cval=self.cval_mask,\n            mode=self.mode,\n            output_shape=output_shape,\n        )\n\n    def apply_to_bboxes(\n        self,\n        bboxes: Sequence[BoxType],\n        bbox_matrix: skimage.transform.AffineTransform,\n        output_shape: SizeType,\n        **params: Any,\n    ) -&gt; list[BoxType]:\n        bboxes_np = np.array(bboxes)\n        result = fgeometric.bboxes_affine(\n            bboxes_np,\n            bbox_matrix,\n            self.rotate_method,\n            params[\"shape\"][:2],\n            self.mode,\n            output_shape,\n        )\n        return result.tolist()\n\n    def apply_to_keypoints(\n        self,\n        keypoints: Sequence[KeypointType],\n        matrix: skimage.transform.AffineTransform,\n        scale: dict[str, Any],\n        **params: Any,\n    ) -&gt; list[KeypointType]:\n        # Convert keypoints to numpy array, including all attributes\n        keypoints_array = np.array([list(kp) for kp in keypoints], dtype=np.float32)\n\n        padded_keypoints = fgeometric.keypoints_affine(keypoints_array, matrix, params[\"shape\"][:2], scale, self.mode)\n\n        # Convert back to list of tuples\n        return [tuple(kp) for kp in padded_keypoints]\n\n    @staticmethod\n    def get_scale(\n        scale: dict[str, tuple[float, float]],\n        keep_ratio: bool,\n        balanced_scale: bool,\n    ) -&gt; fgeometric.ScaleDict:\n        result_scale = {}\n        if balanced_scale:\n            for key, value in scale.items():\n                lower_interval = (value[0], 1.0) if value[0] &lt; 1 else None\n                upper_interval = (1.0, value[1]) if value[1] &gt; 1 else None\n\n                if lower_interval is not None and upper_interval is not None:\n                    selected_interval = random.choice([lower_interval, upper_interval])\n                elif lower_interval is not None:\n                    selected_interval = lower_interval\n                elif upper_interval is not None:\n                    selected_interval = upper_interval\n                else:\n                    raise ValueError(f\"Both lower_interval and upper_interval are None for key: {key}\")\n\n                result_scale[key] = random.uniform(*selected_interval)\n        else:\n            result_scale = {key: random.uniform(*value) for key, value in scale.items()}\n\n        if keep_ratio:\n            result_scale[\"y\"] = result_scale[\"x\"]\n\n        return cast(fgeometric.ScaleDict, result_scale)\n\n    def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n        image_shape = params[\"shape\"][:2]\n\n        translate = self._get_translate_params(image_shape)\n        shear = self._get_shear_params()\n        scale = self.get_scale(self.scale, self.keep_ratio, self.balanced_scale)\n        rotate = -random.uniform(*self.rotate)\n\n        image_shift = center(image_shape)\n        bbox_shift = center_bbox(image_shape)\n\n        matrix = fgeometric.create_affine_transformation_matrix(translate, shear, scale, rotate, image_shift)\n        bbox_matrix = fgeometric.create_affine_transformation_matrix(translate, shear, scale, rotate, bbox_shift)\n\n        if self.fit_output:\n            matrix, output_shape = fgeometric.compute_affine_warp_output_shape(matrix, image_shape)\n            bbox_matrix, _ = fgeometric.compute_affine_warp_output_shape(bbox_matrix, image_shape)\n        else:\n            output_shape = image_shape\n\n        return {\n            \"rotate\": rotate,\n            \"scale\": scale,\n            \"matrix\": matrix,\n            \"bbox_matrix\": bbox_matrix,\n            \"output_shape\": output_shape,\n        }\n\n    def _get_translate_params(self, image_shape: tuple[int, int]) -&gt; fgeometric.TranslateDict:\n        height, width = image_shape[:2]\n        if self.translate_px is not None:\n            return cast(\n                fgeometric.TranslateDict,\n                {key: random.randint(*value) for key, value in self.translate_px.items()},\n            )\n        if self.translate_percent is not None:\n            translate = {key: random.uniform(*value) for key, value in self.translate_percent.items()}\n            return cast(fgeometric.TranslateDict, {\"x\": translate[\"x\"] * width, \"y\": translate[\"y\"] * height})\n        return cast(fgeometric.TranslateDict, {\"x\": 0, \"y\": 0})\n\n    def _get_shear_params(self) -&gt; fgeometric.ShearDict:\n        return cast(fgeometric.ShearDict, {key: -random.uniform(*value) for key, value in self.shear.items()})\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.transforms.Affine.apply","title":"<code>apply (self, img, matrix, output_shape, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def apply(\n    self,\n    img: np.ndarray,\n    matrix: skimage.transform.ProjectiveTransform,\n    output_shape: SizeType,\n    **params: Any,\n) -&gt; np.ndarray:\n    return fgeometric.warp_affine(\n        img,\n        matrix,\n        interpolation=self.interpolation,\n        cval=self.cval,\n        mode=self.mode,\n        output_shape=output_shape,\n    )\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.transforms.Affine.get_params_dependent_on_data","title":"<code>get_params_dependent_on_data (self, params, data)</code>","text":"<p>Returns parameters dependent on input.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n    image_shape = params[\"shape\"][:2]\n\n    translate = self._get_translate_params(image_shape)\n    shear = self._get_shear_params()\n    scale = self.get_scale(self.scale, self.keep_ratio, self.balanced_scale)\n    rotate = -random.uniform(*self.rotate)\n\n    image_shift = center(image_shape)\n    bbox_shift = center_bbox(image_shape)\n\n    matrix = fgeometric.create_affine_transformation_matrix(translate, shear, scale, rotate, image_shift)\n    bbox_matrix = fgeometric.create_affine_transformation_matrix(translate, shear, scale, rotate, bbox_shift)\n\n    if self.fit_output:\n        matrix, output_shape = fgeometric.compute_affine_warp_output_shape(matrix, image_shape)\n        bbox_matrix, _ = fgeometric.compute_affine_warp_output_shape(bbox_matrix, image_shape)\n    else:\n        output_shape = image_shape\n\n    return {\n        \"rotate\": rotate,\n        \"scale\": scale,\n        \"matrix\": matrix,\n        \"bbox_matrix\": bbox_matrix,\n        \"output_shape\": output_shape,\n    }\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.transforms.Affine.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return (\n        \"interpolation\",\n        \"mask_interpolation\",\n        \"cval\",\n        \"mode\",\n        \"scale\",\n        \"translate_percent\",\n        \"translate_px\",\n        \"rotate\",\n        \"fit_output\",\n        \"shear\",\n        \"cval_mask\",\n        \"keep_ratio\",\n        \"rotate_method\",\n        \"balanced_scale\",\n    )\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.transforms.D4","title":"<code>class  D4</code> <code>     (always_apply=None, p=1)                 </code>  [view source on GitHub]","text":"<p>Applies one of the eight possible D4 dihedral group transformations to a square-shaped input,     maintaining the square shape. These transformations correspond to the symmetries of a square,     including rotations and reflections.</p> <p>The D4 group transformations include: - 'e' (identity): No transformation is applied. - 'r90' (rotation by 90 degrees counterclockwise) - 'r180' (rotation by 180 degrees) - 'r270' (rotation by 270 degrees counterclockwise) - 'v' (reflection across the vertical midline) - 'hvt' (reflection across the anti-diagonal) - 'h' (reflection across the horizontal midline) - 't' (reflection across the main diagonal)</p> <p>Even if the probability (<code>p</code>) of applying the transform is set to 1, the identity transformation 'e' may still occur, which means the input will remain unchanged in one out of eight cases.</p> <p>Parameters:</p> Name Type Description <code>p</code> <code>float</code> <p>Probability of applying the transform. Default is 1, meaning the        transform is applied every time it is called.</p> <p>Targets</p> <p>image, mask, bboxes, keypoints</p> <p>Image types:     uint8, float32</p> <p>Note</p> <p>This transform is particularly useful when augmenting data that does not have a clear orientation: - Top view satellite or drone imagery - Medical images</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>class D4(DualTransform):\n    \"\"\"Applies one of the eight possible D4 dihedral group transformations to a square-shaped input,\n        maintaining the square shape. These transformations correspond to the symmetries of a square,\n        including rotations and reflections.\n\n    The D4 group transformations include:\n    - 'e' (identity): No transformation is applied.\n    - 'r90' (rotation by 90 degrees counterclockwise)\n    - 'r180' (rotation by 180 degrees)\n    - 'r270' (rotation by 270 degrees counterclockwise)\n    - 'v' (reflection across the vertical midline)\n    - 'hvt' (reflection across the anti-diagonal)\n    - 'h' (reflection across the horizontal midline)\n    - 't' (reflection across the main diagonal)\n\n    Even if the probability (`p`) of applying the transform is set to 1, the identity transformation\n    'e' may still occur, which means the input will remain unchanged in one out of eight cases.\n\n    Args:\n        p (float): Probability of applying the transform. Default is 1, meaning the\n                   transform is applied every time it is called.\n\n    Targets:\n        image, mask, bboxes, keypoints\n\n    Image types:\n        uint8, float32\n\n    Note:\n        This transform is particularly useful when augmenting data that does not have a clear orientation:\n        - Top view satellite or drone imagery\n        - Medical images\n\n    \"\"\"\n\n    _targets = (Targets.IMAGE, Targets.MASK, Targets.BBOXES, Targets.KEYPOINTS)\n\n    class InitSchema(BaseTransformInitSchema):\n        p: ProbabilityType = 1\n\n    def __init__(\n        self,\n        always_apply: bool | None = None,\n        p: float = 1,\n    ):\n        super().__init__(p=p, always_apply=always_apply)\n\n    def apply(self, img: np.ndarray, group_element: D4Type, **params: Any) -&gt; np.ndarray:\n        return fgeometric.d4(img, group_element)\n\n    def apply_to_bbox(self, bbox: BoxInternalType, group_element: D4Type, **params: Any) -&gt; BoxInternalType:\n        return fgeometric.bbox_d4(bbox, group_element)\n\n    def apply_to_keypoint(\n        self,\n        keypoint: KeypointInternalType,\n        group_element: D4Type,\n        **params: Any,\n    ) -&gt; KeypointInternalType:\n        return fgeometric.keypoint_d4(keypoint, group_element, params[\"shape\"])\n\n    def get_params(self) -&gt; dict[str, D4Type]:\n        return {\n            \"group_element\": random_utils.choice(d4_group_elements),\n        }\n\n    def get_transform_init_args_names(self) -&gt; tuple[()]:\n        return ()\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.transforms.D4.apply","title":"<code>apply (self, img, group_element, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def apply(self, img: np.ndarray, group_element: D4Type, **params: Any) -&gt; np.ndarray:\n    return fgeometric.d4(img, group_element)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.transforms.D4.get_params","title":"<code>get_params (self)</code>","text":"<p>Returns parameters independent of input.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def get_params(self) -&gt; dict[str, D4Type]:\n    return {\n        \"group_element\": random_utils.choice(d4_group_elements),\n    }\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.transforms.D4.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[()]:\n    return ()\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.transforms.ElasticTransform","title":"<code>class  ElasticTransform</code> <code>     (alpha=3, sigma=50, alpha_affine=None, interpolation=1, border_mode=4, value=None, mask_value=None, always_apply=None, approximate=False, same_dxdy=False, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Apply elastic deformation to images, masks, and bounding boxes as described in [Simard2003]_.</p> <p>This transformation introduces random elastic distortions to images, which can be useful for data augmentation in training convolutional neural networks. The transformation can be applied in an approximate or precise manner, with an option to use the same displacement field for both x and y directions to speed up the process.</p> <p>Parameters:</p> Name Type Description <code>alpha</code> <code>float</code> <p>Scaling factor for the random displacement fields.</p> <code>sigma</code> <code>float</code> <p>Standard deviation for Gaussian filter applied to the displacement fields.</p> <code>interpolation</code> <code>int</code> <p>Interpolation method to be used. Should be one of: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4. Default is cv2.INTER_LINEAR.</p> <code>border_mode</code> <code>int</code> <p>Pixel extrapolation method. Should be one of: cv2.BORDER_CONSTANT, cv2.BORDER_REPLICATE, cv2.BORDER_REFLECT, cv2.BORDER_WRAP, cv2.BORDER_REFLECT_101. Default is cv2.BORDER_REFLECT_101.</p> <code>value</code> <code>int, float, list of int, list of float</code> <p>Padding value if border_mode is cv2.BORDER_CONSTANT.</p> <code>mask_value</code> <code>int, float, list of int, list of float</code> <p>Padding value if border_mode is cv2.BORDER_CONSTANT, applied to masks.</p> <code>approximate</code> <code>bool</code> <p>Whether to smooth displacement map with a fixed kernel size. Enabling this option gives ~2X speedup on large images. Default is False.</p> <code>same_dxdy</code> <code>bool</code> <p>Whether to use the same random displacement for x and y directions. Enabling this option gives ~2X speedup. Default is False.</p> <p>Targets</p> <p>image, mask, bboxes</p> <p>Image types:     uint8, float32</p> <p>Reference</p> <p>Simard, Steinkraus and Platt, \"Best Practices for Convolutional Neural Networks applied to Visual Document Analysis\", in Proc. of the International Conference on Document Analysis and Recognition, 2003. https://gist.github.com/ernestum/601cdf56d2b424757de5</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>class ElasticTransform(DualTransform):\n    \"\"\"Apply elastic deformation to images, masks, and bounding boxes as described in [Simard2003]_.\n\n    This transformation introduces random elastic distortions to images, which can be useful for data augmentation\n    in training convolutional neural networks. The transformation can be applied in an approximate or precise manner,\n    with an option to use the same displacement field for both x and y directions to speed up the process.\n\n    Args:\n        alpha (float): Scaling factor for the random displacement fields.\n        sigma (float): Standard deviation for Gaussian filter applied to the displacement fields.\n        interpolation (int): Interpolation method to be used. Should be one of:\n            cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.\n            Default is cv2.INTER_LINEAR.\n        border_mode (int): Pixel extrapolation method. Should be one of:\n            cv2.BORDER_CONSTANT, cv2.BORDER_REPLICATE, cv2.BORDER_REFLECT, cv2.BORDER_WRAP, cv2.BORDER_REFLECT_101.\n            Default is cv2.BORDER_REFLECT_101.\n        value (int, float, list of int, list of float, optional): Padding value if border_mode is cv2.BORDER_CONSTANT.\n        mask_value (int, float, list of int, list of float, optional): Padding value if border_mode is\n            cv2.BORDER_CONSTANT, applied to masks.\n        approximate (bool, optional): Whether to smooth displacement map with a fixed kernel size.\n            Enabling this option gives ~2X speedup on large images. Default is False.\n        same_dxdy (bool, optional): Whether to use the same random displacement for x and y directions.\n            Enabling this option gives ~2X speedup. Default is False.\n\n    Targets:\n        image, mask, bboxes\n\n    Image types:\n        uint8, float32\n\n    Reference:\n        Simard, Steinkraus and Platt, \"Best Practices for Convolutional Neural Networks applied to\n        Visual Document Analysis\", in Proc. of the International Conference on Document Analysis and Recognition, 2003.\n        https://gist.github.com/ernestum/601cdf56d2b424757de5\n    \"\"\"\n\n    _targets = (Targets.IMAGE, Targets.MASK, Targets.BBOXES)\n\n    class InitSchema(BaseTransformInitSchema):\n        alpha: Annotated[float, Field(description=\"Alpha parameter.\", ge=0)]\n        sigma: Annotated[float, Field(default=50, description=\"Sigma parameter for Gaussian filter.\", ge=1)]\n        alpha_affine: None = Field(\n            description=\"Alpha affine parameter.\",\n            deprecated=\"Use Affine transform to get affine effects\",\n        )\n        interpolation: InterpolationType = cv2.INTER_LINEAR\n        border_mode: BorderModeType = cv2.BORDER_REFLECT_101\n        value: int | float | list[int] | list[float] | None = Field(\n            default=None,\n            description=\"Padding value if border_mode is cv2.BORDER_CONSTANT.\",\n        )\n        mask_value: float | list[int] | list[float] | None = Field(\n            default=None,\n            description=\"Padding value if border_mode is cv2.BORDER_CONSTANT applied for masks.\",\n        )\n        approximate: Annotated[bool, Field(default=False, description=\"Approximate displacement map smoothing.\")]\n        same_dxdy: Annotated[bool, Field(default=False, description=\"Use same shift for x and y.\")]\n\n    def __init__(\n        self,\n        alpha: float = 3,\n        sigma: float = 50,\n        alpha_affine: None = None,\n        interpolation: int = cv2.INTER_LINEAR,\n        border_mode: int = cv2.BORDER_REFLECT_101,\n        value: ScalarType | list[ScalarType] | None = None,\n        mask_value: ScalarType | list[ScalarType] | None = None,\n        always_apply: bool | None = None,\n        approximate: bool = False,\n        same_dxdy: bool = False,\n        p: float = 0.5,\n    ):\n        super().__init__(p=p, always_apply=always_apply)\n        self.alpha = alpha\n        self.sigma = sigma\n        self.interpolation = interpolation\n        self.border_mode = border_mode\n        self.value = value\n        self.mask_value = mask_value\n        self.approximate = approximate\n        self.same_dxdy = same_dxdy\n\n    def apply(\n        self,\n        img: np.ndarray,\n        random_seed: int,\n        interpolation: int,\n        **params: Any,\n    ) -&gt; np.ndarray:\n        return fgeometric.elastic_transform(\n            img,\n            self.alpha,\n            self.sigma,\n            interpolation,\n            self.border_mode,\n            self.value,\n            np.random.RandomState(random_seed),\n            self.approximate,\n            self.same_dxdy,\n        )\n\n    def apply_to_mask(self, mask: np.ndarray, random_seed: int, **params: Any) -&gt; np.ndarray:\n        return fgeometric.elastic_transform(\n            mask,\n            self.alpha,\n            self.sigma,\n            cv2.INTER_NEAREST,\n            self.border_mode,\n            self.mask_value,\n            np.random.RandomState(random_seed),\n            self.approximate,\n            self.same_dxdy,\n        )\n\n    def apply_to_bbox(\n        self,\n        bbox: BoxInternalType,\n        random_seed: int,\n        **params: Any,\n    ) -&gt; BoxInternalType:\n        image_shape = params[\"shape\"][:2]\n\n        mask = np.zeros(image_shape, dtype=np.uint8)\n        bbox_denorm = fgeometric.denormalize_bbox(bbox, image_shape)\n        x_min, y_min, x_max, y_max = bbox_denorm[:4]\n        x_min, y_min, x_max, y_max = int(x_min), int(y_min), int(x_max), int(y_max)\n        mask[y_min:y_max, x_min:x_max] = 1\n        mask = fgeometric.elastic_transform(\n            mask,\n            self.alpha,\n            self.sigma,\n            cv2.INTER_NEAREST,\n            self.border_mode,\n            self.mask_value,\n            np.random.RandomState(random_seed),\n            self.approximate,\n        )\n        bbox_returned = bbox_from_mask(mask)\n        return cast(BoxInternalType, fgeometric.normalize_bbox(bbox_returned, image_shape))\n\n    def get_params(self) -&gt; dict[str, int]:\n        return {\"random_seed\": random_utils.get_random_seed()}\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return (\n            \"alpha\",\n            \"sigma\",\n            \"interpolation\",\n            \"border_mode\",\n            \"value\",\n            \"mask_value\",\n            \"approximate\",\n            \"same_dxdy\",\n        )\n\n    @property\n    def targets(self) -&gt; dict[str, Callable[..., Any]]:\n        return {\n            \"image\": self.apply,\n            \"mask\": self.apply_to_mask,\n            \"masks\": self.apply_to_masks,\n            \"bboxes\": self.apply_to_bboxes,\n        }\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.transforms.ElasticTransform.apply","title":"<code>apply (self, img, random_seed, interpolation, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def apply(\n    self,\n    img: np.ndarray,\n    random_seed: int,\n    interpolation: int,\n    **params: Any,\n) -&gt; np.ndarray:\n    return fgeometric.elastic_transform(\n        img,\n        self.alpha,\n        self.sigma,\n        interpolation,\n        self.border_mode,\n        self.value,\n        np.random.RandomState(random_seed),\n        self.approximate,\n        self.same_dxdy,\n    )\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.transforms.ElasticTransform.get_params","title":"<code>get_params (self)</code>","text":"<p>Returns parameters independent of input.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def get_params(self) -&gt; dict[str, int]:\n    return {\"random_seed\": random_utils.get_random_seed()}\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.transforms.ElasticTransform.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return (\n        \"alpha\",\n        \"sigma\",\n        \"interpolation\",\n        \"border_mode\",\n        \"value\",\n        \"mask_value\",\n        \"approximate\",\n        \"same_dxdy\",\n    )\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.transforms.Flip","title":"<code>class  Flip</code> <code>     (always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Deprecated. Consider using HorizontalFlip, VerticalFlip, RandomRotate90 or D4.</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>class Flip(DualTransform):\n    \"\"\"Deprecated. Consider using HorizontalFlip, VerticalFlip, RandomRotate90 or D4.\"\"\"\n\n    _targets = (Targets.IMAGE, Targets.MASK, Targets.BBOXES, Targets.KEYPOINTS)\n\n    def __init__(self, always_apply: bool | None = None, p: float = 0.5):\n        super().__init__(p=p, always_apply=always_apply)\n        warn(\n            \"Flip is deprecated. Consider using HorizontalFlip, VerticalFlip, RandomRotate90 or D4.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n\n    def apply(self, img: np.ndarray, d: int, **params: Any) -&gt; np.ndarray:\n        \"\"\"Args:\n        d (int): code that specifies how to flip the input. 0 for vertical flipping, 1 for horizontal flipping,\n                -1 for both vertical and horizontal flipping (which is also could be seen as rotating the input by\n                180 degrees).\n        \"\"\"\n        return fgeometric.random_flip(img, d)\n\n    def get_params(self) -&gt; dict[str, int]:\n        # Random int in the range [-1, 1]\n        return {\"d\": random.randint(-1, 1)}\n\n    def apply_to_bbox(self, bbox: BoxInternalType, **params: Any) -&gt; BoxInternalType:\n        return fgeometric.bbox_flip(bbox, params[\"d\"])\n\n    def apply_to_keypoint(self, keypoint: KeypointInternalType, **params: Any) -&gt; KeypointInternalType:\n        return fgeometric.keypoint_flip(keypoint, params[\"d\"], params[\"shape\"])\n\n    def get_transform_init_args_names(self) -&gt; tuple[()]:\n        return ()\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.transforms.Flip.__init__","title":"<code>__init__ (self, always_apply=None, p=0.5)</code>  <code>special</code>","text":"<p>Initialize self.  See help(type(self)) for accurate signature.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def __init__(self, always_apply: bool | None = None, p: float = 0.5):\n    super().__init__(p=p, always_apply=always_apply)\n    warn(\n        \"Flip is deprecated. Consider using HorizontalFlip, VerticalFlip, RandomRotate90 or D4.\",\n        DeprecationWarning,\n        stacklevel=2,\n    )\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.transforms.Flip.apply","title":"<code>apply (self, img, d, **params)</code>","text":"<p>d (int): code that specifies how to flip the input. 0 for vertical flipping, 1 for horizontal flipping,         -1 for both vertical and horizontal flipping (which is also could be seen as rotating the input by         180 degrees).</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def apply(self, img: np.ndarray, d: int, **params: Any) -&gt; np.ndarray:\n    \"\"\"Args:\n    d (int): code that specifies how to flip the input. 0 for vertical flipping, 1 for horizontal flipping,\n            -1 for both vertical and horizontal flipping (which is also could be seen as rotating the input by\n            180 degrees).\n    \"\"\"\n    return fgeometric.random_flip(img, d)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.transforms.Flip.get_params","title":"<code>get_params (self)</code>","text":"<p>Returns parameters independent of input.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def get_params(self) -&gt; dict[str, int]:\n    # Random int in the range [-1, 1]\n    return {\"d\": random.randint(-1, 1)}\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.transforms.Flip.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[()]:\n    return ()\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.transforms.GridDistortion","title":"<code>class  GridDistortion</code> <code>     (num_steps=5, distort_limit=(-0.3, 0.3), interpolation=1, border_mode=4, value=None, mask_value=None, normalized=False, always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Applies grid distortion augmentation to images, masks, and bounding boxes. This technique involves dividing the image into a grid of cells and randomly displacing the intersection points of the grid, resulting in localized distortions.</p> <p>Parameters:</p> Name Type Description <code>num_steps</code> <code>int</code> <p>Number of grid cells on each side (minimum 1).</p> <code>distort_limit</code> <code>float, (float, float</code> <p>Range of distortion limits. If a single float is provided, the range will be from (-distort_limit, distort_limit). Default: (-0.3, 0.3).</p> <code>interpolation</code> <code>OpenCV flag</code> <p>Interpolation algorithm used for image transformation. Options are: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4. Default: cv2.INTER_LINEAR.</p> <code>border_mode</code> <code>OpenCV flag</code> <p>Pixel extrapolation method used when pixels outside the image are required. Options are: cv2.BORDER_CONSTANT, cv2.BORDER_REPLICATE, cv2.BORDER_REFLECT, cv2.BORDER_WRAP, cv2.BORDER_REFLECT_101. Default: cv2.BORDER_REFLECT_101.</p> <code>value</code> <code>int, float, list of ints, list of floats</code> <p>Value used for padding when border_mode is cv2.BORDER_CONSTANT.</p> <code>mask_value</code> <code>int, float, list of ints, list of floats</code> <p>Padding value for masks when border_mode is cv2.BORDER_CONSTANT.</p> <code>normalized</code> <code>bool</code> <p>If True, ensures that distortion does not exceed image boundaries. Default: False. Reference: https://github.com/albumentations-team/albumentations/pull/722</p> <p>Targets</p> <p>image, mask, bboxes</p> <p>Image types:     uint8, float32</p> <p>Note</p> <p>This transform is helpful in medical imagery, Optical Character Recognition, and other tasks where local distance may not be preserved.</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>class GridDistortion(DualTransform):\n    \"\"\"Applies grid distortion augmentation to images, masks, and bounding boxes. This technique involves dividing\n    the image into a grid of cells and randomly displacing the intersection points of the grid,\n    resulting in localized distortions.\n\n    Args:\n        num_steps (int): Number of grid cells on each side (minimum 1).\n        distort_limit (float, (float, float)): Range of distortion limits. If a single float is provided,\n            the range will be from (-distort_limit, distort_limit). Default: (-0.3, 0.3).\n        interpolation (OpenCV flag): Interpolation algorithm used for image transformation. Options are:\n            cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.\n            Default: cv2.INTER_LINEAR.\n        border_mode (OpenCV flag): Pixel extrapolation method used when pixels outside the image are required.\n            Options are: cv2.BORDER_CONSTANT, cv2.BORDER_REPLICATE, cv2.BORDER_REFLECT, cv2.BORDER_WRAP,\n            cv2.BORDER_REFLECT_101.\n            Default: cv2.BORDER_REFLECT_101.\n        value (int, float, list of ints, list of floats, optional): Value used for padding when\n            border_mode is cv2.BORDER_CONSTANT.\n        mask_value (int, float, list of ints, list of floats, optional): Padding value for masks when\n            border_mode is cv2.BORDER_CONSTANT.\n        normalized (bool): If True, ensures that distortion does not exceed image boundaries. Default: False.\n            Reference: https://github.com/albumentations-team/albumentations/pull/722\n\n    Targets:\n        image, mask, bboxes\n\n    Image types:\n        uint8, float32\n\n    Note:\n        This transform is helpful in medical imagery, Optical Character Recognition, and other tasks where local\n        distance may not be preserved.\n    \"\"\"\n\n    _targets = (Targets.IMAGE, Targets.MASK, Targets.BBOXES)\n\n    class InitSchema(BaseTransformInitSchema):\n        num_steps: Annotated[int, Field(ge=1, description=\"Count of grid cells on each side.\")]\n        distort_limit: SymmetricRangeType = (-0.3, 0.3)\n        interpolation: InterpolationType = cv2.INTER_LINEAR\n        border_mode: BorderModeType = cv2.BORDER_REFLECT_101\n        value: ColorType | None = Field(\n            default=None,\n            description=\"Padding value if border_mode is cv2.BORDER_CONSTANT.\",\n        )\n        mask_value: ColorType | None = Field(\n            default=None,\n            description=\"Padding value for mask if border_mode is cv2.BORDER_CONSTANT.\",\n        )\n        normalized: bool = Field(\n            default=False,\n            description=\"If true, distortion will be normalized to not go outside the image.\",\n        )\n\n        @field_validator(\"distort_limit\")\n        @classmethod\n        def check_limits(cls, v: tuple[float, float], info: ValidationInfo) -&gt; tuple[float, float]:\n            bounds = -1, 1\n            result = to_tuple(v)\n            check_range(result, *bounds, info.field_name)\n            return result\n\n    def __init__(\n        self,\n        num_steps: int = 5,\n        distort_limit: ScaleFloatType = (-0.3, 0.3),\n        interpolation: int = cv2.INTER_LINEAR,\n        border_mode: int = cv2.BORDER_REFLECT_101,\n        value: ColorType | None = None,\n        mask_value: ColorType | None = None,\n        normalized: bool = False,\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p, always_apply)\n\n        self.num_steps = num_steps\n        self.distort_limit = cast(Tuple[float, float], distort_limit)\n        self.interpolation = interpolation\n        self.border_mode = border_mode\n        self.value = value\n        self.mask_value = mask_value\n        self.normalized = normalized\n\n    def apply(\n        self,\n        img: np.ndarray,\n        stepsx: tuple[()],\n        stepsy: tuple[()],\n        interpolation: int,\n        **params: Any,\n    ) -&gt; np.ndarray:\n        return fgeometric.grid_distortion(\n            img,\n            self.num_steps,\n            stepsx,\n            stepsy,\n            interpolation,\n            self.border_mode,\n            self.value,\n        )\n\n    def apply_to_mask(\n        self,\n        mask: np.ndarray,\n        stepsx: tuple[()],\n        stepsy: tuple[()],\n        **params: Any,\n    ) -&gt; np.ndarray:\n        return fgeometric.grid_distortion(\n            mask,\n            self.num_steps,\n            stepsx,\n            stepsy,\n            cv2.INTER_NEAREST,\n            self.border_mode,\n            self.mask_value,\n        )\n\n    def apply_to_bbox(\n        self,\n        bbox: BoxInternalType,\n        stepsx: tuple[()],\n        stepsy: tuple[()],\n        **params: Any,\n    ) -&gt; BoxInternalType:\n        image_shape = params[\"shape\"][:2]\n        mask = np.zeros(image_shape, dtype=np.uint8)\n        bbox_denorm = fgeometric.denormalize_bbox(bbox, image_shape)\n        x_min, y_min, x_max, y_max = bbox_denorm[:4]\n        x_min, y_min, x_max, y_max = int(x_min), int(y_min), int(x_max), int(y_max)\n        mask[y_min:y_max, x_min:x_max] = 1\n        mask = fgeometric.grid_distortion(\n            mask,\n            self.num_steps,\n            stepsx,\n            stepsy,\n            cv2.INTER_NEAREST,\n            self.border_mode,\n            self.mask_value,\n        )\n        bbox_returned = bbox_from_mask(mask)\n        return cast(BoxInternalType, fgeometric.normalize_bbox(bbox_returned, image_shape))\n\n    def _normalize(self, h: int, w: int, xsteps: list[float], ysteps: list[float]) -&gt; dict[str, Any]:\n        # compensate for smaller last steps in source image.\n        x_step = w // self.num_steps\n        last_x_step = min(w, ((self.num_steps + 1) * x_step)) - (self.num_steps * x_step)\n        xsteps[-1] *= last_x_step / x_step\n\n        y_step = h // self.num_steps\n        last_y_step = min(h, ((self.num_steps + 1) * y_step)) - (self.num_steps * y_step)\n        ysteps[-1] *= last_y_step / y_step\n\n        # now normalize such that distortion never leaves image bounds.\n        tx = w / math.floor(w / self.num_steps)\n        ty = h / math.floor(h / self.num_steps)\n        xsteps = np.array(xsteps) * (tx / np.sum(xsteps))\n        ysteps = np.array(ysteps) * (ty / np.sum(ysteps))\n\n        return {\"stepsx\": xsteps, \"stepsy\": ysteps}\n\n    def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n        height, width = params[\"shape\"][:2]\n\n        stepsx = [1 + random.uniform(self.distort_limit[0], self.distort_limit[1]) for _ in range(self.num_steps + 1)]\n        stepsy = [1 + random.uniform(self.distort_limit[0], self.distort_limit[1]) for _ in range(self.num_steps + 1)]\n\n        if self.normalized:\n            return self._normalize(height, width, stepsx, stepsy)\n\n        return {\"stepsx\": stepsx, \"stepsy\": stepsy}\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return \"num_steps\", \"distort_limit\", \"interpolation\", \"border_mode\", \"value\", \"mask_value\", \"normalized\"\n\n    @property\n    def targets(self) -&gt; dict[str, Callable[..., Any]]:\n        return {\n            \"image\": self.apply,\n            \"mask\": self.apply_to_mask,\n            \"masks\": self.apply_to_masks,\n            \"bboxes\": self.apply_to_bboxes,\n        }\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.transforms.GridDistortion.apply","title":"<code>apply (self, img, stepsx, stepsy, interpolation, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def apply(\n    self,\n    img: np.ndarray,\n    stepsx: tuple[()],\n    stepsy: tuple[()],\n    interpolation: int,\n    **params: Any,\n) -&gt; np.ndarray:\n    return fgeometric.grid_distortion(\n        img,\n        self.num_steps,\n        stepsx,\n        stepsy,\n        interpolation,\n        self.border_mode,\n        self.value,\n    )\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.transforms.GridDistortion.get_params_dependent_on_data","title":"<code>get_params_dependent_on_data (self, params, data)</code>","text":"<p>Returns parameters dependent on input.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n    height, width = params[\"shape\"][:2]\n\n    stepsx = [1 + random.uniform(self.distort_limit[0], self.distort_limit[1]) for _ in range(self.num_steps + 1)]\n    stepsy = [1 + random.uniform(self.distort_limit[0], self.distort_limit[1]) for _ in range(self.num_steps + 1)]\n\n    if self.normalized:\n        return self._normalize(height, width, stepsx, stepsy)\n\n    return {\"stepsx\": stepsx, \"stepsy\": stepsy}\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.transforms.GridDistortion.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return \"num_steps\", \"distort_limit\", \"interpolation\", \"border_mode\", \"value\", \"mask_value\", \"normalized\"\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.transforms.GridElasticDeform","title":"<code>class  GridElasticDeform</code> <code>     (num_grid_xy, magnitude, interpolation=1, mask_interpolation=0, p=1.0, always_apply=None)                 </code>  [view source on GitHub]","text":"<p>Grid-based Elastic deformation Albumentation implementation</p> <p>This class applies elastic transformations using a grid-based approach. The granularity and intensity of the distortions can be controlled using the dimensions of the overlaying distortion grid and the magnitude parameter. Larger grid sizes result in finer, less severe distortions.</p> <p>Parameters:</p> Name Type Description <code>num_grid_xy</code> <code>tuple[int, int]</code> <p>Number of grid cells along the width and height. Specified as (grid_width, grid_height). Each value must be greater than 1.</p> <code>magnitude</code> <code>int</code> <p>Maximum pixel-wise displacement for distortion. Must be greater than 0.</p> <code>interpolation</code> <code>int</code> <p>Interpolation method to be used for the image transformation. Default: cv2.INTER_LINEAR</p> <code>mask_interpolation</code> <code>int</code> <p>Interpolation method to be used for mask transformation. Default: cv2.INTER_NEAREST</p> <code>p</code> <code>float</code> <p>Probability of applying the transform. Default: 1.0.</p> <p>Targets</p> <p>image, mask</p> <p>Image types:     uint8, float32</p> <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; transform = GridElasticDeform(num_grid_xy=(4, 4), magnitude=10, p=1.0)\n&gt;&gt;&gt; result = transform(image=image, mask=mask)\n&gt;&gt;&gt; transformed_image, transformed_mask = result['image'], result['mask']\n</code></pre> <p>Note</p> <p>This transformation is particularly useful for data augmentation in medical imaging and other domains where elastic deformations can simulate realistic variations.</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>class GridElasticDeform(DualTransform):\n    \"\"\"Grid-based Elastic deformation Albumentation implementation\n\n    This class applies elastic transformations using a grid-based approach.\n    The granularity and intensity of the distortions can be controlled using\n    the dimensions of the overlaying distortion grid and the magnitude parameter.\n    Larger grid sizes result in finer, less severe distortions.\n\n    Args:\n        num_grid_xy (tuple[int, int]): Number of grid cells along the width and height.\n            Specified as (grid_width, grid_height). Each value must be greater than 1.\n        magnitude (int): Maximum pixel-wise displacement for distortion. Must be greater than 0.\n        interpolation (int): Interpolation method to be used for the image transformation.\n            Default: cv2.INTER_LINEAR\n        mask_interpolation (int): Interpolation method to be used for mask transformation.\n            Default: cv2.INTER_NEAREST\n        p (float): Probability of applying the transform. Default: 1.0.\n\n    Targets:\n        image, mask\n\n    Image types:\n        uint8, float32\n\n    Example:\n        &gt;&gt;&gt; transform = GridElasticDeform(num_grid_xy=(4, 4), magnitude=10, p=1.0)\n        &gt;&gt;&gt; result = transform(image=image, mask=mask)\n        &gt;&gt;&gt; transformed_image, transformed_mask = result['image'], result['mask']\n\n    Note:\n        This transformation is particularly useful for data augmentation in medical imaging\n        and other domains where elastic deformations can simulate realistic variations.\n    \"\"\"\n\n    _targets = (Targets.IMAGE, Targets.MASK)\n\n    class InitSchema(BaseTransformInitSchema):\n        num_grid_xy: Annotated[tuple[int, int], AfterValidator(check_1plus)]\n        p: ProbabilityType = 1.0\n        magnitude: int = Field(gt=0)\n        interpolation: InterpolationType = cv2.INTER_LINEAR\n        mask_interpolation: InterpolationType = cv2.INTER_NEAREST\n\n    def __init__(\n        self,\n        num_grid_xy: tuple[int, int],\n        magnitude: int,\n        interpolation: int = cv2.INTER_LINEAR,\n        mask_interpolation: int = cv2.INTER_NEAREST,\n        p: float = 1.0,\n        always_apply: bool | None = None,\n    ):\n        super().__init__(p=p, always_apply=always_apply)\n        self.num_grid_xy = num_grid_xy\n        self.magnitude = magnitude\n        self.interpolation = interpolation\n        self.mask_interpolation = mask_interpolation\n\n    @staticmethod\n    def generate_mesh(polygons: np.ndarray, dimensions: np.ndarray) -&gt; np.ndarray:\n        return np.hstack((dimensions.reshape(-1, 4), polygons))\n\n    def get_params_dependent_on_data(\n        self,\n        params: dict[str, Any],\n        data: dict[str, Any],\n    ) -&gt; dict[str, Any]:\n        image_shape = np.array(params[\"shape\"][:2])\n\n        dimensions = fgeometric.calculate_grid_dimensions(image_shape, self.num_grid_xy)\n\n        polygons = fgeometric.generate_distorted_grid_polygons(dimensions, self.magnitude)\n\n        generated_mesh = self.generate_mesh(polygons, dimensions)\n\n        return {\"generated_mesh\": generated_mesh}\n\n    def apply(self, img: np.ndarray, generated_mesh: np.ndarray, **params: Any) -&gt; np.ndarray:\n        return fgeometric.distort_image(img, generated_mesh, self.interpolation)\n\n    def apply_to_mask(self, mask: np.ndarray, generated_mesh: np.ndarray, **params: Any) -&gt; np.ndarray:\n        return fgeometric.distort_image(mask, generated_mesh, self.mask_interpolation)\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return \"num_grid_xy\", \"magnitude\", \"interpolation\", \"mask_interpolation\"\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.transforms.GridElasticDeform.apply","title":"<code>apply (self, img, generated_mesh, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def apply(self, img: np.ndarray, generated_mesh: np.ndarray, **params: Any) -&gt; np.ndarray:\n    return fgeometric.distort_image(img, generated_mesh, self.interpolation)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.transforms.GridElasticDeform.get_params_dependent_on_data","title":"<code>get_params_dependent_on_data (self, params, data)</code>","text":"<p>Returns parameters dependent on input.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def get_params_dependent_on_data(\n    self,\n    params: dict[str, Any],\n    data: dict[str, Any],\n) -&gt; dict[str, Any]:\n    image_shape = np.array(params[\"shape\"][:2])\n\n    dimensions = fgeometric.calculate_grid_dimensions(image_shape, self.num_grid_xy)\n\n    polygons = fgeometric.generate_distorted_grid_polygons(dimensions, self.magnitude)\n\n    generated_mesh = self.generate_mesh(polygons, dimensions)\n\n    return {\"generated_mesh\": generated_mesh}\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.transforms.GridElasticDeform.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return \"num_grid_xy\", \"magnitude\", \"interpolation\", \"mask_interpolation\"\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.transforms.HorizontalFlip","title":"<code>class  HorizontalFlip</code> <code> </code>  [view source on GitHub]","text":"<p>Flip the input horizontally around the y-axis.</p> <p>Parameters:</p> Name Type Description <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 0.5.</p> <p>Targets</p> <p>image, mask, bboxes, keypoints</p> <p>Image types:     uint8, float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>class HorizontalFlip(DualTransform):\n    \"\"\"Flip the input horizontally around the y-axis.\n\n    Args:\n        p (float): probability of applying the transform. Default: 0.5.\n\n    Targets:\n        image, mask, bboxes, keypoints\n\n    Image types:\n        uint8, float32\n\n    \"\"\"\n\n    _targets = (Targets.IMAGE, Targets.MASK, Targets.BBOXES, Targets.KEYPOINTS)\n\n    def apply(self, img: np.ndarray, **params: Any) -&gt; np.ndarray:\n        if get_num_channels(img) &gt; 1 and img.dtype == np.uint8:\n            # Opencv is faster than numpy only in case of\n            # non-gray scale 8bits images\n            return fgeometric.hflip_cv2(img)\n\n        return fgeometric.hflip(img)\n\n    def apply_to_bbox(self, bbox: BoxInternalType, **params: Any) -&gt; BoxInternalType:\n        return fgeometric.bbox_hflip(bbox)\n\n    def apply_to_keypoint(self, keypoint: KeypointInternalType, **params: Any) -&gt; KeypointInternalType:\n        return fgeometric.keypoint_hflip(keypoint, params[\"cols\"])\n\n    def get_transform_init_args_names(self) -&gt; tuple[()]:\n        return ()\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.transforms.HorizontalFlip.apply","title":"<code>apply (self, img, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def apply(self, img: np.ndarray, **params: Any) -&gt; np.ndarray:\n    if get_num_channels(img) &gt; 1 and img.dtype == np.uint8:\n        # Opencv is faster than numpy only in case of\n        # non-gray scale 8bits images\n        return fgeometric.hflip_cv2(img)\n\n    return fgeometric.hflip(img)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.transforms.HorizontalFlip.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[()]:\n    return ()\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.transforms.OpticalDistortion","title":"<code>class  OpticalDistortion</code> <code>     (distort_limit=(-0.05, 0.05), shift_limit=(-0.05, 0.05), interpolation=1, border_mode=4, value=None, mask_value=None, always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Parameters:</p> Name Type Description <code>distort_limit</code> <code>float, (float, float</code> <p>If distort_limit is a single float, the range will be (-distort_limit, distort_limit). Default: (-0.05, 0.05).</p> <code>shift_limit</code> <code>float, (float, float</code> <p>If shift_limit is a single float, the range will be (-shift_limit, shift_limit). Default: (-0.05, 0.05).</p> <code>interpolation</code> <code>OpenCV flag</code> <p>flag that is used to specify the interpolation algorithm. Should be one of: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4. Default: cv2.INTER_LINEAR.</p> <code>border_mode</code> <code>OpenCV flag</code> <p>flag that is used to specify the pixel extrapolation method. Should be one of: cv2.BORDER_CONSTANT, cv2.BORDER_REPLICATE, cv2.BORDER_REFLECT, cv2.BORDER_WRAP, cv2.BORDER_REFLECT_101. Default: cv2.BORDER_REFLECT_101</p> <code>value</code> <code>int, float, list of ints, list of float</code> <p>padding value if border_mode is cv2.BORDER_CONSTANT.</p> <code>mask_value</code> <code>int, float,         list of ints,         list of float</code> <p>padding value if border_mode is cv2.BORDER_CONSTANT applied for masks.</p> <p>Targets</p> <p>image, mask, bboxes</p> <p>Image types:     uint8, float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>class OpticalDistortion(DualTransform):\n    \"\"\"Args:\n        distort_limit (float, (float, float)): If distort_limit is a single float, the range\n            will be (-distort_limit, distort_limit). Default: (-0.05, 0.05).\n        shift_limit (float, (float, float))): If shift_limit is a single float, the range\n            will be (-shift_limit, shift_limit). Default: (-0.05, 0.05).\n        interpolation (OpenCV flag): flag that is used to specify the interpolation algorithm. Should be one of:\n            cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.\n            Default: cv2.INTER_LINEAR.\n        border_mode (OpenCV flag): flag that is used to specify the pixel extrapolation method. Should be one of:\n            cv2.BORDER_CONSTANT, cv2.BORDER_REPLICATE, cv2.BORDER_REFLECT, cv2.BORDER_WRAP, cv2.BORDER_REFLECT_101.\n            Default: cv2.BORDER_REFLECT_101\n        value (int, float, list of ints, list of float): padding value if border_mode is cv2.BORDER_CONSTANT.\n        mask_value (int, float,\n                    list of ints,\n                    list of float): padding value if border_mode is cv2.BORDER_CONSTANT applied for masks.\n\n    Targets:\n        image, mask, bboxes\n\n    Image types:\n        uint8, float32\n\n    \"\"\"\n\n    _targets = (Targets.IMAGE, Targets.MASK, Targets.BBOXES)\n\n    class InitSchema(BaseTransformInitSchema):\n        distort_limit: SymmetricRangeType = (-0.05, 0.05)\n        shift_limit: SymmetricRangeType = (-0.05, 0.05)\n        interpolation: InterpolationType = cv2.INTER_LINEAR\n        border_mode: BorderModeType = cv2.BORDER_REFLECT_101\n        value: ColorType | None = Field(\n            default=None,\n            description=\"Padding value if border_mode is cv2.BORDER_CONSTANT.\",\n        )\n        mask_value: ColorType | None = Field(\n            default=None,\n            description=\"Padding value for mask if border_mode is cv2.BORDER_CONSTANT.\",\n        )\n\n    def __init__(\n        self,\n        distort_limit: ScaleFloatType = (-0.05, 0.05),\n        shift_limit: ScaleFloatType = (-0.05, 0.05),\n        interpolation: int = cv2.INTER_LINEAR,\n        border_mode: int = cv2.BORDER_REFLECT_101,\n        value: ColorType | None = None,\n        mask_value: ColorType | None = None,\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p, always_apply)\n        self.shift_limit = cast(Tuple[float, float], shift_limit)\n        self.distort_limit = cast(Tuple[float, float], distort_limit)\n        self.interpolation = interpolation\n        self.border_mode = border_mode\n        self.value = value\n        self.mask_value = mask_value\n\n    def apply(\n        self,\n        img: np.ndarray,\n        k: int,\n        dx: int,\n        dy: int,\n        interpolation: int,\n        **params: Any,\n    ) -&gt; np.ndarray:\n        return fgeometric.optical_distortion(img, k, dx, dy, interpolation, self.border_mode, self.value)\n\n    def apply_to_mask(self, mask: np.ndarray, k: int, dx: int, dy: int, **params: Any) -&gt; np.ndarray:\n        return fgeometric.optical_distortion(mask, k, dx, dy, cv2.INTER_NEAREST, self.border_mode, self.mask_value)\n\n    def apply_to_bbox(\n        self,\n        bbox: BoxInternalType,\n        k: int,\n        dx: int,\n        dy: int,\n        **params: Any,\n    ) -&gt; BoxInternalType:\n        image_shape = params[\"shape\"]\n        mask = np.zeros(image_shape[:2], dtype=np.uint8)\n        bbox_denorm = fgeometric.denormalize_bbox(bbox, image_shape)\n        x_min, y_min, x_max, y_max = bbox_denorm[:4]\n        x_min, y_min, x_max, y_max = int(x_min), int(y_min), int(x_max), int(y_max)\n        mask[y_min:y_max, x_min:x_max] = 1\n        mask = fgeometric.optical_distortion(mask, k, dx, dy, cv2.INTER_NEAREST, self.border_mode, self.mask_value)\n        bbox_returned = bbox_from_mask(mask)\n        return cast(BoxInternalType, fgeometric.normalize_bbox(bbox_returned, image_shape))\n\n    def get_params(self) -&gt; dict[str, Any]:\n        return {\n            \"k\": random.uniform(*self.distort_limit),\n            \"dx\": round(random.uniform(*self.shift_limit)),\n            \"dy\": round(random.uniform(*self.shift_limit)),\n        }\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return (\n            \"distort_limit\",\n            \"shift_limit\",\n            \"interpolation\",\n            \"border_mode\",\n            \"value\",\n            \"mask_value\",\n        )\n\n    @property\n    def targets(self) -&gt; dict[str, Callable[..., Any]]:\n        return {\n            \"image\": self.apply,\n            \"mask\": self.apply_to_mask,\n            \"masks\": self.apply_to_masks,\n            \"bboxes\": self.apply_to_bboxes,\n        }\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.transforms.OpticalDistortion.apply","title":"<code>apply (self, img, k, dx, dy, interpolation, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def apply(\n    self,\n    img: np.ndarray,\n    k: int,\n    dx: int,\n    dy: int,\n    interpolation: int,\n    **params: Any,\n) -&gt; np.ndarray:\n    return fgeometric.optical_distortion(img, k, dx, dy, interpolation, self.border_mode, self.value)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.transforms.OpticalDistortion.get_params","title":"<code>get_params (self)</code>","text":"<p>Returns parameters independent of input.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def get_params(self) -&gt; dict[str, Any]:\n    return {\n        \"k\": random.uniform(*self.distort_limit),\n        \"dx\": round(random.uniform(*self.shift_limit)),\n        \"dy\": round(random.uniform(*self.shift_limit)),\n    }\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.transforms.OpticalDistortion.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return (\n        \"distort_limit\",\n        \"shift_limit\",\n        \"interpolation\",\n        \"border_mode\",\n        \"value\",\n        \"mask_value\",\n    )\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.transforms.PadIfNeeded","title":"<code>class  PadIfNeeded</code> <code>     (min_height=1024, min_width=1024, pad_height_divisor=None, pad_width_divisor=None, position=&lt;PositionType.CENTER: 'center'&gt;, border_mode=4, value=None, mask_value=None, always_apply=None, p=1.0)                 </code>  [view source on GitHub]","text":"<p>Pads the sides of an image if the image dimensions are less than the specified minimum dimensions. If the <code>pad_height_divisor</code> or <code>pad_width_divisor</code> is specified, the function additionally ensures that the image dimensions are divisible by these values.</p> <p>Parameters:</p> Name Type Description <code>min_height</code> <code>int</code> <p>Minimum desired height of the image. Ensures image height is at least this value.</p> <code>min_width</code> <code>int</code> <p>Minimum desired width of the image. Ensures image width is at least this value.</p> <code>pad_height_divisor</code> <code>int</code> <p>If set, pads the image height to make it divisible by this value.</p> <code>pad_width_divisor</code> <code>int</code> <p>If set, pads the image width to make it divisible by this value.</p> <code>position</code> <code>Union[str, PositionType]</code> <p>Position where the image is to be placed after padding. Can be one of 'center', 'top_left', 'top_right', 'bottom_left', 'bottom_right', or 'random'. Default is 'center'.</p> <code>border_mode</code> <code>int</code> <p>Specifies the border mode to use if padding is required. The default is <code>cv2.BORDER_REFLECT_101</code>.</p> <code>value</code> <code>Union[int, float, list[int], list[float]]</code> <p>Value to fill the border pixels if the border mode is <code>cv2.BORDER_CONSTANT</code>. Default is None.</p> <code>mask_value</code> <code>Union[int, float, list[int], list[float]]</code> <p>Similar to <code>value</code> but used for padding masks. Default is None.</p> <code>p</code> <code>float</code> <p>Probability of applying the transform. Default is 1.0.</p> <p>Targets</p> <p>image, mask, bboxes, keypoints</p> <p>Image types:     uint8, float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>class PadIfNeeded(DualTransform):\n    \"\"\"Pads the sides of an image if the image dimensions are less than the specified minimum dimensions.\n    If the `pad_height_divisor` or `pad_width_divisor` is specified, the function additionally ensures\n    that the image dimensions are divisible by these values.\n\n    Args:\n        min_height (int): Minimum desired height of the image. Ensures image height is at least this value.\n        min_width (int): Minimum desired width of the image. Ensures image width is at least this value.\n        pad_height_divisor (int, optional): If set, pads the image height to make it divisible by this value.\n        pad_width_divisor (int, optional): If set, pads the image width to make it divisible by this value.\n        position (Union[str, PositionType]): Position where the image is to be placed after padding.\n            Can be one of 'center', 'top_left', 'top_right', 'bottom_left', 'bottom_right', or 'random'.\n            Default is 'center'.\n        border_mode (int): Specifies the border mode to use if padding is required.\n            The default is `cv2.BORDER_REFLECT_101`.\n        value (Union[int, float, list[int], list[float]], optional): Value to fill the border pixels if\n            the border mode is `cv2.BORDER_CONSTANT`. Default is None.\n        mask_value (Union[int, float, list[int], list[float]], optional): Similar to `value` but used for padding masks.\n            Default is None.\n        p (float): Probability of applying the transform. Default is 1.0.\n\n    Targets:\n        image, mask, bboxes, keypoints\n\n    Image types:\n        uint8, float32\n\n    \"\"\"\n\n    class PositionType(Enum):\n        \"\"\"Enumerates the types of positions for placing an object within a container.\n\n        This Enum class is utilized to define specific anchor positions that an object can\n        assume relative to a container. It's particularly useful in image processing, UI layout,\n        and graphic design to specify the alignment and positioning of elements.\n\n        Attributes:\n            CENTER (str): Specifies that the object should be placed at the center.\n            TOP_LEFT (str): Specifies that the object should be placed at the top-left corner.\n            TOP_RIGHT (str): Specifies that the object should be placed at the top-right corner.\n            BOTTOM_LEFT (str): Specifies that the object should be placed at the bottom-left corner.\n            BOTTOM_RIGHT (str): Specifies that the object should be placed at the bottom-right corner.\n            RANDOM (str): Indicates that the object's position should be determined randomly.\n\n        \"\"\"\n\n        CENTER = \"center\"\n        TOP_LEFT = \"top_left\"\n        TOP_RIGHT = \"top_right\"\n        BOTTOM_LEFT = \"bottom_left\"\n        BOTTOM_RIGHT = \"bottom_right\"\n        RANDOM = \"random\"\n\n    _targets = (Targets.IMAGE, Targets.MASK, Targets.BBOXES, Targets.KEYPOINTS)\n\n    class InitSchema(BaseTransformInitSchema):\n        min_height: int | None = Field(default=None, ge=1, description=\"Minimal result image height.\")\n        min_width: int | None = Field(default=None, ge=1, description=\"Minimal result image width.\")\n        pad_height_divisor: int | None = Field(\n            default=None,\n            ge=1,\n            description=\"Ensures image height is divisible by this value.\",\n        )\n        pad_width_divisor: int | None = Field(\n            default=None,\n            ge=1,\n            description=\"Ensures image width is divisible by this value.\",\n        )\n        position: str = Field(default=\"center\", description=\"Position of the padded image.\")\n        border_mode: BorderModeType = cv2.BORDER_REFLECT_101\n        value: ColorType | None = Field(default=None, description=\"Value for border if BORDER_CONSTANT is used.\")\n        mask_value: ColorType | None = Field(\n            default=None,\n            description=\"Value for mask border if BORDER_CONSTANT is used.\",\n        )\n        p: ProbabilityType = 1.0\n\n        @model_validator(mode=\"after\")\n        def validate_divisibility(self) -&gt; Self:\n            if (self.min_height is None) == (self.pad_height_divisor is None):\n                msg = \"Only one of 'min_height' and 'pad_height_divisor' parameters must be set\"\n                raise ValueError(msg)\n            if (self.min_width is None) == (self.pad_width_divisor is None):\n                msg = \"Only one of 'min_width' and 'pad_width_divisor' parameters must be set\"\n                raise ValueError(msg)\n\n            if self.border_mode == cv2.BORDER_CONSTANT and self.value is None:\n                msg = \"If 'border_mode' is set to 'BORDER_CONSTANT', 'value' must be provided.\"\n                raise ValueError(msg)\n\n            return self\n\n    def __init__(\n        self,\n        min_height: int | None = 1024,\n        min_width: int | None = 1024,\n        pad_height_divisor: int | None = None,\n        pad_width_divisor: int | None = None,\n        position: PositionType | str = PositionType.CENTER,\n        border_mode: int = cv2.BORDER_REFLECT_101,\n        value: ColorType | None = None,\n        mask_value: ColorType | None = None,\n        always_apply: bool | None = None,\n        p: float = 1.0,\n    ):\n        super().__init__(p, always_apply)\n        self.min_height = min_height\n        self.min_width = min_width\n        self.pad_width_divisor = pad_width_divisor\n        self.pad_height_divisor = pad_height_divisor\n        self.position = PadIfNeeded.PositionType(position)\n        self.border_mode = border_mode\n        self.value = value\n        self.mask_value = mask_value\n\n    def update_params(self, params: dict[str, Any], **kwargs: Any) -&gt; dict[str, Any]:\n        params = super().update_params(params, **kwargs)\n        rows, cols = params[\"shape\"][:2]\n\n        if self.min_height is not None:\n            if rows &lt; self.min_height:\n                h_pad_top = int((self.min_height - rows) / 2.0)\n                h_pad_bottom = self.min_height - rows - h_pad_top\n            else:\n                h_pad_top = 0\n                h_pad_bottom = 0\n        else:\n            pad_remained = rows % self.pad_height_divisor\n            pad_rows = self.pad_height_divisor - pad_remained if pad_remained &gt; 0 else 0\n\n            h_pad_top = pad_rows // 2\n            h_pad_bottom = pad_rows - h_pad_top\n\n        if self.min_width is not None:\n            if cols &lt; self.min_width:\n                w_pad_left = int((self.min_width - cols) / 2.0)\n                w_pad_right = self.min_width - cols - w_pad_left\n            else:\n                w_pad_left = 0\n                w_pad_right = 0\n        else:\n            pad_remainder = cols % self.pad_width_divisor\n            pad_cols = self.pad_width_divisor - pad_remainder if pad_remainder &gt; 0 else 0\n\n            w_pad_left = pad_cols // 2\n            w_pad_right = pad_cols - w_pad_left\n\n        h_pad_top, h_pad_bottom, w_pad_left, w_pad_right = self.__update_position_params(\n            h_top=h_pad_top,\n            h_bottom=h_pad_bottom,\n            w_left=w_pad_left,\n            w_right=w_pad_right,\n        )\n\n        params.update(\n            {\n                \"pad_top\": h_pad_top,\n                \"pad_bottom\": h_pad_bottom,\n                \"pad_left\": w_pad_left,\n                \"pad_right\": w_pad_right,\n            },\n        )\n        return params\n\n    def apply(\n        self,\n        img: np.ndarray,\n        pad_top: int,\n        pad_bottom: int,\n        pad_left: int,\n        pad_right: int,\n        **params: Any,\n    ) -&gt; np.ndarray:\n        return fgeometric.pad_with_params(\n            img,\n            pad_top,\n            pad_bottom,\n            pad_left,\n            pad_right,\n            border_mode=self.border_mode,\n            value=self.value,\n        )\n\n    def apply_to_mask(\n        self,\n        mask: np.ndarray,\n        pad_top: int,\n        pad_bottom: int,\n        pad_left: int,\n        pad_right: int,\n        **params: Any,\n    ) -&gt; np.ndarray:\n        return fgeometric.pad_with_params(\n            mask,\n            pad_top,\n            pad_bottom,\n            pad_left,\n            pad_right,\n            border_mode=self.border_mode,\n            value=self.mask_value,\n        )\n\n    def apply_to_bboxes(\n        self,\n        bboxes: Sequence[BoxType],\n        pad_top: int,\n        pad_bottom: int,\n        pad_left: int,\n        pad_right: int,\n        **params: Any,\n    ) -&gt; list[BoxType]:\n        image_shape = params[\"shape\"][:2]\n\n        bboxes_np = np.array(bboxes)\n        bboxes_np = denormalize_bboxes(bboxes_np, params[\"shape\"])\n\n        result = fgeometric.pad_bboxes(\n            bboxes_np,\n            pad_top,\n            pad_bottom,\n            pad_left,\n            pad_right,\n            self.border_mode,\n            image_shape=image_shape,\n        )\n\n        rows, cols = params[\"shape\"][:2]\n\n        return list(normalize_bboxes(result, (rows + pad_top + pad_bottom, cols + pad_left + pad_right)))\n\n    def apply_to_keypoints(\n        self,\n        keypoints: Sequence[KeypointType],\n        pad_top: int,\n        pad_bottom: int,\n        pad_left: int,\n        pad_right: int,\n        **params: Any,\n    ) -&gt; Sequence[KeypointType]:\n        # Convert keypoints to numpy array, including all attributes\n        keypoints_array = np.array([list(kp) for kp in keypoints])\n\n        padded_keypoints = fgeometric.pad_keypoints(\n            keypoints_array,\n            pad_top,\n            pad_bottom,\n            pad_left,\n            pad_right,\n            self.border_mode,\n            image_shape=params[\"shape\"][:2],\n        )\n\n        # Convert back to list of tuples\n        return [tuple(kp) for kp in padded_keypoints]\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return (\n            \"min_height\",\n            \"min_width\",\n            \"pad_height_divisor\",\n            \"pad_width_divisor\",\n            \"position\",\n            \"border_mode\",\n            \"value\",\n            \"mask_value\",\n        )\n\n    def __update_position_params(\n        self,\n        h_top: int,\n        h_bottom: int,\n        w_left: int,\n        w_right: int,\n    ) -&gt; tuple[int, int, int, int]:\n        if self.position == PadIfNeeded.PositionType.TOP_LEFT:\n            h_bottom += h_top\n            w_right += w_left\n            h_top = 0\n            w_left = 0\n\n        elif self.position == PadIfNeeded.PositionType.TOP_RIGHT:\n            h_bottom += h_top\n            w_left += w_right\n            h_top = 0\n            w_right = 0\n\n        elif self.position == PadIfNeeded.PositionType.BOTTOM_LEFT:\n            h_top += h_bottom\n            w_right += w_left\n            h_bottom = 0\n            w_left = 0\n\n        elif self.position == PadIfNeeded.PositionType.BOTTOM_RIGHT:\n            h_top += h_bottom\n            w_left += w_right\n            h_bottom = 0\n            w_right = 0\n\n        elif self.position == PadIfNeeded.PositionType.RANDOM:\n            h_pad = h_top + h_bottom\n            w_pad = w_left + w_right\n            h_top = random.randint(0, h_pad)\n            h_bottom = h_pad - h_top\n            w_left = random.randint(0, w_pad)\n            w_right = w_pad - w_left\n\n        return h_top, h_bottom, w_left, w_right\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.transforms.PadIfNeeded.PositionType","title":"<code>class  PositionType</code> <code> </code>","text":"<p>Enumerates the types of positions for placing an object within a container.</p> <p>This Enum class is utilized to define specific anchor positions that an object can assume relative to a container. It's particularly useful in image processing, UI layout, and graphic design to specify the alignment and positioning of elements.</p> <p>Attributes:</p> Name Type Description <code>CENTER</code> <code>str</code> <p>Specifies that the object should be placed at the center.</p> <code>TOP_LEFT</code> <code>str</code> <p>Specifies that the object should be placed at the top-left corner.</p> <code>TOP_RIGHT</code> <code>str</code> <p>Specifies that the object should be placed at the top-right corner.</p> <code>BOTTOM_LEFT</code> <code>str</code> <p>Specifies that the object should be placed at the bottom-left corner.</p> <code>BOTTOM_RIGHT</code> <code>str</code> <p>Specifies that the object should be placed at the bottom-right corner.</p> <code>RANDOM</code> <code>str</code> <p>Indicates that the object's position should be determined randomly.</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>class PositionType(Enum):\n    \"\"\"Enumerates the types of positions for placing an object within a container.\n\n    This Enum class is utilized to define specific anchor positions that an object can\n    assume relative to a container. It's particularly useful in image processing, UI layout,\n    and graphic design to specify the alignment and positioning of elements.\n\n    Attributes:\n        CENTER (str): Specifies that the object should be placed at the center.\n        TOP_LEFT (str): Specifies that the object should be placed at the top-left corner.\n        TOP_RIGHT (str): Specifies that the object should be placed at the top-right corner.\n        BOTTOM_LEFT (str): Specifies that the object should be placed at the bottom-left corner.\n        BOTTOM_RIGHT (str): Specifies that the object should be placed at the bottom-right corner.\n        RANDOM (str): Indicates that the object's position should be determined randomly.\n\n    \"\"\"\n\n    CENTER = \"center\"\n    TOP_LEFT = \"top_left\"\n    TOP_RIGHT = \"top_right\"\n    BOTTOM_LEFT = \"bottom_left\"\n    BOTTOM_RIGHT = \"bottom_right\"\n    RANDOM = \"random\"\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.transforms.PadIfNeeded.apply","title":"<code>apply (self, img, pad_top, pad_bottom, pad_left, pad_right, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def apply(\n    self,\n    img: np.ndarray,\n    pad_top: int,\n    pad_bottom: int,\n    pad_left: int,\n    pad_right: int,\n    **params: Any,\n) -&gt; np.ndarray:\n    return fgeometric.pad_with_params(\n        img,\n        pad_top,\n        pad_bottom,\n        pad_left,\n        pad_right,\n        border_mode=self.border_mode,\n        value=self.value,\n    )\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.transforms.PadIfNeeded.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return (\n        \"min_height\",\n        \"min_width\",\n        \"pad_height_divisor\",\n        \"pad_width_divisor\",\n        \"position\",\n        \"border_mode\",\n        \"value\",\n        \"mask_value\",\n    )\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.transforms.PadIfNeeded.update_params","title":"<code>update_params (self, params, **kwargs)</code>","text":"<p>Update parameters with transform specific params. This method is deprecated, use: - <code>get_params</code> for transform specific params like interpolation and - <code>update_params_shape</code> for data like shape.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def update_params(self, params: dict[str, Any], **kwargs: Any) -&gt; dict[str, Any]:\n    params = super().update_params(params, **kwargs)\n    rows, cols = params[\"shape\"][:2]\n\n    if self.min_height is not None:\n        if rows &lt; self.min_height:\n            h_pad_top = int((self.min_height - rows) / 2.0)\n            h_pad_bottom = self.min_height - rows - h_pad_top\n        else:\n            h_pad_top = 0\n            h_pad_bottom = 0\n    else:\n        pad_remained = rows % self.pad_height_divisor\n        pad_rows = self.pad_height_divisor - pad_remained if pad_remained &gt; 0 else 0\n\n        h_pad_top = pad_rows // 2\n        h_pad_bottom = pad_rows - h_pad_top\n\n    if self.min_width is not None:\n        if cols &lt; self.min_width:\n            w_pad_left = int((self.min_width - cols) / 2.0)\n            w_pad_right = self.min_width - cols - w_pad_left\n        else:\n            w_pad_left = 0\n            w_pad_right = 0\n    else:\n        pad_remainder = cols % self.pad_width_divisor\n        pad_cols = self.pad_width_divisor - pad_remainder if pad_remainder &gt; 0 else 0\n\n        w_pad_left = pad_cols // 2\n        w_pad_right = pad_cols - w_pad_left\n\n    h_pad_top, h_pad_bottom, w_pad_left, w_pad_right = self.__update_position_params(\n        h_top=h_pad_top,\n        h_bottom=h_pad_bottom,\n        w_left=w_pad_left,\n        w_right=w_pad_right,\n    )\n\n    params.update(\n        {\n            \"pad_top\": h_pad_top,\n            \"pad_bottom\": h_pad_bottom,\n            \"pad_left\": w_pad_left,\n            \"pad_right\": w_pad_right,\n        },\n    )\n    return params\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.transforms.Perspective","title":"<code>class  Perspective</code> <code>     (scale=(0.05, 0.1), keep_size=True, pad_mode=0, pad_val=0, mask_pad_val=0, fit_output=False, interpolation=1, always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Perform a random four point perspective transform of the input.</p> <p>Parameters:</p> Name Type Description <code>scale</code> <code>ScaleFloatType</code> <p>standard deviation of the normal distributions. These are used to sample the random distances of the subimage's corners from the full image's corners. If scale is a single float value, the range will be (0, scale). Default: (0.05, 0.1).</p> <code>keep_size</code> <code>bool</code> <p>Whether to resize image back to their original size after applying the perspective transform. If set to False, the resulting images may end up having different shapes and will always be a list, never an array. Default: True</p> <code>pad_mode</code> <code>OpenCV flag</code> <p>OpenCV border mode.</p> <code>pad_val</code> <code>int, float, list of int, list of float</code> <p>padding value if border_mode is cv2.BORDER_CONSTANT. Default: 0</p> <code>mask_pad_val</code> <code>int, float, list of int, list of float</code> <p>padding value for mask if border_mode is cv2.BORDER_CONSTANT. Default: 0</p> <code>fit_output</code> <code>bool</code> <p>If True, the image plane size and position will be adjusted to still capture the whole image after perspective transformation. (Followed by image resizing if keep_size is set to True.) Otherwise, parts of the transformed image may be outside of the image plane. This setting should not be set to True when using large scale values as it could lead to very large images. Default: False</p> <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 0.5.</p> <p>Targets</p> <p>image, mask, keypoints, bboxes</p> <p>Image types:     uint8, float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>class Perspective(DualTransform):\n    \"\"\"Perform a random four point perspective transform of the input.\n\n    Args:\n        scale: standard deviation of the normal distributions. These are used to sample\n            the random distances of the subimage's corners from the full image's corners.\n            If scale is a single float value, the range will be (0, scale). Default: (0.05, 0.1).\n        keep_size: Whether to resize image back to their original size after applying the perspective\n            transform. If set to False, the resulting images may end up having different shapes\n            and will always be a list, never an array. Default: True\n        pad_mode (OpenCV flag): OpenCV border mode.\n        pad_val (int, float, list of int, list of float): padding value if border_mode is cv2.BORDER_CONSTANT.\n            Default: 0\n        mask_pad_val (int, float, list of int, list of float): padding value for mask\n            if border_mode is cv2.BORDER_CONSTANT. Default: 0\n        fit_output (bool): If True, the image plane size and position will be adjusted to still capture\n            the whole image after perspective transformation. (Followed by image resizing if keep_size is set to True.)\n            Otherwise, parts of the transformed image may be outside of the image plane.\n            This setting should not be set to True when using large scale values as it could lead to very large images.\n            Default: False\n        p (float): probability of applying the transform. Default: 0.5.\n\n    Targets:\n        image, mask, keypoints, bboxes\n\n    Image types:\n        uint8, float32\n\n    \"\"\"\n\n    _targets = (Targets.IMAGE, Targets.MASK, Targets.KEYPOINTS, Targets.BBOXES)\n\n    class InitSchema(BaseTransformInitSchema):\n        scale: NonNegativeFloatRangeType = (0.05, 0.1)\n        keep_size: Annotated[bool, Field(default=True, description=\"Keep size after transform.\")]\n        pad_mode: BorderModeType = cv2.BORDER_CONSTANT\n        pad_val: ColorType | None = Field(\n            default=0,\n            description=\"Padding value if border_mode is cv2.BORDER_CONSTANT.\",\n        )\n        mask_pad_val: ColorType | None = Field(\n            default=0,\n            description=\"Mask padding value if border_mode is cv2.BORDER_CONSTANT.\",\n        )\n        fit_output: Annotated[bool, Field(default=False, description=\"Adjust image plane to capture whole image.\")]\n        interpolation: InterpolationType = cv2.INTER_LINEAR\n\n    def __init__(\n        self,\n        scale: ScaleFloatType = (0.05, 0.1),\n        keep_size: bool = True,\n        pad_mode: int = cv2.BORDER_CONSTANT,\n        pad_val: ColorType = 0,\n        mask_pad_val: ColorType = 0,\n        fit_output: bool = False,\n        interpolation: int = cv2.INTER_LINEAR,\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p, always_apply)\n        self.scale = cast(Tuple[float, float], scale)\n        self.keep_size = keep_size\n        self.pad_mode = pad_mode\n        self.pad_val = pad_val\n        self.mask_pad_val = mask_pad_val\n        self.fit_output = fit_output\n        self.interpolation = interpolation\n\n    def apply(\n        self,\n        img: np.ndarray,\n        matrix: np.ndarray,\n        max_height: int,\n        max_width: int,\n        **params: Any,\n    ) -&gt; np.ndarray:\n        return fgeometric.perspective(\n            img,\n            matrix,\n            max_width,\n            max_height,\n            self.pad_val,\n            self.pad_mode,\n            self.keep_size,\n            params[\"interpolation\"],\n        )\n\n    def apply_to_bbox(\n        self,\n        bbox: BoxInternalType,\n        matrix: np.ndarray,\n        max_height: int,\n        max_width: int,\n        **params: Any,\n    ) -&gt; BoxInternalType:\n        return fgeometric.perspective_bbox(\n            bbox,\n            params[\"shape\"],\n            matrix,\n            max_width,\n            max_height,\n            self.keep_size,\n        )\n\n    def apply_to_keypoint(\n        self,\n        keypoint: KeypointInternalType,\n        matrix: np.ndarray,\n        max_height: int,\n        max_width: int,\n        **params: Any,\n    ) -&gt; np.ndarray:\n        return fgeometric.perspective_keypoint(\n            keypoint,\n            params[\"shape\"],\n            matrix,\n            max_width,\n            max_height,\n            self.keep_size,\n        )\n\n    def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n        height, width = params[\"shape\"][:2]\n\n        scale = random.uniform(*self.scale)\n        points = random_utils.normal(0, scale, [4, 2])\n        points = np.mod(np.abs(points), 0.32)\n\n        # top left -- no changes needed, just use jitter\n        # top right\n        points[1, 0] = 1.0 - points[1, 0]  # w = 1.0 - jitter\n        # bottom right\n        points[2] = 1.0 - points[2]  # w = 1.0 - jitt\n        # bottom left\n        points[3, 1] = 1.0 - points[3, 1]  # h = 1.0 - jitter\n\n        points[:, 0] *= width\n        points[:, 1] *= height\n\n        # Obtain a consistent order of the points and unpack them individually.\n        # Warning: don't just do (tl, tr, br, bl) = _order_points(...)\n        # here, because the reordered points is used further below.\n        points = self._order_points(points)\n        tl, tr, br, bl = points\n\n        # compute the width of the new image, which will be the\n        # maximum distance between bottom-right and bottom-left\n        # x-coordiates or the top-right and top-left x-coordinates\n        min_width = None\n        max_width = None\n        while min_width is None or min_width &lt; TWO:\n            width_top = np.sqrt(((tr[0] - tl[0]) ** 2) + ((tr[1] - tl[1]) ** 2))\n            width_bottom = np.sqrt(((br[0] - bl[0]) ** 2) + ((br[1] - bl[1]) ** 2))\n            max_width = int(max(width_top, width_bottom))\n            min_width = int(min(width_top, width_bottom))\n            if min_width &lt; TWO:\n                step_size = (2 - min_width) / 2\n                tl[0] -= step_size\n                tr[0] += step_size\n                bl[0] -= step_size\n                br[0] += step_size\n\n        # compute the height of the new image, which will be the maximum distance between the top-right\n        # and bottom-right y-coordinates or the top-left and bottom-left y-coordinates\n        min_height = None\n        max_height = None\n        while min_height is None or min_height &lt; TWO:\n            height_right = np.sqrt(((tr[0] - br[0]) ** 2) + ((tr[1] - br[1]) ** 2))\n            height_left = np.sqrt(((tl[0] - bl[0]) ** 2) + ((tl[1] - bl[1]) ** 2))\n            max_height = int(max(height_right, height_left))\n            min_height = int(min(height_right, height_left))\n            if min_height &lt; TWO:\n                step_size = (2 - min_height) / 2\n                tl[1] -= step_size\n                tr[1] -= step_size\n                bl[1] += step_size\n                br[1] += step_size\n\n        # now that we have the dimensions of the new image, construct\n        # the set of destination points to obtain a \"birds eye view\",\n        # (i.e. top-down view) of the image, again specifying points\n        # in the top-left, top-right, bottom-right, and bottom-left order\n        # do not use width-1 or height-1 here, as for e.g. width=3, height=2\n        # the bottom right coordinate is at (3.0, 2.0) and not (2.0, 1.0)\n        dst = np.array([[0, 0], [max_width, 0], [max_width, max_height], [0, max_height]], dtype=np.float32)\n\n        # compute the perspective transform matrix and then apply it\n        m = cv2.getPerspectiveTransform(points, dst)\n\n        if self.fit_output:\n            m, max_width, max_height = self._expand_transform(m, (height, width))\n\n        return {\"matrix\": m, \"max_height\": max_height, \"max_width\": max_width, \"interpolation\": self.interpolation}\n\n    @classmethod\n    def _expand_transform(cls, matrix: np.ndarray, shape: SizeType) -&gt; tuple[np.ndarray, int, int]:\n        height, width = shape[:2]\n        # do not use width-1 or height-1 here, as for e.g. width=3, height=2, max_height\n        # the bottom right coordinate is at (3.0, 2.0) and not (2.0, 1.0)\n        rect = np.array([[0, 0], [width, 0], [width, height], [0, height]], dtype=np.float32)\n        dst = cv2.perspectiveTransform(np.array([rect]), matrix)[0]\n\n        # get min x, y over transformed 4 points\n        # then modify target points by subtracting these minima  =&gt; shift to (0, 0)\n        dst -= dst.min(axis=0, keepdims=True)\n        dst = np.around(dst, decimals=0)\n\n        matrix_expanded = cv2.getPerspectiveTransform(rect, dst)\n        max_width, max_height = dst.max(axis=0)\n        return matrix_expanded, int(max_width), int(max_height)\n\n    @staticmethod\n    def _order_points(pts: np.ndarray) -&gt; np.ndarray:\n        pts = np.array(sorted(pts, key=lambda x: x[0]))\n        left = pts[:2]  # points with smallest x coordinate - left points\n        right = pts[2:]  # points with greatest x coordinate - right points\n\n        if left[0][1] &lt; left[1][1]:\n            tl, bl = left\n        else:\n            bl, tl = left\n\n        if right[0][1] &lt; right[1][1]:\n            tr, br = right\n        else:\n            br, tr = right\n\n        return np.array([tl, tr, br, bl], dtype=np.float32)\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return \"scale\", \"keep_size\", \"pad_mode\", \"pad_val\", \"mask_pad_val\", \"fit_output\", \"interpolation\"\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.transforms.Perspective.apply","title":"<code>apply (self, img, matrix, max_height, max_width, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def apply(\n    self,\n    img: np.ndarray,\n    matrix: np.ndarray,\n    max_height: int,\n    max_width: int,\n    **params: Any,\n) -&gt; np.ndarray:\n    return fgeometric.perspective(\n        img,\n        matrix,\n        max_width,\n        max_height,\n        self.pad_val,\n        self.pad_mode,\n        self.keep_size,\n        params[\"interpolation\"],\n    )\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.transforms.Perspective.get_params_dependent_on_data","title":"<code>get_params_dependent_on_data (self, params, data)</code>","text":"<p>Returns parameters dependent on input.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n    height, width = params[\"shape\"][:2]\n\n    scale = random.uniform(*self.scale)\n    points = random_utils.normal(0, scale, [4, 2])\n    points = np.mod(np.abs(points), 0.32)\n\n    # top left -- no changes needed, just use jitter\n    # top right\n    points[1, 0] = 1.0 - points[1, 0]  # w = 1.0 - jitter\n    # bottom right\n    points[2] = 1.0 - points[2]  # w = 1.0 - jitt\n    # bottom left\n    points[3, 1] = 1.0 - points[3, 1]  # h = 1.0 - jitter\n\n    points[:, 0] *= width\n    points[:, 1] *= height\n\n    # Obtain a consistent order of the points and unpack them individually.\n    # Warning: don't just do (tl, tr, br, bl) = _order_points(...)\n    # here, because the reordered points is used further below.\n    points = self._order_points(points)\n    tl, tr, br, bl = points\n\n    # compute the width of the new image, which will be the\n    # maximum distance between bottom-right and bottom-left\n    # x-coordiates or the top-right and top-left x-coordinates\n    min_width = None\n    max_width = None\n    while min_width is None or min_width &lt; TWO:\n        width_top = np.sqrt(((tr[0] - tl[0]) ** 2) + ((tr[1] - tl[1]) ** 2))\n        width_bottom = np.sqrt(((br[0] - bl[0]) ** 2) + ((br[1] - bl[1]) ** 2))\n        max_width = int(max(width_top, width_bottom))\n        min_width = int(min(width_top, width_bottom))\n        if min_width &lt; TWO:\n            step_size = (2 - min_width) / 2\n            tl[0] -= step_size\n            tr[0] += step_size\n            bl[0] -= step_size\n            br[0] += step_size\n\n    # compute the height of the new image, which will be the maximum distance between the top-right\n    # and bottom-right y-coordinates or the top-left and bottom-left y-coordinates\n    min_height = None\n    max_height = None\n    while min_height is None or min_height &lt; TWO:\n        height_right = np.sqrt(((tr[0] - br[0]) ** 2) + ((tr[1] - br[1]) ** 2))\n        height_left = np.sqrt(((tl[0] - bl[0]) ** 2) + ((tl[1] - bl[1]) ** 2))\n        max_height = int(max(height_right, height_left))\n        min_height = int(min(height_right, height_left))\n        if min_height &lt; TWO:\n            step_size = (2 - min_height) / 2\n            tl[1] -= step_size\n            tr[1] -= step_size\n            bl[1] += step_size\n            br[1] += step_size\n\n    # now that we have the dimensions of the new image, construct\n    # the set of destination points to obtain a \"birds eye view\",\n    # (i.e. top-down view) of the image, again specifying points\n    # in the top-left, top-right, bottom-right, and bottom-left order\n    # do not use width-1 or height-1 here, as for e.g. width=3, height=2\n    # the bottom right coordinate is at (3.0, 2.0) and not (2.0, 1.0)\n    dst = np.array([[0, 0], [max_width, 0], [max_width, max_height], [0, max_height]], dtype=np.float32)\n\n    # compute the perspective transform matrix and then apply it\n    m = cv2.getPerspectiveTransform(points, dst)\n\n    if self.fit_output:\n        m, max_width, max_height = self._expand_transform(m, (height, width))\n\n    return {\"matrix\": m, \"max_height\": max_height, \"max_width\": max_width, \"interpolation\": self.interpolation}\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.transforms.Perspective.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return \"scale\", \"keep_size\", \"pad_mode\", \"pad_val\", \"mask_pad_val\", \"fit_output\", \"interpolation\"\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.transforms.PiecewiseAffine","title":"<code>class  PiecewiseAffine</code> <code>     (scale=(0.03, 0.05), nb_rows=4, nb_cols=4, interpolation=1, mask_interpolation=0, cval=0, cval_mask=0, mode='constant', absolute_scale=False, always_apply=None, keypoints_threshold=0.01, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Apply affine transformations that differ between local neighborhoods. This augmentation places a regular grid of points on an image and randomly moves the neighborhood of these point around via affine transformations. This leads to local distortions.</p> <p>This is mostly a wrapper around scikit-image's <code>PiecewiseAffine</code>. See also <code>Affine</code> for a similar technique.</p> <p>Note</p> <p>This augmenter is very slow. Try to use <code>ElasticTransformation</code> instead, which is at least 10x faster.</p> <p>Note</p> <p>For coordinate-based inputs (keypoints, bounding boxes, polygons, ...), this augmenter still has to perform an image-based augmentation, which will make it significantly slower and not fully correct for such inputs than other transforms.</p> <p>Parameters:</p> Name Type Description <code>scale</code> <code>float, tuple of float</code> <p>Each point on the regular grid is moved around via a normal distribution. This scale factor is equivalent to the normal distribution's sigma. Note that the jitter (how far each point is moved in which direction) is multiplied by the height/width of the image if <code>absolute_scale=False</code> (default), so this scale can be the same for different sized images. Recommended values are in the range <code>0.01</code> to <code>0.05</code> (weak to strong augmentations).     * If a single <code>float</code>, then that value will always be used as the scale.     * If a tuple <code>(a, b)</code> of <code>float</code> s, then a random value will       be uniformly sampled per image from the interval <code>[a, b]</code>.</p> <code>nb_rows</code> <code>int, tuple of int</code> <p>Number of rows of points that the regular grid should have. Must be at least <code>2</code>. For large images, you might want to pick a higher value than <code>4</code>. You might have to then adjust scale to lower values.     * If a single <code>int</code>, then that value will always be used as the number of rows.     * If a tuple <code>(a, b)</code>, then a value from the discrete interval       <code>[a..b]</code> will be uniformly sampled per image.</p> <code>nb_cols</code> <code>int, tuple of int</code> <p>Number of columns. Analogous to <code>nb_rows</code>.</p> <code>interpolation</code> <code>int</code> <p>The order of interpolation. The order has to be in the range 0-5:  - 0: Nearest-neighbor  - 1: Bi-linear (default)  - 2: Bi-quadratic  - 3: Bi-cubic  - 4: Bi-quartic  - 5: Bi-quintic</p> <code>mask_interpolation</code> <code>int</code> <p>same as interpolation but for mask.</p> <code>cval</code> <code>number</code> <p>The constant value to use when filling in newly created pixels.</p> <code>cval_mask</code> <code>number</code> <p>Same as cval but only for masks.</p> <code>mode</code> <code>str</code> <p>{'constant', 'edge', 'symmetric', 'reflect', 'wrap'}, optional Points outside the boundaries of the input are filled according to the given mode.  Modes match the behaviour of <code>numpy.pad</code>.</p> <code>absolute_scale</code> <code>bool</code> <p>Take <code>scale</code> as an absolute value rather than a relative value.</p> <code>keypoints_threshold</code> <code>float</code> <p>Used as threshold in conversion from distance maps to keypoints. The search for keypoints works by searching for the argmin (non-inverted) or argmax (inverted) in each channel. This parameters contains the maximum (non-inverted) or minimum (inverted) value to accept in order to view a hit as a keypoint. Use <code>None</code> to use no min/max. Default: 0.01</p> <p>Targets</p> <p>image, mask, keypoints, bboxes</p> <p>Image types:     uint8, float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>class PiecewiseAffine(DualTransform):\n    \"\"\"Apply affine transformations that differ between local neighborhoods.\n    This augmentation places a regular grid of points on an image and randomly moves the neighborhood of these point\n    around via affine transformations. This leads to local distortions.\n\n    This is mostly a wrapper around scikit-image's ``PiecewiseAffine``.\n    See also ``Affine`` for a similar technique.\n\n    Note:\n        This augmenter is very slow. Try to use ``ElasticTransformation`` instead, which is at least 10x faster.\n\n    Note:\n        For coordinate-based inputs (keypoints, bounding boxes, polygons, ...),\n        this augmenter still has to perform an image-based augmentation,\n        which will make it significantly slower and not fully correct for such inputs than other transforms.\n\n    Args:\n        scale (float, tuple of float): Each point on the regular grid is moved around via a normal distribution.\n            This scale factor is equivalent to the normal distribution's sigma.\n            Note that the jitter (how far each point is moved in which direction) is multiplied by the height/width of\n            the image if ``absolute_scale=False`` (default), so this scale can be the same for different sized images.\n            Recommended values are in the range ``0.01`` to ``0.05`` (weak to strong augmentations).\n                * If a single ``float``, then that value will always be used as the scale.\n                * If a tuple ``(a, b)`` of ``float`` s, then a random value will\n                  be uniformly sampled per image from the interval ``[a, b]``.\n        nb_rows (int, tuple of int): Number of rows of points that the regular grid should have.\n            Must be at least ``2``. For large images, you might want to pick a higher value than ``4``.\n            You might have to then adjust scale to lower values.\n                * If a single ``int``, then that value will always be used as the number of rows.\n                * If a tuple ``(a, b)``, then a value from the discrete interval\n                  ``[a..b]`` will be uniformly sampled per image.\n        nb_cols (int, tuple of int): Number of columns. Analogous to `nb_rows`.\n        interpolation (int): The order of interpolation. The order has to be in the range 0-5:\n             - 0: Nearest-neighbor\n             - 1: Bi-linear (default)\n             - 2: Bi-quadratic\n             - 3: Bi-cubic\n             - 4: Bi-quartic\n             - 5: Bi-quintic\n        mask_interpolation (int): same as interpolation but for mask.\n        cval (number): The constant value to use when filling in newly created pixels.\n        cval_mask (number): Same as cval but only for masks.\n        mode (str): {'constant', 'edge', 'symmetric', 'reflect', 'wrap'}, optional\n            Points outside the boundaries of the input are filled according\n            to the given mode.  Modes match the behaviour of `numpy.pad`.\n        absolute_scale (bool): Take `scale` as an absolute value rather than a relative value.\n        keypoints_threshold (float): Used as threshold in conversion from distance maps to keypoints.\n            The search for keypoints works by searching for the\n            argmin (non-inverted) or argmax (inverted) in each channel. This\n            parameters contains the maximum (non-inverted) or minimum (inverted) value to accept in order to view a hit\n            as a keypoint. Use ``None`` to use no min/max. Default: 0.01\n\n    Targets:\n        image, mask, keypoints, bboxes\n\n    Image types:\n        uint8, float32\n\n    \"\"\"\n\n    _targets = (Targets.IMAGE, Targets.MASK, Targets.BBOXES, Targets.KEYPOINTS)\n\n    class InitSchema(BaseTransformInitSchema):\n        scale: NonNegativeFloatRangeType = (0.03, 0.05)\n        nb_rows: ScaleIntType = Field(default=4, description=\"Number of rows in the regular grid.\")\n        nb_cols: ScaleIntType = Field(default=4, description=\"Number of columns in the regular grid.\")\n        interpolation: InterpolationType = cv2.INTER_LINEAR\n        mask_interpolation: InterpolationType = cv2.INTER_NEAREST\n        cval: int = Field(default=0, description=\"Constant value used for newly created pixels.\")\n        cval_mask: int = Field(default=0, description=\"Constant value used for newly created mask pixels.\")\n        mode: Literal[\"constant\", \"edge\", \"symmetric\", \"reflect\", \"wrap\"] = \"constant\"\n        absolute_scale: bool = Field(\n            default=False,\n            description=\"Whether scale is an absolute value rather than relative.\",\n        )\n        keypoints_threshold: float = Field(\n            default=0.01,\n            description=\"Threshold for conversion from distance maps to keypoints.\",\n        )\n\n        @field_validator(\"nb_rows\", \"nb_cols\")\n        @classmethod\n        def process_range(cls, value: ScaleFloatType, info: ValidationInfo) -&gt; tuple[float, float]:\n            bounds = 2, BIG_INTEGER\n            result = to_tuple(value, value)\n            check_range(result, *bounds, info.field_name)\n            return result\n\n    def __init__(\n        self,\n        scale: ScaleFloatType = (0.03, 0.05),\n        nb_rows: ScaleIntType = 4,\n        nb_cols: ScaleIntType = 4,\n        interpolation: int = cv2.INTER_LINEAR,\n        mask_interpolation: int = cv2.INTER_NEAREST,\n        cval: int = 0,\n        cval_mask: int = 0,\n        mode: Literal[\"constant\", \"edge\", \"symmetric\", \"reflect\", \"wrap\"] = \"constant\",\n        absolute_scale: bool = False,\n        always_apply: bool | None = None,\n        keypoints_threshold: float = 0.01,\n        p: float = 0.5,\n    ):\n        super().__init__(p, always_apply)\n\n        warn(\n            \"This augmenter is very slow. Try to use ``ElasticTransformation`` instead, which is at least 10x faster.\",\n            stacklevel=2,\n        )\n\n        self.scale = cast(Tuple[float, float], scale)\n        self.nb_rows = cast(Tuple[int, int], nb_rows)\n        self.nb_cols = cast(Tuple[int, int], nb_cols)\n        self.interpolation = interpolation\n        self.mask_interpolation = mask_interpolation\n        self.cval = cval\n        self.cval_mask = cval_mask\n        self.mode = mode\n        self.absolute_scale = absolute_scale\n        self.keypoints_threshold = keypoints_threshold\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return (\n            \"scale\",\n            \"nb_rows\",\n            \"nb_cols\",\n            \"interpolation\",\n            \"mask_interpolation\",\n            \"cval\",\n            \"cval_mask\",\n            \"mode\",\n            \"absolute_scale\",\n            \"keypoints_threshold\",\n        )\n\n    def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n        height, width = params[\"shape\"][:2]\n\n        nb_rows = np.clip(random.randint(*self.nb_rows), 2, None)\n        nb_cols = np.clip(random.randint(*self.nb_cols), 2, None)\n        nb_cells = nb_cols * nb_rows\n        scale = random.uniform(*self.scale)\n\n        jitter: np.ndarray = random_utils.normal(0, scale, (nb_cells, 2))\n        if not np.any(jitter &gt; 0):\n            for _ in range(10):  # See: https://github.com/albumentations-team/albumentations/issues/1442\n                jitter = random_utils.normal(0, scale, (nb_cells, 2))\n                if np.any(jitter &gt; 0):\n                    break\n            if not np.any(jitter &gt; 0):\n                return {\"matrix\": None}\n\n        y = np.linspace(0, height, nb_rows)\n        x = np.linspace(0, width, nb_cols)\n\n        # (H, W) and (H, W) for H=rows, W=cols\n        xx_src, yy_src = np.meshgrid(x, y)\n\n        # (1, HW, 2) =&gt; (HW, 2) for H=rows, W=cols\n        points_src = np.dstack([yy_src.flat, xx_src.flat])[0]\n\n        if self.absolute_scale:\n            jitter[:, 0] = jitter[:, 0] / height if height &gt; 0 else 0.0\n            jitter[:, 1] = jitter[:, 1] / width if width &gt; 0 else 0.0\n\n        jitter[:, 0] = jitter[:, 0] * height\n        jitter[:, 1] = jitter[:, 1] * width\n\n        points_dest = np.copy(points_src)\n        points_dest[:, 0] = points_dest[:, 0] + jitter[:, 0]\n        points_dest[:, 1] = points_dest[:, 1] + jitter[:, 1]\n\n        # Restrict all destination points to be inside the image plane.\n        # This is necessary, as otherwise keypoints could be augmented\n        # outside of the image plane and these would be replaced by\n        # (-1, -1), which would not conform with the behaviour of the other augmenters.\n        points_dest[:, 0] = np.clip(points_dest[:, 0], 0, height - 1)\n        points_dest[:, 1] = np.clip(points_dest[:, 1], 0, width - 1)\n\n        matrix = skimage.transform.PiecewiseAffineTransform()\n        matrix.estimate(points_src[:, ::-1], points_dest[:, ::-1])\n\n        return {\n            \"matrix\": matrix,\n        }\n\n    def apply(\n        self,\n        img: np.ndarray,\n        matrix: skimage.transform.PiecewiseAffineTransform,\n        **params: Any,\n    ) -&gt; np.ndarray:\n        return fgeometric.piecewise_affine(img, matrix, self.interpolation, self.mode, self.cval)\n\n    def apply_to_mask(\n        self,\n        mask: np.ndarray,\n        matrix: skimage.transform.PiecewiseAffineTransform,\n        **params: Any,\n    ) -&gt; np.ndarray:\n        return fgeometric.piecewise_affine(mask, matrix, self.mask_interpolation, self.mode, self.cval_mask)\n\n    def apply_to_bbox(\n        self,\n        bbox: BoxInternalType,\n        rows: int,\n        cols: int,\n        matrix: skimage.transform.PiecewiseAffineTransform,\n        **params: Any,\n    ) -&gt; BoxInternalType:\n        return fgeometric.bbox_piecewise_affine(bbox, matrix, params[\"shape\"], self.keypoints_threshold)\n\n    def apply_to_keypoint(\n        self,\n        keypoint: KeypointInternalType,\n        rows: int,\n        cols: int,\n        matrix: skimage.transform.PiecewiseAffineTransform,\n        **params: Any,\n    ) -&gt; KeypointInternalType:\n        return fgeometric.keypoint_piecewise_affine(keypoint, matrix, params[\"shape\"], self.keypoints_threshold)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.transforms.PiecewiseAffine.apply","title":"<code>apply (self, img, matrix, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def apply(\n    self,\n    img: np.ndarray,\n    matrix: skimage.transform.PiecewiseAffineTransform,\n    **params: Any,\n) -&gt; np.ndarray:\n    return fgeometric.piecewise_affine(img, matrix, self.interpolation, self.mode, self.cval)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.transforms.PiecewiseAffine.get_params_dependent_on_data","title":"<code>get_params_dependent_on_data (self, params, data)</code>","text":"<p>Returns parameters dependent on input.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n    height, width = params[\"shape\"][:2]\n\n    nb_rows = np.clip(random.randint(*self.nb_rows), 2, None)\n    nb_cols = np.clip(random.randint(*self.nb_cols), 2, None)\n    nb_cells = nb_cols * nb_rows\n    scale = random.uniform(*self.scale)\n\n    jitter: np.ndarray = random_utils.normal(0, scale, (nb_cells, 2))\n    if not np.any(jitter &gt; 0):\n        for _ in range(10):  # See: https://github.com/albumentations-team/albumentations/issues/1442\n            jitter = random_utils.normal(0, scale, (nb_cells, 2))\n            if np.any(jitter &gt; 0):\n                break\n        if not np.any(jitter &gt; 0):\n            return {\"matrix\": None}\n\n    y = np.linspace(0, height, nb_rows)\n    x = np.linspace(0, width, nb_cols)\n\n    # (H, W) and (H, W) for H=rows, W=cols\n    xx_src, yy_src = np.meshgrid(x, y)\n\n    # (1, HW, 2) =&gt; (HW, 2) for H=rows, W=cols\n    points_src = np.dstack([yy_src.flat, xx_src.flat])[0]\n\n    if self.absolute_scale:\n        jitter[:, 0] = jitter[:, 0] / height if height &gt; 0 else 0.0\n        jitter[:, 1] = jitter[:, 1] / width if width &gt; 0 else 0.0\n\n    jitter[:, 0] = jitter[:, 0] * height\n    jitter[:, 1] = jitter[:, 1] * width\n\n    points_dest = np.copy(points_src)\n    points_dest[:, 0] = points_dest[:, 0] + jitter[:, 0]\n    points_dest[:, 1] = points_dest[:, 1] + jitter[:, 1]\n\n    # Restrict all destination points to be inside the image plane.\n    # This is necessary, as otherwise keypoints could be augmented\n    # outside of the image plane and these would be replaced by\n    # (-1, -1), which would not conform with the behaviour of the other augmenters.\n    points_dest[:, 0] = np.clip(points_dest[:, 0], 0, height - 1)\n    points_dest[:, 1] = np.clip(points_dest[:, 1], 0, width - 1)\n\n    matrix = skimage.transform.PiecewiseAffineTransform()\n    matrix.estimate(points_src[:, ::-1], points_dest[:, ::-1])\n\n    return {\n        \"matrix\": matrix,\n    }\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.transforms.PiecewiseAffine.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return (\n        \"scale\",\n        \"nb_rows\",\n        \"nb_cols\",\n        \"interpolation\",\n        \"mask_interpolation\",\n        \"cval\",\n        \"cval_mask\",\n        \"mode\",\n        \"absolute_scale\",\n        \"keypoints_threshold\",\n    )\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.transforms.ShiftScaleRotate","title":"<code>class  ShiftScaleRotate</code> <code>     (shift_limit=(-0.0625, 0.0625), scale_limit=(-0.1, 0.1), rotate_limit=(-45, 45), interpolation=1, border_mode=4, value=0, mask_value=0, shift_limit_x=None, shift_limit_y=None, rotate_method='largest_box', always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Randomly apply affine transforms: translate, scale and rotate the input.</p> <p>Parameters:</p> Name Type Description <code>shift_limit</code> <code>float, float) or float</code> <p>shift factor range for both height and width. If shift_limit is a single float value, the range will be (-shift_limit, shift_limit). Absolute values for lower and upper bounds should lie in range [-1, 1]. Default: (-0.0625, 0.0625).</p> <code>scale_limit</code> <code>float, float) or float</code> <p>scaling factor range. If scale_limit is a single float value, the range will be (-scale_limit, scale_limit). Note that the scale_limit will be biased by 1. If scale_limit is a tuple, like (low, high), sampling will be done from the range (1 + low, 1 + high). Default: (-0.1, 0.1).</p> <code>rotate_limit</code> <code>int, int) or int</code> <p>rotation range. If rotate_limit is a single int value, the range will be (-rotate_limit, rotate_limit). Default: (-45, 45).</p> <code>interpolation</code> <code>OpenCV flag</code> <p>flag that is used to specify the interpolation algorithm. Should be one of: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4. Default: cv2.INTER_LINEAR.</p> <code>border_mode</code> <code>OpenCV flag</code> <p>flag that is used to specify the pixel extrapolation method. Should be one of: cv2.BORDER_CONSTANT, cv2.BORDER_REPLICATE, cv2.BORDER_REFLECT, cv2.BORDER_WRAP, cv2.BORDER_REFLECT_101. Default: cv2.BORDER_REFLECT_101</p> <code>value</code> <code>int, float, list of int, list of float</code> <p>padding value if border_mode is cv2.BORDER_CONSTANT.</p> <code>mask_value</code> <code>int, float,         list of int,         list of float</code> <p>padding value if border_mode is cv2.BORDER_CONSTANT applied for masks.</p> <code>shift_limit_x</code> <code>float, float) or float</code> <p>shift factor range for width. If it is set then this value instead of shift_limit will be used for shifting width.  If shift_limit_x is a single float value, the range will be (-shift_limit_x, shift_limit_x). Absolute values for lower and upper bounds should lie in the range [-1, 1]. Default: None.</p> <code>shift_limit_y</code> <code>float, float) or float</code> <p>shift factor range for height. If it is set then this value instead of shift_limit will be used for shifting height.  If shift_limit_y is a single float value, the range will be (-shift_limit_y, shift_limit_y). Absolute values for lower and upper bounds should lie in the range [-, 1]. Default: None.</p> <code>rotate_method</code> <code>str</code> <p>rotation method used for the bounding boxes. Should be one of \"largest_box\" or \"ellipse\". Default: \"largest_box\"</p> <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 0.5.</p> <p>Targets</p> <p>image, mask, keypoints, bboxes</p> <p>Image types:     uint8, float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>class ShiftScaleRotate(Affine):\n    \"\"\"Randomly apply affine transforms: translate, scale and rotate the input.\n\n    Args:\n        shift_limit ((float, float) or float): shift factor range for both height and width. If shift_limit\n            is a single float value, the range will be (-shift_limit, shift_limit). Absolute values for lower and\n            upper bounds should lie in range [-1, 1]. Default: (-0.0625, 0.0625).\n        scale_limit ((float, float) or float): scaling factor range. If scale_limit is a single float value, the\n            range will be (-scale_limit, scale_limit). Note that the scale_limit will be biased by 1.\n            If scale_limit is a tuple, like (low, high), sampling will be done from the range (1 + low, 1 + high).\n            Default: (-0.1, 0.1).\n        rotate_limit ((int, int) or int): rotation range. If rotate_limit is a single int value, the\n            range will be (-rotate_limit, rotate_limit). Default: (-45, 45).\n        interpolation (OpenCV flag): flag that is used to specify the interpolation algorithm. Should be one of:\n            cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.\n            Default: cv2.INTER_LINEAR.\n        border_mode (OpenCV flag): flag that is used to specify the pixel extrapolation method. Should be one of:\n            cv2.BORDER_CONSTANT, cv2.BORDER_REPLICATE, cv2.BORDER_REFLECT, cv2.BORDER_WRAP, cv2.BORDER_REFLECT_101.\n            Default: cv2.BORDER_REFLECT_101\n        value (int, float, list of int, list of float): padding value if border_mode is cv2.BORDER_CONSTANT.\n        mask_value (int, float,\n                    list of int,\n                    list of float): padding value if border_mode is cv2.BORDER_CONSTANT applied for masks.\n        shift_limit_x ((float, float) or float): shift factor range for width. If it is set then this value\n            instead of shift_limit will be used for shifting width.  If shift_limit_x is a single float value,\n            the range will be (-shift_limit_x, shift_limit_x). Absolute values for lower and upper bounds should lie in\n            the range [-1, 1]. Default: None.\n        shift_limit_y ((float, float) or float): shift factor range for height. If it is set then this value\n            instead of shift_limit will be used for shifting height.  If shift_limit_y is a single float value,\n            the range will be (-shift_limit_y, shift_limit_y). Absolute values for lower and upper bounds should lie\n            in the range [-, 1]. Default: None.\n        rotate_method (str): rotation method used for the bounding boxes. Should be one of \"largest_box\" or \"ellipse\".\n            Default: \"largest_box\"\n        p (float): probability of applying the transform. Default: 0.5.\n\n    Targets:\n        image, mask, keypoints, bboxes\n\n    Image types:\n        uint8, float32\n\n    \"\"\"\n\n    _targets = (Targets.IMAGE, Targets.MASK, Targets.KEYPOINTS, Targets.BBOXES)\n\n    class InitSchema(BaseTransformInitSchema):\n        shift_limit: SymmetricRangeType = (-0.0625, 0.0625)\n        scale_limit: SymmetricRangeType = (-0.1, 0.1)\n        rotate_limit: SymmetricRangeType = (-45, 45)\n        interpolation: InterpolationType = cv2.INTER_LINEAR\n        border_mode: BorderModeType = cv2.BORDER_REFLECT_101\n        value: ColorType = 0\n        mask_value: ColorType = 0\n        shift_limit_x: ScaleFloatType | None = Field(default=None)\n        shift_limit_y: ScaleFloatType | None = Field(default=None)\n        rotate_method: Literal[\"largest_box\", \"ellipse\"] = \"largest_box\"\n\n        @model_validator(mode=\"after\")\n        def check_shift_limit(self) -&gt; Self:\n            bounds = -1, 1\n            self.shift_limit_x = to_tuple(self.shift_limit_x if self.shift_limit_x is not None else self.shift_limit)\n            check_range(self.shift_limit_x, *bounds, \"shift_limit_x\")\n            self.shift_limit_y = to_tuple(self.shift_limit_y if self.shift_limit_y is not None else self.shift_limit)\n            check_range(self.shift_limit_y, *bounds, \"shift_limit_y\")\n            return self\n\n        @field_validator(\"scale_limit\")\n        @classmethod\n        def check_scale_limit(cls, value: ScaleFloatType, info: ValidationInfo) -&gt; ScaleFloatType:\n            bounds = 0, float(\"inf\")\n            result = to_tuple(value, bias=1.0)\n            check_range(result, *bounds, str(info.field_name))\n            return result\n\n    def __init__(\n        self,\n        shift_limit: ScaleFloatType = (-0.0625, 0.0625),\n        scale_limit: ScaleFloatType = (-0.1, 0.1),\n        rotate_limit: ScaleFloatType = (-45, 45),\n        interpolation: int = cv2.INTER_LINEAR,\n        border_mode: int = cv2.BORDER_REFLECT_101,\n        value: ColorType = 0,\n        mask_value: ColorType = 0,\n        shift_limit_x: ScaleFloatType | None = None,\n        shift_limit_y: ScaleFloatType | None = None,\n        rotate_method: Literal[\"largest_box\", \"ellipse\"] = \"largest_box\",\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(\n            scale=scale_limit,\n            translate_percent={\"x\": shift_limit_x, \"y\": shift_limit_y},\n            rotate=rotate_limit,\n            shear=(0, 0),\n            interpolation=interpolation,\n            mask_interpolation=cv2.INTER_NEAREST,\n            cval=value,\n            cval_mask=mask_value,\n            mode=border_mode,\n            fit_output=False,\n            keep_ratio=False,\n            rotate_method=rotate_method,\n            always_apply=always_apply,\n            p=p,\n        )\n        warn(\n            \"ShiftScaleRotate is deprecated. Please use Affine transform instead .\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        self.shift_limit_x = cast(Tuple[float, float], shift_limit_x)\n        self.shift_limit_y = cast(Tuple[float, float], shift_limit_y)\n        self.scale_limit = cast(Tuple[float, float], scale_limit)\n        self.rotate_limit = cast(Tuple[int, int], rotate_limit)\n        self.border_mode = border_mode\n        self.value = value\n        self.mask_value = mask_value\n\n    def get_transform_init_args(self) -&gt; dict[str, Any]:\n        return {\n            \"shift_limit_x\": self.shift_limit_x,\n            \"shift_limit_y\": self.shift_limit_y,\n            \"scale_limit\": to_tuple(self.scale_limit, bias=-1.0),\n            \"rotate_limit\": self.rotate_limit,\n            \"interpolation\": self.interpolation,\n            \"border_mode\": self.border_mode,\n            \"value\": self.value,\n            \"mask_value\": self.mask_value,\n            \"rotate_method\": self.rotate_method,\n        }\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.transforms.Transpose","title":"<code>class  Transpose</code> <code> </code>  [view source on GitHub]","text":"<p>Transpose the input by swapping rows and columns.</p> <p>Parameters:</p> Name Type Description <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 0.5.</p> <p>Targets</p> <p>image, mask, bboxes, keypoints</p> <p>Image types:     uint8, float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>class Transpose(DualTransform):\n    \"\"\"Transpose the input by swapping rows and columns.\n\n    Args:\n        p (float): probability of applying the transform. Default: 0.5.\n\n    Targets:\n        image, mask, bboxes, keypoints\n\n    Image types:\n        uint8, float32\n\n    \"\"\"\n\n    _targets = (Targets.IMAGE, Targets.MASK, Targets.BBOXES, Targets.KEYPOINTS)\n\n    def apply(self, img: np.ndarray, **params: Any) -&gt; np.ndarray:\n        return fgeometric.transpose(img)\n\n    def apply_to_bbox(self, bbox: BoxInternalType, **params: Any) -&gt; BoxInternalType:\n        return fgeometric.bbox_transpose(bbox)\n\n    def apply_to_keypoint(self, keypoint: KeypointInternalType, **params: Any) -&gt; KeypointInternalType:\n        return fgeometric.keypoint_transpose(keypoint)\n\n    def get_transform_init_args_names(self) -&gt; tuple[()]:\n        return ()\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.transforms.Transpose.apply","title":"<code>apply (self, img, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def apply(self, img: np.ndarray, **params: Any) -&gt; np.ndarray:\n    return fgeometric.transpose(img)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.transforms.Transpose.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[()]:\n    return ()\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.transforms.VerticalFlip","title":"<code>class  VerticalFlip</code> <code> </code>  [view source on GitHub]","text":"<p>Flip the input vertically around the x-axis.</p> <p>Parameters:</p> Name Type Description <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 0.5.</p> <p>Targets</p> <p>image, mask, bboxes, keypoints</p> <p>Image types:     uint8, float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>class VerticalFlip(DualTransform):\n    \"\"\"Flip the input vertically around the x-axis.\n\n    Args:\n        p (float): probability of applying the transform. Default: 0.5.\n\n    Targets:\n        image, mask, bboxes, keypoints\n\n    Image types:\n        uint8, float32\n\n    \"\"\"\n\n    _targets = (Targets.IMAGE, Targets.MASK, Targets.BBOXES, Targets.KEYPOINTS)\n\n    def apply(self, img: np.ndarray, **params: Any) -&gt; np.ndarray:\n        return fgeometric.vflip(img)\n\n    def apply_to_bbox(self, bbox: BoxInternalType, **params: Any) -&gt; BoxInternalType:\n        return fgeometric.bbox_vflip(bbox)\n\n    def apply_to_keypoint(self, keypoint: KeypointInternalType, **params: Any) -&gt; KeypointInternalType:\n        return fgeometric.keypoint_vflip(keypoint, params[\"rows\"])\n\n    def get_transform_init_args_names(self) -&gt; tuple[()]:\n        return ()\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.transforms.VerticalFlip.apply","title":"<code>apply (self, img, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def apply(self, img: np.ndarray, **params: Any) -&gt; np.ndarray:\n    return fgeometric.vflip(img)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.geometric.transforms.VerticalFlip.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[()]:\n    return ()\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.mixing","title":"<code>mixing</code>  <code>special</code>","text":""},{"location":"api_reference/full_reference/#albumentations.augmentations.mixing.transforms","title":"<code>transforms</code>","text":""},{"location":"api_reference/full_reference/#albumentations.augmentations.mixing.transforms.MixUp","title":"<code>class  MixUp</code> <code>     (reference_data=None, read_fn=&lt;function MixUp.&lt;lambda&gt; at 0x7f4aecf6f7e0&gt;, alpha=0.4, mix_coef_return_name='mix_coef', always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Performs MixUp data augmentation, blending images, masks, and class labels with reference data.</p> <p>MixUp augmentation linearly combines an input (image, mask, and class label) with another set from a predefined reference dataset. The mixing degree is controlled by a parameter \u03bb (lambda), sampled from a Beta distribution. This method is known for improving model generalization by promoting linear behavior between classes and smoothing decision boundaries.</p> <p>Reference</p> <ul> <li>Zhang, H., Cisse, M., Dauphin, Y.N., and Lopez-Paz, D. (2018). mixup: Beyond Empirical Risk Minimization. In International Conference on Learning Representations. https://arxiv.org/abs/1710.09412</li> </ul> <p>Parameters:</p> Name Type Description <code>reference_data</code> <code>Optional[Union[Generator[ReferenceImage, None, None], Sequence[Any]]]</code> <p>A sequence or generator of dictionaries containing the reference data for mixing If None or an empty sequence is provided, no operation is performed and a warning is issued.</p> <code>read_fn</code> <code>Callable[[ReferenceImage], dict[str, Any]]</code> <p>A function to process items from reference_data. It should accept items from reference_data and return a dictionary containing processed data:     - The returned dictionary must include an 'image' key with a numpy array value.     - It may also include 'mask', 'global_label' each associated with numpy array values. Defaults to a function that assumes input dictionary contains numpy arrays and directly returns it.</p> <code>mix_coef_return_name</code> <code>str</code> <p>Name used for the applied alpha coefficient in the returned dictionary. Defaults to \"mix_coef\".</p> <code>alpha</code> <code>float</code> <p>The alpha parameter for the Beta distribution, influencing the mix's balance. Must be \u2265 0. Higher values lead to more uniform mixing. Defaults to 0.4.</p> <code>p</code> <code>float</code> <p>The probability of applying the transformation. Defaults to 0.5.</p> <p>Targets</p> <p>image, mask, global_label</p> <p>Image types:     - uint8, float32</p> <p>Exceptions:</p> Type Description <code>- ValueError</code> <p>If the alpha parameter is negative.</p> <code>- NotImplementedError</code> <p>If the transform is applied to bounding boxes or keypoints.</p> <p>Notes</p> <ul> <li>If no reference data is provided, a warning is issued, and the transform acts as a no-op.</li> <li>Notes if images are in float32 format, they should be within [0, 1] range.</li> </ul> <p>Example Usage:     import albumentations as A     import numpy as np     from albumentations.core.types import ReferenceImage</p> <pre><code># Prepare reference data\n# Note: This code generates random reference data for demonstration purposes only.\n# In real-world applications, it's crucial to use meaningful and representative data.\n# The quality and relevance of your input data significantly impact the effectiveness\n# of the augmentation process. Ensure your data closely aligns with your specific\n# use case and application requirements.\nreference_data = [ReferenceImage(image=np.random.randint(0, 256, [100, 100, 3], dtype=np.uint8),\n                                 mask=np.random.randint(0, 4, (100, 100, 1), dtype=np.uint8),\n                                 global_label=np.random.choice([0, 1], size=3)) for i in range(10)]\n\n# In this example, the lambda function simply returns its input, which works well for\n# data already in the expected format. For more complex scenarios, where the data might not be in\n# the required format or additional processing is needed, a more sophisticated function can be implemented.\n# Below is a hypothetical example where the input data is a file path, # and the function reads the image\n# file, converts it to a specific format, and possibly performs other preprocessing steps.\n\n# Example of a more complex read_fn that reads an image from a file path, converts it to RGB, and resizes it.\n# def custom_read_fn(file_path):\n#     from PIL import Image\n#     image = Image.open(file_path).convert('RGB')\n#     image = image.resize((100, 100))  # Example resize, adjust as needed.\n#     return np.array(image)\n\n# aug = A.Compose([A.RandomRotate90(), A.MixUp(p=1, reference_data=reference_data, read_fn=lambda x: x)])\n\n# For simplicity, the original lambda function is used in this example.\n# Replace `lambda x: x` with `custom_read_fn`if you need to process the data more extensively.\n\n# Apply augmentations\nimage = np.empty([100, 100, 3], dtype=np.uint8)\nmask = np.empty([100, 100], dtype=np.uint8)\nglobal_label = np.array([0, 1, 0])\ndata = aug(image=image, global_label=global_label, mask=mask)\ntransformed_image = data[\"image\"]\ntransformed_mask = data[\"mask\"]\ntransformed_global_label = data[\"global_label\"]\n\n# Print applied mix coefficient\nprint(data[\"mix_coef\"])  # Output: e.g., 0.9991580344142427\n</code></pre> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/mixing/transforms.py</code> Python<pre><code>class MixUp(ReferenceBasedTransform):\n    \"\"\"Performs MixUp data augmentation, blending images, masks, and class labels with reference data.\n\n    MixUp augmentation linearly combines an input (image, mask, and class label) with another set from a predefined\n    reference dataset. The mixing degree is controlled by a parameter \u03bb (lambda), sampled from a Beta distribution.\n    This method is known for improving model generalization by promoting linear behavior between classes and\n    smoothing decision boundaries.\n\n    Reference:\n        - Zhang, H., Cisse, M., Dauphin, Y.N., and Lopez-Paz, D. (2018). mixup: Beyond Empirical Risk Minimization.\n        In International Conference on Learning Representations. https://arxiv.org/abs/1710.09412\n\n    Args:\n        reference_data (Optional[Union[Generator[ReferenceImage, None, None], Sequence[Any]]]):\n            A sequence or generator of dictionaries containing the reference data for mixing\n            If None or an empty sequence is provided, no operation is performed and a warning is issued.\n        read_fn (Callable[[ReferenceImage], dict[str, Any]]):\n            A function to process items from reference_data. It should accept items from reference_data\n            and return a dictionary containing processed data:\n                - The returned dictionary must include an 'image' key with a numpy array value.\n                - It may also include 'mask', 'global_label' each associated with numpy array values.\n            Defaults to a function that assumes input dictionary contains numpy arrays and directly returns it.\n        mix_coef_return_name (str): Name used for the applied alpha coefficient in the returned dictionary.\n            Defaults to \"mix_coef\".\n        alpha (float):\n            The alpha parameter for the Beta distribution, influencing the mix's balance. Must be \u2265 0.\n            Higher values lead to more uniform mixing. Defaults to 0.4.\n        p (float):\n            The probability of applying the transformation. Defaults to 0.5.\n\n    Targets:\n        image, mask, global_label\n\n    Image types:\n        - uint8, float32\n\n    Raises:\n        - ValueError: If the alpha parameter is negative.\n        - NotImplementedError: If the transform is applied to bounding boxes or keypoints.\n\n    Notes:\n        - If no reference data is provided, a warning is issued, and the transform acts as a no-op.\n        - Notes if images are in float32 format, they should be within [0, 1] range.\n\n    Example Usage:\n        import albumentations as A\n        import numpy as np\n        from albumentations.core.types import ReferenceImage\n\n        # Prepare reference data\n        # Note: This code generates random reference data for demonstration purposes only.\n        # In real-world applications, it's crucial to use meaningful and representative data.\n        # The quality and relevance of your input data significantly impact the effectiveness\n        # of the augmentation process. Ensure your data closely aligns with your specific\n        # use case and application requirements.\n        reference_data = [ReferenceImage(image=np.random.randint(0, 256, [100, 100, 3], dtype=np.uint8),\n                                         mask=np.random.randint(0, 4, (100, 100, 1), dtype=np.uint8),\n                                         global_label=np.random.choice([0, 1], size=3)) for i in range(10)]\n\n        # In this example, the lambda function simply returns its input, which works well for\n        # data already in the expected format. For more complex scenarios, where the data might not be in\n        # the required format or additional processing is needed, a more sophisticated function can be implemented.\n        # Below is a hypothetical example where the input data is a file path, # and the function reads the image\n        # file, converts it to a specific format, and possibly performs other preprocessing steps.\n\n        # Example of a more complex read_fn that reads an image from a file path, converts it to RGB, and resizes it.\n        # def custom_read_fn(file_path):\n        #     from PIL import Image\n        #     image = Image.open(file_path).convert('RGB')\n        #     image = image.resize((100, 100))  # Example resize, adjust as needed.\n        #     return np.array(image)\n\n        # aug = A.Compose([A.RandomRotate90(), A.MixUp(p=1, reference_data=reference_data, read_fn=lambda x: x)])\n\n        # For simplicity, the original lambda function is used in this example.\n        # Replace `lambda x: x` with `custom_read_fn`if you need to process the data more extensively.\n\n        # Apply augmentations\n        image = np.empty([100, 100, 3], dtype=np.uint8)\n        mask = np.empty([100, 100], dtype=np.uint8)\n        global_label = np.array([0, 1, 0])\n        data = aug(image=image, global_label=global_label, mask=mask)\n        transformed_image = data[\"image\"]\n        transformed_mask = data[\"mask\"]\n        transformed_global_label = data[\"global_label\"]\n\n        # Print applied mix coefficient\n        print(data[\"mix_coef\"])  # Output: e.g., 0.9991580344142427\n    \"\"\"\n\n    _targets = (Targets.IMAGE, Targets.MASK, Targets.GLOBAL_LABEL)\n\n    class InitSchema(BaseTransformInitSchema):\n        reference_data: Generator[Any, None, None] | Sequence[Any] | None = None\n        read_fn: Callable[[ReferenceImage], Any]\n        alpha: Annotated[float, Field(default=0.4, ge=0, le=1)]\n        mix_coef_return_name: str = \"mix_coef\"\n\n    def __init__(\n        self,\n        reference_data: Generator[Any, None, None] | Sequence[Any] | None = None,\n        read_fn: Callable[[ReferenceImage], Any] = lambda x: {\"image\": x, \"mask\": None, \"class_label\": None},\n        alpha: float = 0.4,\n        mix_coef_return_name: str = \"mix_coef\",\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p=p, always_apply=always_apply)\n        self.mix_coef_return_name = mix_coef_return_name\n\n        self.read_fn = read_fn\n        self.alpha = alpha\n\n        if reference_data is None:\n            warn(\"No reference data provided for MixUp. This transform will act as a no-op.\", stacklevel=2)\n            # Create an empty generator\n            self.reference_data: list[Any] = []\n        elif (\n            isinstance(reference_data, types.GeneratorType)\n            or isinstance(reference_data, Iterable)\n            and not isinstance(reference_data, str)\n        ):\n            self.reference_data = reference_data  # type: ignore[assignment]\n        else:\n            msg = \"reference_data must be a list, tuple, generator, or None.\"\n            raise TypeError(msg)\n\n    def apply(self, img: np.ndarray, mix_data: ReferenceImage, mix_coef: float, **params: Any) -&gt; np.ndarray:\n        if not mix_data:\n            return img\n\n        mix_img = mix_data[\"image\"]\n\n        if img.shape != mix_img.shape and not is_grayscale_image(img):\n            msg = \"The shape of the reference image should be the same as the input image.\"\n            raise ValueError(msg)\n\n        return add_weighted(img, mix_coef, mix_img.reshape(img.shape), 1 - mix_coef) if mix_img is not None else img\n\n    def apply_to_mask(self, mask: np.ndarray, mix_data: ReferenceImage, mix_coef: float, **params: Any) -&gt; np.ndarray:\n        mix_mask = mix_data.get(\"mask\")\n        return (\n            add_weighted(mask, mix_coef, mix_mask.reshape(mask.shape), 1 - mix_coef) if mix_mask is not None else mask\n        )\n\n    def apply_to_global_label(\n        self,\n        label: np.ndarray,\n        mix_data: ReferenceImage,\n        mix_coef: float,\n        **params: Any,\n    ) -&gt; np.ndarray:\n        mix_label = mix_data.get(\"global_label\")\n        if mix_label is not None and label is not None:\n            return mix_coef * label + (1 - mix_coef) * mix_label\n        return label\n\n    def apply_to_bboxes(self, bboxes: Sequence[BoxType], mix_data: ReferenceImage, **params: Any) -&gt; Sequence[BoxType]:\n        msg = \"MixUp does not support bounding boxes yet, feel free to submit pull request to https://github.com/albumentations-team/albumentations/.\"\n        raise NotImplementedError(msg)\n\n    def apply_to_keypoints(\n        self,\n        keypoints: Sequence[KeypointType],\n        *args: Any,\n        **params: Any,\n    ) -&gt; Sequence[KeypointType]:\n        msg = \"MixUp does not support keypoints yet, feel free to submit pull request to https://github.com/albumentations-team/albumentations/.\"\n        raise NotImplementedError(msg)\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return \"reference_data\", \"alpha\"\n\n    def get_params(self) -&gt; dict[str, None | float | dict[str, Any]]:\n        mix_data = None\n        # Check if reference_data is not empty and is a sequence (list, tuple, np.array)\n        if isinstance(self.reference_data, Sequence) and not isinstance(self.reference_data, (str, bytes)):\n            if len(self.reference_data) &gt; 0:  # Additional check to ensure it's not empty\n                mix_idx = random.randint(0, len(self.reference_data) - 1)\n                mix_data = self.reference_data[mix_idx]\n        # Check if reference_data is an iterator or generator\n        elif isinstance(self.reference_data, Iterator):\n            try:\n                mix_data = next(self.reference_data)  # Attempt to get the next item\n            except StopIteration:\n                warn(\n                    \"Reference data iterator/generator has been exhausted. \"\n                    \"Further mixing augmentations will not be applied.\",\n                    RuntimeWarning,\n                    stacklevel=2,\n                )\n                return {\"mix_data\": {}, \"mix_coef\": 1}\n\n        # If mix_data is None or empty after the above checks, return default values\n        if mix_data is None:\n            return {\"mix_data\": {}, \"mix_coef\": 1}\n\n        # If mix_data is not None, calculate mix_coef and apply read_fn\n        mix_coef = beta(self.alpha, self.alpha)  # Assuming beta is defined elsewhere\n        return {\"mix_data\": self.read_fn(mix_data), \"mix_coef\": mix_coef}\n\n    def apply_with_params(self, params: dict[str, Any], *args: Any, **kwargs: Any) -&gt; dict[str, Any]:\n        res = super().apply_with_params(params, *args, **kwargs)\n        if self.mix_coef_return_name:\n            res[self.mix_coef_return_name] = params[\"mix_coef\"]\n            res[\"mix_data\"] = params[\"mix_data\"]\n        return res\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.mixing.transforms.MixUp.apply","title":"<code>apply (self, img, mix_data, mix_coef, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/mixing/transforms.py</code> Python<pre><code>def apply(self, img: np.ndarray, mix_data: ReferenceImage, mix_coef: float, **params: Any) -&gt; np.ndarray:\n    if not mix_data:\n        return img\n\n    mix_img = mix_data[\"image\"]\n\n    if img.shape != mix_img.shape and not is_grayscale_image(img):\n        msg = \"The shape of the reference image should be the same as the input image.\"\n        raise ValueError(msg)\n\n    return add_weighted(img, mix_coef, mix_img.reshape(img.shape), 1 - mix_coef) if mix_img is not None else img\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.mixing.transforms.MixUp.apply_with_params","title":"<code>apply_with_params (self, params, *args, **kwargs)</code>","text":"<p>Apply transforms with parameters.</p> Source code in <code>albumentations/augmentations/mixing/transforms.py</code> Python<pre><code>def apply_with_params(self, params: dict[str, Any], *args: Any, **kwargs: Any) -&gt; dict[str, Any]:\n    res = super().apply_with_params(params, *args, **kwargs)\n    if self.mix_coef_return_name:\n        res[self.mix_coef_return_name] = params[\"mix_coef\"]\n        res[\"mix_data\"] = params[\"mix_data\"]\n    return res\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.mixing.transforms.MixUp.get_params","title":"<code>get_params (self)</code>","text":"<p>Returns parameters independent of input.</p> Source code in <code>albumentations/augmentations/mixing/transforms.py</code> Python<pre><code>def get_params(self) -&gt; dict[str, None | float | dict[str, Any]]:\n    mix_data = None\n    # Check if reference_data is not empty and is a sequence (list, tuple, np.array)\n    if isinstance(self.reference_data, Sequence) and not isinstance(self.reference_data, (str, bytes)):\n        if len(self.reference_data) &gt; 0:  # Additional check to ensure it's not empty\n            mix_idx = random.randint(0, len(self.reference_data) - 1)\n            mix_data = self.reference_data[mix_idx]\n    # Check if reference_data is an iterator or generator\n    elif isinstance(self.reference_data, Iterator):\n        try:\n            mix_data = next(self.reference_data)  # Attempt to get the next item\n        except StopIteration:\n            warn(\n                \"Reference data iterator/generator has been exhausted. \"\n                \"Further mixing augmentations will not be applied.\",\n                RuntimeWarning,\n                stacklevel=2,\n            )\n            return {\"mix_data\": {}, \"mix_coef\": 1}\n\n    # If mix_data is None or empty after the above checks, return default values\n    if mix_data is None:\n        return {\"mix_data\": {}, \"mix_coef\": 1}\n\n    # If mix_data is not None, calculate mix_coef and apply read_fn\n    mix_coef = beta(self.alpha, self.alpha)  # Assuming beta is defined elsewhere\n    return {\"mix_data\": self.read_fn(mix_data), \"mix_coef\": mix_coef}\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.mixing.transforms.MixUp.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/mixing/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return \"reference_data\", \"alpha\"\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.mixing.transforms.OverlayElements","title":"<code>class  OverlayElements</code> <code>     (metadata_key='overlay_metadata', p=0.5, always_apply=None)                 </code>  [view source on GitHub]","text":"<p>Apply overlay elements such as images and masks onto an input image. This transformation can be used to add various objects (e.g., stickers, logos) to images with optional masks and bounding boxes for better placement control.</p> <p>Parameters:</p> Name Type Description <code>metadata_key</code> <code>str</code> <p>Additional target key for metadata. Default <code>overlay_metadata</code>.</p> <code>p</code> <code>float</code> <p>Probability of applying the transformation. Default: 0.5.</p> <p>Possible Metadata Fields:     - image (np.ndarray): The overlay image to be applied. This is a required field.     - bbox (list[int]): The bounding box specifying the region where the overlay should be applied. It should                         contain four floats: [y_min, x_min, y_max, x_max]. If <code>label_id</code> is provided, it should                         be appended as the fifth element in the bbox. BBox should be in Albumentations format,                         that is the same as normalized Pascal VOC format                         [x_min / width, y_min / height, x_max / width, y_max / height]     - mask (np.ndarray): An optional mask that defines the non-rectangular region of the overlay image. If not                          provided, the entire overlay image is used.     - mask_id (int): An optional identifier for the mask. If provided, the regions specified by the mask will                      be labeled with this identifier in the output mask.</p> <p>Targets</p> <p>image, mask</p> <p>Image types:     uint8, float32</p> <p>Reference</p> <p>https://github.com/danaaubakirova/doc-augmentation</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/mixing/transforms.py</code> Python<pre><code>class OverlayElements(ReferenceBasedTransform):\n    \"\"\"Apply overlay elements such as images and masks onto an input image. This transformation can be used to add\n    various objects (e.g., stickers, logos) to images with optional masks and bounding boxes for better placement\n    control.\n\n    Args:\n        metadata_key (str): Additional target key for metadata. Default `overlay_metadata`.\n        p (float): Probability of applying the transformation. Default: 0.5.\n\n    Possible Metadata Fields:\n        - image (np.ndarray): The overlay image to be applied. This is a required field.\n        - bbox (list[int]): The bounding box specifying the region where the overlay should be applied. It should\n                            contain four floats: [y_min, x_min, y_max, x_max]. If `label_id` is provided, it should\n                            be appended as the fifth element in the bbox. BBox should be in Albumentations format,\n                            that is the same as normalized Pascal VOC format\n                            [x_min / width, y_min / height, x_max / width, y_max / height]\n        - mask (np.ndarray): An optional mask that defines the non-rectangular region of the overlay image. If not\n                             provided, the entire overlay image is used.\n        - mask_id (int): An optional identifier for the mask. If provided, the regions specified by the mask will\n                         be labeled with this identifier in the output mask.\n\n    Targets:\n        image, mask\n\n    Image types:\n        uint8, float32\n\n    Reference:\n        https://github.com/danaaubakirova/doc-augmentation\n\n    \"\"\"\n\n    _targets = (Targets.IMAGE, Targets.MASK)\n\n    class InitSchema(BaseTransformInitSchema):\n        metadata_key: str\n\n    def __init__(\n        self,\n        metadata_key: str = \"overlay_metadata\",\n        p: float = 0.5,\n        always_apply: bool | None = None,\n    ):\n        super().__init__(p=p, always_apply=always_apply)\n        self.metadata_key = metadata_key\n\n    @property\n    def targets_as_params(self) -&gt; list[str]:\n        return [self.metadata_key]\n\n    @staticmethod\n    def preprocess_metadata(metadata: dict[str, Any], img_shape: SizeType) -&gt; dict[str, Any]:\n        overlay_image = metadata[\"image\"]\n        overlay_height, overlay_width = overlay_image.shape[:2]\n        image_height, image_width = img_shape[:2]\n\n        if \"bbox\" in metadata:\n            bbox = metadata[\"bbox\"]\n            check_bbox(bbox)\n            denormalized_bbox = denormalize_bbox(bbox[:4], img_shape[:2])\n\n            x_min, y_min, x_max, y_max = (int(x) for x in denormalized_bbox[:4])\n\n            if \"mask\" in metadata:\n                mask = metadata[\"mask\"]\n                mask = cv2.resize(mask, (x_max - x_min, y_max - y_min), interpolation=cv2.INTER_NEAREST)\n            else:\n                mask = np.ones((y_max - y_min, x_max - x_min), dtype=np.uint8)\n\n            overlay_image = cv2.resize(overlay_image, (x_max - x_min, y_max - y_min), interpolation=cv2.INTER_AREA)\n            offset = (y_min, x_min)\n\n            if len(bbox) == LENGTH_RAW_BBOX and \"bbox_id\" in metadata:\n                bbox = [x_min, y_min, x_max, y_max, metadata[\"bbox_id\"]]\n            else:\n                bbox = (x_min, y_min, x_max, y_max, *bbox[4:])\n        else:\n            if image_height &lt; overlay_height or image_width &lt; overlay_width:\n                overlay_image = cv2.resize(overlay_image, (image_width, image_height), interpolation=cv2.INTER_AREA)\n                overlay_height, overlay_width = overlay_image.shape[:2]\n\n            mask = metadata[\"mask\"] if \"mask\" in metadata else np.ones_like(overlay_image, dtype=np.uint8)\n\n            max_x_offset = image_width - overlay_width\n            max_y_offset = image_height - overlay_height\n\n            offset_x = random.randint(0, max_x_offset)\n            offset_y = random.randint(0, max_y_offset)\n\n            offset = (offset_y, offset_x)\n\n            bbox = [\n                offset_x,\n                offset_y,\n                offset_x + overlay_width,\n                offset_y + overlay_height,\n            ]\n\n            if \"bbox_id\" in metadata:\n                bbox = [*bbox, metadata[\"bbox_id\"]]\n\n        result = {\n            \"overlay_image\": overlay_image,\n            \"overlay_mask\": mask,\n            \"offset\": offset,\n            \"bbox\": bbox,\n        }\n\n        if \"mask_id\" in metadata:\n            result[\"mask_id\"] = metadata[\"mask_id\"]\n\n        return result\n\n    def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n        metadata = data[self.metadata_key]\n        img_shape = params[\"shape\"]\n\n        if isinstance(metadata, list):\n            overlay_data = [self.preprocess_metadata(md, img_shape) for md in metadata]\n        else:\n            overlay_data = [self.preprocess_metadata(metadata, img_shape)]\n\n        return {\n            \"overlay_data\": overlay_data,\n        }\n\n    def apply(\n        self,\n        img: np.ndarray,\n        overlay_data: list[dict[str, Any]],\n        **params: Any,\n    ) -&gt; np.ndarray:\n        for data in overlay_data:\n            overlay_image = data[\"overlay_image\"]\n            overlay_mask = data[\"overlay_mask\"]\n            offset = data[\"offset\"]\n            img = fmixing.copy_and_paste_blend(img, overlay_image, overlay_mask, offset=offset)\n        return img\n\n    def apply_to_mask(\n        self,\n        mask: np.ndarray,\n        overlay_data: list[dict[str, Any]],\n        **params: Any,\n    ) -&gt; np.ndarray:\n        for data in overlay_data:\n            if \"mask_id\" in data and data[\"mask_id\"] is not None:\n                overlay_mask = data[\"overlay_mask\"]\n                offset = data[\"offset\"]\n                mask_id = data[\"mask_id\"]\n\n                y_min, x_min = offset\n                y_max = y_min + overlay_mask.shape[0]\n                x_max = x_min + overlay_mask.shape[1]\n\n                mask_section = mask[y_min:y_max, x_min:x_max]\n                mask_section[overlay_mask &gt; 0] = mask_id\n\n        return mask\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return (\"metadata_key\",)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.mixing.transforms.OverlayElements.targets_as_params","title":"<code>targets_as_params: list[str]</code>  <code>property</code> <code>readonly</code>","text":"<p>Targets used to get params dependent on targets. This is used to check input has all required targets.</p>"},{"location":"api_reference/full_reference/#albumentations.augmentations.mixing.transforms.OverlayElements.apply","title":"<code>apply (self, img, overlay_data, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/mixing/transforms.py</code> Python<pre><code>def apply(\n    self,\n    img: np.ndarray,\n    overlay_data: list[dict[str, Any]],\n    **params: Any,\n) -&gt; np.ndarray:\n    for data in overlay_data:\n        overlay_image = data[\"overlay_image\"]\n        overlay_mask = data[\"overlay_mask\"]\n        offset = data[\"offset\"]\n        img = fmixing.copy_and_paste_blend(img, overlay_image, overlay_mask, offset=offset)\n    return img\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.mixing.transforms.OverlayElements.get_params_dependent_on_data","title":"<code>get_params_dependent_on_data (self, params, data)</code>","text":"<p>Returns parameters dependent on input.</p> Source code in <code>albumentations/augmentations/mixing/transforms.py</code> Python<pre><code>def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n    metadata = data[self.metadata_key]\n    img_shape = params[\"shape\"]\n\n    if isinstance(metadata, list):\n        overlay_data = [self.preprocess_metadata(md, img_shape) for md in metadata]\n    else:\n        overlay_data = [self.preprocess_metadata(metadata, img_shape)]\n\n    return {\n        \"overlay_data\": overlay_data,\n    }\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.mixing.transforms.OverlayElements.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/mixing/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return (\"metadata_key\",)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.text","title":"<code>text</code>  <code>special</code>","text":""},{"location":"api_reference/full_reference/#albumentations.augmentations.text.functional","title":"<code>functional</code>","text":""},{"location":"api_reference/full_reference/#albumentations.augmentations.text.functional.convert_image_to_pil","title":"<code>def convert_image_to_pil    (image)    </code> [view source on GitHub]","text":"<p>Convert a NumPy array image to a PIL image.</p> Source code in <code>albumentations/augmentations/text/functional.py</code> Python<pre><code>def convert_image_to_pil(image: np.ndarray) -&gt; Image:\n    \"\"\"Convert a NumPy array image to a PIL image.\"\"\"\n    try:\n        from PIL import Image\n    except ImportError:\n        raise ImportError(\"Pillow is not installed\") from ImportError\n\n    if len(image.shape) == MONO_CHANNEL_DIMENSIONS:  # (height, width)\n        return Image.fromarray(image)\n    if len(image.shape) == NUM_MULTI_CHANNEL_DIMENSIONS and image.shape[2] == 1:  # (height, width, 1)\n        return Image.fromarray(image[:, :, 0], mode=\"L\")\n    if len(image.shape) == NUM_MULTI_CHANNEL_DIMENSIONS and image.shape[2] == NUM_RGB_CHANNELS:  # (height, width, 3)\n        return Image.fromarray(image)\n\n    raise TypeError(f\"Unsupported image shape: {image.shape}\")\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.text.functional.draw_text_on_multi_channel_image","title":"<code>def draw_text_on_multi_channel_image    (image, metadata_list)    </code> [view source on GitHub]","text":"<p>Draw text on a multi-channel image with more than three channels.</p> Source code in <code>albumentations/augmentations/text/functional.py</code> Python<pre><code>def draw_text_on_multi_channel_image(image: np.ndarray, metadata_list: list[dict[str, Any]]) -&gt; np.ndarray:\n    \"\"\"Draw text on a multi-channel image with more than three channels.\"\"\"\n    try:\n        from PIL import Image, ImageDraw\n    except ImportError:\n        raise ImportError(\"Pillow is not installed\") from ImportError\n\n    channels = [Image.fromarray(image[:, :, i]) for i in range(image.shape[2])]\n    pil_images = [ImageDraw.Draw(channel) for channel in channels]\n\n    for metadata in metadata_list:\n        bbox_coords = metadata[\"bbox_coords\"]\n        text = metadata[\"text\"]\n        font = metadata[\"font\"]\n        font_color = metadata[\"font_color\"]\n        if isinstance(font_color, Sequence):\n            font_color = tuple(int(c) for c in font_color)\n        position = bbox_coords[:2]\n\n        for channel_id, pil_image in enumerate(pil_images):\n            pil_image.text(position, text, font=font, fill=font_color[channel_id])\n\n    return np.stack([np.array(channel) for channel in channels], axis=2)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.text.functional.draw_text_on_pil_image","title":"<code>def draw_text_on_pil_image    (pil_image, metadata_list)    </code> [view source on GitHub]","text":"<p>Draw text on a PIL image using metadata information.</p> Source code in <code>albumentations/augmentations/text/functional.py</code> Python<pre><code>def draw_text_on_pil_image(pil_image: Image, metadata_list: list[dict[str, Any]]) -&gt; Image:\n    \"\"\"Draw text on a PIL image using metadata information.\"\"\"\n    try:\n        from PIL import ImageDraw\n    except ImportError:\n        raise ImportError(\"Pillow is not installed\") from ImportError\n\n    draw = ImageDraw.Draw(pil_image)\n    for metadata in metadata_list:\n        bbox_coords = metadata[\"bbox_coords\"]\n        text = metadata[\"text\"]\n        font = metadata[\"font\"]\n        font_color = metadata[\"font_color\"]\n        if isinstance(font_color, (list, tuple)):\n            font_color = tuple(int(c) for c in font_color)\n        elif isinstance(font_color, float):\n            font_color = int(font_color)\n        position = bbox_coords[:2]\n        draw.text(position, text, font=font, fill=font_color)\n    return pil_image\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.text.transforms","title":"<code>transforms</code>","text":""},{"location":"api_reference/full_reference/#albumentations.augmentations.text.transforms.TextImage","title":"<code>class  TextImage</code> <code>     (font_path, stopwords=None, augmentations=(None,), fraction_range=(1.0, 1.0), font_size_fraction_range=(0.8, 0.9), font_color='black', clear_bg=False, metadata_key='textimage_metadata', always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Apply text rendering transformations on images.</p> <p>This class supports rendering text directly onto images using a variety of configurations, such as custom fonts, font sizes, colors, and augmentation methods. The text can be placed inside specified bounding boxes.</p> <p>Parameters:</p> Name Type Description <code>font_path</code> <code>str | Path</code> <p>Path to the font file to use for rendering text.</p> <code>stopwords</code> <code>list[str] | None</code> <p>List of stopwords for text augmentation.</p> <code>augmentations</code> <code>tuple[str | None, ...] | list[str | None]</code> <p>List of text augmentations to apply. None: text is printed as is \"insertion\": insert random stop words into the text. \"swap\": swap random words in the text. \"deletion\": delete random words from the text.</p> <code>fraction_range</code> <code>tuple[float, float]</code> <p>Range for selecting a fraction of bounding boxes to modify.</p> <code>font_size_fraction_range</code> <code>tuple[float, float]</code> <p>Range for selecting the font size as a fraction of bounding box height.</p> <code>font_color</code> <code>list[str] | str</code> <p>List of possible font colors or a single font color.</p> <code>clear_bg</code> <code>bool</code> <p>Whether to clear the background before rendering text.</p> <code>metadata_key</code> <code>str</code> <p>Key to access metadata in the parameters.</p> <code>p</code> <code>float</code> <p>Probability of applying the transform.</p> <p>Targets</p> <p>image</p> <p>Image types:     uint8, float32</p> <p>Reference</p> <p>https://github.com/danaaubakirova/doc-augmentation</p> <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; import albumentations as A\n&gt;&gt;&gt; transform = A.Compose([\n    A.TextImage(\n        font_path=Path(\"/path/to/font.ttf\"),\n        stopwords=[\"the\", \"is\", \"in\"],\n        augmentations=(\"insertion\", \"deletion\"),\n        fraction_range=(0.5, 1.0),\n        font_size_fraction_range=(0.5, 0.9),\n        font_color=[\"red\", \"green\", \"blue\"],\n        metadata_key=\"text_metadata\",\n        p=0.5\n    )\n])\n&gt;&gt;&gt; transformed = transform(image=my_image, text_metadata=my_metadata)\n&gt;&gt;&gt; image = transformed['image']\n# This will render text on `my_image` based on the metadata provided in `my_metadata`.\n</code></pre> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/text/transforms.py</code> Python<pre><code>class TextImage(ImageOnlyTransform):\n    \"\"\"Apply text rendering transformations on images.\n\n    This class supports rendering text directly onto images using a variety of configurations,\n    such as custom fonts, font sizes, colors, and augmentation methods. The text can be placed\n    inside specified bounding boxes.\n\n    Args:\n        font_path (str | Path): Path to the font file to use for rendering text.\n        stopwords (list[str] | None): List of stopwords for text augmentation.\n        augmentations (tuple[str | None, ...] | list[str | None]): List of text augmentations to apply.\n            None: text is printed as is\n            \"insertion\": insert random stop words into the text.\n            \"swap\": swap random words in the text.\n            \"deletion\": delete random words from the text.\n        fraction_range (tuple[float, float]): Range for selecting a fraction of bounding boxes to modify.\n        font_size_fraction_range (tuple[float, float]): Range for selecting the font size as a fraction of\n            bounding box height.\n        font_color (list[str] | str): List of possible font colors or a single font color.\n        clear_bg (bool): Whether to clear the background before rendering text.\n        metadata_key (str): Key to access metadata in the parameters.\n        p (float): Probability of applying the transform.\n\n    Targets:\n        image\n\n    Image types:\n        uint8, float32\n\n    Reference:\n        https://github.com/danaaubakirova/doc-augmentation\n\n    Examples:\n        &gt;&gt;&gt; import albumentations as A\n        &gt;&gt;&gt; transform = A.Compose([\n            A.TextImage(\n                font_path=Path(\"/path/to/font.ttf\"),\n                stopwords=[\"the\", \"is\", \"in\"],\n                augmentations=(\"insertion\", \"deletion\"),\n                fraction_range=(0.5, 1.0),\n                font_size_fraction_range=(0.5, 0.9),\n                font_color=[\"red\", \"green\", \"blue\"],\n                metadata_key=\"text_metadata\",\n                p=0.5\n            )\n        ])\n        &gt;&gt;&gt; transformed = transform(image=my_image, text_metadata=my_metadata)\n        &gt;&gt;&gt; image = transformed['image']\n        # This will render text on `my_image` based on the metadata provided in `my_metadata`.\n    \"\"\"\n\n    class InitSchema(BaseTransformInitSchema):\n        font_path: str\n        stopwords: list[str] | None\n        augmentations: tuple[str | None, ...] | list[str | None]\n        fraction_range: Annotated[tuple[float, float], AfterValidator(nondecreasing), AfterValidator(check_01)]\n        font_size_fraction_range: Annotated[\n            tuple[float, float],\n            AfterValidator(nondecreasing),\n            AfterValidator(check_01),\n        ]\n        font_color: list[ColorType | str] | ColorType | str\n        clear_bg: bool\n        metadata_key: str\n\n        @model_validator(mode=\"after\")\n        def validate_input(self) -&gt; Self:\n            if not self.stopwords:\n                self.augmentations = [aug for aug in self.augmentations if aug != \"insertion\"]\n\n            self.stopwords = self.stopwords or [\"the\", \"is\", \"in\", \"at\", \"of\"]\n\n            return self\n\n    def __init__(\n        self,\n        font_path: str,\n        stopwords: list[str] | None = None,\n        augmentations: tuple[Literal[\"insertion\", \"swap\", \"deletion\"] | None] = (None,),\n        fraction_range: tuple[float, float] = (1.0, 1.0),\n        font_size_fraction_range: tuple[float, float] = (0.8, 0.9),\n        font_color: list[ColorType | str] | ColorType | str = \"black\",\n        clear_bg: bool = False,\n        metadata_key: str = \"textimage_metadata\",\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ) -&gt; None:\n        super().__init__(p=p, always_apply=always_apply)\n        self.metadata_key = metadata_key\n        self.font_path = font_path\n        self.fraction_range = fraction_range\n        self.stopwords = stopwords\n        self.augmentations = list(augmentations)\n        self.font_size_fraction_range = font_size_fraction_range\n        self.font_color = font_color\n        self.clear_bg = clear_bg\n\n    @property\n    def targets_as_params(self) -&gt; list[str]:\n        return [self.metadata_key]\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return (\n            \"font_path\",\n            \"stopwords\",\n            \"augmentations\",\n            \"fraction_range\",\n            \"font_size_fraction_range\",\n            \"font_color\",\n            \"metadata_key\",\n            \"clear_bg\",\n        )\n\n    def random_aug(\n        self,\n        text: str,\n        fraction: float,\n        choice: Literal[\"insertion\", \"swap\", \"deletion\"],\n    ) -&gt; str:\n        words = [word for word in text.strip().split() if word]\n        num_words = len(words)\n        num_words_to_modify = max(1, int(fraction * num_words))\n\n        if choice == \"insertion\":\n            result_sentence = ftext.insert_random_stopwords(words, num_words_to_modify, self.stopwords)\n        elif choice == \"swap\":\n            result_sentence = ftext.swap_random_words(words, num_words_to_modify)\n        elif choice == \"deletion\":\n            result_sentence = ftext.delete_random_words(words, num_words_to_modify)\n        else:\n            raise ValueError(\"Invalid choice. Choose from 'insertion', 'swap', or 'deletion'.\")\n\n        result_sentence = re.sub(\" +\", \" \", result_sentence).strip()\n        return result_sentence if result_sentence != text else \"\"\n\n    def preprocess_metadata(self, image: np.ndarray, bbox: BoxType, text: str, bbox_index: int) -&gt; dict[str, Any]:\n        check_bbox(bbox)\n        denormalized_bbox = denormalize_bbox(bbox[:4], image.shape[:2])\n\n        x_min, y_min, x_max, y_max = (int(x) for x in denormalized_bbox[:4])\n        bbox_height = y_max - y_min\n\n        font_size_fraction = random.uniform(*self.font_size_fraction_range)\n\n        font = ImageFont.truetype(str(self.font_path), int(font_size_fraction * bbox_height))\n\n        if not self.augmentations or self.augmentations is None:\n            augmented_text = text\n        else:\n            augmentation = random.choice(self.augmentations)\n\n            augmented_text = text if augmentation is None else self.random_aug(text, 0.5, choice=augmentation)\n\n        font_color = random.choice(self.font_color) if isinstance(self.font_color, list) else self.font_color\n\n        return {\n            \"bbox_coords\": (x_min, y_min, x_max, y_max),\n            \"bbox_index\": bbox_index,\n            \"original_text\": text,\n            \"text\": augmented_text,\n            \"font\": font,\n            \"font_color\": font_color,\n        }\n\n    def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n        image = data[\"image\"]\n\n        metadata = data[self.metadata_key]\n\n        if metadata == []:\n            return {\n                \"overlay_data\": [],\n            }\n\n        if isinstance(metadata, dict):\n            metadata = [metadata]\n\n        fraction = random.uniform(*self.fraction_range)\n\n        num_bboxes_to_modify = int(len(metadata) * fraction)\n\n        bbox_indices_to_update = random.sample(range(len(metadata)), num_bboxes_to_modify)\n\n        overlay_data = [\n            self.preprocess_metadata(image, metadata[bbox_index][\"bbox\"], metadata[bbox_index][\"text\"], bbox_index)\n            for bbox_index in bbox_indices_to_update\n        ]\n\n        return {\n            \"overlay_data\": overlay_data,\n        }\n\n    def apply(\n        self,\n        img: np.ndarray,\n        overlay_data: list[dict[str, Any]],\n        **params: Any,\n    ) -&gt; np.ndarray:\n        return ftext.render_text(img, overlay_data, clear_bg=self.clear_bg)\n\n    def apply_with_params(self, params: dict[str, Any], *args: Any, **kwargs: Any) -&gt; dict[str, Any]:\n        res = super().apply_with_params(params, *args, **kwargs)\n        res[\"overlay_data\"] = [\n            {\n                \"bbox_coords\": overlay[\"bbox_coords\"],\n                \"text\": overlay[\"text\"],\n                \"original_text\": overlay[\"original_text\"],\n                \"bbox_index\": overlay[\"bbox_index\"],\n                \"font_color\": overlay[\"font_color\"],\n            }\n            for overlay in params[\"overlay_data\"]\n        ]\n\n        return res\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.text.transforms.TextImage.targets_as_params","title":"<code>targets_as_params: list[str]</code>  <code>property</code> <code>readonly</code>","text":"<p>Targets used to get params dependent on targets. This is used to check input has all required targets.</p>"},{"location":"api_reference/full_reference/#albumentations.augmentations.text.transforms.TextImage.apply","title":"<code>apply (self, img, overlay_data, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/text/transforms.py</code> Python<pre><code>def apply(\n    self,\n    img: np.ndarray,\n    overlay_data: list[dict[str, Any]],\n    **params: Any,\n) -&gt; np.ndarray:\n    return ftext.render_text(img, overlay_data, clear_bg=self.clear_bg)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.text.transforms.TextImage.apply_with_params","title":"<code>apply_with_params (self, params, *args, **kwargs)</code>","text":"<p>Apply transforms with parameters.</p> Source code in <code>albumentations/augmentations/text/transforms.py</code> Python<pre><code>def apply_with_params(self, params: dict[str, Any], *args: Any, **kwargs: Any) -&gt; dict[str, Any]:\n    res = super().apply_with_params(params, *args, **kwargs)\n    res[\"overlay_data\"] = [\n        {\n            \"bbox_coords\": overlay[\"bbox_coords\"],\n            \"text\": overlay[\"text\"],\n            \"original_text\": overlay[\"original_text\"],\n            \"bbox_index\": overlay[\"bbox_index\"],\n            \"font_color\": overlay[\"font_color\"],\n        }\n        for overlay in params[\"overlay_data\"]\n    ]\n\n    return res\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.text.transforms.TextImage.get_params_dependent_on_data","title":"<code>get_params_dependent_on_data (self, params, data)</code>","text":"<p>Returns parameters dependent on input.</p> Source code in <code>albumentations/augmentations/text/transforms.py</code> Python<pre><code>def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n    image = data[\"image\"]\n\n    metadata = data[self.metadata_key]\n\n    if metadata == []:\n        return {\n            \"overlay_data\": [],\n        }\n\n    if isinstance(metadata, dict):\n        metadata = [metadata]\n\n    fraction = random.uniform(*self.fraction_range)\n\n    num_bboxes_to_modify = int(len(metadata) * fraction)\n\n    bbox_indices_to_update = random.sample(range(len(metadata)), num_bboxes_to_modify)\n\n    overlay_data = [\n        self.preprocess_metadata(image, metadata[bbox_index][\"bbox\"], metadata[bbox_index][\"text\"], bbox_index)\n        for bbox_index in bbox_indices_to_update\n    ]\n\n    return {\n        \"overlay_data\": overlay_data,\n    }\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.text.transforms.TextImage.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/text/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return (\n        \"font_path\",\n        \"stopwords\",\n        \"augmentations\",\n        \"fraction_range\",\n        \"font_size_fraction_range\",\n        \"font_color\",\n        \"metadata_key\",\n        \"clear_bg\",\n    )\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms","title":"<code>transforms</code>","text":""},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.CLAHE","title":"<code>class  CLAHE</code> <code>     (clip_limit=4.0, tile_grid_size=(8, 8), always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Apply Contrast Limited Adaptive Histogram Equalization to the input image.</p> <p>Parameters:</p> Name Type Description <code>clip_limit</code> <code>ScaleFloatType</code> <p>upper threshold value for contrast limiting. If clip_limit is a single float value, the range will be (1, clip_limit). Default: (1, 4).</p> <code>tile_grid_size</code> <code>tuple[int, int]</code> <p>size of grid for histogram equalization. Default: (8, 8).</p> <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 0.5.</p> <p>Targets</p> <p>image</p> <p>Image types:     uint8</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>class CLAHE(ImageOnlyTransform):\n    \"\"\"Apply Contrast Limited Adaptive Histogram Equalization to the input image.\n\n    Args:\n        clip_limit: upper threshold value for contrast limiting.\n            If clip_limit is a single float value, the range will be (1, clip_limit). Default: (1, 4).\n        tile_grid_size: size of grid for histogram equalization. Default: (8, 8).\n        p: probability of applying the transform. Default: 0.5.\n\n    Targets:\n        image\n\n    Image types:\n        uint8\n\n    \"\"\"\n\n    class InitSchema(BaseTransformInitSchema):\n        clip_limit: OnePlusFloatRangeType = (1.0, 4.0)\n        tile_grid_size: OnePlusIntRangeType = (8, 8)\n\n    def __init__(\n        self,\n        clip_limit: ScaleFloatType = 4.0,\n        tile_grid_size: tuple[int, int] = (8, 8),\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p=p, always_apply=always_apply)\n        self.clip_limit = cast(Tuple[float, float], clip_limit)\n        self.tile_grid_size = tile_grid_size\n\n    def apply(self, img: np.ndarray, clip_limit: float, **params: Any) -&gt; np.ndarray:\n        if not is_rgb_image(img) and not is_grayscale_image(img):\n            msg = \"CLAHE transformation expects 1-channel or 3-channel images.\"\n            raise TypeError(msg)\n\n        return fmain.clahe(img, clip_limit, self.tile_grid_size)\n\n    def get_params(self) -&gt; dict[str, float]:\n        return {\"clip_limit\": random.uniform(self.clip_limit[0], self.clip_limit[1])}\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, str]:\n        return (\"clip_limit\", \"tile_grid_size\")\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.CLAHE.apply","title":"<code>apply (self, img, clip_limit, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def apply(self, img: np.ndarray, clip_limit: float, **params: Any) -&gt; np.ndarray:\n    if not is_rgb_image(img) and not is_grayscale_image(img):\n        msg = \"CLAHE transformation expects 1-channel or 3-channel images.\"\n        raise TypeError(msg)\n\n    return fmain.clahe(img, clip_limit, self.tile_grid_size)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.CLAHE.get_params","title":"<code>get_params (self)</code>","text":"<p>Returns parameters independent of input.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_params(self) -&gt; dict[str, float]:\n    return {\"clip_limit\": random.uniform(self.clip_limit[0], self.clip_limit[1])}\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.CLAHE.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, str]:\n    return (\"clip_limit\", \"tile_grid_size\")\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.ChannelShuffle","title":"<code>class  ChannelShuffle</code> <code> </code>  [view source on GitHub]","text":"<p>Randomly rearrange channels of the image.</p> <p>Parameters:</p> Name Type Description <code>p</code> <p>probability of applying the transform. Default: 0.5.</p> <p>Targets</p> <p>image</p> <p>Image types:     uint8, float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>class ChannelShuffle(ImageOnlyTransform):\n    \"\"\"Randomly rearrange channels of the image.\n\n    Args:\n        p: probability of applying the transform. Default: 0.5.\n\n    Targets:\n        image\n\n    Image types:\n        uint8, float32\n\n    \"\"\"\n\n    def apply(self, img: np.ndarray, channels_shuffled: tuple[int, ...], **params: Any) -&gt; np.ndarray:\n        return fmain.channel_shuffle(img, channels_shuffled)\n\n    def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n        ch_arr = list(range(params[\"shape\"][2]))\n        ch_arr = random_utils.shuffle(ch_arr)\n        return {\"channels_shuffled\": ch_arr}\n\n    def get_transform_init_args_names(self) -&gt; tuple[()]:\n        return ()\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.ChannelShuffle.apply","title":"<code>apply (self, img, channels_shuffled, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def apply(self, img: np.ndarray, channels_shuffled: tuple[int, ...], **params: Any) -&gt; np.ndarray:\n    return fmain.channel_shuffle(img, channels_shuffled)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.ChannelShuffle.get_params_dependent_on_data","title":"<code>get_params_dependent_on_data (self, params, data)</code>","text":"<p>Returns parameters dependent on input.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n    ch_arr = list(range(params[\"shape\"][2]))\n    ch_arr = random_utils.shuffle(ch_arr)\n    return {\"channels_shuffled\": ch_arr}\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.ChannelShuffle.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[()]:\n    return ()\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.ChromaticAberration","title":"<code>class  ChromaticAberration</code> <code>     (primary_distortion_limit=(-0.02, 0.02), secondary_distortion_limit=(-0.05, 0.05), mode='green_purple', interpolation=1, always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Add lateral chromatic aberration by distorting the red and blue channels of the input image.</p> <p>Parameters:</p> Name Type Description <code>primary_distortion_limit</code> <code>ScaleFloatType</code> <p>range of the primary radial distortion coefficient. If primary_distortion_limit is a single float value, the range will be (-primary_distortion_limit, primary_distortion_limit). Controls the distortion in the center of the image (positive values result in pincushion distortion, negative values result in barrel distortion). Default: 0.02.</p> <code>secondary_distortion_limit</code> <code>ScaleFloatType</code> <p>range of the secondary radial distortion coefficient. If secondary_distortion_limit is a single float value, the range will be (-secondary_distortion_limit, secondary_distortion_limit). Controls the distortion in the corners of the image (positive values result in pincushion distortion, negative values result in barrel distortion). Default: 0.05.</p> <code>mode</code> <code>ChromaticAberrationMode</code> <p>type of color fringing. Supported modes are 'green_purple', 'red_blue' and 'random'. 'random' will choose one of the modes 'green_purple' or 'red_blue' randomly. Default: 'green_purple'.</p> <code>interpolation</code> <code>int</code> <p>flag that is used to specify the interpolation algorithm. Should be one of: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4. Default: cv2.INTER_LINEAR.</p> <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 0.5.</p> <p>Targets</p> <p>image</p> <p>Image types:     uint8, float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>class ChromaticAberration(ImageOnlyTransform):\n    \"\"\"Add lateral chromatic aberration by distorting the red and blue channels of the input image.\n\n    Args:\n        primary_distortion_limit: range of the primary radial distortion coefficient.\n            If primary_distortion_limit is a single float value, the range will be\n            (-primary_distortion_limit, primary_distortion_limit).\n            Controls the distortion in the center of the image (positive values result in pincushion distortion,\n            negative values result in barrel distortion).\n            Default: 0.02.\n        secondary_distortion_limit: range of the secondary radial distortion coefficient.\n            If secondary_distortion_limit is a single float value, the range will be\n            (-secondary_distortion_limit, secondary_distortion_limit).\n            Controls the distortion in the corners of the image (positive values result in pincushion distortion,\n            negative values result in barrel distortion).\n            Default: 0.05.\n        mode: type of color fringing.\n            Supported modes are 'green_purple', 'red_blue' and 'random'.\n            'random' will choose one of the modes 'green_purple' or 'red_blue' randomly.\n            Default: 'green_purple'.\n        interpolation: flag that is used to specify the interpolation algorithm. Should be one of:\n            cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.\n            Default: cv2.INTER_LINEAR.\n        p: probability of applying the transform.\n            Default: 0.5.\n\n    Targets:\n        image\n\n    Image types:\n        uint8, float32\n\n    \"\"\"\n\n    class InitSchema(BaseTransformInitSchema):\n        primary_distortion_limit: SymmetricRangeType = (-0.02, 0.02)\n        secondary_distortion_limit: SymmetricRangeType = (-0.05, 0.05)\n        mode: ChromaticAberrationMode = Field(default=\"green_purple\", description=\"Type of color fringing.\")\n        interpolation: InterpolationType = cv2.INTER_LINEAR\n\n    def __init__(\n        self,\n        primary_distortion_limit: ScaleFloatType = (-0.02, 0.02),\n        secondary_distortion_limit: ScaleFloatType = (-0.05, 0.05),\n        mode: ChromaticAberrationMode = \"green_purple\",\n        interpolation: int = cv2.INTER_LINEAR,\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p=p, always_apply=always_apply)\n        self.primary_distortion_limit = cast(Tuple[float, float], primary_distortion_limit)\n        self.secondary_distortion_limit = cast(Tuple[float, float], secondary_distortion_limit)\n        self.mode = mode\n        self.interpolation = interpolation\n\n    def apply(\n        self,\n        img: np.ndarray,\n        primary_distortion_red: float,\n        secondary_distortion_red: float,\n        primary_distortion_blue: float,\n        secondary_distortion_blue: float,\n        **params: Any,\n    ) -&gt; np.ndarray:\n        return fmain.chromatic_aberration(\n            img,\n            primary_distortion_red,\n            secondary_distortion_red,\n            primary_distortion_blue,\n            secondary_distortion_blue,\n            self.interpolation,\n        )\n\n    def get_params(self) -&gt; dict[str, float]:\n        primary_distortion_red = random.uniform(*self.primary_distortion_limit)\n        secondary_distortion_red = random.uniform(*self.secondary_distortion_limit)\n        primary_distortion_blue = random.uniform(*self.primary_distortion_limit)\n        secondary_distortion_blue = random.uniform(*self.secondary_distortion_limit)\n\n        secondary_distortion_red = self._match_sign(primary_distortion_red, secondary_distortion_red)\n        secondary_distortion_blue = self._match_sign(primary_distortion_blue, secondary_distortion_blue)\n\n        if self.mode == \"green_purple\":\n            # distortion coefficients of the red and blue channels have the same sign\n            primary_distortion_blue = self._match_sign(primary_distortion_red, primary_distortion_blue)\n            secondary_distortion_blue = self._match_sign(secondary_distortion_red, secondary_distortion_blue)\n        if self.mode == \"red_blue\":\n            # distortion coefficients of the red and blue channels have the opposite sign\n            primary_distortion_blue = self._unmatch_sign(primary_distortion_red, primary_distortion_blue)\n            secondary_distortion_blue = self._unmatch_sign(secondary_distortion_red, secondary_distortion_blue)\n\n        return {\n            \"primary_distortion_red\": primary_distortion_red,\n            \"secondary_distortion_red\": secondary_distortion_red,\n            \"primary_distortion_blue\": primary_distortion_blue,\n            \"secondary_distortion_blue\": secondary_distortion_blue,\n        }\n\n    @staticmethod\n    def _match_sign(a: float, b: float) -&gt; float:\n        # Match the sign of b to a\n        if (a &lt; 0 &lt; b) or (a &gt; 0 &gt; b):\n            return -b\n        return b\n\n    @staticmethod\n    def _unmatch_sign(a: float, b: float) -&gt; float:\n        # Unmatch the sign of b to a\n        if (a &lt; 0 and b &lt; 0) or (a &gt; 0 and b &gt; 0):\n            return -b\n        return b\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, str, str, str]:\n        return \"primary_distortion_limit\", \"secondary_distortion_limit\", \"mode\", \"interpolation\"\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.ChromaticAberration.apply","title":"<code>apply (self, img, primary_distortion_red, secondary_distortion_red, primary_distortion_blue, secondary_distortion_blue, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def apply(\n    self,\n    img: np.ndarray,\n    primary_distortion_red: float,\n    secondary_distortion_red: float,\n    primary_distortion_blue: float,\n    secondary_distortion_blue: float,\n    **params: Any,\n) -&gt; np.ndarray:\n    return fmain.chromatic_aberration(\n        img,\n        primary_distortion_red,\n        secondary_distortion_red,\n        primary_distortion_blue,\n        secondary_distortion_blue,\n        self.interpolation,\n    )\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.ChromaticAberration.get_params","title":"<code>get_params (self)</code>","text":"<p>Returns parameters independent of input.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_params(self) -&gt; dict[str, float]:\n    primary_distortion_red = random.uniform(*self.primary_distortion_limit)\n    secondary_distortion_red = random.uniform(*self.secondary_distortion_limit)\n    primary_distortion_blue = random.uniform(*self.primary_distortion_limit)\n    secondary_distortion_blue = random.uniform(*self.secondary_distortion_limit)\n\n    secondary_distortion_red = self._match_sign(primary_distortion_red, secondary_distortion_red)\n    secondary_distortion_blue = self._match_sign(primary_distortion_blue, secondary_distortion_blue)\n\n    if self.mode == \"green_purple\":\n        # distortion coefficients of the red and blue channels have the same sign\n        primary_distortion_blue = self._match_sign(primary_distortion_red, primary_distortion_blue)\n        secondary_distortion_blue = self._match_sign(secondary_distortion_red, secondary_distortion_blue)\n    if self.mode == \"red_blue\":\n        # distortion coefficients of the red and blue channels have the opposite sign\n        primary_distortion_blue = self._unmatch_sign(primary_distortion_red, primary_distortion_blue)\n        secondary_distortion_blue = self._unmatch_sign(secondary_distortion_red, secondary_distortion_blue)\n\n    return {\n        \"primary_distortion_red\": primary_distortion_red,\n        \"secondary_distortion_red\": secondary_distortion_red,\n        \"primary_distortion_blue\": primary_distortion_blue,\n        \"secondary_distortion_blue\": secondary_distortion_blue,\n    }\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.ChromaticAberration.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, str, str, str]:\n    return \"primary_distortion_limit\", \"secondary_distortion_limit\", \"mode\", \"interpolation\"\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.ColorJitter","title":"<code>class  ColorJitter</code> <code>     (brightness=(0.8, 1), contrast=(0.8, 1), saturation=(0.8, 1), hue=(-0.5, 0.5), always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Randomly changes the brightness, contrast, and saturation of an image. Compared to ColorJitter from torchvision, this transform gives a little bit different results because Pillow (used in torchvision) and OpenCV (used in Albumentations) transform an image to HSV format by different formulas. Another difference - Pillow uses uint8 overflow, but we use value saturation.</p> <p>Parameters:</p> Name Type Description <code>brightness</code> <code>float or tuple of float (min, max</code> <p>How much to jitter brightness. If float:     brightness_factor is chosen uniformly from [max(0, 1 - brightness), 1 + brightness] If tuple[float, float]] will be sampled from that range. Both values should be non negative numbers.</p> <code>contrast</code> <code>float or tuple of float (min, max</code> <p>How much to jitter contrast. If float:     contrast_factor is chosen uniformly from [max(0, 1 - brightness), 1 + brightness] If tuple[float, float]] will be sampled from that range. Both values should be non negative numbers.</p> <code>saturation</code> <code>float or tuple of float (min, max</code> <p>How much to jitter saturation. If float:    saturation_factor is chosen uniformly from [max(0, 1 - brightness), 1 + brightness] If tuple[float, float]] will be sampled from that range. Both values should be non negative numbers.</p> <code>hue</code> <code>float or tuple of float (min, max</code> <p>How much to jitter hue. If float:    saturation_factor is chosen uniformly from [-hue, hue]. Should have 0 &lt;= hue &lt;= 0.5. If tuple[float, float]] will be sampled from that range. Both values should be in range [-0.5, 0.5].</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>class ColorJitter(ImageOnlyTransform):\n    \"\"\"Randomly changes the brightness, contrast, and saturation of an image. Compared to ColorJitter from torchvision,\n    this transform gives a little bit different results because Pillow (used in torchvision) and OpenCV (used in\n    Albumentations) transform an image to HSV format by different formulas. Another difference - Pillow uses uint8\n    overflow, but we use value saturation.\n\n    Args:\n        brightness (float or tuple of float (min, max)): How much to jitter brightness.\n            If float:\n                brightness_factor is chosen uniformly from [max(0, 1 - brightness), 1 + brightness]\n            If tuple[float, float]] will be sampled from that range. Both values should be non negative numbers.\n        contrast (float or tuple of float (min, max)): How much to jitter contrast.\n            If float:\n                contrast_factor is chosen uniformly from [max(0, 1 - brightness), 1 + brightness]\n            If tuple[float, float]] will be sampled from that range. Both values should be non negative numbers.\n        saturation (float or tuple of float (min, max)): How much to jitter saturation.\n            If float:\n               saturation_factor is chosen uniformly from [max(0, 1 - brightness), 1 + brightness]\n            If tuple[float, float]] will be sampled from that range. Both values should be non negative numbers.\n        hue (float or tuple of float (min, max)): How much to jitter hue.\n            If float:\n               saturation_factor is chosen uniformly from [-hue, hue]. Should have 0 &lt;= hue &lt;= 0.5.\n            If tuple[float, float]] will be sampled from that range. Both values should be in range [-0.5, 0.5].\n\n    \"\"\"\n\n    class InitSchema(BaseTransformInitSchema):\n        brightness: Annotated[ScaleFloatType, Field(default=0.2, description=\"Range for jittering brightness.\")]\n        contrast: Annotated[ScaleFloatType, Field(default=0.2, description=\"Range for jittering contrast.\")]\n        saturation: Annotated[ScaleFloatType, Field(default=0.2, description=\"Range for jittering saturation.\")]\n        hue: Annotated[ScaleFloatType, Field(default=0.2, description=\"Range for jittering hue.\")]\n\n        @field_validator(\"brightness\", \"contrast\", \"saturation\", \"hue\")\n        @classmethod\n        def check_ranges(cls, value: ScaleFloatType, info: ValidationInfo) -&gt; tuple[float, float]:\n            if info.field_name == \"hue\":\n                bounds = -0.5, 0.5\n                bias = 0\n                clip = False\n            elif info.field_name in [\"brightness\", \"contrast\", \"saturation\"]:\n                bounds = 0, float(\"inf\")\n                bias = 1\n                clip = True\n\n            if isinstance(value, numbers.Number):\n                if value &lt; 0:\n                    raise ValueError(f\"If {info.field_name} is a single number, it must be non negative.\")\n                left = bias - value\n                if clip:\n                    left = max(left, 0)\n                value = (left, bias + value)\n            elif isinstance(value, tuple) and len(value) == PAIR:\n                check_range(value, *bounds, info.field_name)\n\n            return cast(Tuple[float, float], value)\n\n    def __init__(\n        self,\n        brightness: ScaleFloatType = (0.8, 1),\n        contrast: ScaleFloatType = (0.8, 1),\n        saturation: ScaleFloatType = (0.8, 1),\n        hue: ScaleFloatType = (-0.5, 0.5),\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p=p, always_apply=always_apply)\n\n        self.brightness = cast(Tuple[float, float], brightness)\n        self.contrast = cast(Tuple[float, float], contrast)\n        self.saturation = cast(Tuple[float, float], saturation)\n        self.hue = cast(Tuple[float, float], hue)\n\n        self.transforms = [\n            fmain.adjust_brightness_torchvision,\n            fmain.adjust_contrast_torchvision,\n            fmain.adjust_saturation_torchvision,\n            fmain.adjust_hue_torchvision,\n        ]\n\n    def get_params(self) -&gt; dict[str, Any]:\n        brightness = random.uniform(self.brightness[0], self.brightness[1])\n        contrast = random.uniform(self.contrast[0], self.contrast[1])\n        saturation = random.uniform(self.saturation[0], self.saturation[1])\n        hue = random.uniform(self.hue[0], self.hue[1])\n\n        order = [0, 1, 2, 3]\n        order = random_utils.shuffle(order)\n\n        return {\n            \"brightness\": brightness,\n            \"contrast\": contrast,\n            \"saturation\": saturation,\n            \"hue\": hue,\n            \"order\": order,\n        }\n\n    def apply(\n        self,\n        img: np.ndarray,\n        brightness: float,\n        contrast: float,\n        saturation: float,\n        hue: float,\n        order: list[int],\n        **params: Any,\n    ) -&gt; np.ndarray:\n        if order is None:\n            order = [0, 1, 2, 3]\n        if not is_rgb_image(img) and not is_grayscale_image(img):\n            msg = \"ColorJitter transformation expects 1-channel or 3-channel images.\"\n            raise TypeError(msg)\n        color_transforms = [brightness, contrast, saturation, hue]\n        for i in order:\n            img = self.transforms[i](img, color_transforms[i])\n        return img\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, str, str, str]:\n        return (\"brightness\", \"contrast\", \"saturation\", \"hue\")\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.ColorJitter.apply","title":"<code>apply (self, img, brightness, contrast, saturation, hue, order, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def apply(\n    self,\n    img: np.ndarray,\n    brightness: float,\n    contrast: float,\n    saturation: float,\n    hue: float,\n    order: list[int],\n    **params: Any,\n) -&gt; np.ndarray:\n    if order is None:\n        order = [0, 1, 2, 3]\n    if not is_rgb_image(img) and not is_grayscale_image(img):\n        msg = \"ColorJitter transformation expects 1-channel or 3-channel images.\"\n        raise TypeError(msg)\n    color_transforms = [brightness, contrast, saturation, hue]\n    for i in order:\n        img = self.transforms[i](img, color_transforms[i])\n    return img\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.ColorJitter.get_params","title":"<code>get_params (self)</code>","text":"<p>Returns parameters independent of input.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_params(self) -&gt; dict[str, Any]:\n    brightness = random.uniform(self.brightness[0], self.brightness[1])\n    contrast = random.uniform(self.contrast[0], self.contrast[1])\n    saturation = random.uniform(self.saturation[0], self.saturation[1])\n    hue = random.uniform(self.hue[0], self.hue[1])\n\n    order = [0, 1, 2, 3]\n    order = random_utils.shuffle(order)\n\n    return {\n        \"brightness\": brightness,\n        \"contrast\": contrast,\n        \"saturation\": saturation,\n        \"hue\": hue,\n        \"order\": order,\n    }\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.ColorJitter.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, str, str, str]:\n    return (\"brightness\", \"contrast\", \"saturation\", \"hue\")\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.Downscale","title":"<code>class  Downscale</code> <code>     (scale_min=None, scale_max=None, interpolation=None, scale_range=(0.25, 0.25), interpolation_pair={'upscale': 0, 'downscale': 0}, always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Decreases image quality by downscaling and then upscaling it back to its original size.</p> <p>Parameters:</p> Name Type Description <code>scale_range</code> <code>tuple[float, float]</code> <p>A tuple defining the minimum and maximum scale to which the image will be downscaled. The range should be between 0 and 1, inclusive at minimum and exclusive at maximum. The first value should be less than or equal to the second value.</p> <code>interpolation_pair</code> <code>InterpolationDict</code> <p>A dictionary specifying the interpolation methods to use for downscaling and upscaling. Should include keys 'downscale' and 'upscale' with cv2 interpolation     flags as values. Example: {\"downscale\": cv2.INTER_NEAREST, \"upscale\": cv2.INTER_LINEAR}.</p> <p>Targets</p> <p>image</p> <p>Image types:     uint8, float32</p> <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; transform = Downscale(scale_range=(0.5, 0.9), interpolation_pair={\"downscale\": cv2.INTER_AREA,\n                                                  \"upscale\": cv2.INTER_CUBIC})\n&gt;&gt;&gt; transformed = transform(image=img)\n</code></pre> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>class Downscale(ImageOnlyTransform):\n    \"\"\"Decreases image quality by downscaling and then upscaling it back to its original size.\n\n    Args:\n        scale_range (tuple[float, float]): A tuple defining the minimum and maximum scale to which the image\n            will be downscaled. The range should be between 0 and 1, inclusive at minimum and exclusive at maximum.\n            The first value should be less than or equal to the second value.\n        interpolation_pair (InterpolationDict): A dictionary specifying the interpolation methods to use for\n            downscaling and upscaling. Should include keys 'downscale' and 'upscale' with cv2 interpolation\n                flags as values.\n            Example: {\"downscale\": cv2.INTER_NEAREST, \"upscale\": cv2.INTER_LINEAR}.\n\n    Targets:\n        image\n\n    Image types:\n        uint8, float32\n\n    Example:\n        &gt;&gt;&gt; transform = Downscale(scale_range=(0.5, 0.9), interpolation_pair={\"downscale\": cv2.INTER_AREA,\n                                                          \"upscale\": cv2.INTER_CUBIC})\n        &gt;&gt;&gt; transformed = transform(image=img)\n    \"\"\"\n\n    class InitSchema(BaseTransformInitSchema):\n        scale_min: float | None = Field(\n            default=None,\n            ge=0,\n            le=1,\n            description=\"Lower bound on the image scale.\",\n        )\n        scale_max: float | None = Field(\n            default=None,\n            ge=0,\n            lt=1,\n            description=\"Upper bound on the image scale.\",\n        )\n\n        interpolation: int | Interpolation | InterpolationDict | None = Field(\n            default_factory=lambda: Interpolation(downscale=cv2.INTER_NEAREST, upscale=cv2.INTER_NEAREST),\n        )\n        interpolation_pair: InterpolationPydantic\n\n        scale_range: Annotated[tuple[float, float], AfterValidator(check_01), AfterValidator(nondecreasing)] = (\n            0.25,\n            0.25,\n        )\n\n        @model_validator(mode=\"after\")\n        def validate_params(self) -&gt; Self:\n            if self.scale_min is not None and self.scale_max is not None:\n                warn(\n                    \"scale_min and scale_max are deprecated. Use scale_range instead.\",\n                    DeprecationWarning,\n                    stacklevel=2,\n                )\n\n                self.scale_range = (self.scale_min, self.scale_max)\n                self.scale_min = None\n                self.scale_max = None\n\n            if self.interpolation is not None:\n                warn(\n                    \"Downscale.interpolation is deprecated. Use Downscale.interpolation_pair instead.\",\n                    DeprecationWarning,\n                    stacklevel=2,\n                )\n\n                if isinstance(self.interpolation, dict):\n                    self.interpolation_pair = InterpolationPydantic(**self.interpolation)\n                elif isinstance(self.interpolation, int):\n                    self.interpolation_pair = InterpolationPydantic(\n                        upscale=self.interpolation,\n                        downscale=self.interpolation,\n                    )\n                elif isinstance(self.interpolation, Interpolation):\n                    self.interpolation_pair = InterpolationPydantic(\n                        upscale=self.interpolation.upscale,\n                        downscale=self.interpolation.downscale,\n                    )\n                self.interpolation = None\n\n            return self\n\n    def __init__(\n        self,\n        scale_min: float | None = None,\n        scale_max: float | None = None,\n        interpolation: int | Interpolation | InterpolationDict | None = None,\n        scale_range: tuple[float, float] = (0.25, 0.25),\n        interpolation_pair: InterpolationDict = InterpolationDict(\n            {\"upscale\": cv2.INTER_NEAREST, \"downscale\": cv2.INTER_NEAREST},\n        ),\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p=p, always_apply=always_apply)\n        self.scale_range = scale_range\n        self.interpolation_pair = interpolation_pair\n\n    def apply(self, img: np.ndarray, scale: float, **params: Any) -&gt; np.ndarray:\n        return fmain.downscale(\n            img,\n            scale=scale,\n            down_interpolation=self.interpolation_pair[\"downscale\"],\n            up_interpolation=self.interpolation_pair[\"upscale\"],\n        )\n\n    def get_params(self) -&gt; dict[str, Any]:\n        return {\"scale\": random.uniform(self.scale_range[0], self.scale_range[1])}\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, str]:\n        return (\"scale_range\", \"interpolation_pair\")\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.Downscale.apply","title":"<code>apply (self, img, scale, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def apply(self, img: np.ndarray, scale: float, **params: Any) -&gt; np.ndarray:\n    return fmain.downscale(\n        img,\n        scale=scale,\n        down_interpolation=self.interpolation_pair[\"downscale\"],\n        up_interpolation=self.interpolation_pair[\"upscale\"],\n    )\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.Downscale.get_params","title":"<code>get_params (self)</code>","text":"<p>Returns parameters independent of input.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_params(self) -&gt; dict[str, Any]:\n    return {\"scale\": random.uniform(self.scale_range[0], self.scale_range[1])}\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.Downscale.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, str]:\n    return (\"scale_range\", \"interpolation_pair\")\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.Emboss","title":"<code>class  Emboss</code> <code>     (alpha=(0.2, 0.5), strength=(0.2, 0.7), always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Emboss the input image and overlays the result with the original image.</p> <p>Parameters:</p> Name Type Description <code>alpha</code> <code>tuple[float, float]</code> <p>range to choose the visibility of the embossed image. At 0, only the original image is visible,at 1.0 only its embossed version is visible. Default: (0.2, 0.5).</p> <code>strength</code> <code>tuple[float, float]</code> <p>strength range of the embossing. Default: (0.2, 0.7).</p> <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 0.5.</p> <p>Targets</p> <p>image</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>class Emboss(ImageOnlyTransform):\n    \"\"\"Emboss the input image and overlays the result with the original image.\n\n    Args:\n        alpha: range to choose the visibility of the embossed image. At 0, only the original image is\n            visible,at 1.0 only its embossed version is visible. Default: (0.2, 0.5).\n        strength: strength range of the embossing. Default: (0.2, 0.7).\n        p: probability of applying the transform. Default: 0.5.\n\n    Targets:\n        image\n\n    \"\"\"\n\n    class InitSchema(BaseTransformInitSchema):\n        alpha: ZeroOneRangeType = (0.2, 0.5)\n        strength: NonNegativeFloatRangeType = (0.2, 0.7)\n\n    def __init__(\n        self,\n        alpha: tuple[float, float] = (0.2, 0.5),\n        strength: tuple[float, float] = (0.2, 0.7),\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p=p, always_apply=always_apply)\n        self.alpha = alpha\n        self.strength = strength\n\n    @staticmethod\n    def __generate_emboss_matrix(alpha_sample: np.ndarray, strength_sample: np.ndarray) -&gt; np.ndarray:\n        matrix_nochange = np.array([[0, 0, 0], [0, 1, 0], [0, 0, 0]], dtype=np.float32)\n        matrix_effect = np.array(\n            [\n                [-1 - strength_sample, 0 - strength_sample, 0],\n                [0 - strength_sample, 1, 0 + strength_sample],\n                [0, 0 + strength_sample, 1 + strength_sample],\n            ],\n            dtype=np.float32,\n        )\n        return (1 - alpha_sample) * matrix_nochange + alpha_sample * matrix_effect\n\n    def get_params(self) -&gt; dict[str, np.ndarray]:\n        alpha = random.uniform(*self.alpha)\n        strength = random.uniform(*self.strength)\n        emboss_matrix = self.__generate_emboss_matrix(alpha_sample=alpha, strength_sample=strength)\n        return {\"emboss_matrix\": emboss_matrix}\n\n    def apply(self, img: np.ndarray, emboss_matrix: np.ndarray, **params: Any) -&gt; np.ndarray:\n        return fmain.convolve(img, emboss_matrix)\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, str]:\n        return (\"alpha\", \"strength\")\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.Emboss.apply","title":"<code>apply (self, img, emboss_matrix, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def apply(self, img: np.ndarray, emboss_matrix: np.ndarray, **params: Any) -&gt; np.ndarray:\n    return fmain.convolve(img, emboss_matrix)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.Emboss.get_params","title":"<code>get_params (self)</code>","text":"<p>Returns parameters independent of input.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_params(self) -&gt; dict[str, np.ndarray]:\n    alpha = random.uniform(*self.alpha)\n    strength = random.uniform(*self.strength)\n    emboss_matrix = self.__generate_emboss_matrix(alpha_sample=alpha, strength_sample=strength)\n    return {\"emboss_matrix\": emboss_matrix}\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.Emboss.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, str]:\n    return (\"alpha\", \"strength\")\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.Equalize","title":"<code>class  Equalize</code> <code>     (mode='cv', by_channels=True, mask=None, mask_params=(), always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Equalize the image histogram.</p> <p>Parameters:</p> Name Type Description <code>mode</code> <code>str</code> <p>{'cv', 'pil'}. Use OpenCV or Pillow equalization method.</p> <code>by_channels</code> <code>bool</code> <p>If True, use equalization by channels separately, else convert image to YCbCr representation and use equalization by <code>Y</code> channel.</p> <code>mask</code> <code>np.ndarray, callable</code> <p>If given, only the pixels selected by the mask are included in the analysis. Maybe 1 channel or 3 channel array or callable. Function signature must include <code>image</code> argument.</p> <code>mask_params</code> <code>list of str</code> <p>Params for mask function.</p> <p>Targets</p> <p>image</p> <p>Image types:     uint8</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>class Equalize(ImageOnlyTransform):\n    \"\"\"Equalize the image histogram.\n\n    Args:\n        mode (str): {'cv', 'pil'}. Use OpenCV or Pillow equalization method.\n        by_channels (bool): If True, use equalization by channels separately,\n            else convert image to YCbCr representation and use equalization by `Y` channel.\n        mask (np.ndarray, callable): If given, only the pixels selected by\n            the mask are included in the analysis. Maybe 1 channel or 3 channel array or callable.\n            Function signature must include `image` argument.\n        mask_params (list of str): Params for mask function.\n\n    Targets:\n        image\n\n    Image types:\n        uint8\n\n    \"\"\"\n\n    class InitSchema(BaseTransformInitSchema):\n        mode: ImageMode = \"cv\"\n        by_channels: Annotated[bool, Field(default=True, description=\"Equalize channels separately if True\")]\n        mask: Annotated[\n            np.ndarray | Callable[..., Any] | None,\n            Field(default=None, description=\"Mask to apply for equalization\"),\n        ]\n        mask_params: Annotated[Sequence[str], Field(default=[], description=\"Parameters for mask function\")]\n\n    def __init__(\n        self,\n        mode: ImageMode = \"cv\",\n        by_channels: bool = True,\n        mask: np.ndarray | Callable[..., Any] | None = None,\n        mask_params: Sequence[str] = (),\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p=p, always_apply=always_apply)\n\n        self.mode = mode\n        self.by_channels = by_channels\n        self.mask = mask\n        self.mask_params = mask_params\n\n    def apply(self, img: np.ndarray, mask: np.ndarray, **params: Any) -&gt; np.ndarray:\n        return fmain.equalize(img, mode=self.mode, by_channels=self.by_channels, mask=mask)\n\n    def get_params_dependent_on_targets(self, params: dict[str, Any]) -&gt; dict[str, Any]:\n        if not callable(self.mask):\n            return {\"mask\": self.mask}\n\n        return {\"mask\": self.mask(**params)}\n\n    @property\n    def targets_as_params(self) -&gt; list[str]:\n        return [\"image\", *list(self.mask_params)]\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return (\"mode\", \"by_channels\", \"mask\", \"mask_params\")\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.Equalize.targets_as_params","title":"<code>targets_as_params: list[str]</code>  <code>property</code> <code>readonly</code>","text":"<p>Targets used to get params dependent on targets. This is used to check input has all required targets.</p>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.Equalize.apply","title":"<code>apply (self, img, mask, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def apply(self, img: np.ndarray, mask: np.ndarray, **params: Any) -&gt; np.ndarray:\n    return fmain.equalize(img, mode=self.mode, by_channels=self.by_channels, mask=mask)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.Equalize.get_params_dependent_on_targets","title":"<code>get_params_dependent_on_targets (self, params)</code>","text":"<p>This method is deprecated. Use <code>get_params_dependent_on_data</code> instead. Returns parameters dependent on targets. Dependent target is defined in <code>self.targets_as_params</code></p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_params_dependent_on_targets(self, params: dict[str, Any]) -&gt; dict[str, Any]:\n    if not callable(self.mask):\n        return {\"mask\": self.mask}\n\n    return {\"mask\": self.mask(**params)}\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.Equalize.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return (\"mode\", \"by_channels\", \"mask\", \"mask_params\")\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.FancyPCA","title":"<code>class  FancyPCA</code> <code>     (alpha=0.1, p=0.5, always_apply=None)                 </code>  [view source on GitHub]","text":"<p>Augment RGB image using FancyPCA from Krizhevsky's paper \"ImageNet Classification with Deep Convolutional Neural Networks\"</p> <p>Parameters:</p> Name Type Description <code>alpha</code> <code>float</code> <p>how much to perturb/scale the eigen vectors and eigenvalues. scale is samples from gaussian distribution (mu=0, sigma=alpha)</p> <p>Targets</p> <p>image</p> <p>Image types:     3-channel uint8 images only</p> <p>Credit</p> <p>http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf https://deshanadesai.github.io/notes/Fancy-PCA-with-Scikit-Image https://pixelatedbrian.github.io/2018-04-29-fancy_pca/</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>class FancyPCA(ImageOnlyTransform):\n    \"\"\"Augment RGB image using FancyPCA from Krizhevsky's paper\n    \"ImageNet Classification with Deep Convolutional Neural Networks\"\n\n    Args:\n        alpha:  how much to perturb/scale the eigen vectors and eigenvalues.\n            scale is samples from gaussian distribution (mu=0, sigma=alpha)\n\n    Targets:\n        image\n\n    Image types:\n        3-channel uint8 images only\n\n    Credit:\n        http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf\n        https://deshanadesai.github.io/notes/Fancy-PCA-with-Scikit-Image\n        https://pixelatedbrian.github.io/2018-04-29-fancy_pca/\n\n    \"\"\"\n\n    class InitSchema(BaseTransformInitSchema):\n        alpha: float = Field(default=0.1, description=\"Scale for perturbing the eigen vectors and values\", ge=0)\n\n    def __init__(self, alpha: float = 0.1, p: float = 0.5, always_apply: bool | None = None):\n        super().__init__(p=p, always_apply=always_apply)\n        self.alpha = alpha\n\n    def apply(self, img: np.ndarray, alpha: float, **params: Any) -&gt; np.ndarray:\n        return fmain.fancy_pca(img, alpha)\n\n    def get_params(self) -&gt; dict[str, float]:\n        return {\"alpha\": random.gauss(0, self.alpha)}\n\n    def get_transform_init_args_names(self) -&gt; tuple[str]:\n        return (\"alpha\",)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.FancyPCA.apply","title":"<code>apply (self, img, alpha, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def apply(self, img: np.ndarray, alpha: float, **params: Any) -&gt; np.ndarray:\n    return fmain.fancy_pca(img, alpha)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.FancyPCA.get_params","title":"<code>get_params (self)</code>","text":"<p>Returns parameters independent of input.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_params(self) -&gt; dict[str, float]:\n    return {\"alpha\": random.gauss(0, self.alpha)}\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.FancyPCA.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str]:\n    return (\"alpha\",)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.FromFloat","title":"<code>class  FromFloat</code> <code>     (dtype='uint16', max_value=None, always_apply=None, p=1.0)                 </code>  [view source on GitHub]","text":"<p>Take an input array where all values should lie in the range [0, 1.0], multiply them by <code>max_value</code> and then cast the resulted value to a type specified by <code>dtype</code>. If <code>max_value</code> is None the transform will try to infer the maximum value for the data type from the <code>dtype</code> argument.</p> <p>This is the inverse transform for :class:<code>~albumentations.augmentations.transforms.ToFloat</code>.</p> <p>Parameters:</p> Name Type Description <code>max_value</code> <code>float | None</code> <p>maximum possible input value. Default: None.</p> <code>dtype</code> <code>Literal['uint8', 'uint16', 'float32', 'float64']</code> <p>data type of the output. See the <code>'Data types' page from the NumPy docs</code>_. Default: 'uint16'.</p> <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 1.0.</p> <p>Targets</p> <p>image</p> <p>Image types:     float32</p> <p>.. _'Data types' page from the NumPy docs:    https://docs.scipy.org/doc/numpy/user/basics.types.html</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>class FromFloat(ImageOnlyTransform):\n    \"\"\"Take an input array where all values should lie in the range [0, 1.0], multiply them by `max_value` and then\n    cast the resulted value to a type specified by `dtype`. If `max_value` is None the transform will try to infer\n    the maximum value for the data type from the `dtype` argument.\n\n    This is the inverse transform for :class:`~albumentations.augmentations.transforms.ToFloat`.\n\n    Args:\n        max_value: maximum possible input value. Default: None.\n        dtype: data type of the output. See the `'Data types' page from the NumPy docs`_.\n            Default: 'uint16'.\n        p: probability of applying the transform. Default: 1.0.\n\n    Targets:\n        image\n\n    Image types:\n        float32\n\n    .. _'Data types' page from the NumPy docs:\n       https://docs.scipy.org/doc/numpy/user/basics.types.html\n\n    \"\"\"\n\n    class InitSchema(BaseTransformInitSchema):\n        dtype: Literal[\"uint8\", \"uint16\", \"float32\", \"float64\"]\n        max_value: float | None = Field(default=None, description=\"Maximum possible input value.\")\n        p: ProbabilityType = 1\n\n    def __init__(\n        self,\n        dtype: Literal[\"uint8\", \"uint16\", \"float32\", \"float64\"] = \"uint16\",\n        max_value: float | None = None,\n        always_apply: bool | None = None,\n        p: float = 1.0,\n    ):\n        super().__init__(p=p, always_apply=always_apply)\n        self.dtype = np.dtype(dtype)\n        self.max_value = max_value\n\n    def apply(self, img: np.ndarray, **params: Any) -&gt; np.ndarray:\n        return fmain.from_float(img, self.dtype, self.max_value)\n\n    def get_transform_init_args(self) -&gt; dict[str, Any]:\n        return {\"dtype\": self.dtype.name, \"max_value\": self.max_value}\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.FromFloat.apply","title":"<code>apply (self, img, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def apply(self, img: np.ndarray, **params: Any) -&gt; np.ndarray:\n    return fmain.from_float(img, self.dtype, self.max_value)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.GaussNoise","title":"<code>class  GaussNoise</code> <code>     (var_limit=(10.0, 50.0), mean=0, per_channel=True, noise_scale_factor=1, always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Apply Gaussian noise to the input image.</p> <p>Parameters:</p> Name Type Description <code>var_limit</code> <code>Union[float, tuple[float, float]]</code> <p>Variance range for noise. If var_limit is a single float, the range will be (0, var_limit). Default: (10.0, 50.0).</p> <code>mean</code> <code>float</code> <p>Mean of the noise. Default: 0</p> <code>per_channel</code> <code>bool</code> <p>If set to True, noise will be sampled for each channel independently. Otherwise, the noise will be sampled once for all channels. Faster when <code>per_channel = False</code>. Default: True</p> <code>noise_scale_factor</code> <code>float</code> <p>Scaling factor for noise generation. Value should be in the range (0, 1]. When set to 1, noise is sampled for each pixel independently. If less, noise is sampled for a smaller size and resized to fit the shape of the image. Smaller values make the transform faster. Default: 1.0.</p> <code>p</code> <code>float</code> <p>Probability of applying the transform. Default: 0.5.</p> <p>Targets</p> <p>image</p> <p>Image types:     uint8, float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>class GaussNoise(ImageOnlyTransform):\n    \"\"\"Apply Gaussian noise to the input image.\n\n    Args:\n        var_limit (Union[float, tuple[float, float]]): Variance range for noise.\n            If var_limit is a single float, the range will be (0, var_limit). Default: (10.0, 50.0).\n        mean (float): Mean of the noise. Default: 0\n        per_channel (bool): If set to True, noise will be sampled for each channel independently.\n            Otherwise, the noise will be sampled once for all channels.\n            Faster when `per_channel = False`.\n            Default: True\n        noise_scale_factor (float): Scaling factor for noise generation. Value should be in the range (0, 1].\n            When set to 1, noise is sampled for each pixel independently. If less, noise is sampled for a smaller size\n            and resized to fit the shape of the image. Smaller values make the transform faster. Default: 1.0.\n        p (float): Probability of applying the transform. Default: 0.5.\n\n    Targets:\n        image\n\n    Image types:\n        uint8, float32\n\n    \"\"\"\n\n    class InitSchema(BaseTransformInitSchema):\n        var_limit: NonNegativeFloatRangeType = Field(default=(10.0, 50.0), description=\"Variance range for noise.\")\n        mean: float = Field(default=0, description=\"Mean of the noise.\")\n        per_channel: bool = Field(default=True, description=\"Apply noise per channel.\")\n        noise_scale_factor: float = Field(gt=0, le=1)\n\n    def __init__(\n        self,\n        var_limit: ScaleFloatType = (10.0, 50.0),\n        mean: float = 0,\n        per_channel: bool = True,\n        noise_scale_factor: float = 1,\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p=p, always_apply=always_apply)\n        self.var_limit = cast(Tuple[float, float], var_limit)\n        self.mean = mean\n        self.per_channel = per_channel\n        self.noise_scale_factor = noise_scale_factor\n\n    def apply(self, img: np.ndarray, gauss: np.ndarray, **params: Any) -&gt; np.ndarray:\n        return fmain.add_noise(img, gauss)\n\n    def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, float]:\n        image = data[\"image\"] if \"image\" in data else data[\"images\"][0]\n        var = random.uniform(self.var_limit[0], self.var_limit[1])\n        sigma = math.sqrt(var)\n\n        if self.per_channel:\n            target_shape = image.shape\n            if self.noise_scale_factor == 1:\n                gauss = random_utils.normal(self.mean, sigma, target_shape)\n            else:\n                gauss = fmain.generate_approx_gaussian_noise(target_shape, self.mean, sigma, self.noise_scale_factor)\n        else:\n            target_shape = image.shape[:2]\n            if self.noise_scale_factor == 1:\n                gauss = random_utils.normal(self.mean, sigma, target_shape)\n            else:\n                gauss = fmain.generate_approx_gaussian_noise(target_shape, self.mean, sigma, self.noise_scale_factor)\n\n            if image.ndim &gt; MONO_CHANNEL_DIMENSIONS:\n                gauss = np.expand_dims(gauss, -1)\n\n        return {\"gauss\": gauss}\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return \"var_limit\", \"per_channel\", \"mean\", \"noise_scale_factor\"\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.GaussNoise.apply","title":"<code>apply (self, img, gauss, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def apply(self, img: np.ndarray, gauss: np.ndarray, **params: Any) -&gt; np.ndarray:\n    return fmain.add_noise(img, gauss)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.GaussNoise.get_params_dependent_on_data","title":"<code>get_params_dependent_on_data (self, params, data)</code>","text":"<p>Returns parameters dependent on input.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, float]:\n    image = data[\"image\"] if \"image\" in data else data[\"images\"][0]\n    var = random.uniform(self.var_limit[0], self.var_limit[1])\n    sigma = math.sqrt(var)\n\n    if self.per_channel:\n        target_shape = image.shape\n        if self.noise_scale_factor == 1:\n            gauss = random_utils.normal(self.mean, sigma, target_shape)\n        else:\n            gauss = fmain.generate_approx_gaussian_noise(target_shape, self.mean, sigma, self.noise_scale_factor)\n    else:\n        target_shape = image.shape[:2]\n        if self.noise_scale_factor == 1:\n            gauss = random_utils.normal(self.mean, sigma, target_shape)\n        else:\n            gauss = fmain.generate_approx_gaussian_noise(target_shape, self.mean, sigma, self.noise_scale_factor)\n\n        if image.ndim &gt; MONO_CHANNEL_DIMENSIONS:\n            gauss = np.expand_dims(gauss, -1)\n\n    return {\"gauss\": gauss}\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.GaussNoise.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return \"var_limit\", \"per_channel\", \"mean\", \"noise_scale_factor\"\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.HueSaturationValue","title":"<code>class  HueSaturationValue</code> <code>     (hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Randomly change hue, saturation and value of the input image.</p> <p>Parameters:</p> Name Type Description <code>hue_shift_limit</code> <code>ScaleIntType</code> <p>range for changing hue. If hue_shift_limit is a single int, the range will be (-hue_shift_limit, hue_shift_limit). Default: (-20, 20).</p> <code>sat_shift_limit</code> <code>ScaleIntType</code> <p>range for changing saturation. If sat_shift_limit is a single int, the range will be (-sat_shift_limit, sat_shift_limit). Default: (-30, 30).</p> <code>val_shift_limit</code> <code>ScaleIntType</code> <p>range for changing value. If val_shift_limit is a single int, the range will be (-val_shift_limit, val_shift_limit). Default: (-20, 20).</p> <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 0.5.</p> <p>Targets</p> <p>image</p> <p>Image types:     uint8, float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>class HueSaturationValue(ImageOnlyTransform):\n    \"\"\"Randomly change hue, saturation and value of the input image.\n\n    Args:\n        hue_shift_limit: range for changing hue. If hue_shift_limit is a single int, the range\n            will be (-hue_shift_limit, hue_shift_limit). Default: (-20, 20).\n        sat_shift_limit: range for changing saturation. If sat_shift_limit is a single int,\n            the range will be (-sat_shift_limit, sat_shift_limit). Default: (-30, 30).\n        val_shift_limit: range for changing value. If val_shift_limit is a single int, the range\n            will be (-val_shift_limit, val_shift_limit). Default: (-20, 20).\n        p (float): probability of applying the transform. Default: 0.5.\n\n    Targets:\n        image\n\n    Image types:\n        uint8, float32\n\n    \"\"\"\n\n    class InitSchema(BaseTransformInitSchema):\n        hue_shift_limit: SymmetricRangeType = (-20, 20)\n        sat_shift_limit: SymmetricRangeType = (-30, 30)\n        val_shift_limit: SymmetricRangeType = (-20, 20)\n\n    def __init__(\n        self,\n        hue_shift_limit: ScaleIntType = 20,\n        sat_shift_limit: ScaleIntType = 30,\n        val_shift_limit: ScaleIntType = 20,\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p=p, always_apply=always_apply)\n        self.hue_shift_limit = cast(Tuple[float, float], hue_shift_limit)\n        self.sat_shift_limit = cast(Tuple[float, float], sat_shift_limit)\n        self.val_shift_limit = cast(Tuple[float, float], val_shift_limit)\n\n    def apply(\n        self,\n        img: np.ndarray,\n        hue_shift: int,\n        sat_shift: int,\n        val_shift: int,\n        **params: Any,\n    ) -&gt; np.ndarray:\n        if not is_rgb_image(img) and not is_grayscale_image(img):\n            msg = \"HueSaturationValue transformation expects 1-channel or 3-channel images.\"\n            raise TypeError(msg)\n        return fmain.shift_hsv(img, hue_shift, sat_shift, val_shift)\n\n    def get_params(self) -&gt; dict[str, float]:\n        return {\n            \"hue_shift\": random.uniform(self.hue_shift_limit[0], self.hue_shift_limit[1]),\n            \"sat_shift\": random.uniform(self.sat_shift_limit[0], self.sat_shift_limit[1]),\n            \"val_shift\": random.uniform(self.val_shift_limit[0], self.val_shift_limit[1]),\n        }\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return (\"hue_shift_limit\", \"sat_shift_limit\", \"val_shift_limit\")\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.HueSaturationValue.apply","title":"<code>apply (self, img, hue_shift, sat_shift, val_shift, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def apply(\n    self,\n    img: np.ndarray,\n    hue_shift: int,\n    sat_shift: int,\n    val_shift: int,\n    **params: Any,\n) -&gt; np.ndarray:\n    if not is_rgb_image(img) and not is_grayscale_image(img):\n        msg = \"HueSaturationValue transformation expects 1-channel or 3-channel images.\"\n        raise TypeError(msg)\n    return fmain.shift_hsv(img, hue_shift, sat_shift, val_shift)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.HueSaturationValue.get_params","title":"<code>get_params (self)</code>","text":"<p>Returns parameters independent of input.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_params(self) -&gt; dict[str, float]:\n    return {\n        \"hue_shift\": random.uniform(self.hue_shift_limit[0], self.hue_shift_limit[1]),\n        \"sat_shift\": random.uniform(self.sat_shift_limit[0], self.sat_shift_limit[1]),\n        \"val_shift\": random.uniform(self.val_shift_limit[0], self.val_shift_limit[1]),\n    }\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.HueSaturationValue.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return (\"hue_shift_limit\", \"sat_shift_limit\", \"val_shift_limit\")\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.ISONoise","title":"<code>class  ISONoise</code> <code>     (color_shift=(0.01, 0.05), intensity=(0.1, 0.5), always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Apply camera sensor noise.</p> <p>Parameters:</p> Name Type Description <code>color_shift</code> <code>float, float</code> <p>variance range for color hue change. Measured as a fraction of 360 degree Hue angle in HLS colorspace.</p> <code>intensity</code> <code>float, float</code> <p>Multiplicative factor that control strength of color and luminace noise.</p> <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 0.5.</p> <p>Targets</p> <p>image</p> <p>Image types:     uint8, float32</p> <p>Exceptions:</p> Type Description <code>TypeError</code> <p>If the input image is not RGB.</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>class ISONoise(ImageOnlyTransform):\n    \"\"\"Apply camera sensor noise.\n\n    Args:\n        color_shift (float, float): variance range for color hue change.\n            Measured as a fraction of 360 degree Hue angle in HLS colorspace.\n        intensity ((float, float): Multiplicative factor that control strength\n            of color and luminace noise.\n        p (float): probability of applying the transform. Default: 0.5.\n\n    Targets:\n        image\n\n    Image types:\n        uint8, float32\n\n    Raises:\n        TypeError: If the input image is not RGB.\n\n    \"\"\"\n\n    class InitSchema(BaseTransformInitSchema):\n        color_shift: Annotated[tuple[float, float], AfterValidator(check_01), AfterValidator(nondecreasing)] = Field(\n            default=(0.01, 0.05),\n            description=(\n                \"Variance range for color hue change. Measured as a fraction of 360 degree Hue angle in HLS colorspace.\"\n            ),\n        )\n        intensity: Annotated[tuple[float, float], AfterValidator(check_0plus), AfterValidator(nondecreasing)] = Field(\n            default=(0.1, 0.5),\n            description=\"Multiplicative factor that control strength of color and luminance noise.\",\n        )\n\n    def __init__(\n        self,\n        color_shift: tuple[float, float] = (0.01, 0.05),\n        intensity: tuple[float, float] = (0.1, 0.5),\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p=p, always_apply=always_apply)\n        self.intensity = intensity\n        self.color_shift = color_shift\n\n    def apply(\n        self,\n        img: np.ndarray,\n        color_shift: float,\n        intensity: float,\n        random_seed: int,\n        **params: Any,\n    ) -&gt; np.ndarray:\n        return fmain.iso_noise(img, color_shift, intensity, np.random.RandomState(random_seed))\n\n    def get_params(self) -&gt; dict[str, Any]:\n        return {\n            \"color_shift\": random.uniform(self.color_shift[0], self.color_shift[1]),\n            \"intensity\": random.uniform(self.intensity[0], self.intensity[1]),\n            \"random_seed\": random_utils.get_random_seed(),\n        }\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, str]:\n        return \"intensity\", \"color_shift\"\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.ISONoise.apply","title":"<code>apply (self, img, color_shift, intensity, random_seed, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def apply(\n    self,\n    img: np.ndarray,\n    color_shift: float,\n    intensity: float,\n    random_seed: int,\n    **params: Any,\n) -&gt; np.ndarray:\n    return fmain.iso_noise(img, color_shift, intensity, np.random.RandomState(random_seed))\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.ISONoise.get_params","title":"<code>get_params (self)</code>","text":"<p>Returns parameters independent of input.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_params(self) -&gt; dict[str, Any]:\n    return {\n        \"color_shift\": random.uniform(self.color_shift[0], self.color_shift[1]),\n        \"intensity\": random.uniform(self.intensity[0], self.intensity[1]),\n        \"random_seed\": random_utils.get_random_seed(),\n    }\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.ISONoise.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, str]:\n    return \"intensity\", \"color_shift\"\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.ImageCompression","title":"<code>class  ImageCompression</code> <code>     (quality_lower=None, quality_upper=None, compression_type=&lt;ImageCompressionType.JPEG: 0&gt;, quality_range=(99, 100), always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Decreases image quality by Jpeg, WebP compression of an image.</p> <p>Parameters:</p> Name Type Description <code>quality_range</code> <code>tuple[int, int]</code> <p>tuple of bounds on the image quality i.e. (quality_lower, quality_upper). Both values should be in [1, 100] range.</p> <code>compression_type</code> <code>ImageCompressionType</code> <p>should be ImageCompressionType.JPEG or ImageCompressionType.WEBP. Default: ImageCompressionType.JPEG</p> <p>Targets</p> <p>image</p> <p>Image types:     uint8, float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>class ImageCompression(ImageOnlyTransform):\n    \"\"\"Decreases image quality by Jpeg, WebP compression of an image.\n\n    Args:\n        quality_range: tuple of bounds on the image quality i.e. (quality_lower, quality_upper).\n            Both values should be in [1, 100] range.\n        compression_type (ImageCompressionType): should be ImageCompressionType.JPEG or ImageCompressionType.WEBP.\n            Default: ImageCompressionType.JPEG\n\n    Targets:\n        image\n\n    Image types:\n        uint8, float32\n\n    \"\"\"\n\n    class InitSchema(BaseTransformInitSchema):\n        quality_range: Annotated[tuple[int, int], AfterValidator(check_1plus), AfterValidator(nondecreasing)] = (\n            99,\n            100,\n        )\n\n        quality_lower: int | None = Field(\n            default=None,\n            description=\"Lower bound on the image quality\",\n            ge=1,\n            le=100,\n        )\n        quality_upper: int | None = Field(\n            default=None,\n            description=\"Upper bound on the image quality\",\n            ge=1,\n            le=100,\n        )\n        compression_type: ImageCompressionType = Field(\n            default=ImageCompressionType.JPEG,\n            description=\"Image compression format\",\n        )\n\n        @model_validator(mode=\"after\")\n        def validate_ranges(self) -&gt; Self:\n            # Update the quality_range based on the non-None values of quality_lower and quality_upper\n            if self.quality_lower is not None or self.quality_upper is not None:\n                if self.quality_lower is not None:\n                    warn(\n                        \"`quality_lower` is deprecated. Use `quality_range` as tuple\"\n                        \" (quality_lower, quality_upper) instead.\",\n                        DeprecationWarning,\n                        stacklevel=2,\n                    )\n                if self.quality_upper is not None:\n                    warn(\n                        \"`quality_upper` is deprecated. Use `quality_range` as tuple\"\n                        \" (quality_lower, quality_upper) instead.\",\n                        DeprecationWarning,\n                        stacklevel=2,\n                    )\n                lower = self.quality_lower if self.quality_lower is not None else self.quality_range[0]\n                upper = self.quality_upper if self.quality_upper is not None else self.quality_range[1]\n                self.quality_range = (lower, upper)\n                # Clear the deprecated individual quality settings\n                self.quality_lower = None\n                self.quality_upper = None\n\n            # Validate the quality_range\n            if not (1 &lt;= self.quality_range[0] &lt;= MAX_JPEG_QUALITY and 1 &lt;= self.quality_range[1] &lt;= MAX_JPEG_QUALITY):\n                raise ValueError(f\"Quality range values should be within [1, {MAX_JPEG_QUALITY}] range.\")\n\n            return self\n\n    def __init__(\n        self,\n        quality_lower: int | None = None,\n        quality_upper: int | None = None,\n        compression_type: ImageCompressionType = ImageCompressionType.JPEG,\n        quality_range: tuple[int, int] = (99, 100),\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p, always_apply)\n        self.quality_range = quality_range\n        self.compression_type = compression_type\n\n    def apply(self, img: np.ndarray, quality: int, image_type: Literal[\".jpg\", \".webp\"], **params: Any) -&gt; np.ndarray:\n        if img.ndim != MONO_CHANNEL_DIMENSIONS and img.shape[-1] not in (1, 3, 4):\n            msg = \"ImageCompression transformation expects 1, 3 or 4 channel images.\"\n            raise TypeError(msg)\n        return fmain.image_compression(img, quality, image_type)\n\n    def get_params(self) -&gt; dict[str, int | str]:\n        if self.compression_type == ImageCompressionType.JPEG:\n            image_type = \".jpg\"\n        elif self.compression_type == ImageCompressionType.WEBP:\n            image_type = \".webp\"\n        else:\n            raise ValueError(f\"Unknown image compression type: {self.compression_type}\")\n\n        return {\n            \"quality\": random.randint(self.quality_range[0], self.quality_range[1]),\n            \"image_type\": image_type,\n        }\n\n    def get_transform_init_args(self) -&gt; dict[str, Any]:\n        return {\n            \"quality_range\": self.quality_range,\n            \"compression_type\": self.compression_type.value,\n        }\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.ImageCompression.apply","title":"<code>apply (self, img, quality, image_type, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def apply(self, img: np.ndarray, quality: int, image_type: Literal[\".jpg\", \".webp\"], **params: Any) -&gt; np.ndarray:\n    if img.ndim != MONO_CHANNEL_DIMENSIONS and img.shape[-1] not in (1, 3, 4):\n        msg = \"ImageCompression transformation expects 1, 3 or 4 channel images.\"\n        raise TypeError(msg)\n    return fmain.image_compression(img, quality, image_type)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.ImageCompression.get_params","title":"<code>get_params (self)</code>","text":"<p>Returns parameters independent of input.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_params(self) -&gt; dict[str, int | str]:\n    if self.compression_type == ImageCompressionType.JPEG:\n        image_type = \".jpg\"\n    elif self.compression_type == ImageCompressionType.WEBP:\n        image_type = \".webp\"\n    else:\n        raise ValueError(f\"Unknown image compression type: {self.compression_type}\")\n\n    return {\n        \"quality\": random.randint(self.quality_range[0], self.quality_range[1]),\n        \"image_type\": image_type,\n    }\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.InvertImg","title":"<code>class  InvertImg</code> <code> </code>  [view source on GitHub]","text":"<p>Invert the input image by subtracting pixel values from max values of the image types, i.e., 255 for uint8 and 1.0 for float32.</p> <p>Parameters:</p> Name Type Description <code>p</code> <p>probability of applying the transform. Default: 0.5.</p> <p>Targets</p> <p>image</p> <p>Image types:     uint8, float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>class InvertImg(ImageOnlyTransform):\n    \"\"\"Invert the input image by subtracting pixel values from max values of the image types,\n    i.e., 255 for uint8 and 1.0 for float32.\n\n    Args:\n        p: probability of applying the transform. Default: 0.5.\n\n    Targets:\n        image\n\n    Image types:\n        uint8, float32\n\n    \"\"\"\n\n    def apply(self, img: np.ndarray, **params: Any) -&gt; np.ndarray:\n        return fmain.invert(img)\n\n    def get_transform_init_args_names(self) -&gt; tuple[()]:\n        return ()\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.InvertImg.apply","title":"<code>apply (self, img, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def apply(self, img: np.ndarray, **params: Any) -&gt; np.ndarray:\n    return fmain.invert(img)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.InvertImg.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[()]:\n    return ()\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.Lambda","title":"<code>class  Lambda</code> <code>     (image=None, mask=None, keypoint=None, bbox=None, global_label=None, name=None, always_apply=None, p=1.0)                 </code>  [view source on GitHub]","text":"<p>A flexible transformation class for using user-defined transformation functions per targets. Function signature must include **kwargs to accept optional arguments like interpolation method, image size, etc:</p> <p>Parameters:</p> Name Type Description <code>image</code> <code>Callable[..., Any] | None</code> <p>Image transformation function.</p> <code>mask</code> <code>Callable[..., Any] | None</code> <p>Mask transformation function.</p> <code>keypoint</code> <code>Callable[..., Any] | None</code> <p>Keypoint transformation function.</p> <code>bbox</code> <code>Callable[..., Any] | None</code> <p>BBox transformation function.</p> <code>global_label</code> <code>Callable[..., Any] | None</code> <p>Global label transformation function.</p> <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 1.0.</p> <p>Targets</p> <p>image, mask, bboxes, keypoints, global_label</p> <p>Image types:     Any</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>class Lambda(NoOp):\n    \"\"\"A flexible transformation class for using user-defined transformation functions per targets.\n    Function signature must include **kwargs to accept optional arguments like interpolation method, image size, etc:\n\n    Args:\n        image: Image transformation function.\n        mask: Mask transformation function.\n        keypoint: Keypoint transformation function.\n        bbox: BBox transformation function.\n        global_label: Global label transformation function.\n        p: probability of applying the transform. Default: 1.0.\n\n    Targets:\n        image, mask, bboxes, keypoints, global_label\n\n    Image types:\n        Any\n\n    \"\"\"\n\n    def __init__(\n        self,\n        image: Callable[..., Any] | None = None,\n        mask: Callable[..., Any] | None = None,\n        keypoint: Callable[..., Any] | None = None,\n        bbox: Callable[..., Any] | None = None,\n        global_label: Callable[..., Any] | None = None,\n        name: str | None = None,\n        always_apply: bool | None = None,\n        p: float = 1.0,\n    ):\n        super().__init__(p, always_apply)\n\n        self.name = name\n        self.custom_apply_fns = {\n            target_name: fmain.noop for target_name in (\"image\", \"mask\", \"keypoint\", \"bbox\", \"global_label\")\n        }\n        for target_name, custom_apply_fn in {\n            \"image\": image,\n            \"mask\": mask,\n            \"keypoint\": keypoint,\n            \"bbox\": bbox,\n            \"global_label\": global_label,\n        }.items():\n            if custom_apply_fn is not None:\n                if isinstance(custom_apply_fn, LambdaType) and custom_apply_fn.__name__ == \"&lt;lambda&gt;\":\n                    warnings.warn(\n                        \"Using lambda is incompatible with multiprocessing. \"\n                        \"Consider using regular functions or partial().\",\n                        stacklevel=2,\n                    )\n\n                self.custom_apply_fns[target_name] = custom_apply_fn\n\n    def apply(self, img: np.ndarray, **params: Any) -&gt; np.ndarray:\n        fn = self.custom_apply_fns[\"image\"]\n        return fn(img, **params)\n\n    def apply_to_mask(self, mask: np.ndarray, **params: Any) -&gt; np.ndarray:\n        fn = self.custom_apply_fns[\"mask\"]\n        return fn(mask, **params)\n\n    def apply_to_bbox(self, bbox: BoxInternalType, **params: Any) -&gt; BoxInternalType:\n        fn = self.custom_apply_fns[\"bbox\"]\n        return fn(bbox, **params)\n\n    def apply_to_keypoint(self, keypoint: KeypointInternalType, **params: Any) -&gt; KeypointInternalType:\n        fn = self.custom_apply_fns[\"keypoint\"]\n        return fn(keypoint, **params)\n\n    def apply_to_global_label(self, label: np.ndarray, **params: Any) -&gt; np.ndarray:\n        fn = self.custom_apply_fns[\"global_label\"]\n        return fn(label, **params)\n\n    @classmethod\n    def is_serializable(cls) -&gt; bool:\n        return False\n\n    def to_dict_private(self) -&gt; dict[str, Any]:\n        if self.name is None:\n            msg = (\n                \"To make a Lambda transform serializable you should provide the `name` argument, \"\n                \"e.g. `Lambda(name='my_transform', image=&lt;some func&gt;, ...)`.\"\n            )\n            raise ValueError(msg)\n        return {\"__class_fullname__\": self.get_class_fullname(), \"__name__\": self.name}\n\n    def __repr__(self) -&gt; str:\n        state = {\"name\": self.name}\n        state.update(self.custom_apply_fns.items())  # type: ignore[arg-type]\n        state.update(self.get_base_init_args())\n        return f\"{self.__class__.__name__}({format_args(state)})\"\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.Lambda.__init__","title":"<code>__init__ (self, image=None, mask=None, keypoint=None, bbox=None, global_label=None, name=None, always_apply=None, p=1.0)</code>  <code>special</code>","text":"<p>Initialize self.  See help(type(self)) for accurate signature.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def __init__(\n    self,\n    image: Callable[..., Any] | None = None,\n    mask: Callable[..., Any] | None = None,\n    keypoint: Callable[..., Any] | None = None,\n    bbox: Callable[..., Any] | None = None,\n    global_label: Callable[..., Any] | None = None,\n    name: str | None = None,\n    always_apply: bool | None = None,\n    p: float = 1.0,\n):\n    super().__init__(p, always_apply)\n\n    self.name = name\n    self.custom_apply_fns = {\n        target_name: fmain.noop for target_name in (\"image\", \"mask\", \"keypoint\", \"bbox\", \"global_label\")\n    }\n    for target_name, custom_apply_fn in {\n        \"image\": image,\n        \"mask\": mask,\n        \"keypoint\": keypoint,\n        \"bbox\": bbox,\n        \"global_label\": global_label,\n    }.items():\n        if custom_apply_fn is not None:\n            if isinstance(custom_apply_fn, LambdaType) and custom_apply_fn.__name__ == \"&lt;lambda&gt;\":\n                warnings.warn(\n                    \"Using lambda is incompatible with multiprocessing. \"\n                    \"Consider using regular functions or partial().\",\n                    stacklevel=2,\n                )\n\n            self.custom_apply_fns[target_name] = custom_apply_fn\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.Lambda.apply","title":"<code>apply (self, img, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def apply(self, img: np.ndarray, **params: Any) -&gt; np.ndarray:\n    fn = self.custom_apply_fns[\"image\"]\n    return fn(img, **params)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.Morphological","title":"<code>class  Morphological</code> <code>     (scale=(2, 3), operation='dilation', always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Apply a morphological operation (dilation or erosion) to an image, with particular value for enhancing document scans.</p> <p>Morphological operations modify the structure of the image. Dilation expands the white (foreground) regions in a binary or grayscale image, while erosion shrinks them. These operations are beneficial in document processing, for example: - Dilation helps in closing up gaps within text or making thin lines thicker,     enhancing legibility for OCR (Optical Character Recognition). - Erosion can remove small white noise and detach connected objects,     making the structure of larger objects more pronounced.</p> <p>Parameters:</p> Name Type Description <code>scale</code> <code>int or tuple/list of int</code> <p>Specifies the size of the structuring element (kernel) used for the operation. - If an integer is provided, a square kernel of that size will be used. - If a tuple or list is provided, it should contain two integers representing the minimum     and maximum sizes for the dilation kernel.</p> <code>operation</code> <code>str</code> <p>The morphological operation to apply. Options are 'dilation' or 'erosion'. Default is 'dilation'.</p> <code>p</code> <code>float</code> <p>The probability of applying this transformation. Default is 0.5.</p> <p>Targets</p> <p>image, mask</p> <p>Image types:     uint8, float32</p> <p>Reference</p> <p>https://github.com/facebookresearch/nougat</p> <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; import albumentations as A\n&gt;&gt;&gt; transform = A.Compose([\n&gt;&gt;&gt;     A.Morphological(scale=(2, 3), operation='dilation', p=0.5)\n&gt;&gt;&gt; ])\n&gt;&gt;&gt; image = transform(image=image)[\"image\"]\n</code></pre> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>class Morphological(DualTransform):\n    \"\"\"Apply a morphological operation (dilation or erosion) to an image,\n    with particular value for enhancing document scans.\n\n    Morphological operations modify the structure of the image.\n    Dilation expands the white (foreground) regions in a binary or grayscale image, while erosion shrinks them.\n    These operations are beneficial in document processing, for example:\n    - Dilation helps in closing up gaps within text or making thin lines thicker,\n        enhancing legibility for OCR (Optical Character Recognition).\n    - Erosion can remove small white noise and detach connected objects,\n        making the structure of larger objects more pronounced.\n\n    Args:\n        scale (int or tuple/list of int): Specifies the size of the structuring element (kernel) used for the operation.\n            - If an integer is provided, a square kernel of that size will be used.\n            - If a tuple or list is provided, it should contain two integers representing the minimum\n                and maximum sizes for the dilation kernel.\n        operation (str, optional): The morphological operation to apply. Options are 'dilation' or 'erosion'.\n            Default is 'dilation'.\n        p (float, optional): The probability of applying this transformation. Default is 0.5.\n\n    Targets:\n        image, mask\n\n    Image types:\n        uint8, float32\n\n    Reference:\n        https://github.com/facebookresearch/nougat\n\n    Example:\n        &gt;&gt;&gt; import albumentations as A\n        &gt;&gt;&gt; transform = A.Compose([\n        &gt;&gt;&gt;     A.Morphological(scale=(2, 3), operation='dilation', p=0.5)\n        &gt;&gt;&gt; ])\n        &gt;&gt;&gt; image = transform(image=image)[\"image\"]\n    \"\"\"\n\n    _targets = (Targets.IMAGE, Targets.MASK)\n\n    class InitSchema(BaseTransformInitSchema):\n        scale: OnePlusIntRangeType = (2, 3)\n        operation: MorphologyMode = \"dilation\"\n\n    def __init__(\n        self,\n        scale: ScaleIntType = (2, 3),\n        operation: MorphologyMode = \"dilation\",\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p, always_apply)\n        self.scale = cast(Tuple[int, int], scale)\n        self.operation = operation\n\n    def apply(self, img: np.ndarray, kernel: tuple[int, int], **params: Any) -&gt; np.ndarray:\n        return fmain.morphology(img, kernel, self.operation)\n\n    def apply_to_mask(self, mask: np.ndarray, kernel: tuple[int, int], **params: Any) -&gt; np.ndarray:\n        return fmain.morphology(mask, kernel, self.operation)\n\n    def get_params(self) -&gt; dict[str, float]:\n        return {\n            \"kernel\": cv2.getStructuringElement(cv2.MORPH_ELLIPSE, self.scale),\n        }\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return (\"scale\", \"operation\")\n\n    @property\n    def targets(self) -&gt; dict[str, Callable[..., Any]]:\n        return {\n            \"image\": self.apply,\n            \"mask\": self.apply_to_mask,\n            \"masks\": self.apply_to_masks,\n        }\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.Morphological.apply","title":"<code>apply (self, img, kernel, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def apply(self, img: np.ndarray, kernel: tuple[int, int], **params: Any) -&gt; np.ndarray:\n    return fmain.morphology(img, kernel, self.operation)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.Morphological.get_params","title":"<code>get_params (self)</code>","text":"<p>Returns parameters independent of input.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_params(self) -&gt; dict[str, float]:\n    return {\n        \"kernel\": cv2.getStructuringElement(cv2.MORPH_ELLIPSE, self.scale),\n    }\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.Morphological.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return (\"scale\", \"operation\")\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.MultiplicativeNoise","title":"<code>class  MultiplicativeNoise</code> <code>     (multiplier=(0.9, 1.1), per_channel=None, elementwise=False, always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Multiply image by a random number or array of numbers.</p> <p>Parameters:</p> Name Type Description <code>multiplier</code> <code>ScaleFloatType</code> <p>If a single float, the image will be multiplied by this number. If a tuple of floats, the multiplier will be a random number in the range <code>[multiplier[0], multiplier[1])</code>. Default: (0.9, 1.1).</p> <code>elementwise</code> <code>bool</code> <p>If <code>False</code>, multiply all pixels in the image by a single random value sampled once. If <code>True</code>, multiply image pixels by values that are pixelwise randomly sampled. Default: False.</p> <code>p</code> <code>float</code> <p>Probability of applying the transform. Default: 0.5.</p> <p>Targets</p> <p>image</p> <p>Image types:     uint8, np.float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>class MultiplicativeNoise(ImageOnlyTransform):\n    \"\"\"Multiply image by a random number or array of numbers.\n\n    Args:\n        multiplier: If a single float, the image will be multiplied by this number.\n            If a tuple of floats, the multiplier will be a random number in the range `[multiplier[0], multiplier[1])`.\n            Default: (0.9, 1.1).\n        elementwise: If `False`, multiply all pixels in the image by a single random value sampled once.\n            If `True`, multiply image pixels by values that are pixelwise randomly sampled. Default: False.\n        p: Probability of applying the transform. Default: 0.5.\n\n    Targets:\n        image\n\n    Image types:\n        uint8, np.float32\n\n    \"\"\"\n\n    class InitSchema(BaseTransformInitSchema):\n        multiplier: Annotated[tuple[float, float], AfterValidator(check_0plus), AfterValidator(nondecreasing)] = (\n            0.9,\n            1.1,\n        )\n        per_channel: bool | None = Field(\n            default=False,\n            description=\"Apply multiplier per channel.\",\n            deprecated=\"Does not have any effect. Will be removed in future releases.\",\n        )\n        elementwise: bool = Field(default=False, description=\"Apply multiplier element-wise to pixels.\")\n\n    def __init__(\n        self,\n        multiplier: ScaleFloatType = (0.9, 1.1),\n        per_channel: bool | None = None,\n        elementwise: bool = False,\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p=p, always_apply=always_apply)\n        self.multiplier = cast(Tuple[float, float], multiplier)\n        self.elementwise = elementwise\n\n    def apply(\n        self,\n        img: np.ndarray,\n        multiplier: float | np.ndarray,\n        **kwargs: Any,\n    ) -&gt; np.ndarray:\n        return multiply(img, multiplier)\n\n    def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n        if self.multiplier[0] == self.multiplier[1]:\n            return {\"multiplier\": self.multiplier[0]}\n\n        img = data[\"image\"] if \"image\" in data else data[\"images\"][0]\n        shape = img.shape if self.elementwise else get_num_channels(img)\n\n        multiplier = random_utils.uniform(self.multiplier[0], self.multiplier[1], shape).astype(np.float32)\n\n        return {\"multiplier\": multiplier}\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return \"multiplier\", \"elementwise\"\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.MultiplicativeNoise.apply","title":"<code>apply (self, img, multiplier, **kwargs)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def apply(\n    self,\n    img: np.ndarray,\n    multiplier: float | np.ndarray,\n    **kwargs: Any,\n) -&gt; np.ndarray:\n    return multiply(img, multiplier)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.MultiplicativeNoise.get_params_dependent_on_data","title":"<code>get_params_dependent_on_data (self, params, data)</code>","text":"<p>Returns parameters dependent on input.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n    if self.multiplier[0] == self.multiplier[1]:\n        return {\"multiplier\": self.multiplier[0]}\n\n    img = data[\"image\"] if \"image\" in data else data[\"images\"][0]\n    shape = img.shape if self.elementwise else get_num_channels(img)\n\n    multiplier = random_utils.uniform(self.multiplier[0], self.multiplier[1], shape).astype(np.float32)\n\n    return {\"multiplier\": multiplier}\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.MultiplicativeNoise.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return \"multiplier\", \"elementwise\"\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.Normalize","title":"<code>class  Normalize</code> <code>     (mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, normalization='standard', always_apply=None, p=1.0)                 </code>  [view source on GitHub]","text":"<p>Applies various normalization techniques to an image. The specific normalization technique can be selected     with the <code>normalization</code> parameter.</p> <p>Standard normalization is applied using the formula:     <code>img = (img - mean * max_pixel_value) / (std * max_pixel_value)</code>.     Other normalization techniques adjust the image based on global or per-channel statistics,     or scale pixel values to a specified range.</p> <p>Parameters:</p> Name Type Description <code>mean</code> <code>ColorType | None</code> <p>Mean values for standard normalization. For \"standard\" normalization, the default values are ImageNet mean values: (0.485, 0.456, 0.406). For \"inception\" normalization, use mean values of (0.5, 0.5, 0.5).</p> <code>std</code> <code>ColorType | None</code> <p>Standard deviation values for standard normalization. For \"standard\" normalization, the default values are ImageNet standard deviation :(0.229, 0.224, 0.225). For \"inception\" normalization, use standard deviation values of (0.5, 0.5, 0.5).</p> <code>max_pixel_value</code> <code>float | None</code> <p>Maximum possible pixel value, used for scaling in standard normalization. Defaults to 255.0.</p> <code>normalization</code> <code>Literal[\"standard\", \"image\", \"image_per_channel\", \"min_max\", \"min_max_per_channel\", \"inception\"]) Specifies the normalization technique to apply. Defaults to \"standard\". - \"standard\"</code> <p>Applies the formula <code>(img - mean * max_pixel_value) / (std * max_pixel_value)</code>.     The default mean and std are based on ImageNet. - \"image\": Normalizes the whole image based on its global mean and standard deviation. - \"image_per_channel\": Normalizes the image per channel based on each channel's mean and standard deviation. - \"min_max\": Scales the image pixel values to a [0, 1] range based on the global     minimum and maximum pixel values. - \"min_max_per_channel\": Scales each channel of the image pixel values to a [0, 1]     range based on the per-channel minimum and maximum pixel values.</p> <code>p</code> <code>float</code> <p>Probability of applying the transform. Defaults to 1.0.</p> <p>Targets</p> <p>image</p> <p>Image types:     uint8, float32</p> <p>Note</p> <p>For \"standard\" normalization, <code>mean</code>, <code>std</code>, and <code>max_pixel_value</code> must be provided. For other normalization types, these parameters are ignored.</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>class Normalize(ImageOnlyTransform):\n    \"\"\"Applies various normalization techniques to an image. The specific normalization technique can be selected\n        with the `normalization` parameter.\n\n    Standard normalization is applied using the formula:\n        `img = (img - mean * max_pixel_value) / (std * max_pixel_value)`.\n        Other normalization techniques adjust the image based on global or per-channel statistics,\n        or scale pixel values to a specified range.\n\n    Args:\n        mean (ColorType | None): Mean values for standard normalization.\n            For \"standard\" normalization, the default values are ImageNet mean values: (0.485, 0.456, 0.406).\n            For \"inception\" normalization, use mean values of (0.5, 0.5, 0.5).\n        std (ColorType | None): Standard deviation values for standard normalization.\n            For \"standard\" normalization, the default values are ImageNet standard deviation :(0.229, 0.224, 0.225).\n            For \"inception\" normalization, use standard deviation values of (0.5, 0.5, 0.5).\n        max_pixel_value (float | None): Maximum possible pixel value, used for scaling in standard normalization.\n            Defaults to 255.0.\n        normalization (Literal[\"standard\", \"image\", \"image_per_channel\", \"min_max\", \"min_max_per_channel\", \"inception\"])\n            Specifies the normalization technique to apply. Defaults to \"standard\".\n            - \"standard\": Applies the formula `(img - mean * max_pixel_value) / (std * max_pixel_value)`.\n                The default mean and std are based on ImageNet.\n            - \"image\": Normalizes the whole image based on its global mean and standard deviation.\n            - \"image_per_channel\": Normalizes the image per channel based on each channel's mean and standard deviation.\n            - \"min_max\": Scales the image pixel values to a [0, 1] range based on the global\n                minimum and maximum pixel values.\n            - \"min_max_per_channel\": Scales each channel of the image pixel values to a [0, 1]\n                range based on the per-channel minimum and maximum pixel values.\n\n        p (float): Probability of applying the transform. Defaults to 1.0.\n\n    Targets:\n        image\n\n    Image types:\n        uint8, float32\n\n    Note:\n        For \"standard\" normalization, `mean`, `std`, and `max_pixel_value` must be provided.\n        For other normalization types, these parameters are ignored.\n    \"\"\"\n\n    class InitSchema(BaseTransformInitSchema):\n        mean: ColorType | None = Field(\n            default=(0.485, 0.456, 0.406),\n            description=\"Mean values for normalization, defaulting to ImageNet mean values.\",\n        )\n        std: ColorType | None = Field(\n            default=(0.229, 0.224, 0.225),\n            description=\"Standard deviation values for normalization, defaulting to ImageNet std values.\",\n        )\n        max_pixel_value: float | None = Field(default=255.0, description=\"Maximum possible pixel value.\")\n        normalization: Literal[\n            \"standard\",\n            \"image\",\n            \"image_per_channel\",\n            \"min_max\",\n            \"min_max_per_channel\",\n        ] = \"standard\"\n        p: ProbabilityType = 1\n\n        @model_validator(mode=\"after\")\n        def validate_normalization(self) -&gt; Self:\n            if (\n                self.mean is None\n                or self.std is None\n                or self.max_pixel_value is None\n                and self.normalization == \"standard\"\n            ):\n                raise ValueError(\"mean, std, and max_pixel_value must be provided for standard normalization.\")\n            return self\n\n    def __init__(\n        self,\n        mean: ColorType | None = (0.485, 0.456, 0.406),\n        std: ColorType | None = (0.229, 0.224, 0.225),\n        max_pixel_value: float | None = 255.0,\n        normalization: Literal[\"standard\", \"image\", \"image_per_channel\", \"min_max\", \"min_max_per_channel\"] = \"standard\",\n        always_apply: bool | None = None,\n        p: float = 1.0,\n    ):\n        super().__init__(p=p, always_apply=always_apply)\n        self.mean = mean\n        self.mean_np = np.array(mean, dtype=np.float32) * max_pixel_value\n        self.std = std\n        self.denominator = np.reciprocal(np.array(std, dtype=np.float32) * max_pixel_value)\n        self.max_pixel_value = max_pixel_value\n        if normalization not in {\"standard\", \"image\", \"image_per_channel\", \"min_max\", \"min_max_per_channel\"}:\n            raise ValueError(\n                f\"Error during Normalize initialization. Unknown normalization type: {normalization}\",\n            )\n        self.normalization = normalization\n\n    def apply(self, img: np.ndarray, **params: Any) -&gt; np.ndarray:\n        if self.normalization == \"standard\":\n            return normalize(\n                img,\n                self.mean_np,\n                self.denominator,\n            )\n        return normalize_per_image(img, self.normalization)\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return \"mean\", \"std\", \"max_pixel_value\", \"normalization\"\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.Normalize.apply","title":"<code>apply (self, img, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def apply(self, img: np.ndarray, **params: Any) -&gt; np.ndarray:\n    if self.normalization == \"standard\":\n        return normalize(\n            img,\n            self.mean_np,\n            self.denominator,\n        )\n    return normalize_per_image(img, self.normalization)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.Normalize.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return \"mean\", \"std\", \"max_pixel_value\", \"normalization\"\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.PixelDropout","title":"<code>class  PixelDropout</code> <code>     (dropout_prob=0.01, per_channel=False, drop_value=0, mask_drop_value=None, always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Set pixels to 0 with some probability.</p> <p>Parameters:</p> Name Type Description <code>dropout_prob</code> <code>float</code> <p>pixel drop probability. Default: 0.01</p> <code>per_channel</code> <code>bool</code> <p>if set to <code>True</code> drop mask will be sampled for each channel, otherwise the same mask will be sampled for all channels. Default: False</p> <code>drop_value</code> <code>number or sequence of numbers or None</code> <p>Value that will be set in dropped place. If set to None value will be sampled randomly, default ranges will be used:     - uint8 - [0, 255]     - uint16 - [0, 65535]     - uint32 - [0, 4294967295]     - float, double - [0, 1] Default: 0</p> <code>mask_drop_value</code> <code>number or sequence of numbers or None</code> <p>Value that will be set in dropped place in masks. If set to None masks will be unchanged. Default: 0</p> <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 0.5.</p> <p>Targets</p> <p>image, mask</p> <p>Image types:     any</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>class PixelDropout(DualTransform):\n    \"\"\"Set pixels to 0 with some probability.\n\n    Args:\n        dropout_prob (float): pixel drop probability. Default: 0.01\n        per_channel (bool): if set to `True` drop mask will be sampled for each channel,\n            otherwise the same mask will be sampled for all channels. Default: False\n        drop_value (number or sequence of numbers or None): Value that will be set in dropped place.\n            If set to None value will be sampled randomly, default ranges will be used:\n                - uint8 - [0, 255]\n                - uint16 - [0, 65535]\n                - uint32 - [0, 4294967295]\n                - float, double - [0, 1]\n            Default: 0\n        mask_drop_value (number or sequence of numbers or None): Value that will be set in dropped place in masks.\n            If set to None masks will be unchanged. Default: 0\n        p (float): probability of applying the transform. Default: 0.5.\n\n    Targets:\n        image, mask\n    Image types:\n        any\n\n    \"\"\"\n\n    class InitSchema(BaseTransformInitSchema):\n        dropout_prob: ProbabilityType = 0.01\n        per_channel: bool = Field(default=False, description=\"Sample drop mask per channel.\")\n        drop_value: ScaleFloatType | None = Field(\n            default=0,\n            description=\"Value to set in dropped pixels. None for random sampling.\",\n        )\n        mask_drop_value: ScaleFloatType | None = Field(\n            default=None,\n            description=\"Value to set in dropped pixels in masks. None to leave masks unchanged.\",\n        )\n\n        @model_validator(mode=\"after\")\n        def validate_mask_drop_value(self) -&gt; Self:\n            if self.mask_drop_value is not None and self.per_channel:\n                msg = \"PixelDropout supports mask only with per_channel=False.\"\n                raise ValueError(msg)\n            return self\n\n    _targets = (Targets.IMAGE, Targets.MASK)\n\n    def __init__(\n        self,\n        dropout_prob: float = 0.01,\n        per_channel: bool = False,\n        drop_value: ScaleFloatType | None = 0,\n        mask_drop_value: ScaleFloatType | None = None,\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p=p, always_apply=always_apply)\n        self.dropout_prob = dropout_prob\n        self.per_channel = per_channel\n        self.drop_value = drop_value\n        self.mask_drop_value = mask_drop_value\n\n    def apply(\n        self,\n        img: np.ndarray,\n        drop_mask: np.ndarray,\n        drop_value: float | Sequence[float],\n        **params: Any,\n    ) -&gt; np.ndarray:\n        return fmain.pixel_dropout(img, drop_mask, drop_value)\n\n    def apply_to_mask(self, mask: np.ndarray, drop_mask: np.ndarray, **params: Any) -&gt; np.ndarray:\n        if self.mask_drop_value is None:\n            return mask\n\n        if mask.ndim == MONO_CHANNEL_DIMENSIONS:\n            drop_mask = np.squeeze(drop_mask)\n\n        return fmain.pixel_dropout(mask, drop_mask, self.mask_drop_value)\n\n    def apply_to_bbox(self, bbox: BoxInternalType, **params: Any) -&gt; BoxInternalType:\n        return bbox\n\n    def apply_to_keypoint(self, keypoint: KeypointInternalType, **params: Any) -&gt; KeypointInternalType:\n        return keypoint\n\n    def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n        img = data[\"image\"] if \"image\" in data else data[\"images\"][0]\n        shape = img.shape if self.per_channel else img.shape[:2]\n\n        rnd = np.random.RandomState(random.randint(0, 1 &lt;&lt; 31))\n        # Use choice to create boolean matrix, if we will use binomial after that we will need type conversion\n        drop_mask = rnd.choice([True, False], shape, p=[self.dropout_prob, 1 - self.dropout_prob])\n\n        drop_value: float | Sequence[float] | np.ndarray\n        if drop_mask.ndim != img.ndim:\n            drop_mask = np.expand_dims(drop_mask, -1)\n        if self.drop_value is None:\n            drop_shape = 1 if is_grayscale_image(img) else int(img.shape[-1])\n\n            if img.dtype in (np.uint8, np.uint16, np.uint32):\n                drop_value = rnd.randint(0, int(MAX_VALUES_BY_DTYPE[img.dtype]), drop_shape, img.dtype)\n            elif img.dtype in [np.float32, np.double]:\n                drop_value = rnd.uniform(0, 1, drop_shape).astype(img.dtype)\n            else:\n                raise ValueError(f\"Unsupported dtype: {img.dtype}\")\n        else:\n            drop_value = self.drop_value\n\n        return {\"drop_mask\": drop_mask, \"drop_value\": drop_value}\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, str, str, str]:\n        return (\"dropout_prob\", \"per_channel\", \"drop_value\", \"mask_drop_value\")\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.PixelDropout.apply","title":"<code>apply (self, img, drop_mask, drop_value, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def apply(\n    self,\n    img: np.ndarray,\n    drop_mask: np.ndarray,\n    drop_value: float | Sequence[float],\n    **params: Any,\n) -&gt; np.ndarray:\n    return fmain.pixel_dropout(img, drop_mask, drop_value)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.PixelDropout.get_params_dependent_on_data","title":"<code>get_params_dependent_on_data (self, params, data)</code>","text":"<p>Returns parameters dependent on input.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n    img = data[\"image\"] if \"image\" in data else data[\"images\"][0]\n    shape = img.shape if self.per_channel else img.shape[:2]\n\n    rnd = np.random.RandomState(random.randint(0, 1 &lt;&lt; 31))\n    # Use choice to create boolean matrix, if we will use binomial after that we will need type conversion\n    drop_mask = rnd.choice([True, False], shape, p=[self.dropout_prob, 1 - self.dropout_prob])\n\n    drop_value: float | Sequence[float] | np.ndarray\n    if drop_mask.ndim != img.ndim:\n        drop_mask = np.expand_dims(drop_mask, -1)\n    if self.drop_value is None:\n        drop_shape = 1 if is_grayscale_image(img) else int(img.shape[-1])\n\n        if img.dtype in (np.uint8, np.uint16, np.uint32):\n            drop_value = rnd.randint(0, int(MAX_VALUES_BY_DTYPE[img.dtype]), drop_shape, img.dtype)\n        elif img.dtype in [np.float32, np.double]:\n            drop_value = rnd.uniform(0, 1, drop_shape).astype(img.dtype)\n        else:\n            raise ValueError(f\"Unsupported dtype: {img.dtype}\")\n    else:\n        drop_value = self.drop_value\n\n    return {\"drop_mask\": drop_mask, \"drop_value\": drop_value}\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.PixelDropout.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, str, str, str]:\n    return (\"dropout_prob\", \"per_channel\", \"drop_value\", \"mask_drop_value\")\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.PlanckianJitter","title":"<code>class  PlanckianJitter</code> <code>     (mode='blackbody', temperature_limit=None, sampling_method='uniform', always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Randomly jitter the image illuminant along the Planckian locus.</p> <p>Physics-based color augmentation creates realistic variations in chromaticity, simulating illumination changes in a scene.</p> <p>Parameters:</p> Name Type Description <code>mode</code> <code>Literal[\"blackbody\", \"cied\"]</code> <p>The mode of the transformation. <code>blackbody</code> simulates blackbody radiation, and <code>cied</code> uses the CIED illuminant series.</p> <code>temperature_limit</code> <code>tuple[int, int]</code> <p>Temperature range to sample from. For <code>blackbody</code> mode, the range should be within <code>[3000K, 15000K]</code>. For \"cied\" mode, the range should be within <code>[4000K, 15000K]</code>. Range should include white temperature <code>6000</code> Higher temperatures produce cooler (bluish) images. If not defined, it defaults to: - <code>[3000, 15000]</code> for <code>blackbody</code> mode - <code>[4000, 15000]</code> for <code>cied</code> mode</p> <code>p</code> <code>float</code> <p>Probability of applying the transform. Defaults to 0.5.</p> <code>sampling_method</code> <code>Literal[\"uniform\", \"gaussian\"]</code> <p>Method to sample the temperature. \"uniform\" samples uniformly across the range, while \"gaussian\" samples from a Gaussian distribution.</p> <code>p</code> <code>float</code> <p>Probability of applying the transform. Defaults to 0.5.</p> <p>If <code>temperature_limit</code> is not defined, it defaults to:     - <code>[3000, 15000]</code> for <code>blackbody</code> mode     - <code>[4000, 15000]</code> for <code>cied</code> mode</p> <p>Targets</p> <p>image</p> <p>Image types:     uint8, float32</p> <p>References</p> <ul> <li>https://github.com/TheZino/PlanckianJitter</li> <li>https://arxiv.org/pdf/2202.07993.pdf</li> </ul> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>class PlanckianJitter(ImageOnlyTransform):\n    r\"\"\"Randomly jitter the image illuminant along the Planckian locus.\n\n    Physics-based color augmentation creates realistic variations in chromaticity, simulating illumination changes\n    in a scene.\n\n    Args:\n        mode (Literal[\"blackbody\", \"cied\"]): The mode of the transformation. `blackbody` simulates blackbody radiation,\n            and `cied` uses the CIED illuminant series.\n        temperature_limit (tuple[int, int]): Temperature range to sample from. For `blackbody` mode, the range should\n            be within `[3000K, 15000K]`. For \"cied\" mode, the range should be within `[4000K, 15000K]`. Range should\n            include white temperature `6000`\n            Higher temperatures produce cooler (bluish) images. If not defined, it defaults to:\n            - `[3000, 15000]` for `blackbody` mode\n            - `[4000, 15000]` for `cied` mode\n        p (float): Probability of applying the transform. Defaults to 0.5.\n        sampling_method (Literal[\"uniform\", \"gaussian\"]): Method to sample the temperature.\n            \"uniform\" samples uniformly across the range, while \"gaussian\" samples from a Gaussian distribution.\n        p (float): Probability of applying the transform. Defaults to 0.5.\n\n    If `temperature_limit` is not defined, it defaults to:\n        - `[3000, 15000]` for `blackbody` mode\n        - `[4000, 15000]` for `cied` mode\n\n    Targets:\n        image\n\n    Image types:\n        uint8, float32\n\n    References:\n        - https://github.com/TheZino/PlanckianJitter\n        - https://arxiv.org/pdf/2202.07993.pdf\n\n    \"\"\"\n\n    class InitSchema(BaseTransformInitSchema):\n        mode: PlanckianJitterMode = \"blackbody\"\n        temperature_limit: Annotated[tuple[int, int], AfterValidator(nondecreasing)] | None = None\n        sampling_method: Literal[\"uniform\", \"gaussian\"] = \"uniform\"\n\n        @model_validator(mode=\"after\")\n        def validate_temperature(self) -&gt; Self:\n            max_temp = int(PLANKIAN_JITTER_CONST[\"MAX_TEMP\"])\n\n            if self.temperature_limit is None:\n                if self.mode == \"blackbody\":\n                    self.temperature_limit = int(PLANKIAN_JITTER_CONST[\"MIN_BLACKBODY_TEMP\"]), max_temp\n                elif self.mode == \"cied\":\n                    self.temperature_limit = int(PLANKIAN_JITTER_CONST[\"MIN_CIED_TEMP\"]), max_temp\n            else:\n                if self.mode == \"blackbody\" and (\n                    min(self.temperature_limit) &lt; PLANKIAN_JITTER_CONST[\"MIN_BLACKBODY_TEMP\"]\n                    or max(self.temperature_limit) &gt; max_temp\n                ):\n                    raise ValueError(\"Temperature limits for blackbody should be in [3000, 15000] range\")\n                if self.mode == \"cied\" and (\n                    min(self.temperature_limit) &lt; PLANKIAN_JITTER_CONST[\"MIN_CIED_TEMP\"]\n                    or max(self.temperature_limit) &gt; max_temp\n                ):\n                    raise ValueError(\"Temperature limits for CIED should be in [4000, 15000] range\")\n\n                if not self.temperature_limit[0] &lt;= PLANKIAN_JITTER_CONST[\"WHITE_TEMP\"] &lt;= self.temperature_limit[1]:\n                    raise ValueError(\"White temperature should be within the temperature limits\")\n\n            return self\n\n    def __init__(\n        self,\n        mode: PlanckianJitterMode = \"blackbody\",\n        temperature_limit: tuple[int, int] | None = None,\n        sampling_method: Literal[\"uniform\", \"gaussian\"] = \"uniform\",\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ) -&gt; None:\n        super().__init__(p=p, always_apply=always_apply)\n\n        self.mode = mode\n        self.temperature_limit = cast(Tuple[int, int], temperature_limit)\n        self.sampling_method = sampling_method\n\n    def apply(self, img: np.ndarray, temperature: int, **params: Any) -&gt; np.ndarray:\n        if not is_rgb_image(img):\n            raise TypeError(\"PlanckianJitter transformation expects 3-channel images.\")\n        return fmain.planckian_jitter(img, temperature, mode=self.mode)\n\n    def get_params(self) -&gt; dict[str, Any]:\n        sampling_prob_boundary = PLANKIAN_JITTER_CONST[\"SAMPLING_TEMP_PROB\"]\n        sampling_temp_boundary = PLANKIAN_JITTER_CONST[\"WHITE_TEMP\"]\n\n        if self.sampling_method == \"uniform\":\n            # Split into 2 cases to avoid selecting cold temperatures (&gt;6000) too often\n            if random.random() &lt; sampling_prob_boundary:\n                temperature = (\n                    random.uniform(\n                        self.temperature_limit[0],\n                        sampling_temp_boundary,\n                    ),\n                )\n            else:\n                temperature = (\n                    random.uniform(\n                        sampling_temp_boundary,\n                        self.temperature_limit[1],\n                    ),\n                )\n        elif self.sampling_method == \"gaussian\":\n            # Sample values from asymmetric gaussian distribution\n            if random.random() &lt; sampling_prob_boundary:\n                # Left side\n                shift = np.abs(\n                    random.gauss(\n                        0,\n                        np.abs(sampling_temp_boundary - self.temperature_limit[0]) / 3,\n                    ),\n                )\n            else:\n                # Right side\n                shift = -np.abs(\n                    random.gauss(\n                        0,\n                        np.abs(self.temperature_limit[1] - sampling_temp_boundary) / 3,\n                    ),\n                )\n\n            temperature = sampling_temp_boundary - shift\n        else:\n            raise ValueError(f\"Unknown sampling method: {self.sampling_method}\")\n\n        return {\"temperature\": int(np.clip(temperature, self.temperature_limit[0], self.temperature_limit[1]))}\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return \"mode\", \"temperature_limit\", \"sampling_method\"\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.PlanckianJitter.apply","title":"<code>apply (self, img, temperature, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def apply(self, img: np.ndarray, temperature: int, **params: Any) -&gt; np.ndarray:\n    if not is_rgb_image(img):\n        raise TypeError(\"PlanckianJitter transformation expects 3-channel images.\")\n    return fmain.planckian_jitter(img, temperature, mode=self.mode)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.PlanckianJitter.get_params","title":"<code>get_params (self)</code>","text":"<p>Returns parameters independent of input.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_params(self) -&gt; dict[str, Any]:\n    sampling_prob_boundary = PLANKIAN_JITTER_CONST[\"SAMPLING_TEMP_PROB\"]\n    sampling_temp_boundary = PLANKIAN_JITTER_CONST[\"WHITE_TEMP\"]\n\n    if self.sampling_method == \"uniform\":\n        # Split into 2 cases to avoid selecting cold temperatures (&gt;6000) too often\n        if random.random() &lt; sampling_prob_boundary:\n            temperature = (\n                random.uniform(\n                    self.temperature_limit[0],\n                    sampling_temp_boundary,\n                ),\n            )\n        else:\n            temperature = (\n                random.uniform(\n                    sampling_temp_boundary,\n                    self.temperature_limit[1],\n                ),\n            )\n    elif self.sampling_method == \"gaussian\":\n        # Sample values from asymmetric gaussian distribution\n        if random.random() &lt; sampling_prob_boundary:\n            # Left side\n            shift = np.abs(\n                random.gauss(\n                    0,\n                    np.abs(sampling_temp_boundary - self.temperature_limit[0]) / 3,\n                ),\n            )\n        else:\n            # Right side\n            shift = -np.abs(\n                random.gauss(\n                    0,\n                    np.abs(self.temperature_limit[1] - sampling_temp_boundary) / 3,\n                ),\n            )\n\n        temperature = sampling_temp_boundary - shift\n    else:\n        raise ValueError(f\"Unknown sampling method: {self.sampling_method}\")\n\n    return {\"temperature\": int(np.clip(temperature, self.temperature_limit[0], self.temperature_limit[1]))}\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.PlanckianJitter.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return \"mode\", \"temperature_limit\", \"sampling_method\"\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.Posterize","title":"<code>class  Posterize</code> <code>     (num_bits=4, always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Reduce the number of bits for each color channel.</p> <p>Parameters:</p> Name Type Description <code>num_bits</code> <code>int, int) or int,       or list of ints [r, g, b],       or list of ints [[r1, r1], [g1, g2], [b1, b2]]</code> <p>number of high bits. If num_bits is a single value, the range will be [num_bits, num_bits]. Must be in range [0, 8]. Default: 4.</p> <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 0.5.</p> <p>Targets: image</p> <p>Image types:     uint8</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>class Posterize(ImageOnlyTransform):\n    \"\"\"Reduce the number of bits for each color channel.\n\n    Args:\n        num_bits ((int, int) or int,\n                  or list of ints [r, g, b],\n                  or list of ints [[r1, r1], [g1, g2], [b1, b2]]): number of high bits.\n            If num_bits is a single value, the range will be [num_bits, num_bits].\n            Must be in range [0, 8]. Default: 4.\n        p: probability of applying the transform. Default: 0.5.\n\n    Targets:\n    image\n\n    Image types:\n        uint8\n\n    \"\"\"\n\n    class InitSchema(BaseTransformInitSchema):\n        num_bits: Annotated[\n            int | tuple[int, int] | tuple[int, int, int],\n            Field(default=4, description=\"Number of high bits\"),\n        ]\n\n        @field_validator(\"num_bits\")\n        @classmethod\n        def validate_num_bits(cls, num_bits: Any) -&gt; tuple[int, int] | list[tuple[int, int]]:\n            if isinstance(num_bits, int):\n                return cast(Tuple[int, int], to_tuple(num_bits, num_bits))\n            if isinstance(num_bits, Sequence) and len(num_bits) == NUM_BITS_ARRAY_LENGTH:\n                return [cast(Tuple[int, int], to_tuple(i, 0)) for i in num_bits]\n            return cast(Tuple[int, int], to_tuple(num_bits, 0))\n\n    def __init__(\n        self,\n        num_bits: int | tuple[int, int] | tuple[int, int, int] = 4,\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p=p, always_apply=always_apply)\n        self.num_bits = cast(Union[Tuple[int, ...], List[Tuple[int, ...]]], num_bits)\n\n    def apply(self, img: np.ndarray, num_bits: int, **params: Any) -&gt; np.ndarray:\n        return fmain.posterize(img, num_bits)\n\n    def get_params(self) -&gt; dict[str, Any]:\n        if len(self.num_bits) == NUM_BITS_ARRAY_LENGTH:\n            return {\"num_bits\": [random.randint(int(i[0]), int(i[1])) for i in self.num_bits]}  # type: ignore[index]\n        num_bits = self.num_bits\n        return {\"num_bits\": random.randint(int(num_bits[0]), int(num_bits[1]))}  # type: ignore[arg-type]\n\n    def get_transform_init_args_names(self) -&gt; tuple[str]:\n        return (\"num_bits\",)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.Posterize.apply","title":"<code>apply (self, img, num_bits, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def apply(self, img: np.ndarray, num_bits: int, **params: Any) -&gt; np.ndarray:\n    return fmain.posterize(img, num_bits)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.Posterize.get_params","title":"<code>get_params (self)</code>","text":"<p>Returns parameters independent of input.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_params(self) -&gt; dict[str, Any]:\n    if len(self.num_bits) == NUM_BITS_ARRAY_LENGTH:\n        return {\"num_bits\": [random.randint(int(i[0]), int(i[1])) for i in self.num_bits]}  # type: ignore[index]\n    num_bits = self.num_bits\n    return {\"num_bits\": random.randint(int(num_bits[0]), int(num_bits[1]))}  # type: ignore[arg-type]\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.Posterize.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str]:\n    return (\"num_bits\",)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.RGBShift","title":"<code>class  RGBShift</code> <code>     (r_shift_limit=(-20, 20), g_shift_limit=(-20, 20), b_shift_limit=(-20, 20), always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Randomly shift values for each channel of the input RGB image.</p> <p>Parameters:</p> Name Type Description <code>r_shift_limit</code> <code>ScaleIntType</code> <p>range for changing values for the red channel. If r_shift_limit is a single int, the range will be (-r_shift_limit, r_shift_limit). Default: (-20, 20).</p> <code>g_shift_limit</code> <code>ScaleIntType</code> <p>range for changing values for the green channel. If g_shift_limit is a single int, the range  will be (-g_shift_limit, g_shift_limit). Default: (-20, 20).</p> <code>b_shift_limit</code> <code>ScaleIntType</code> <p>range for changing values for the blue channel. If b_shift_limit is a single int, the range will be (-b_shift_limit, b_shift_limit). Default: (-20, 20).</p> <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 0.5.</p> <p>Targets</p> <p>image</p> <p>Image types:     uint8, float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>class RGBShift(ImageOnlyTransform):\n    \"\"\"Randomly shift values for each channel of the input RGB image.\n\n    Args:\n        r_shift_limit: range for changing values for the red channel. If r_shift_limit is a single\n            int, the range will be (-r_shift_limit, r_shift_limit). Default: (-20, 20).\n        g_shift_limit: range for changing values for the green channel. If g_shift_limit is a\n            single int, the range  will be (-g_shift_limit, g_shift_limit). Default: (-20, 20).\n        b_shift_limit: range for changing values for the blue channel. If b_shift_limit is a single\n            int, the range will be (-b_shift_limit, b_shift_limit). Default: (-20, 20).\n        p: probability of applying the transform. Default: 0.5.\n\n    Targets:\n        image\n\n    Image types:\n        uint8, float32\n\n    \"\"\"\n\n    class InitSchema(BaseTransformInitSchema):\n        r_shift_limit: SymmetricRangeType = (-20, 20)\n        g_shift_limit: SymmetricRangeType = (-20, 20)\n        b_shift_limit: SymmetricRangeType = (-20, 20)\n\n    def __init__(\n        self,\n        r_shift_limit: ScaleIntType = (-20, 20),\n        g_shift_limit: ScaleIntType = (-20, 20),\n        b_shift_limit: ScaleIntType = (-20, 20),\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p=p, always_apply=always_apply)\n        self.r_shift_limit = cast(Tuple[float, float], r_shift_limit)\n        self.g_shift_limit = cast(Tuple[float, float], g_shift_limit)\n        self.b_shift_limit = cast(Tuple[float, float], b_shift_limit)\n\n    def apply(self, img: np.ndarray, shift: np.ndarray, **params: Any) -&gt; np.ndarray:\n        if not is_rgb_image(img):\n            msg = \"RGBShift transformation expects 3-channel images.\"\n            raise TypeError(msg)\n\n        return albucore.add_vector(img, shift)\n\n    def get_params(self) -&gt; dict[str, Any]:\n        return {\n            \"shift\": np.array(\n                [\n                    random.uniform(self.r_shift_limit[0], self.r_shift_limit[1]),\n                    random.uniform(self.g_shift_limit[0], self.g_shift_limit[1]),\n                    random.uniform(self.b_shift_limit[0], self.b_shift_limit[1]),\n                ],\n            ),\n        }\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return \"r_shift_limit\", \"g_shift_limit\", \"b_shift_limit\"\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.RGBShift.apply","title":"<code>apply (self, img, shift, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def apply(self, img: np.ndarray, shift: np.ndarray, **params: Any) -&gt; np.ndarray:\n    if not is_rgb_image(img):\n        msg = \"RGBShift transformation expects 3-channel images.\"\n        raise TypeError(msg)\n\n    return albucore.add_vector(img, shift)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.RGBShift.get_params","title":"<code>get_params (self)</code>","text":"<p>Returns parameters independent of input.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_params(self) -&gt; dict[str, Any]:\n    return {\n        \"shift\": np.array(\n            [\n                random.uniform(self.r_shift_limit[0], self.r_shift_limit[1]),\n                random.uniform(self.g_shift_limit[0], self.g_shift_limit[1]),\n                random.uniform(self.b_shift_limit[0], self.b_shift_limit[1]),\n            ],\n        ),\n    }\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.RGBShift.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return \"r_shift_limit\", \"g_shift_limit\", \"b_shift_limit\"\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.RandomBrightnessContrast","title":"<code>class  RandomBrightnessContrast</code> <code>     (brightness_limit=(-0.2, 0.2), contrast_limit=(-0.2, 0.2), brightness_by_max=True, always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Randomly change brightness and contrast of the input image.</p> <p>Parameters:</p> Name Type Description <code>brightness_limit</code> <code>ScaleFloatType</code> <p>factor range for changing brightness. If limit is a single float, the range will be (-limit, limit). Default: (-0.2, 0.2).</p> <code>contrast_limit</code> <code>ScaleFloatType</code> <p>factor range for changing contrast. If limit is a single float, the range will be (-limit, limit). Default: (-0.2, 0.2).</p> <code>brightness_by_max</code> <code>bool</code> <p>If True adjust contrast by image dtype maximum, else adjust contrast by image mean.</p> <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 0.5.</p> <p>Targets</p> <p>image</p> <p>Image types:     uint8, float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>class RandomBrightnessContrast(ImageOnlyTransform):\n    \"\"\"Randomly change brightness and contrast of the input image.\n\n    Args:\n        brightness_limit: factor range for changing brightness.\n            If limit is a single float, the range will be (-limit, limit). Default: (-0.2, 0.2).\n        contrast_limit: factor range for changing contrast.\n            If limit is a single float, the range will be (-limit, limit). Default: (-0.2, 0.2).\n        brightness_by_max: If True adjust contrast by image dtype maximum,\n            else adjust contrast by image mean.\n        p: probability of applying the transform. Default: 0.5.\n\n    Targets:\n        image\n\n    Image types:\n        uint8, float32\n\n    \"\"\"\n\n    class InitSchema(BaseTransformInitSchema):\n        brightness_limit: SymmetricRangeType = (-0.2, 0.2)\n        contrast_limit: SymmetricRangeType = (-0.2, 0.2)\n        brightness_by_max: bool = Field(default=True, description=\"Adjust brightness by image dtype maximum if True.\")\n\n    def __init__(\n        self,\n        brightness_limit: ScaleFloatType = (-0.2, 0.2),\n        contrast_limit: ScaleFloatType = (-0.2, 0.2),\n        brightness_by_max: bool = True,\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p=p, always_apply=always_apply)\n        self.brightness_limit = cast(Tuple[float, float], brightness_limit)\n        self.contrast_limit = cast(Tuple[float, float], contrast_limit)\n        self.brightness_by_max = brightness_by_max\n\n    def apply(self, img: np.ndarray, alpha: float, beta: float, **params: Any) -&gt; np.ndarray:\n        return fmain.brightness_contrast_adjust(img, alpha, beta, self.brightness_by_max)\n\n    def get_params(self) -&gt; dict[str, float]:\n        return {\n            \"alpha\": 1.0 + random.uniform(self.contrast_limit[0], self.contrast_limit[1]),\n            \"beta\": 0.0 + random.uniform(self.brightness_limit[0], self.brightness_limit[1]),\n        }\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, str, str]:\n        return (\"brightness_limit\", \"contrast_limit\", \"brightness_by_max\")\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.RandomBrightnessContrast.apply","title":"<code>apply (self, img, alpha, beta, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def apply(self, img: np.ndarray, alpha: float, beta: float, **params: Any) -&gt; np.ndarray:\n    return fmain.brightness_contrast_adjust(img, alpha, beta, self.brightness_by_max)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.RandomBrightnessContrast.get_params","title":"<code>get_params (self)</code>","text":"<p>Returns parameters independent of input.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_params(self) -&gt; dict[str, float]:\n    return {\n        \"alpha\": 1.0 + random.uniform(self.contrast_limit[0], self.contrast_limit[1]),\n        \"beta\": 0.0 + random.uniform(self.brightness_limit[0], self.brightness_limit[1]),\n    }\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.RandomBrightnessContrast.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, str, str]:\n    return (\"brightness_limit\", \"contrast_limit\", \"brightness_by_max\")\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.RandomFog","title":"<code>class  RandomFog</code> <code>     (fog_coef_lower=None, fog_coef_upper=None, alpha_coef=0.08, fog_coef_range=(0.3, 1), always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Simulates fog for the image.</p> <p>Parameters:</p> Name Type Description <code>fog_coef_range</code> <code>tuple</code> <p>tuple of bounds on the fog intensity coefficient (fog_coef_lower, fog_coef_upper). Default: (0.3, 1).</p> <code>alpha_coef</code> <code>float</code> <p>Transparency of the fog circles. Should be in [0, 1] range. Default: 0.08.</p> <code>p</code> <code>float</code> <p>Probability of applying the transform. Default: 0.5.</p> <p>Targets</p> <p>image</p> <p>Image types:     uint8, float32</p> <p>Reference</p> <p>https://github.com/UjjwalSaxena/Automold--Road-Augmentation-Library</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>class RandomFog(ImageOnlyTransform):\n    \"\"\"Simulates fog for the image.\n\n    Args:\n        fog_coef_range (tuple): tuple of bounds on the fog intensity coefficient (fog_coef_lower, fog_coef_upper).\n            Default: (0.3, 1).\n        alpha_coef (float): Transparency of the fog circles. Should be in [0, 1] range. Default: 0.08.\n        p (float): Probability of applying the transform. Default: 0.5.\n\n    Targets:\n        image\n\n    Image types:\n        uint8, float32\n\n    Reference:\n        https://github.com/UjjwalSaxena/Automold--Road-Augmentation-Library\n    \"\"\"\n\n    class InitSchema(BaseTransformInitSchema):\n        fog_coef_lower: float | None = Field(\n            default=None,\n            description=\"Lower limit for fog intensity coefficient\",\n            ge=0,\n            le=1,\n        )\n        fog_coef_upper: float | None = Field(\n            default=None,\n            description=\"Upper limit for fog intensity coefficient\",\n            ge=0,\n            le=1,\n        )\n        fog_coef_range: Annotated[tuple[float, float], AfterValidator(check_01), AfterValidator(nondecreasing)] = (\n            0.3,\n            1,\n        )\n\n        alpha_coef: float = Field(default=0.08, description=\"Transparency of the fog circles\", ge=0, le=1)\n\n        @model_validator(mode=\"after\")\n        def validate_fog_coefficients(self) -&gt; Self:\n            if self.fog_coef_lower is not None:\n                warn(\"`fog_coef_lower` is deprecated, use `fog_coef_range` instead.\", DeprecationWarning, stacklevel=2)\n            if self.fog_coef_upper is not None:\n                warn(\"`fog_coef_upper` is deprecated, use `fog_coef_range` instead.\", DeprecationWarning, stacklevel=2)\n\n            lower = self.fog_coef_lower if self.fog_coef_lower is not None else self.fog_coef_range[0]\n            upper = self.fog_coef_upper if self.fog_coef_upper is not None else self.fog_coef_range[1]\n            self.fog_coef_range = (lower, upper)\n\n            self.fog_coef_lower = None\n            self.fog_coef_upper = None\n\n            return self\n\n    def __init__(\n        self,\n        fog_coef_lower: float | None = None,\n        fog_coef_upper: float | None = None,\n        alpha_coef: float = 0.08,\n        fog_coef_range: tuple[float, float] = (0.3, 1),\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p=p, always_apply=always_apply)\n        self.fog_coef_range = fog_coef_range\n        self.alpha_coef = alpha_coef\n\n    def apply(\n        self,\n        img: np.ndarray,\n        fog_coef: np.ndarray,\n        haze_list: list[tuple[int, int]],\n        **params: Any,\n    ) -&gt; np.ndarray:\n        return fmain.add_fog(img, fog_coef, self.alpha_coef, haze_list)\n\n    def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n        fog_coef = random.uniform(*self.fog_coef_range)\n\n        height, width = imshape = params[\"shape\"][:2]\n\n        hw = max(1, int(width // 3 * fog_coef))\n\n        haze_list = []\n        midx = width // 2 - 2 * hw\n        midy = height // 2 - hw\n        index = 1\n\n        while midx &gt; -hw or midy &gt; -hw:\n            for _ in range(hw // 10 * index):\n                x = random.randint(midx, width - midx - hw)\n                y = random.randint(midy, height - midy - hw)\n                haze_list.append((x, y))\n\n            midx -= 3 * hw * width // sum(imshape)\n            midy -= 3 * hw * height // sum(imshape)\n            index += 1\n\n        return {\"haze_list\": haze_list, \"fog_coef\": fog_coef}\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, str]:\n        return \"fog_coef_range\", \"alpha_coef\"\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.RandomFog.apply","title":"<code>apply (self, img, fog_coef, haze_list, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def apply(\n    self,\n    img: np.ndarray,\n    fog_coef: np.ndarray,\n    haze_list: list[tuple[int, int]],\n    **params: Any,\n) -&gt; np.ndarray:\n    return fmain.add_fog(img, fog_coef, self.alpha_coef, haze_list)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.RandomFog.get_params_dependent_on_data","title":"<code>get_params_dependent_on_data (self, params, data)</code>","text":"<p>Returns parameters dependent on input.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n    fog_coef = random.uniform(*self.fog_coef_range)\n\n    height, width = imshape = params[\"shape\"][:2]\n\n    hw = max(1, int(width // 3 * fog_coef))\n\n    haze_list = []\n    midx = width // 2 - 2 * hw\n    midy = height // 2 - hw\n    index = 1\n\n    while midx &gt; -hw or midy &gt; -hw:\n        for _ in range(hw // 10 * index):\n            x = random.randint(midx, width - midx - hw)\n            y = random.randint(midy, height - midy - hw)\n            haze_list.append((x, y))\n\n        midx -= 3 * hw * width // sum(imshape)\n        midy -= 3 * hw * height // sum(imshape)\n        index += 1\n\n    return {\"haze_list\": haze_list, \"fog_coef\": fog_coef}\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.RandomFog.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, str]:\n    return \"fog_coef_range\", \"alpha_coef\"\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.RandomGamma","title":"<code>class  RandomGamma</code> <code>     (gamma_limit=(80, 120), always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Applies random gamma correction to an image as a form of data augmentation.</p> <p>This class adjusts the luminance of an image by applying gamma correction with a randomly selected gamma value from a specified range. Gamma correction can simulate various lighting conditions, potentially enhancing model generalization.</p> <p>Attributes:</p> Name Type Description <code>gamma_limit</code> <code>Union[int, tuple[int, int]]</code> <p>The range for gamma adjustment. If <code>gamma_limit</code> is a single int, the range will be interpreted as (-gamma_limit, gamma_limit), defining how much to adjust the image's gamma. Default is (80, 120).</p> <code>always_apply</code> <p>Depreciated. Use <code>p=1</code> instead.</p> <code>p</code> <code>float</code> <p>The probability that the transform will be applied. Default is 0.5.</p> <p>Targets</p> <p>image</p> <p>Image types:     uint8, float32</p> <p>Reference</p> <p>https://en.wikipedia.org/wiki/Gamma_correction</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>class RandomGamma(ImageOnlyTransform):\n    \"\"\"Applies random gamma correction to an image as a form of data augmentation.\n\n    This class adjusts the luminance of an image by applying gamma correction with a randomly\n    selected gamma value from a specified range. Gamma correction can simulate various lighting\n    conditions, potentially enhancing model generalization.\n\n    Attributes:\n        gamma_limit (Union[int, tuple[int, int]]): The range for gamma adjustment. If `gamma_limit` is a single\n            int, the range will be interpreted as (-gamma_limit, gamma_limit), defining how much\n            to adjust the image's gamma. Default is (80, 120).\n        always_apply: Depreciated. Use `p=1` instead.\n        p (float): The probability that the transform will be applied. Default is 0.5.\n\n    Targets:\n        image\n\n    Image types:\n        uint8, float32\n\n    Reference:\n         https://en.wikipedia.org/wiki/Gamma_correction\n\n    \"\"\"\n\n    class InitSchema(BaseTransformInitSchema):\n        gamma_limit: OnePlusFloatRangeType = (80, 120)\n\n    def __init__(\n        self,\n        gamma_limit: ScaleIntType = (80, 120),\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p, always_apply)\n        self.gamma_limit = cast(Tuple[float, float], gamma_limit)\n\n    def apply(self, img: np.ndarray, gamma: float, **params: Any) -&gt; np.ndarray:\n        return fmain.gamma_transform(img, gamma=gamma)\n\n    def get_params(self) -&gt; dict[str, float]:\n        return {\"gamma\": random.uniform(self.gamma_limit[0], self.gamma_limit[1]) / 100.0}\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return (\"gamma_limit\",)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.RandomGamma.apply","title":"<code>apply (self, img, gamma, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def apply(self, img: np.ndarray, gamma: float, **params: Any) -&gt; np.ndarray:\n    return fmain.gamma_transform(img, gamma=gamma)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.RandomGamma.get_params","title":"<code>get_params (self)</code>","text":"<p>Returns parameters independent of input.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_params(self) -&gt; dict[str, float]:\n    return {\"gamma\": random.uniform(self.gamma_limit[0], self.gamma_limit[1]) / 100.0}\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.RandomGamma.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return (\"gamma_limit\",)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.RandomGravel","title":"<code>class  RandomGravel</code> <code>     (gravel_roi=(0.1, 0.4, 0.9, 0.9), number_of_patches=2, always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Add gravels.</p> <p>Parameters:</p> Name Type Description <code>gravel_roi</code> <code>tuple[float, float, float, float]</code> <p>(top-left x, top-left y, bottom-right x, bottom right y). Should be in [0, 1] range</p> <code>number_of_patches</code> <code>int</code> <p>no. of gravel patches required</p> <p>Targets</p> <p>image</p> <p>Image types:     uint8, float32</p> <p>Reference</p> <p>https://github.com/UjjwalSaxena/Automold--Road-Augmentation-Library</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>class RandomGravel(ImageOnlyTransform):\n    \"\"\"Add gravels.\n\n    Args:\n        gravel_roi: (top-left x, top-left y,\n            bottom-right x, bottom right y). Should be in [0, 1] range\n        number_of_patches: no. of gravel patches required\n\n    Targets:\n        image\n\n    Image types:\n        uint8, float32\n\n    Reference:\n        https://github.com/UjjwalSaxena/Automold--Road-Augmentation-Library\n\n    \"\"\"\n\n    class InitSchema(BaseTransformInitSchema):\n        gravel_roi: tuple[float, float, float, float] = Field(\n            default=(0.1, 0.4, 0.9, 0.9),\n            description=\"Region of interest for gravel placement\",\n        )\n        number_of_patches: int = Field(default=2, description=\"Number of gravel patches\", ge=1)\n\n        @model_validator(mode=\"after\")\n        def validate_gravel_roi(self) -&gt; Self:\n            gravel_lower_x, gravel_lower_y, gravel_upper_x, gravel_upper_y = self.gravel_roi\n            if not 0 &lt;= gravel_lower_x &lt; gravel_upper_x &lt;= 1 or not 0 &lt;= gravel_lower_y &lt; gravel_upper_y &lt;= 1:\n                raise ValueError(f\"Invalid gravel_roi. Got: {self.gravel_roi}.\")\n            return self\n\n    def __init__(\n        self,\n        gravel_roi: tuple[float, float, float, float] = (0.1, 0.4, 0.9, 0.9),\n        number_of_patches: int = 2,\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p, always_apply)\n        self.gravel_roi = gravel_roi\n        self.number_of_patches = number_of_patches\n\n    def generate_gravel_patch(self, rectangular_roi: tuple[int, int, int, int]) -&gt; np.ndarray:\n        x1, y1, x2, y2 = rectangular_roi\n        area = abs((x2 - x1) * (y2 - y1))\n        count = area // 10\n        gravels = np.empty([count, 2], dtype=np.int64)\n        gravels[:, 0] = random_utils.randint(x1, x2, count)\n        gravels[:, 1] = random_utils.randint(y1, y2, count)\n        return gravels\n\n    def apply(self, img: np.ndarray, gravels_infos: list[Any], **params: Any) -&gt; np.ndarray:\n        if gravels_infos is None:\n            gravels_infos = []\n        return fmain.add_gravel(img, gravels_infos)\n\n    def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, np.ndarray]:\n        height, width = params[\"shape\"][:2]\n\n        x_min, y_min, x_max, y_max = self.gravel_roi\n        x_min = int(x_min * width)\n        x_max = int(x_max * width)\n        y_min = int(y_min * height)\n        y_max = int(y_max * height)\n\n        max_height = 200\n        max_width = 30\n\n        rectangular_rois = np.zeros([self.number_of_patches, 4], dtype=np.int64)\n        xx1 = random_utils.randint(x_min + 1, x_max, self.number_of_patches)  # xmax\n        xx2 = random_utils.randint(x_min, xx1)  # xmin\n        yy1 = random_utils.randint(y_min + 1, y_max, self.number_of_patches)  # ymax\n        yy2 = random_utils.randint(y_min, yy1)  # ymin\n\n        rectangular_rois[:, 0] = xx2\n        rectangular_rois[:, 1] = yy2\n        rectangular_rois[:, 2] = [min(tup) for tup in zip(xx1, xx2 + max_height)]\n        rectangular_rois[:, 3] = [min(tup) for tup in zip(yy1, yy2 + max_width)]\n\n        minx = []\n        maxx = []\n        miny = []\n        maxy = []\n        val = []\n        for roi in rectangular_rois:\n            gravels = self.generate_gravel_patch(roi)\n            x = gravels[:, 0]\n            y = gravels[:, 1]\n            r = random_utils.randint(1, 4, len(gravels))\n            sat = random_utils.randint(0, 255, len(gravels))\n            miny.append(np.maximum(y - r, 0))\n            maxy.append(np.minimum(y + r, y))\n            minx.append(np.maximum(x - r, 0))\n            maxx.append(np.minimum(x + r, x))\n            val.append(sat)\n\n        return {\n            \"gravels_infos\": np.stack(\n                [\n                    np.concatenate(miny),\n                    np.concatenate(maxy),\n                    np.concatenate(minx),\n                    np.concatenate(maxx),\n                    np.concatenate(val),\n                ],\n                1,\n            ),\n        }\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, str]:\n        return \"gravel_roi\", \"number_of_patches\"\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.RandomGravel.apply","title":"<code>apply (self, img, gravels_infos, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def apply(self, img: np.ndarray, gravels_infos: list[Any], **params: Any) -&gt; np.ndarray:\n    if gravels_infos is None:\n        gravels_infos = []\n    return fmain.add_gravel(img, gravels_infos)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.RandomGravel.get_params_dependent_on_data","title":"<code>get_params_dependent_on_data (self, params, data)</code>","text":"<p>Returns parameters dependent on input.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, np.ndarray]:\n    height, width = params[\"shape\"][:2]\n\n    x_min, y_min, x_max, y_max = self.gravel_roi\n    x_min = int(x_min * width)\n    x_max = int(x_max * width)\n    y_min = int(y_min * height)\n    y_max = int(y_max * height)\n\n    max_height = 200\n    max_width = 30\n\n    rectangular_rois = np.zeros([self.number_of_patches, 4], dtype=np.int64)\n    xx1 = random_utils.randint(x_min + 1, x_max, self.number_of_patches)  # xmax\n    xx2 = random_utils.randint(x_min, xx1)  # xmin\n    yy1 = random_utils.randint(y_min + 1, y_max, self.number_of_patches)  # ymax\n    yy2 = random_utils.randint(y_min, yy1)  # ymin\n\n    rectangular_rois[:, 0] = xx2\n    rectangular_rois[:, 1] = yy2\n    rectangular_rois[:, 2] = [min(tup) for tup in zip(xx1, xx2 + max_height)]\n    rectangular_rois[:, 3] = [min(tup) for tup in zip(yy1, yy2 + max_width)]\n\n    minx = []\n    maxx = []\n    miny = []\n    maxy = []\n    val = []\n    for roi in rectangular_rois:\n        gravels = self.generate_gravel_patch(roi)\n        x = gravels[:, 0]\n        y = gravels[:, 1]\n        r = random_utils.randint(1, 4, len(gravels))\n        sat = random_utils.randint(0, 255, len(gravels))\n        miny.append(np.maximum(y - r, 0))\n        maxy.append(np.minimum(y + r, y))\n        minx.append(np.maximum(x - r, 0))\n        maxx.append(np.minimum(x + r, x))\n        val.append(sat)\n\n    return {\n        \"gravels_infos\": np.stack(\n            [\n                np.concatenate(miny),\n                np.concatenate(maxy),\n                np.concatenate(minx),\n                np.concatenate(maxx),\n                np.concatenate(val),\n            ],\n            1,\n        ),\n    }\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.RandomGravel.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, str]:\n    return \"gravel_roi\", \"number_of_patches\"\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.RandomGridShuffle","title":"<code>class  RandomGridShuffle</code> <code>     (grid=(3, 3), p=0.5, always_apply=None)                 </code>  [view source on GitHub]","text":"<p>Randomly shuffles the grid's cells on an image, mask, or keypoints, effectively rearranging patches within the image. This transformation divides the image into a grid and then permutes these grid cells based on a random mapping.</p> <p>Parameters:</p> Name Type Description <code>grid</code> <code>tuple[int, int]</code> <p>Size of the grid for splitting the image into cells. Each cell is shuffled randomly.</p> <code>p</code> <code>float</code> <p>Probability that the transform will be applied.</p> <p>Targets</p> <p>image, mask, keypoints</p> <p>Image types:     uint8, float32</p> <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; import albumentations as A\n&gt;&gt;&gt; transform = A.Compose([\n    A.RandomGridShuffle(grid=(3, 3), p=1.0)\n])\n&gt;&gt;&gt; transformed = transform(image=my_image, mask=my_mask)\n&gt;&gt;&gt; image, mask = transformed['image'], transformed['mask']\n# This will shuffle the 3x3 grid cells of `my_image` and `my_mask` randomly.\n# Mask and image are shuffled in a consistent way\n</code></pre> <p>Note</p> <p>This transform could be useful when only micro features are important for the model, and memorizing the global structure could be harmful. For example: - Identifying the type of cell phone used to take a picture based on micro artifacts generated by phone post-processing algorithms, rather than the semantic features of the photo. See more at https://ieeexplore.ieee.org/abstract/document/8622031 - Identifying stress, glucose, hydration levels based on skin images.</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>class RandomGridShuffle(DualTransform):\n    \"\"\"Randomly shuffles the grid's cells on an image, mask, or keypoints,\n    effectively rearranging patches within the image.\n    This transformation divides the image into a grid and then permutes these grid cells based on a random mapping.\n\n\n    Args:\n        grid (tuple[int, int]): Size of the grid for splitting the image into cells. Each cell is shuffled randomly.\n        p (float): Probability that the transform will be applied.\n\n    Targets:\n        image, mask, keypoints\n\n    Image types:\n        uint8, float32\n\n    Examples:\n        &gt;&gt;&gt; import albumentations as A\n        &gt;&gt;&gt; transform = A.Compose([\n            A.RandomGridShuffle(grid=(3, 3), p=1.0)\n        ])\n        &gt;&gt;&gt; transformed = transform(image=my_image, mask=my_mask)\n        &gt;&gt;&gt; image, mask = transformed['image'], transformed['mask']\n        # This will shuffle the 3x3 grid cells of `my_image` and `my_mask` randomly.\n        # Mask and image are shuffled in a consistent way\n    Note:\n        This transform could be useful when only micro features are important for the model, and memorizing\n        the global structure could be harmful. For example:\n        - Identifying the type of cell phone used to take a picture based on micro artifacts generated by\n        phone post-processing algorithms, rather than the semantic features of the photo.\n        See more at https://ieeexplore.ieee.org/abstract/document/8622031\n        - Identifying stress, glucose, hydration levels based on skin images.\n    \"\"\"\n\n    class InitSchema(BaseTransformInitSchema):\n        grid: Annotated[tuple[int, int], AfterValidator(check_1plus)] = (3, 3)\n\n    _targets = (Targets.IMAGE, Targets.MASK, Targets.KEYPOINTS)\n\n    def __init__(self, grid: tuple[int, int] = (3, 3), p: float = 0.5, always_apply: bool | None = None):\n        super().__init__(p=p, always_apply=always_apply)\n        self.grid = grid\n\n    def apply(self, img: np.ndarray, tiles: np.ndarray, mapping: list[int], **params: Any) -&gt; np.ndarray:\n        return fmain.swap_tiles_on_image(img, tiles, mapping)\n\n    def apply_to_mask(self, mask: np.ndarray, tiles: np.ndarray, mapping: list[int], **params: Any) -&gt; np.ndarray:\n        return fmain.swap_tiles_on_image(mask, tiles, mapping)\n\n    def apply_to_keypoint(\n        self,\n        keypoint: KeypointInternalType,\n        tiles: np.ndarray,\n        mapping: list[int],\n        **params: Any,\n    ) -&gt; KeypointInternalType:\n        x, y = keypoint[:2]\n\n        # Find which original tile the keypoint belongs to\n        for original_index, new_index in enumerate(mapping):\n            start_y, start_x, end_y, end_x = tiles[original_index]\n            # check if the keypoint is in this tile\n            if start_y &lt;= y &lt; end_y and start_x &lt;= x &lt; end_x:\n                # Get the new tile's coordinates\n                new_start_y, new_start_x = tiles[new_index][:2]\n\n                # Map the keypoint to the new tile's position\n                new_x = (x - start_x) + new_start_x\n                new_y = (y - start_y) + new_start_y\n\n                return (new_x, new_y, *keypoint[2:])\n\n        # If the keypoint wasn't in any tile (shouldn't happen), log a warning for debugging purposes\n        warn(\n            \"Keypoint not in any tile, returning it unchanged. This is unexpected and should be investigated.\",\n            RuntimeWarning,\n            stacklevel=2,\n        )\n        return keypoint\n\n    def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, np.ndarray]:\n        height, width = params[\"shape\"][:2]\n        random_state = random_utils.get_random_state()\n        original_tiles = fmain.split_uniform_grid(\n            (height, width),\n            self.grid,\n            random_state=random_state,\n        )\n        shape_groups = fmain.create_shape_groups(original_tiles)\n        mapping = fmain.shuffle_tiles_within_shape_groups(shape_groups, random_state=random_state)\n\n        return {\"tiles\": original_tiles, \"mapping\": mapping}\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return (\"grid\",)\n\n    @property\n    def targets(self) -&gt; dict[str, Callable[..., Any]]:\n        return {\n            \"image\": self.apply,\n            \"mask\": self.apply_to_mask,\n            \"masks\": self.apply_to_masks,\n            \"keypoints\": self.apply_to_keypoints,\n        }\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.RandomGridShuffle.apply","title":"<code>apply (self, img, tiles, mapping, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def apply(self, img: np.ndarray, tiles: np.ndarray, mapping: list[int], **params: Any) -&gt; np.ndarray:\n    return fmain.swap_tiles_on_image(img, tiles, mapping)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.RandomGridShuffle.get_params_dependent_on_data","title":"<code>get_params_dependent_on_data (self, params, data)</code>","text":"<p>Returns parameters dependent on input.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, np.ndarray]:\n    height, width = params[\"shape\"][:2]\n    random_state = random_utils.get_random_state()\n    original_tiles = fmain.split_uniform_grid(\n        (height, width),\n        self.grid,\n        random_state=random_state,\n    )\n    shape_groups = fmain.create_shape_groups(original_tiles)\n    mapping = fmain.shuffle_tiles_within_shape_groups(shape_groups, random_state=random_state)\n\n    return {\"tiles\": original_tiles, \"mapping\": mapping}\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.RandomGridShuffle.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return (\"grid\",)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.RandomRain","title":"<code>class  RandomRain</code> <code>     (slant_lower=None, slant_upper=None, slant_range=(-10, 10), drop_length=20, drop_width=1, drop_color=(200, 200, 200), blur_value=7, brightness_coefficient=0.7, rain_type=None, always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Adds rain effects to an image.</p> <p>Parameters:</p> Name Type Description <code>slant_range</code> <code>tuple[int, int]</code> <p>tuple of type (slant_lower, slant_upper) representing the range for rain slant angle.</p> <code>drop_length</code> <code>int</code> <p>Length of the raindrops.</p> <code>drop_width</code> <code>int</code> <p>Width of the raindrops.</p> <code>drop_color</code> <code>tuple[int, int, int]</code> <p>Color of the rain drops in RGB format.</p> <code>blur_value</code> <code>int</code> <p>Blur value for simulating rain effect. Rainy views are blurry.</p> <code>brightness_coefficient</code> <code>float</code> <p>Coefficient to adjust the brightness of the image. Rainy days are usually shady. Should be in the range (0, 1].</p> <code>rain_type</code> <code>Optional[str]</code> <p>Type of rain to simulate. One of [None, \"drizzle\", \"heavy\", \"torrential\"].</p> <p>Targets</p> <p>image</p> <p>Image types:     uint8, float32</p> <p>Reference</p> <p>https://github.com/UjjwalSaxena/Automold--Road-Augmentation-Library</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>class RandomRain(ImageOnlyTransform):\n    \"\"\"Adds rain effects to an image.\n\n    Args:\n        slant_range (tuple[int, int]): tuple of type (slant_lower, slant_upper) representing the range for\n            rain slant angle.\n        drop_length (int): Length of the raindrops.\n        drop_width (int): Width of the raindrops.\n        drop_color (tuple[int, int, int]): Color of the rain drops in RGB format.\n        blur_value (int): Blur value for simulating rain effect. Rainy views are blurry.\n        brightness_coefficient (float): Coefficient to adjust the brightness of the image.\n            Rainy days are usually shady. Should be in the range (0, 1].\n        rain_type (Optional[str]): Type of rain to simulate. One of [None, \"drizzle\", \"heavy\", \"torrential\"].\n\n\n    Targets:\n        image\n\n    Image types:\n        uint8, float32\n\n    Reference:\n        https://github.com/UjjwalSaxena/Automold--Road-Augmentation-Library\n\n    \"\"\"\n\n    class InitSchema(BaseTransformInitSchema):\n        slant_lower: int | None = Field(\n            default=None,\n            description=\"Lower bound for rain slant angle\",\n        )\n        slant_upper: int | None = Field(\n            default=None,\n            description=\"Upper bound for rain slant angle\",\n        )\n        slant_range: Annotated[tuple[float, float], AfterValidator(nondecreasing)] = Field(\n            default=(-10, 10),\n            description=\"tuple like (slant_lower, slant_upper) for rain slant angle\",\n        )\n        drop_length: int = Field(default=20, description=\"Length of raindrops\", ge=1)\n        drop_width: int = Field(default=1, description=\"Width of raindrops\", ge=1)\n        drop_color: tuple[int, int, int] = Field(default=(200, 200, 200), description=\"Color of raindrops\")\n        blur_value: int = Field(default=7, description=\"Blur value for simulating rain effect\", ge=1)\n        brightness_coefficient: float = Field(\n            default=0.7,\n            description=\"Brightness coefficient for rainy effect\",\n            gt=0,\n            le=1,\n        )\n        rain_type: RainMode | None = Field(default=None, description=\"Type of rain to simulate\")\n\n        @model_validator(mode=\"after\")\n        def validate_ranges(self) -&gt; Self:\n            if self.slant_lower is not None or self.slant_upper is not None:\n                if self.slant_lower is not None:\n                    warn(\n                        \"`slant_lower` deprecated. Use `slant_range` as tuple (slant_lower, slant_upper) instead.\",\n                        DeprecationWarning,\n                        stacklevel=2,\n                    )\n                if self.slant_upper is not None:\n                    warn(\n                        \"`slant_upper` deprecated. Use `slant_range` as tuple (slant_lower, slant_upper) instead.\",\n                        DeprecationWarning,\n                        stacklevel=2,\n                    )\n                lower = self.slant_lower if self.slant_lower is not None else self.slant_range[0]\n                upper = self.slant_upper if self.slant_upper is not None else self.slant_range[1]\n                self.slant_range = (lower, upper)\n                self.slant_lower = None\n                self.slant_upper = None\n\n            # Validate the slant_range\n            if not (-MAX_RAIN_ANGLE &lt;= self.slant_range[0] &lt;= self.slant_range[1] &lt;= MAX_RAIN_ANGLE):\n                raise ValueError(\n                    f\"slant_range values should be increasing within [-{MAX_RAIN_ANGLE}, {MAX_RAIN_ANGLE}] range.\",\n                )\n            return self\n\n    def __init__(\n        self,\n        slant_lower: int | None = None,\n        slant_upper: int | None = None,\n        slant_range: tuple[int, int] = (-10, 10),\n        drop_length: int = 20,\n        drop_width: int = 1,\n        drop_color: tuple[int, int, int] = (200, 200, 200),\n        blur_value: int = 7,\n        brightness_coefficient: float = 0.7,\n        rain_type: RainMode | None = None,\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p=p, always_apply=always_apply)\n        self.slant_range = slant_range\n        self.drop_length = drop_length\n        self.drop_width = drop_width\n        self.drop_color = drop_color\n        self.blur_value = blur_value\n        self.brightness_coefficient = brightness_coefficient\n        self.rain_type = rain_type\n\n    def apply(\n        self,\n        img: np.ndarray,\n        slant: int,\n        drop_length: int,\n        rain_drops: list[tuple[int, int]],\n        **params: Any,\n    ) -&gt; np.ndarray:\n        return fmain.add_rain(\n            img,\n            slant,\n            drop_length,\n            self.drop_width,\n            self.drop_color,\n            self.blur_value,\n            self.brightness_coefficient,\n            rain_drops,\n        )\n\n    def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n        slant = int(random.uniform(*self.slant_range))\n\n        height, width = params[\"shape\"][:2]\n        area = height * width\n\n        if self.rain_type == \"drizzle\":\n            num_drops = area // 770\n            drop_length = 10\n        elif self.rain_type == \"heavy\":\n            num_drops = width * height // 600\n            drop_length = 30\n        elif self.rain_type == \"torrential\":\n            num_drops = area // 500\n            drop_length = 60\n        else:\n            drop_length = self.drop_length\n            num_drops = area // 600\n\n        rain_drops = []\n\n        for _ in range(num_drops):  # If You want heavy rain, try increasing this\n            x = random.randint(slant, width) if slant &lt; 0 else random.randint(0, width - slant)\n\n            y = random.randint(0, height - drop_length)\n\n            rain_drops.append((x, y))\n\n        return {\"drop_length\": drop_length, \"slant\": slant, \"rain_drops\": rain_drops}\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return (\n            \"slant_range\",\n            \"drop_length\",\n            \"drop_width\",\n            \"drop_color\",\n            \"blur_value\",\n            \"brightness_coefficient\",\n            \"rain_type\",\n        )\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.RandomRain.apply","title":"<code>apply (self, img, slant, drop_length, rain_drops, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def apply(\n    self,\n    img: np.ndarray,\n    slant: int,\n    drop_length: int,\n    rain_drops: list[tuple[int, int]],\n    **params: Any,\n) -&gt; np.ndarray:\n    return fmain.add_rain(\n        img,\n        slant,\n        drop_length,\n        self.drop_width,\n        self.drop_color,\n        self.blur_value,\n        self.brightness_coefficient,\n        rain_drops,\n    )\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.RandomRain.get_params_dependent_on_data","title":"<code>get_params_dependent_on_data (self, params, data)</code>","text":"<p>Returns parameters dependent on input.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n    slant = int(random.uniform(*self.slant_range))\n\n    height, width = params[\"shape\"][:2]\n    area = height * width\n\n    if self.rain_type == \"drizzle\":\n        num_drops = area // 770\n        drop_length = 10\n    elif self.rain_type == \"heavy\":\n        num_drops = width * height // 600\n        drop_length = 30\n    elif self.rain_type == \"torrential\":\n        num_drops = area // 500\n        drop_length = 60\n    else:\n        drop_length = self.drop_length\n        num_drops = area // 600\n\n    rain_drops = []\n\n    for _ in range(num_drops):  # If You want heavy rain, try increasing this\n        x = random.randint(slant, width) if slant &lt; 0 else random.randint(0, width - slant)\n\n        y = random.randint(0, height - drop_length)\n\n        rain_drops.append((x, y))\n\n    return {\"drop_length\": drop_length, \"slant\": slant, \"rain_drops\": rain_drops}\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.RandomRain.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return (\n        \"slant_range\",\n        \"drop_length\",\n        \"drop_width\",\n        \"drop_color\",\n        \"blur_value\",\n        \"brightness_coefficient\",\n        \"rain_type\",\n    )\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.RandomShadow","title":"<code>class  RandomShadow</code> <code>     (shadow_roi=(0, 0.5, 1, 1), num_shadows_limit=(1, 2), num_shadows_lower=None, num_shadows_upper=None, shadow_dimension=5, shadow_intensity_range=(0.5, 0.5), always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Simulates shadows for the image by reducing the brightness of the image in shadow regions.</p> <p>Parameters:</p> Name Type Description <code>shadow_roi</code> <code>tuple</code> <p>region of the image where shadows will appear (x_min, y_min, x_max, y_max). All values should be in range [0, 1].</p> <code>num_shadows_limit</code> <code>tuple</code> <p>Lower and upper limits for the possible number of shadows. Default: (1, 2).</p> <code>shadow_dimension</code> <code>int</code> <p>number of edges in the shadow polygons. Default: 5.</p> <code>shadow_intensity_range</code> <code>tuple</code> <p>Range for the shadow intensity. Should be two float values between 0 and 1. Default: (0.5, 0.5).</p> <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 0.5.</p> <p>Targets</p> <p>image</p> <p>Image types:     uint8, float32</p> <p>Reference</p> <p>https://github.com/UjjwalSaxena/Automold--Road-Augmentation-Library</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>class RandomShadow(ImageOnlyTransform):\n    \"\"\"Simulates shadows for the image by reducing the brightness of the image in shadow regions.\n\n    Args:\n        shadow_roi (tuple): region of the image where shadows\n            will appear (x_min, y_min, x_max, y_max). All values should be in range [0, 1].\n        num_shadows_limit (tuple): Lower and upper limits for the possible number of shadows.\n            Default: (1, 2).\n        shadow_dimension (int): number of edges in the shadow polygons. Default: 5.\n        shadow_intensity_range (tuple): Range for the shadow intensity.\n            Should be two float values between 0 and 1. Default: (0.5, 0.5).\n        p (float): probability of applying the transform. Default: 0.5.\n\n    Targets:\n        image\n\n    Image types:\n        uint8, float32\n\n    Reference:\n        https://github.com/UjjwalSaxena/Automold--Road-Augmentation-Library\n    \"\"\"\n\n    class InitSchema(BaseTransformInitSchema):\n        shadow_roi: tuple[float, float, float, float] = Field(\n            default=(0, 0.5, 1, 1),\n            description=\"Region of the image where shadows will appear\",\n        )\n        num_shadows_limit: Annotated[tuple[int, int], AfterValidator(check_1plus), AfterValidator(nondecreasing)] = (\n            1,\n            2,\n        )\n        num_shadows_lower: int | None = Field(\n            default=None,\n            description=\"Lower limit for the possible number of shadows\",\n        )\n        num_shadows_upper: int | None = Field(\n            default=None,\n            description=\"Upper limit for the possible number of shadows\",\n        )\n        shadow_dimension: int = Field(default=5, description=\"Number of edges in the shadow polygons\", ge=1)\n\n        shadow_intensity_range: Annotated[\n            tuple[float, float],\n            AfterValidator(check_01),\n            AfterValidator(nondecreasing),\n        ] = Field(\n            default=(0.5, 0.5),\n            description=\"Range for the shadow intensity\",\n        )\n\n        @model_validator(mode=\"after\")\n        def validate_shadows(self) -&gt; Self:\n            if self.num_shadows_lower is not None:\n                warn(\n                    \"`num_shadows_lower` is deprecated. Use `num_shadows_limit` instead.\",\n                    DeprecationWarning,\n                    stacklevel=2,\n                )\n\n            if self.num_shadows_upper is not None:\n                warn(\n                    \"`num_shadows_upper` is deprecated. Use `num_shadows_limit` instead.\",\n                    DeprecationWarning,\n                    stacklevel=2,\n                )\n\n            if self.num_shadows_lower is not None or self.num_shadows_upper is not None:\n                num_shadows_lower = (\n                    self.num_shadows_lower if self.num_shadows_lower is not None else self.num_shadows_limit[0]\n                )\n                num_shadows_upper = (\n                    self.num_shadows_upper if self.num_shadows_upper is not None else self.num_shadows_limit[1]\n                )\n\n                self.num_shadows_limit = (num_shadows_lower, num_shadows_upper)\n                self.num_shadows_lower = None\n                self.num_shadows_upper = None\n\n            shadow_lower_x, shadow_lower_y, shadow_upper_x, shadow_upper_y = self.shadow_roi\n\n            if not 0 &lt;= shadow_lower_x &lt;= shadow_upper_x &lt;= 1 or not 0 &lt;= shadow_lower_y &lt;= shadow_upper_y &lt;= 1:\n                raise ValueError(f\"Invalid shadow_roi. Got: {self.shadow_roi}\")\n\n            if isinstance(self.shadow_intensity_range, float):\n                if not (0 &lt;= self.shadow_intensity_range &lt;= 1):\n                    raise ValueError(\n                        f\"shadow_intensity_range value should be within [0, 1] range. \"\n                        f\"Got: {self.shadow_intensity_range}\",\n                    )\n            elif isinstance(self.shadow_intensity_range, tuple):\n                if not (0 &lt;= self.shadow_intensity_range[0] &lt;= self.shadow_intensity_range[1] &lt;= 1):\n                    raise ValueError(\n                        f\"shadow_intensity_range values should be within [0, 1] range and increasing. \"\n                        f\"Got: {self.shadow_intensity_range}\",\n                    )\n            else:\n                raise TypeError(\"shadow_intensity_range should be an float or a tuple of floats.\")\n\n            return self\n\n    def __init__(\n        self,\n        shadow_roi: tuple[float, float, float, float] = (0, 0.5, 1, 1),\n        num_shadows_limit: tuple[int, int] = (1, 2),\n        num_shadows_lower: int | None = None,\n        num_shadows_upper: int | None = None,\n        shadow_dimension: int = 5,\n        shadow_intensity_range: tuple[float, float] = (0.5, 0.5),\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p=p, always_apply=always_apply)\n\n        self.shadow_roi = shadow_roi\n        self.shadow_dimension = shadow_dimension\n        self.num_shadows_limit = num_shadows_limit\n        self.shadow_intensity_range = shadow_intensity_range\n\n    def apply(\n        self,\n        img: np.ndarray,\n        vertices_list: list[np.ndarray],\n        intensities: np.ndarray,\n        **params: Any,\n    ) -&gt; np.ndarray:\n        return fmain.add_shadow(img, vertices_list, intensities)\n\n    def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, list[np.ndarray]]:\n        height, width = params[\"shape\"][:2]\n\n        num_shadows = random.randint(self.num_shadows_limit[0], self.num_shadows_limit[1])\n\n        x_min, y_min, x_max, y_max = self.shadow_roi\n\n        x_min = int(x_min * width)\n        x_max = int(x_max * width)\n        y_min = int(y_min * height)\n        y_max = int(y_max * height)\n\n        vertices_list = [\n            np.stack(\n                [\n                    random_utils.randint(x_min, x_max, size=5),\n                    random_utils.randint(y_min, y_max, size=5),\n                ],\n                axis=1,\n            )\n            for _ in range(num_shadows)\n        ]\n\n        # Sample shadow intensity for each shadow\n        intensities = random_utils.uniform(\n            self.shadow_intensity_range[0],\n            self.shadow_intensity_range[1],\n            size=num_shadows,\n        )\n\n        return {\"vertices_list\": vertices_list, \"intensities\": intensities}\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return (\n            \"shadow_roi\",\n            \"num_shadows_limit\",\n            \"shadow_dimension\",\n        )\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.RandomShadow.apply","title":"<code>apply (self, img, vertices_list, intensities, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def apply(\n    self,\n    img: np.ndarray,\n    vertices_list: list[np.ndarray],\n    intensities: np.ndarray,\n    **params: Any,\n) -&gt; np.ndarray:\n    return fmain.add_shadow(img, vertices_list, intensities)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.RandomShadow.get_params_dependent_on_data","title":"<code>get_params_dependent_on_data (self, params, data)</code>","text":"<p>Returns parameters dependent on input.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, list[np.ndarray]]:\n    height, width = params[\"shape\"][:2]\n\n    num_shadows = random.randint(self.num_shadows_limit[0], self.num_shadows_limit[1])\n\n    x_min, y_min, x_max, y_max = self.shadow_roi\n\n    x_min = int(x_min * width)\n    x_max = int(x_max * width)\n    y_min = int(y_min * height)\n    y_max = int(y_max * height)\n\n    vertices_list = [\n        np.stack(\n            [\n                random_utils.randint(x_min, x_max, size=5),\n                random_utils.randint(y_min, y_max, size=5),\n            ],\n            axis=1,\n        )\n        for _ in range(num_shadows)\n    ]\n\n    # Sample shadow intensity for each shadow\n    intensities = random_utils.uniform(\n        self.shadow_intensity_range[0],\n        self.shadow_intensity_range[1],\n        size=num_shadows,\n    )\n\n    return {\"vertices_list\": vertices_list, \"intensities\": intensities}\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.RandomShadow.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return (\n        \"shadow_roi\",\n        \"num_shadows_limit\",\n        \"shadow_dimension\",\n    )\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.RandomSnow","title":"<code>class  RandomSnow</code> <code>     (snow_point_lower=None, snow_point_upper=None, brightness_coeff=2.5, snow_point_range=(0.1, 0.3), always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Bleach out some pixel values imitating snow.</p> <p>Parameters:</p> Name Type Description <code>snow_point_range</code> <code>tuple</code> <p>tuple of bounds on the amount of snow i.e. (snow_point_lower, snow_point_upper). Both values should be in the (0, 1) range. Default: (0.1, 0.3).</p> <code>brightness_coeff</code> <code>float</code> <p>Coefficient applied to increase the brightness of pixels below the snow_point threshold. Larger values lead to more pronounced snow effects. Should be &gt; 0. Default: 2.5.</p> <code>p</code> <code>float</code> <p>Probability of applying the transform. Default: 0.5.</p> <p>Targets</p> <p>image</p> <p>Image types:     uint8, float32</p> <p>Reference</p> <p>https://github.com/UjjwalSaxena/Automold--Road-Augmentation-Library</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>class RandomSnow(ImageOnlyTransform):\n    \"\"\"Bleach out some pixel values imitating snow.\n\n    Args:\n        snow_point_range (tuple): tuple of bounds on the amount of snow i.e. (snow_point_lower, snow_point_upper).\n            Both values should be in the (0, 1) range. Default: (0.1, 0.3).\n        brightness_coeff (float): Coefficient applied to increase the brightness of pixels\n            below the snow_point threshold. Larger values lead to more pronounced snow effects.\n            Should be &gt; 0. Default: 2.5.\n        p (float): Probability of applying the transform. Default: 0.5.\n\n    Targets:\n        image\n\n    Image types:\n        uint8, float32\n\n    Reference:\n        https://github.com/UjjwalSaxena/Automold--Road-Augmentation-Library\n\n    \"\"\"\n\n    class InitSchema(BaseTransformInitSchema):\n        snow_point_range: Annotated[tuple[float, float], AfterValidator(check_01), AfterValidator(nondecreasing)] = (\n            Field(\n                default=(0.1, 0.3),\n                description=\"lower and upper bound on the amount of snow as tuple (snow_point_lower, snow_point_upper)\",\n            )\n        )\n        snow_point_lower: float | None = Field(\n            default=None,\n            description=\"Lower bound of the amount of snow\",\n            gt=0,\n            lt=1,\n        )\n        snow_point_upper: float | None = Field(\n            default=None,\n            description=\"Upper bound of the amount of snow\",\n            gt=0,\n            lt=1,\n        )\n        brightness_coeff: float = Field(default=2.5, description=\"Brightness coefficient, must be &gt; 0\", gt=0)\n\n        @model_validator(mode=\"after\")\n        def validate_ranges(self) -&gt; Self:\n            if self.snow_point_lower is not None or self.snow_point_upper is not None:\n                if self.snow_point_lower is not None:\n                    warn(\n                        \"`snow_point_lower` deprecated. Use `snow_point_range` as tuple\"\n                        \" (snow_point_lower, snow_point_upper) instead.\",\n                        DeprecationWarning,\n                        stacklevel=2,\n                    )\n                if self.snow_point_upper is not None:\n                    warn(\n                        \"`snow_point_upper` deprecated. Use `snow_point_range` as tuple\"\n                        \"(snow_point_lower, snow_point_upper) instead.\",\n                        DeprecationWarning,\n                        stacklevel=2,\n                    )\n                lower = self.snow_point_lower if self.snow_point_lower is not None else self.snow_point_range[0]\n                upper = self.snow_point_upper if self.snow_point_upper is not None else self.snow_point_range[1]\n                self.snow_point_range = (lower, upper)\n                self.snow_point_lower = None\n                self.snow_point_upper = None\n\n            # Validate the snow_point_range\n            if not (0 &lt; self.snow_point_range[0] &lt;= self.snow_point_range[1] &lt; 1):\n                raise ValueError(\"snow_point_range values should be increasing within (0, 1) range.\")\n\n            return self\n\n    def __init__(\n        self,\n        snow_point_lower: float | None = None,\n        snow_point_upper: float | None = None,\n        brightness_coeff: float = 2.5,\n        snow_point_range: tuple[float, float] = (0.1, 0.3),\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p, always_apply)\n\n        self.snow_point_range = snow_point_range\n        self.brightness_coeff = brightness_coeff\n\n    def apply(self, img: np.ndarray, snow_point: float, **params: Any) -&gt; np.ndarray:\n        return fmain.add_snow(img, snow_point, self.brightness_coeff)\n\n    def get_params(self) -&gt; dict[str, np.ndarray]:\n        return {\"snow_point\": random.uniform(*self.snow_point_range)}\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, str]:\n        return \"snow_point_range\", \"brightness_coeff\"\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.RandomSnow.apply","title":"<code>apply (self, img, snow_point, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def apply(self, img: np.ndarray, snow_point: float, **params: Any) -&gt; np.ndarray:\n    return fmain.add_snow(img, snow_point, self.brightness_coeff)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.RandomSnow.get_params","title":"<code>get_params (self)</code>","text":"<p>Returns parameters independent of input.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_params(self) -&gt; dict[str, np.ndarray]:\n    return {\"snow_point\": random.uniform(*self.snow_point_range)}\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.RandomSnow.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, str]:\n    return \"snow_point_range\", \"brightness_coeff\"\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.RandomSunFlare","title":"<code>class  RandomSunFlare</code> <code>     (flare_roi=(0, 0, 1, 0.5), angle_lower=None, angle_upper=None, num_flare_circles_lower=None, num_flare_circles_upper=None, src_radius=400, src_color=(255, 255, 255), angle_range=(0, 1), num_flare_circles_range=(6, 10), always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Simulates Sun Flare for the image</p> <p>Parameters:</p> Name Type Description <code>flare_roi</code> <code>tuple[float, float, float, float]</code> <p>Tuple specifying the region of the image where flare will appear (x_min, y_min, x_max, y_max). All values should be in range [0, 1].</p> <code>src_radius</code> <code>int</code> <p>Radius of the source for the flare.</p> <code>src_color</code> <code>tuple[int, int, int]</code> <p>Color of the flare as an (R, G, B) tuple.</p> <code>angle_range</code> <code>tuple[float, float]</code> <p>tuple specifying the range of angles for the flare. Both ends of the range are in the [0, 1] interval.</p> <code>num_flare_circles_range</code> <code>tuple[int, int]</code> <p>tuple specifying the range for the number of flare circles.</p> <code>p</code> <code>float</code> <p>Probability of applying the transform.</p> <p>Targets</p> <p>image</p> <p>Image types:     uint8</p> <p>Reference</p> <p>https://github.com/UjjwalSaxena/Automold--Road-Augmentation-Library</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>class RandomSunFlare(ImageOnlyTransform):\n    \"\"\"Simulates Sun Flare for the image\n\n    Args:\n        flare_roi (tuple[float, float, float, float]): Tuple specifying the region of the image where flare will\n            appear (x_min, y_min, x_max, y_max). All values should be in range [0, 1].\n        src_radius (int): Radius of the source for the flare.\n        src_color (tuple[int, int, int]): Color of the flare as an (R, G, B) tuple.\n        angle_range (tuple[float, float]): tuple specifying the range of angles for the flare.\n            Both ends of the range are in the [0, 1] interval.\n        num_flare_circles_range (tuple[int, int]): tuple specifying the range for the number of flare circles.\n        p (float): Probability of applying the transform.\n\n    Targets:\n        image\n\n    Image types:\n        uint8\n\n    Reference:\n        https://github.com/UjjwalSaxena/Automold--Road-Augmentation-Library\n    \"\"\"\n\n    class InitSchema(BaseTransformInitSchema):\n        flare_roi: tuple[float, float, float, float] = Field(\n            default=(0, 0, 1, 0.5),\n            description=\"Region of the image where flare will appear\",\n        )\n        angle_lower: float | None = Field(default=None, description=\"Lower bound for the angle\", ge=0, le=1)\n        angle_upper: float | None = Field(default=None, description=\"Upper bound for the angle\", ge=0, le=1)\n\n        num_flare_circles_lower: int | None = Field(\n            default=6,\n            description=\"Lower limit for the number of flare circles\",\n            ge=0,\n        )\n        num_flare_circles_upper: int | None = Field(\n            default=10,\n            description=\"Upper limit for the number of flare circles\",\n            gt=0,\n        )\n        src_radius: int = Field(default=400, description=\"Source radius for the flare\")\n        src_color: tuple[int, ...] = Field(default=(255, 255, 255), description=\"Color of the flare\")\n\n        angle_range: Annotated[tuple[float, float], AfterValidator(check_01), AfterValidator(nondecreasing)] = Field(\n            default=(0, 1),\n            description=\"Angle range\",\n        )\n\n        num_flare_circles_range: Annotated[\n            tuple[int, int],\n            AfterValidator(check_1plus),\n            AfterValidator(nondecreasing),\n        ] = Field(default=(6, 10), description=\"Number of flare circles range\")\n\n        @model_validator(mode=\"after\")\n        def validate_parameters(self) -&gt; Self:\n            flare_center_lower_x, flare_center_lower_y, flare_center_upper_x, flare_center_upper_y = self.flare_roi\n            if (\n                not 0 &lt;= flare_center_lower_x &lt; flare_center_upper_x &lt;= 1\n                or not 0 &lt;= flare_center_lower_y &lt; flare_center_upper_y &lt;= 1\n            ):\n                raise ValueError(f\"Invalid flare_roi. Got: {self.flare_roi}\")\n\n            if self.angle_lower is not None or self.angle_upper is not None:\n                if self.angle_lower is not None:\n                    warn(\n                        \"`angle_lower` deprecated. Use `angle_range` as tuple (angle_lower, angle_upper) instead.\",\n                        DeprecationWarning,\n                        stacklevel=2,\n                    )\n                if self.angle_upper is not None:\n                    warn(\n                        \"`angle_upper` deprecated. Use `angle_range` as tuple(angle_lower, angle_upper) instead.\",\n                        DeprecationWarning,\n                        stacklevel=2,\n                    )\n                lower = self.angle_lower if self.angle_lower is not None else self.angle_range[0]\n                upper = self.angle_upper if self.angle_upper is not None else self.angle_range[1]\n                self.angle_range = (lower, upper)\n\n            if self.num_flare_circles_lower is not None or self.num_flare_circles_upper is not None:\n                if self.num_flare_circles_lower is not None:\n                    warn(\n                        \"`num_flare_circles_lower` deprecated. Use `num_flare_circles_range` as tuple\"\n                        \" (num_flare_circles_lower, num_flare_circles_upper) instead.\",\n                        DeprecationWarning,\n                        stacklevel=2,\n                    )\n                if self.num_flare_circles_upper is not None:\n                    warn(\n                        \"`num_flare_circles_upper` deprecated. Use `num_flare_circles_range` as tuple\"\n                        \" (num_flare_circles_lower, num_flare_circles_upper) instead.\",\n                        DeprecationWarning,\n                        stacklevel=2,\n                    )\n                lower = (\n                    self.num_flare_circles_lower\n                    if self.num_flare_circles_lower is not None\n                    else self.num_flare_circles_range[0]\n                )\n                upper = (\n                    self.num_flare_circles_upper\n                    if self.num_flare_circles_upper is not None\n                    else self.num_flare_circles_range[1]\n                )\n                self.num_flare_circles_range = (lower, upper)\n\n            return self\n\n    def __init__(\n        self,\n        flare_roi: tuple[float, float, float, float] = (0, 0, 1, 0.5),\n        angle_lower: float | None = None,\n        angle_upper: float | None = None,\n        num_flare_circles_lower: int | None = None,\n        num_flare_circles_upper: int | None = None,\n        src_radius: int = 400,\n        src_color: tuple[int, ...] = (255, 255, 255),\n        angle_range: tuple[float, float] = (0, 1),\n        num_flare_circles_range: tuple[int, int] = (6, 10),\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p=p, always_apply=always_apply)\n\n        self.angle_range = angle_range\n        self.num_flare_circles_range = num_flare_circles_range\n\n        self.src_radius = src_radius\n        self.src_color = src_color\n        self.flare_roi = flare_roi\n\n    def apply(\n        self,\n        img: np.ndarray,\n        flare_center: tuple[float, float],\n        circles: list[Any],\n        **params: Any,\n    ) -&gt; np.ndarray:\n        if circles is None:\n            circles = []\n        return fmain.add_sun_flare(\n            img,\n            flare_center,\n            self.src_radius,\n            self.src_color,\n            circles,\n        )\n\n    def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n        height, width = params[\"shape\"][:2]\n\n        angle = 2 * math.pi * random.uniform(*self.angle_range)\n\n        (flare_center_lower_x, flare_center_lower_y, flare_center_upper_x, flare_center_upper_y) = self.flare_roi\n\n        flare_center_x = random.uniform(flare_center_lower_x, flare_center_upper_x)\n        flare_center_y = random.uniform(flare_center_lower_y, flare_center_upper_y)\n\n        flare_center_x = int(width * flare_center_x)\n        flare_center_y = int(height * flare_center_y)\n\n        num_circles = random.randint(*self.num_flare_circles_range)\n\n        circles = []\n\n        x = []\n        y = []\n\n        def line(t: float) -&gt; tuple[float, float]:\n            return (flare_center_x + t * math.cos(angle), flare_center_y + t * math.sin(angle))\n\n        for t_val in range(-flare_center_x, width - flare_center_x, 10):\n            rand_x, rand_y = line(t_val)\n            x.append(rand_x)\n            y.append(rand_y)\n\n        for _ in range(num_circles):\n            alpha = random.uniform(0.05, 0.2)\n            r = random.randint(0, len(x) - 1)\n            rad = random.randint(1, max(height // 100 - 2, 2))\n\n            r_color = random.randint(max(self.src_color[0] - 50, 0), self.src_color[0])\n            g_color = random.randint(max(self.src_color[1] - 50, 0), self.src_color[1])\n            b_color = random.randint(max(self.src_color[2] - 50, 0), self.src_color[2])\n\n            circles += [\n                (\n                    alpha,\n                    (int(x[r]), int(y[r])),\n                    pow(rad, 3),\n                    (r_color, g_color, b_color),\n                ),\n            ]\n\n        return {\n            \"circles\": circles,\n            \"flare_center\": (flare_center_x, flare_center_y),\n        }\n\n    def get_transform_init_args(self) -&gt; dict[str, Any]:\n        return {\n            \"flare_roi\": self.flare_roi,\n            \"angle_range\": self.angle_range,\n            \"num_flare_circles_range\": self.num_flare_circles_range,\n            \"src_radius\": self.src_radius,\n            \"src_color\": self.src_color,\n        }\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.RandomSunFlare.apply","title":"<code>apply (self, img, flare_center, circles, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def apply(\n    self,\n    img: np.ndarray,\n    flare_center: tuple[float, float],\n    circles: list[Any],\n    **params: Any,\n) -&gt; np.ndarray:\n    if circles is None:\n        circles = []\n    return fmain.add_sun_flare(\n        img,\n        flare_center,\n        self.src_radius,\n        self.src_color,\n        circles,\n    )\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.RandomSunFlare.get_params_dependent_on_data","title":"<code>get_params_dependent_on_data (self, params, data)</code>","text":"<p>Returns parameters dependent on input.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n    height, width = params[\"shape\"][:2]\n\n    angle = 2 * math.pi * random.uniform(*self.angle_range)\n\n    (flare_center_lower_x, flare_center_lower_y, flare_center_upper_x, flare_center_upper_y) = self.flare_roi\n\n    flare_center_x = random.uniform(flare_center_lower_x, flare_center_upper_x)\n    flare_center_y = random.uniform(flare_center_lower_y, flare_center_upper_y)\n\n    flare_center_x = int(width * flare_center_x)\n    flare_center_y = int(height * flare_center_y)\n\n    num_circles = random.randint(*self.num_flare_circles_range)\n\n    circles = []\n\n    x = []\n    y = []\n\n    def line(t: float) -&gt; tuple[float, float]:\n        return (flare_center_x + t * math.cos(angle), flare_center_y + t * math.sin(angle))\n\n    for t_val in range(-flare_center_x, width - flare_center_x, 10):\n        rand_x, rand_y = line(t_val)\n        x.append(rand_x)\n        y.append(rand_y)\n\n    for _ in range(num_circles):\n        alpha = random.uniform(0.05, 0.2)\n        r = random.randint(0, len(x) - 1)\n        rad = random.randint(1, max(height // 100 - 2, 2))\n\n        r_color = random.randint(max(self.src_color[0] - 50, 0), self.src_color[0])\n        g_color = random.randint(max(self.src_color[1] - 50, 0), self.src_color[1])\n        b_color = random.randint(max(self.src_color[2] - 50, 0), self.src_color[2])\n\n        circles += [\n            (\n                alpha,\n                (int(x[r]), int(y[r])),\n                pow(rad, 3),\n                (r_color, g_color, b_color),\n            ),\n        ]\n\n    return {\n        \"circles\": circles,\n        \"flare_center\": (flare_center_x, flare_center_y),\n    }\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.RandomToneCurve","title":"<code>class  RandomToneCurve</code> <code>     (scale=0.1, per_channel=False, always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Randomly change the relationship between bright and dark areas of the image by manipulating its tone curve.</p> <p>Parameters:</p> Name Type Description <code>scale</code> <code>float</code> <p>Standard deviation of the normal distribution used to sample random distances to move two control points that modify the image's curve. Values should be in range [0, 1]. Default: 0.1</p> <code>per_channel</code> <code>bool</code> <p>If <code>True</code>, the tone curve will be applied to each channel of the input image separately, which can lead to color distortion. Default: False.</p> <code>p</code> <code>float</code> <p>Probability of applying the transform. Default: 0.5</p> <p>Targets</p> <p>image</p> <p>Image types:     uint8, float32</p> <p>Reference</p> <ul> <li>\"What Else Can Fool Deep Learning? Addressing Color Constancy Errors on Deep Neural Network Performance\"   by Mahmoud Afifi and Michael S. Brown, ICCV 2019.</li> <li>GitHub repository: https://github.com/mahmoudnafifi/WB_color_augmenter</li> </ul> <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from albumentations import RandomToneCurve\n&gt;&gt;&gt; img = np.random.randint(0, 256, (100, 100, 3), dtype=np.uint8)\n&gt;&gt;&gt; transform = RandomToneCurve(scale=0.1, per_channel=True, p=1.0)\n&gt;&gt;&gt; transformed_img = transform(image=img)['image']\n</code></pre> <p>This transform applies a random tone curve to the input image by adjusting the relationship between bright and dark areas. When <code>per_channel</code> is set to True, each channel is adjusted separately, potentially causing color distortions. Otherwise, the same adjustment is applied to all channels, preserving the original color relationships.</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>class RandomToneCurve(ImageOnlyTransform):\n    \"\"\"Randomly change the relationship between bright and dark areas of the image by manipulating its tone curve.\n\n    Args:\n        scale (float): Standard deviation of the normal distribution used to sample random distances\n            to move two control points that modify the image's curve. Values should be in range [0, 1]. Default: 0.1\n        per_channel (bool): If `True`, the tone curve will be applied to each channel of the input image separately,\n            which can lead to color distortion. Default: False.\n        p (float): Probability of applying the transform. Default: 0.5\n\n    Targets:\n        image\n\n    Image types:\n        uint8, float32\n\n    Reference:\n        - \"What Else Can Fool Deep Learning? Addressing Color Constancy Errors on Deep Neural Network Performance\"\n          by Mahmoud Afifi and Michael S. Brown, ICCV 2019.\n        - GitHub repository: https://github.com/mahmoudnafifi/WB_color_augmenter\n\n    Example:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from albumentations import RandomToneCurve\n        &gt;&gt;&gt; img = np.random.randint(0, 256, (100, 100, 3), dtype=np.uint8)\n        &gt;&gt;&gt; transform = RandomToneCurve(scale=0.1, per_channel=True, p=1.0)\n        &gt;&gt;&gt; transformed_img = transform(image=img)['image']\n\n    This transform applies a random tone curve to the input image by adjusting the relationship between bright and\n    dark areas. When `per_channel` is set to True, each channel is adjusted separately, potentially causing color\n    distortions. Otherwise, the same adjustment is applied to all channels, preserving the original color relationships.\n    \"\"\"\n\n    class InitSchema(BaseTransformInitSchema):\n        scale: float = Field(\n            default=0.1,\n            description=\"Standard deviation of the normal distribution used to sample random distances\",\n            ge=0,\n            le=1,\n        )\n        per_channel: bool = Field(default=False, description=\"Apply the tone curve to each channel separately\")\n\n    def __init__(\n        self,\n        scale: float = 0.1,\n        per_channel: bool = False,\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p=p, always_apply=always_apply)\n        self.scale = scale\n        self.per_channel = per_channel\n\n    def apply(\n        self,\n        img: np.ndarray,\n        low_y: float | np.ndarray,\n        high_y: float | np.ndarray,\n        **params: Any,\n    ) -&gt; np.ndarray:\n        return fmain.move_tone_curve(img, low_y, high_y)\n\n    def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n        image = data[\"image\"] if \"image\" in data else data[\"images\"][0]\n        num_channels = get_num_channels(image)\n\n        if self.per_channel and num_channels != 1:\n            return {\n                \"low_y\": np.clip(random_utils.normal(loc=0.25, scale=self.scale, size=[num_channels]), 0, 1),\n                \"high_y\": np.clip(random_utils.normal(loc=0.75, scale=self.scale, size=[num_channels]), 0, 1),\n            }\n        # Same values for all channels\n        low_y = np.clip(random_utils.normal(loc=0.25, scale=self.scale), 0, 1)\n        high_y = np.clip(random_utils.normal(loc=0.75, scale=self.scale), 0, 1)\n\n        return {\"low_y\": low_y, \"high_y\": high_y}\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return \"scale\", \"per_channel\"\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.RandomToneCurve.apply","title":"<code>apply (self, img, low_y, high_y, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def apply(\n    self,\n    img: np.ndarray,\n    low_y: float | np.ndarray,\n    high_y: float | np.ndarray,\n    **params: Any,\n) -&gt; np.ndarray:\n    return fmain.move_tone_curve(img, low_y, high_y)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.RandomToneCurve.get_params_dependent_on_data","title":"<code>get_params_dependent_on_data (self, params, data)</code>","text":"<p>Returns parameters dependent on input.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n    image = data[\"image\"] if \"image\" in data else data[\"images\"][0]\n    num_channels = get_num_channels(image)\n\n    if self.per_channel and num_channels != 1:\n        return {\n            \"low_y\": np.clip(random_utils.normal(loc=0.25, scale=self.scale, size=[num_channels]), 0, 1),\n            \"high_y\": np.clip(random_utils.normal(loc=0.75, scale=self.scale, size=[num_channels]), 0, 1),\n        }\n    # Same values for all channels\n    low_y = np.clip(random_utils.normal(loc=0.25, scale=self.scale), 0, 1)\n    high_y = np.clip(random_utils.normal(loc=0.75, scale=self.scale), 0, 1)\n\n    return {\"low_y\": low_y, \"high_y\": high_y}\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.RandomToneCurve.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return \"scale\", \"per_channel\"\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.RingingOvershoot","title":"<code>class  RingingOvershoot</code> <code>     (blur_limit=(7, 15), cutoff=(0.7853981633974483, 1.5707963267948966), always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Create ringing or overshoot artefacts by conlvolving image with 2D sinc filter.</p> <p>Parameters:</p> Name Type Description <code>blur_limit</code> <code>ScaleIntType</code> <p>maximum kernel size for sinc filter. Should be in range [3, inf). Default: (7, 15).</p> <code>cutoff</code> <code>ScaleFloatType</code> <p>range to choose the cutoff frequency in radians. Should be in range (0, np.pi) Default: (np.pi / 4, np.pi / 2).</p> <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 0.5.</p> <p>Reference</p> <p>dsp.stackexchange.com/questions/58301/2-d-circularly-symmetric-low-pass-filter https://arxiv.org/abs/2107.10833</p> <p>Targets</p> <p>image</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>class RingingOvershoot(ImageOnlyTransform):\n    \"\"\"Create ringing or overshoot artefacts by conlvolving image with 2D sinc filter.\n\n    Args:\n        blur_limit: maximum kernel size for sinc filter.\n            Should be in range [3, inf). Default: (7, 15).\n        cutoff: range to choose the cutoff frequency in radians.\n            Should be in range (0, np.pi)\n            Default: (np.pi / 4, np.pi / 2).\n        p: probability of applying the transform. Default: 0.5.\n\n    Reference:\n        dsp.stackexchange.com/questions/58301/2-d-circularly-symmetric-low-pass-filter\n        https://arxiv.org/abs/2107.10833\n\n    Targets:\n        image\n\n    \"\"\"\n\n    class InitSchema(BlurInitSchema):\n        blur_limit: ScaleIntType = Field(default=(7, 15), description=\"Maximum kernel size for sinc filter.\")\n        cutoff: ScaleFloatType = Field(default=(np.pi / 4, np.pi / 2), description=\"Cutoff frequency range in radians.\")\n\n        @field_validator(\"cutoff\")\n        @classmethod\n        def check_cutoff(cls, v: ScaleFloatType, info: ValidationInfo) -&gt; tuple[float, float]:\n            bounds = 0, np.pi\n            result = to_tuple(v, v)\n            check_range(result, *bounds, info.field_name)\n            return result\n\n    def __init__(\n        self,\n        blur_limit: ScaleIntType = (7, 15),\n        cutoff: ScaleFloatType = (np.pi / 4, np.pi / 2),\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p=p, always_apply=always_apply)\n        self.blur_limit = cast(Tuple[int, int], blur_limit)\n        self.cutoff = cast(Tuple[float, float], cutoff)\n\n    def get_params(self) -&gt; dict[str, np.ndarray]:\n        ksize = random.randrange(self.blur_limit[0], self.blur_limit[1] + 1, 2)\n        if ksize % 2 == 0:\n            raise ValueError(f\"Kernel size must be odd. Got: {ksize}\")\n\n        cutoff = random.uniform(*self.cutoff)\n\n        # From dsp.stackexchange.com/questions/58301/2-d-circularly-symmetric-low-pass-filter\n        with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n            kernel = np.fromfunction(\n                lambda x, y: cutoff\n                * special.j1(cutoff * np.sqrt((x - (ksize - 1) / 2) ** 2 + (y - (ksize - 1) / 2) ** 2))\n                / (2 * np.pi * np.sqrt((x - (ksize - 1) / 2) ** 2 + (y - (ksize - 1) / 2) ** 2)),\n                [ksize, ksize],\n            )\n        kernel[(ksize - 1) // 2, (ksize - 1) // 2] = cutoff**2 / (4 * np.pi)\n\n        # Normalize kernel\n        kernel = kernel.astype(np.float32) / np.sum(kernel)\n\n        return {\"kernel\": kernel}\n\n    def apply(self, img: np.ndarray, kernel: int, **params: Any) -&gt; np.ndarray:\n        return fmain.convolve(img, kernel)\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, str]:\n        return (\"blur_limit\", \"cutoff\")\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.RingingOvershoot.apply","title":"<code>apply (self, img, kernel, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def apply(self, img: np.ndarray, kernel: int, **params: Any) -&gt; np.ndarray:\n    return fmain.convolve(img, kernel)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.RingingOvershoot.get_params","title":"<code>get_params (self)</code>","text":"<p>Returns parameters independent of input.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_params(self) -&gt; dict[str, np.ndarray]:\n    ksize = random.randrange(self.blur_limit[0], self.blur_limit[1] + 1, 2)\n    if ksize % 2 == 0:\n        raise ValueError(f\"Kernel size must be odd. Got: {ksize}\")\n\n    cutoff = random.uniform(*self.cutoff)\n\n    # From dsp.stackexchange.com/questions/58301/2-d-circularly-symmetric-low-pass-filter\n    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n        kernel = np.fromfunction(\n            lambda x, y: cutoff\n            * special.j1(cutoff * np.sqrt((x - (ksize - 1) / 2) ** 2 + (y - (ksize - 1) / 2) ** 2))\n            / (2 * np.pi * np.sqrt((x - (ksize - 1) / 2) ** 2 + (y - (ksize - 1) / 2) ** 2)),\n            [ksize, ksize],\n        )\n    kernel[(ksize - 1) // 2, (ksize - 1) // 2] = cutoff**2 / (4 * np.pi)\n\n    # Normalize kernel\n    kernel = kernel.astype(np.float32) / np.sum(kernel)\n\n    return {\"kernel\": kernel}\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.RingingOvershoot.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, str]:\n    return (\"blur_limit\", \"cutoff\")\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.Sharpen","title":"<code>class  Sharpen</code> <code>     (alpha=(0.2, 0.5), lightness=(0.5, 1.0), always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Sharpen the input image and overlays the result with the original image.</p> <p>Parameters:</p> Name Type Description <code>alpha</code> <code>tuple[float, float]</code> <p>range to choose the visibility of the sharpened image. At 0, only the original image is visible, at 1.0 only its sharpened version is visible. Default: (0.2, 0.5).</p> <code>lightness</code> <code>tuple[float, float]</code> <p>range to choose the lightness of the sharpened image. Default: (0.5, 1.0).</p> <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 0.5.</p> <p>Targets</p> <p>image</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>class Sharpen(ImageOnlyTransform):\n    \"\"\"Sharpen the input image and overlays the result with the original image.\n\n    Args:\n        alpha: range to choose the visibility of the sharpened image. At 0, only the original image is\n            visible, at 1.0 only its sharpened version is visible. Default: (0.2, 0.5).\n        lightness: range to choose the lightness of the sharpened image. Default: (0.5, 1.0).\n        p: probability of applying the transform. Default: 0.5.\n\n    Targets:\n        image\n\n    \"\"\"\n\n    class InitSchema(BaseTransformInitSchema):\n        alpha: ZeroOneRangeType = (0.2, 0.5)\n        lightness: NonNegativeFloatRangeType = (0.5, 1.0)\n\n    def __init__(\n        self,\n        alpha: tuple[float, float] = (0.2, 0.5),\n        lightness: tuple[float, float] = (0.5, 1.0),\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p=p, always_apply=always_apply)\n        self.alpha = alpha\n        self.lightness = lightness\n\n    @staticmethod\n    def __generate_sharpening_matrix(alpha_sample: np.ndarray, lightness_sample: np.ndarray) -&gt; np.ndarray:\n        matrix_nochange = np.array([[0, 0, 0], [0, 1, 0], [0, 0, 0]], dtype=np.float32)\n        matrix_effect = np.array(\n            [[-1, -1, -1], [-1, 8 + lightness_sample, -1], [-1, -1, -1]],\n            dtype=np.float32,\n        )\n\n        return (1 - alpha_sample) * matrix_nochange + alpha_sample * matrix_effect\n\n    def get_params(self) -&gt; dict[str, np.ndarray]:\n        alpha = random.uniform(*self.alpha)\n        lightness = random.uniform(*self.lightness)\n        sharpening_matrix = self.__generate_sharpening_matrix(alpha_sample=alpha, lightness_sample=lightness)\n        return {\"sharpening_matrix\": sharpening_matrix}\n\n    def apply(self, img: np.ndarray, sharpening_matrix: np.ndarray, **params: Any) -&gt; np.ndarray:\n        return fmain.convolve(img, sharpening_matrix)\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, str]:\n        return (\"alpha\", \"lightness\")\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.Sharpen.apply","title":"<code>apply (self, img, sharpening_matrix, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def apply(self, img: np.ndarray, sharpening_matrix: np.ndarray, **params: Any) -&gt; np.ndarray:\n    return fmain.convolve(img, sharpening_matrix)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.Sharpen.get_params","title":"<code>get_params (self)</code>","text":"<p>Returns parameters independent of input.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_params(self) -&gt; dict[str, np.ndarray]:\n    alpha = random.uniform(*self.alpha)\n    lightness = random.uniform(*self.lightness)\n    sharpening_matrix = self.__generate_sharpening_matrix(alpha_sample=alpha, lightness_sample=lightness)\n    return {\"sharpening_matrix\": sharpening_matrix}\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.Sharpen.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, str]:\n    return (\"alpha\", \"lightness\")\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.Solarize","title":"<code>class  Solarize</code> <code>     (threshold=(128, 128), p=0.5, always_apply=None)                 </code>  [view source on GitHub]","text":"<p>Invert all pixel values above a threshold.</p> <p>Parameters:</p> Name Type Description <code>threshold</code> <code>ScaleType</code> <p>range for solarizing threshold. If threshold is a single value, the range will be [1, threshold]. Default: 128.</p> <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 0.5.</p> <p>Targets</p> <p>image</p> <p>Image types:     uint8, float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>class Solarize(ImageOnlyTransform):\n    \"\"\"Invert all pixel values above a threshold.\n\n    Args:\n        threshold: range for solarizing threshold.\n            If threshold is a single value, the range will be [1, threshold]. Default: 128.\n        p: probability of applying the transform. Default: 0.5.\n\n    Targets:\n        image\n\n    Image types:\n        uint8, float32\n\n    \"\"\"\n\n    class InitSchema(BaseTransformInitSchema):\n        threshold: OnePlusFloatRangeType = (128, 128)\n\n    def __init__(self, threshold: ScaleType = (128, 128), p: float = 0.5, always_apply: bool | None = None):\n        super().__init__(p=p, always_apply=always_apply)\n        self.threshold = cast(Tuple[float, float], threshold)\n\n    def apply(self, img: np.ndarray, threshold: int, **params: Any) -&gt; np.ndarray:\n        return fmain.solarize(img, threshold)\n\n    def get_params(self) -&gt; dict[str, float]:\n        return {\"threshold\": random.uniform(self.threshold[0], self.threshold[1])}\n\n    def get_transform_init_args_names(self) -&gt; tuple[str]:\n        return (\"threshold\",)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.Solarize.apply","title":"<code>apply (self, img, threshold, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def apply(self, img: np.ndarray, threshold: int, **params: Any) -&gt; np.ndarray:\n    return fmain.solarize(img, threshold)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.Solarize.get_params","title":"<code>get_params (self)</code>","text":"<p>Returns parameters independent of input.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_params(self) -&gt; dict[str, float]:\n    return {\"threshold\": random.uniform(self.threshold[0], self.threshold[1])}\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.Solarize.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str]:\n    return (\"threshold\",)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.Spatter","title":"<code>class  Spatter</code> <code>     (mean=(0.65, 0.65), std=(0.3, 0.3), gauss_sigma=(2, 2), cutout_threshold=(0.68, 0.68), intensity=(0.6, 0.6), mode='rain', color=None, always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Apply spatter transform. It simulates corruption which can occlude a lens in the form of rain or mud.</p> <p>Parameters:</p> Name Type Description <code>mean</code> <code>float, or tuple of floats</code> <p>Mean value of normal distribution for generating liquid layer. If single float mean will be sampled from <code>(0, mean)</code> If tuple of float mean will be sampled from range <code>(mean[0], mean[1])</code>. If you want constant value use (mean, mean). Default (0.65, 0.65)</p> <code>std</code> <code>float, or tuple of floats</code> <p>Standard deviation value of normal distribution for generating liquid layer. If single float the number will be sampled from <code>(0, std)</code>. If tuple of float std will be sampled from range <code>(std[0], std[1])</code>. If you want constant value use (std, std). Default: (0.3, 0.3).</p> <code>gauss_sigma</code> <code>float, or tuple of floats</code> <p>Sigma value for gaussian filtering of liquid layer. If single float the number will be sampled from <code>(0, gauss_sigma)</code>. If tuple of float gauss_sigma will be sampled from range <code>(gauss_sigma[0], gauss_sigma[1])</code>. If you want constant value use (gauss_sigma, gauss_sigma). Default: (2, 3).</p> <code>cutout_threshold</code> <code>float, or tuple of floats</code> <p>Threshold for filtering liqued layer (determines number of drops). If single float it will used as cutout_threshold. If single float the number will be sampled from <code>(0, cutout_threshold)</code>. If tuple of float cutout_threshold will be sampled from range <code>(cutout_threshold[0], cutout_threshold[1])</code>. If you want constant value use <code>(cutout_threshold, cutout_threshold)</code>. Default: (0.68, 0.68).</p> <code>intensity</code> <code>float, or tuple of floats</code> <p>Intensity of corruption. If single float the number will be sampled from <code>(0, intensity)</code>. If tuple of float intensity will be sampled from range <code>(intensity[0], intensity[1])</code>. If you want constant value use <code>(intensity, intensity)</code>. Default: (0.6, 0.6).</p> <code>mode</code> <code>string, or list of strings</code> <p>Type of corruption. Currently, supported options are 'rain' and 'mud'.  If list is provided type of corruption will be sampled list. Default: (\"rain\").</p> <code>color</code> <code>list of (r, g, b) or dict or None</code> <p>Corruption elements color. If list uses provided list as color for specified mode. If dict uses provided color for specified mode. Color for each specified mode should be provided in dict. If None uses default colors (rain: (238, 238, 175), mud: (20, 42, 63)).</p> <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 0.5.</p> <p>Targets</p> <p>image</p> <p>Image types:     uint8, float32</p> <p>Reference</p> <p>https://arxiv.org/abs/1903.12261 https://github.com/hendrycks/robustness/blob/master/ImageNet-C/create_c/make_imagenet_c.py</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>class Spatter(ImageOnlyTransform):\n    \"\"\"Apply spatter transform. It simulates corruption which can occlude a lens in the form of rain or mud.\n\n    Args:\n        mean (float, or tuple of floats): Mean value of normal distribution for generating liquid layer.\n            If single float mean will be sampled from `(0, mean)`\n            If tuple of float mean will be sampled from range `(mean[0], mean[1])`.\n            If you want constant value use (mean, mean).\n            Default (0.65, 0.65)\n        std (float, or tuple of floats): Standard deviation value of normal distribution for generating liquid layer.\n            If single float the number will be sampled from `(0, std)`.\n            If tuple of float std will be sampled from range `(std[0], std[1])`.\n            If you want constant value use (std, std).\n            Default: (0.3, 0.3).\n        gauss_sigma (float, or tuple of floats): Sigma value for gaussian filtering of liquid layer.\n            If single float the number will be sampled from `(0, gauss_sigma)`.\n            If tuple of float gauss_sigma will be sampled from range `(gauss_sigma[0], gauss_sigma[1])`.\n            If you want constant value use (gauss_sigma, gauss_sigma).\n            Default: (2, 3).\n        cutout_threshold (float, or tuple of floats): Threshold for filtering liqued layer\n            (determines number of drops). If single float it will used as cutout_threshold.\n            If single float the number will be sampled from `(0, cutout_threshold)`.\n            If tuple of float cutout_threshold will be sampled from range `(cutout_threshold[0], cutout_threshold[1])`.\n            If you want constant value use `(cutout_threshold, cutout_threshold)`.\n            Default: (0.68, 0.68).\n        intensity (float, or tuple of floats): Intensity of corruption.\n            If single float the number will be sampled from `(0, intensity)`.\n            If tuple of float intensity will be sampled from range `(intensity[0], intensity[1])`.\n            If you want constant value use `(intensity, intensity)`.\n            Default: (0.6, 0.6).\n        mode (string, or list of strings): Type of corruption. Currently, supported options are 'rain' and 'mud'.\n             If list is provided type of corruption will be sampled list. Default: (\"rain\").\n        color (list of (r, g, b) or dict or None): Corruption elements color.\n            If list uses provided list as color for specified mode.\n            If dict uses provided color for specified mode. Color for each specified mode should be provided in dict.\n            If None uses default colors (rain: (238, 238, 175), mud: (20, 42, 63)).\n        p (float): probability of applying the transform. Default: 0.5.\n\n    Targets:\n        image\n\n    Image types:\n        uint8, float32\n\n    Reference:\n        https://arxiv.org/abs/1903.12261\n        https://github.com/hendrycks/robustness/blob/master/ImageNet-C/create_c/make_imagenet_c.py\n\n    \"\"\"\n\n    class InitSchema(BaseTransformInitSchema):\n        mean: ZeroOneRangeType = (0.65, 0.65)\n        std: ZeroOneRangeType = (0.3, 0.3)\n        gauss_sigma: NonNegativeFloatRangeType = (2, 2)\n        cutout_threshold: ZeroOneRangeType = (0.68, 0.68)\n        intensity: ZeroOneRangeType = (0.6, 0.6)\n        mode: SpatterMode | Sequence[SpatterMode] = Field(\n            default=\"rain\",\n            description=\"Type of corruption ('rain', 'mud').\",\n        )\n        color: Sequence[int] | dict[str, Sequence[int]] | None = None\n\n        @field_validator(\"mode\")\n        @classmethod\n        def check_mode(cls, mode: SpatterMode | Sequence[SpatterMode]) -&gt; Sequence[SpatterMode]:\n            if isinstance(mode, str):\n                return [mode]\n            return mode\n\n        @model_validator(mode=\"after\")\n        def check_color(self) -&gt; Self:\n            if self.color is None:\n                self.color = {\"rain\": [238, 238, 175], \"mud\": [20, 42, 63]}\n\n            elif isinstance(self.color, (list, tuple)) and len(self.mode) == 1:\n                if len(self.color) != NUM_RGB_CHANNELS:\n                    msg = \"Color must be a list of three integers for RGB format.\"\n                    raise ValueError(msg)\n                self.color = {self.mode[0]: self.color}\n            elif isinstance(self.color, dict):\n                result = {}\n                for mode in self.mode:\n                    if mode not in self.color:\n                        raise ValueError(f\"Color for mode {mode} is not specified.\")\n                    if len(self.color[mode]) != NUM_RGB_CHANNELS:\n                        raise ValueError(f\"Color for mode {mode} must be in RGB format.\")\n                    result[mode] = self.color[mode]\n            else:\n                msg = \"Color must be a list of RGB values or a dict mapping mode to RGB values.\"\n                raise ValueError(msg)\n            return self\n\n    def __init__(\n        self,\n        mean: ScaleFloatType = (0.65, 0.65),\n        std: ScaleFloatType = (0.3, 0.3),\n        gauss_sigma: ScaleFloatType = (2, 2),\n        cutout_threshold: ScaleFloatType = (0.68, 0.68),\n        intensity: ScaleFloatType = (0.6, 0.6),\n        mode: SpatterMode | Sequence[SpatterMode] = \"rain\",\n        color: Sequence[int] | dict[str, Sequence[int]] | None = None,\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p=p, always_apply=always_apply)\n        self.mean = cast(Tuple[float, float], mean)\n        self.std = cast(Tuple[float, float], std)\n        self.gauss_sigma = cast(Tuple[float, float], gauss_sigma)\n        self.cutout_threshold = cast(Tuple[float, float], cutout_threshold)\n        self.intensity = cast(Tuple[float, float], intensity)\n        self.mode = mode\n        self.color = cast(Dict[str, Sequence[int]], color)\n\n    def apply(\n        self,\n        img: np.ndarray,\n        non_mud: np.ndarray,\n        mud: np.ndarray,\n        drops: np.ndarray,\n        mode: SpatterMode,\n        **params: dict[str, Any],\n    ) -&gt; np.ndarray:\n        return fmain.spatter(img, non_mud, mud, drops, mode)\n\n    def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n        height, width = params[\"shape\"][:2]\n\n        mean = random.uniform(self.mean[0], self.mean[1])\n        std = random.uniform(self.std[0], self.std[1])\n        cutout_threshold = random.uniform(self.cutout_threshold[0], self.cutout_threshold[1])\n        sigma = random.uniform(self.gauss_sigma[0], self.gauss_sigma[1])\n        mode = random.choice(self.mode)\n        intensity = random.uniform(self.intensity[0], self.intensity[1])\n        color = np.array(self.color[mode]) / 255.0\n\n        liquid_layer = random_utils.normal(size=(height, width), loc=mean, scale=std)\n        liquid_layer = gaussian_filter(liquid_layer, sigma=sigma, mode=\"nearest\")\n        liquid_layer[liquid_layer &lt; cutout_threshold] = 0\n\n        if mode == \"rain\":\n            liquid_layer = clip(liquid_layer * 255, np.uint8)\n            dist = 255 - cv2.Canny(liquid_layer, 50, 150)\n            dist = cv2.distanceTransform(dist, cv2.DIST_L2, 5)\n            _, dist = cv2.threshold(dist, 20, 20, cv2.THRESH_TRUNC)\n            dist = clip(blur(dist, 3), np.uint8)\n            dist = fmain.equalize(dist)\n\n            ker = np.array([[-2, -1, 0], [-1, 1, 1], [0, 1, 2]])\n            dist = fmain.convolve(dist, ker)\n            dist = blur(dist, 3).astype(np.float32)\n\n            m = liquid_layer * dist\n            m *= 1 / np.max(m, axis=(0, 1))\n\n            drops = m[:, :, None] * color * intensity\n            mud = None\n            non_mud = None\n        else:\n            m = np.where(liquid_layer &gt; cutout_threshold, 1, 0)\n            m = gaussian_filter(m.astype(np.float32), sigma=sigma, mode=\"nearest\")\n            m[m &lt; 1.2 * cutout_threshold] = 0\n            m = m[..., np.newaxis]\n\n            mud = m * color\n            non_mud = 1 - m\n            drops = None\n\n        return {\n            \"non_mud\": non_mud,\n            \"mud\": mud,\n            \"drops\": drops,\n            \"mode\": mode,\n        }\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, str, str, str, str, str, str]:\n        return \"mean\", \"std\", \"gauss_sigma\", \"intensity\", \"cutout_threshold\", \"mode\", \"color\"\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.Spatter.apply","title":"<code>apply (self, img, non_mud, mud, drops, mode, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def apply(\n    self,\n    img: np.ndarray,\n    non_mud: np.ndarray,\n    mud: np.ndarray,\n    drops: np.ndarray,\n    mode: SpatterMode,\n    **params: dict[str, Any],\n) -&gt; np.ndarray:\n    return fmain.spatter(img, non_mud, mud, drops, mode)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.Spatter.get_params_dependent_on_data","title":"<code>get_params_dependent_on_data (self, params, data)</code>","text":"<p>Returns parameters dependent on input.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n    height, width = params[\"shape\"][:2]\n\n    mean = random.uniform(self.mean[0], self.mean[1])\n    std = random.uniform(self.std[0], self.std[1])\n    cutout_threshold = random.uniform(self.cutout_threshold[0], self.cutout_threshold[1])\n    sigma = random.uniform(self.gauss_sigma[0], self.gauss_sigma[1])\n    mode = random.choice(self.mode)\n    intensity = random.uniform(self.intensity[0], self.intensity[1])\n    color = np.array(self.color[mode]) / 255.0\n\n    liquid_layer = random_utils.normal(size=(height, width), loc=mean, scale=std)\n    liquid_layer = gaussian_filter(liquid_layer, sigma=sigma, mode=\"nearest\")\n    liquid_layer[liquid_layer &lt; cutout_threshold] = 0\n\n    if mode == \"rain\":\n        liquid_layer = clip(liquid_layer * 255, np.uint8)\n        dist = 255 - cv2.Canny(liquid_layer, 50, 150)\n        dist = cv2.distanceTransform(dist, cv2.DIST_L2, 5)\n        _, dist = cv2.threshold(dist, 20, 20, cv2.THRESH_TRUNC)\n        dist = clip(blur(dist, 3), np.uint8)\n        dist = fmain.equalize(dist)\n\n        ker = np.array([[-2, -1, 0], [-1, 1, 1], [0, 1, 2]])\n        dist = fmain.convolve(dist, ker)\n        dist = blur(dist, 3).astype(np.float32)\n\n        m = liquid_layer * dist\n        m *= 1 / np.max(m, axis=(0, 1))\n\n        drops = m[:, :, None] * color * intensity\n        mud = None\n        non_mud = None\n    else:\n        m = np.where(liquid_layer &gt; cutout_threshold, 1, 0)\n        m = gaussian_filter(m.astype(np.float32), sigma=sigma, mode=\"nearest\")\n        m[m &lt; 1.2 * cutout_threshold] = 0\n        m = m[..., np.newaxis]\n\n        mud = m * color\n        non_mud = 1 - m\n        drops = None\n\n    return {\n        \"non_mud\": non_mud,\n        \"mud\": mud,\n        \"drops\": drops,\n        \"mode\": mode,\n    }\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.Spatter.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, str, str, str, str, str, str]:\n    return \"mean\", \"std\", \"gauss_sigma\", \"intensity\", \"cutout_threshold\", \"mode\", \"color\"\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.Superpixels","title":"<code>class  Superpixels</code> <code>     (p_replace=(0, 0.1), n_segments=(100, 100), max_size=128, interpolation=1, always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Transform images partially/completely to their superpixel representation. This implementation uses skimage's version of the SLIC algorithm.</p> <p>Parameters:</p> Name Type Description <code>p_replace</code> <code>float or tuple of float</code> <p>Defines for any segment the probability that the pixels within that segment are replaced by their average color (otherwise, the pixels are not changed).</p> <p>Examples:</p> <ul> <li>A probability of <code>0.0</code> would mean, that the pixels in no   segment are replaced by their average color (image is not   changed at all).</li> <li>A probability of <code>0.5</code> would mean, that around half of all   segments are replaced by their average color.</li> <li>A probability of <code>1.0</code> would mean, that all segments are   replaced by their average color (resulting in a Voronoi   image).</li> </ul> <pre><code>    Behavior based on chosen data types for this parameter:\n        * If a ``float``, then that ``flat`` will always be used.\n        * If ``tuple`` ``(a, b)``, then a random probability will be\n          sampled from the interval ``[a, b]`` per image.\nn_segments (tuple of int): Rough target number of how many superpixels to generate (the algorithm\n    may deviate from this number). Lower value will lead to coarser superpixels.\n    Higher values are computationally more intensive and will hence lead to a slowdown\n    Then a value from the discrete interval ``[a..b]`` will be sampled per image.\n    If input is a single integer, the range will be ``(1, n_segments)``.\n    If interested in a fixed number of segments, use ``(n_segments, n_segments)``.\nmax_size (int or None): Maximum image size at which the augmentation is performed.\n    If the width or height of an image exceeds this value, it will be\n    downscaled before the augmentation so that the longest side matches `max_size`.\n    This is done to speed up the process. The final output image has the same size as the input image.\n    Note that in case `p_replace` is below ``1.0``,\n    the down-/upscaling will affect the not-replaced pixels too.\n    Use ``None`` to apply no down-/upscaling.\ninterpolation (OpenCV flag): flag that is used to specify the interpolation algorithm. Should be one of:\n    cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.\n    Default: cv2.INTER_LINEAR.\np (float): probability of applying the transform. Default: 0.5.\n</code></pre> <p>Targets</p> <p>image</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>class Superpixels(ImageOnlyTransform):\n    \"\"\"Transform images partially/completely to their superpixel representation.\n    This implementation uses skimage's version of the SLIC algorithm.\n\n    Args:\n        p_replace (float or tuple of float): Defines for any segment the probability that the pixels within that\n            segment are replaced by their average color (otherwise, the pixels are not changed).\n\n    Examples:\n                * A probability of ``0.0`` would mean, that the pixels in no\n                  segment are replaced by their average color (image is not\n                  changed at all).\n                * A probability of ``0.5`` would mean, that around half of all\n                  segments are replaced by their average color.\n                * A probability of ``1.0`` would mean, that all segments are\n                  replaced by their average color (resulting in a Voronoi\n                  image).\n            Behavior based on chosen data types for this parameter:\n                * If a ``float``, then that ``flat`` will always be used.\n                * If ``tuple`` ``(a, b)``, then a random probability will be\n                  sampled from the interval ``[a, b]`` per image.\n        n_segments (tuple of int): Rough target number of how many superpixels to generate (the algorithm\n            may deviate from this number). Lower value will lead to coarser superpixels.\n            Higher values are computationally more intensive and will hence lead to a slowdown\n            Then a value from the discrete interval ``[a..b]`` will be sampled per image.\n            If input is a single integer, the range will be ``(1, n_segments)``.\n            If interested in a fixed number of segments, use ``(n_segments, n_segments)``.\n        max_size (int or None): Maximum image size at which the augmentation is performed.\n            If the width or height of an image exceeds this value, it will be\n            downscaled before the augmentation so that the longest side matches `max_size`.\n            This is done to speed up the process. The final output image has the same size as the input image.\n            Note that in case `p_replace` is below ``1.0``,\n            the down-/upscaling will affect the not-replaced pixels too.\n            Use ``None`` to apply no down-/upscaling.\n        interpolation (OpenCV flag): flag that is used to specify the interpolation algorithm. Should be one of:\n            cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.\n            Default: cv2.INTER_LINEAR.\n        p (float): probability of applying the transform. Default: 0.5.\n\n    Targets:\n        image\n\n    \"\"\"\n\n    class InitSchema(BaseTransformInitSchema):\n        p_replace: ZeroOneRangeType = (0, 0.1)\n        n_segments: OnePlusIntRangeType = (100, 100)\n        max_size: int | None = Field(default=128, ge=1, description=\"Maximum image size for the transformation.\")\n        interpolation: InterpolationType = cv2.INTER_LINEAR\n\n    def __init__(\n        self,\n        p_replace: ScaleFloatType = (0, 0.1),\n        n_segments: ScaleIntType = (100, 100),\n        max_size: int | None = 128,\n        interpolation: int = cv2.INTER_LINEAR,\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p=p, always_apply=always_apply)\n        self.p_replace = cast(Tuple[float, float], p_replace)\n        self.n_segments = cast(Tuple[int, int], n_segments)\n        self.max_size = max_size\n        self.interpolation = interpolation\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return \"p_replace\", \"n_segments\", \"max_size\", \"interpolation\"\n\n    def get_params(self) -&gt; dict[str, Any]:\n        n_segments = random.randint(self.n_segments[0], self.n_segments[1])\n        p = random.uniform(*self.p_replace)\n        return {\"replace_samples\": random_utils.random(n_segments) &lt; p, \"n_segments\": n_segments}\n\n    def apply(\n        self,\n        img: np.ndarray,\n        replace_samples: Sequence[bool],\n        n_segments: int,\n        **kwargs: Any,\n    ) -&gt; np.ndarray:\n        return fmain.superpixels(img, n_segments, replace_samples, self.max_size, self.interpolation)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.Superpixels.apply","title":"<code>apply (self, img, replace_samples, n_segments, **kwargs)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def apply(\n    self,\n    img: np.ndarray,\n    replace_samples: Sequence[bool],\n    n_segments: int,\n    **kwargs: Any,\n) -&gt; np.ndarray:\n    return fmain.superpixels(img, n_segments, replace_samples, self.max_size, self.interpolation)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.Superpixels.get_params","title":"<code>get_params (self)</code>","text":"<p>Returns parameters independent of input.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_params(self) -&gt; dict[str, Any]:\n    n_segments = random.randint(self.n_segments[0], self.n_segments[1])\n    p = random.uniform(*self.p_replace)\n    return {\"replace_samples\": random_utils.random(n_segments) &lt; p, \"n_segments\": n_segments}\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.Superpixels.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return \"p_replace\", \"n_segments\", \"max_size\", \"interpolation\"\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.TemplateTransform","title":"<code>class  TemplateTransform</code> <code>     (templates, img_weight=(0.5, 0.5), template_weight=(0.5, 0.5), template_transform=None, name=None, always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Apply blending of input image with specified templates</p> <p>Parameters:</p> Name Type Description <code>templates</code> <code>numpy array or list of numpy arrays</code> <p>Images as template for transform.</p> <code>img_weight</code> <code>ScaleFloatType</code> <p>If single float weight will be sampled from (0, img_weight). If tuple of float img_weight will be in range <code>[img_weight[0], img_weight[1])</code>. If you want fixed weight, use (img_weight, img_weight) Default: (0.5, 0.5).</p> <code>template_weight</code> <code>ScaleFloatType</code> <p>If single float weight will be sampled from (0, template_weight). If tuple of float template_weight will be in range <code>[template_weight[0], template_weight[1])</code>. If you want fixed weight, use (template_weight, template_weight) Default: (0.5, 0.5).</p> <code>template_transform</code> <code>Callable[..., Any] | None</code> <p>transformation object which could be applied to template, must produce template the same size as input image.</p> <code>name</code> <code>str | None</code> <p>(Optional) Name of transform, used only for deserialization.</p> <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 0.5.</p> <p>Targets</p> <p>image</p> <p>Image types:     uint8, float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>class TemplateTransform(ImageOnlyTransform):\n    \"\"\"Apply blending of input image with specified templates\n    Args:\n        templates (numpy array or list of numpy arrays): Images as template for transform.\n        img_weight: If single float weight will be sampled from (0, img_weight).\n            If tuple of float img_weight will be in range `[img_weight[0], img_weight[1])`.\n            If you want fixed weight, use (img_weight, img_weight)\n            Default: (0.5, 0.5).\n        template_weight: If single float weight will be sampled from (0, template_weight).\n            If tuple of float template_weight will be in range `[template_weight[0], template_weight[1])`.\n            If you want fixed weight, use (template_weight, template_weight)\n            Default: (0.5, 0.5).\n        template_transform: transformation object which could be applied to template,\n            must produce template the same size as input image.\n        name: (Optional) Name of transform, used only for deserialization.\n        p: probability of applying the transform. Default: 0.5.\n    Targets:\n        image\n    Image types:\n        uint8, float32\n    \"\"\"\n\n    class InitSchema(BaseTransformInitSchema):\n        templates: np.ndarray | Sequence[np.ndarray] = Field(..., description=\"Images as template for transform.\")\n        img_weight: ZeroOneRangeType = (0.5, 0.5)\n        template_weight: ZeroOneRangeType = (0.5, 0.5)\n        template_transform: Callable[..., Any] | None = Field(\n            default=None,\n            description=\"Transformation object applied to template.\",\n        )\n        name: str | None = Field(default=None, description=\"Name of transform, used only for deserialization.\")\n\n        @field_validator(\"templates\")\n        @classmethod\n        def validate_templates(cls, v: np.ndarray | list[np.ndarray]) -&gt; list[np.ndarray]:\n            if isinstance(v, np.ndarray):\n                return [v]\n            if isinstance(v, list):\n                if not all(isinstance(item, np.ndarray) for item in v):\n                    msg = \"All templates must be numpy arrays.\"\n                    raise ValueError(msg)\n                return v\n            msg = \"Templates must be a numpy array or a list of numpy arrays.\"\n            raise TypeError(msg)\n\n    def __init__(\n        self,\n        templates: np.ndarray | list[np.ndarray],\n        img_weight: ScaleFloatType = (0.5, 0.5),\n        template_weight: ScaleFloatType = (0.5, 0.5),\n        template_transform: Callable[..., Any] | None = None,\n        name: str | None = None,\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p=p, always_apply=always_apply)\n        self.templates = templates\n        self.img_weight = cast(Tuple[float, float], img_weight)\n        self.template_weight = cast(Tuple[float, float], template_weight)\n        self.template_transform = template_transform\n        self.name = name\n\n    def apply(\n        self,\n        img: np.ndarray,\n        template: np.ndarray,\n        img_weight: float,\n        template_weight: float,\n        **params: Any,\n    ) -&gt; np.ndarray:\n        return add_weighted(img, img_weight, template, template_weight)\n\n    def get_params(self) -&gt; dict[str, float]:\n        return {\n            \"img_weight\": random.uniform(self.img_weight[0], self.img_weight[1]),\n            \"template_weight\": random.uniform(self.template_weight[0], self.template_weight[1]),\n        }\n\n    def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n        img = data[\"image\"] if \"image\" in data else data[\"images\"][0]\n        template = random.choice(self.templates)\n\n        if self.template_transform is not None:\n            template = self.template_transform(image=template)[\"image\"]\n\n        if get_num_channels(template) not in [1, get_num_channels(img)]:\n            msg = (\n                \"Template must be a single channel or \"\n                \"has the same number of channels as input \"\n                f\"image ({get_num_channels(img)}), got {get_num_channels(template)}\"\n            )\n            raise ValueError(msg)\n\n        if template.dtype != img.dtype:\n            msg = \"Image and template must be the same image type\"\n            raise ValueError(msg)\n\n        if img.shape[:2] != template.shape[:2]:\n            raise ValueError(f\"Image and template must be the same size, got {img.shape[:2]} and {template.shape[:2]}\")\n\n        if get_num_channels(template) == 1 and get_num_channels(img) &gt; 1:\n            template = np.stack((template,) * get_num_channels(img), axis=-1)\n\n        # in order to support grayscale image with dummy dim\n        template = template.reshape(img.shape)\n\n        return {\"template\": template}\n\n    @classmethod\n    def is_serializable(cls) -&gt; bool:\n        return False\n\n    def to_dict_private(self) -&gt; dict[str, Any]:\n        if self.name is None:\n            msg = (\n                \"To make a TemplateTransform serializable you should provide the `name` argument, \"\n                \"e.g. `TemplateTransform(name='my_transform', ...)`.\"\n            )\n            raise ValueError(msg)\n        return {\"__class_fullname__\": self.get_class_fullname(), \"__name__\": self.name}\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.TemplateTransform.apply","title":"<code>apply (self, img, template, img_weight, template_weight, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def apply(\n    self,\n    img: np.ndarray,\n    template: np.ndarray,\n    img_weight: float,\n    template_weight: float,\n    **params: Any,\n) -&gt; np.ndarray:\n    return add_weighted(img, img_weight, template, template_weight)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.TemplateTransform.get_params","title":"<code>get_params (self)</code>","text":"<p>Returns parameters independent of input.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_params(self) -&gt; dict[str, float]:\n    return {\n        \"img_weight\": random.uniform(self.img_weight[0], self.img_weight[1]),\n        \"template_weight\": random.uniform(self.template_weight[0], self.template_weight[1]),\n    }\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.TemplateTransform.get_params_dependent_on_data","title":"<code>get_params_dependent_on_data (self, params, data)</code>","text":"<p>Returns parameters dependent on input.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n    img = data[\"image\"] if \"image\" in data else data[\"images\"][0]\n    template = random.choice(self.templates)\n\n    if self.template_transform is not None:\n        template = self.template_transform(image=template)[\"image\"]\n\n    if get_num_channels(template) not in [1, get_num_channels(img)]:\n        msg = (\n            \"Template must be a single channel or \"\n            \"has the same number of channels as input \"\n            f\"image ({get_num_channels(img)}), got {get_num_channels(template)}\"\n        )\n        raise ValueError(msg)\n\n    if template.dtype != img.dtype:\n        msg = \"Image and template must be the same image type\"\n        raise ValueError(msg)\n\n    if img.shape[:2] != template.shape[:2]:\n        raise ValueError(f\"Image and template must be the same size, got {img.shape[:2]} and {template.shape[:2]}\")\n\n    if get_num_channels(template) == 1 and get_num_channels(img) &gt; 1:\n        template = np.stack((template,) * get_num_channels(img), axis=-1)\n\n    # in order to support grayscale image with dummy dim\n    template = template.reshape(img.shape)\n\n    return {\"template\": template}\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.ToFloat","title":"<code>class  ToFloat</code> <code>     (max_value=None, p=1.0, always_apply=None)                 </code>  [view source on GitHub]","text":"<p>Divide pixel values by <code>max_value</code> to get a float32 output array where all values lie in the range [0, 1.0]. If <code>max_value</code> is None the transform will try to infer the maximum value by inspecting the data type of the input image.</p> <p>See Also:     :class:<code>~albumentations.augmentations.transforms.FromFloat</code></p> <p>Parameters:</p> Name Type Description <code>max_value</code> <code>float | None</code> <p>maximum possible input value. Default: None.</p> <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 1.0.</p> <p>Targets</p> <p>image</p> <p>Image types:     any type</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>class ToFloat(ImageOnlyTransform):\n    \"\"\"Divide pixel values by `max_value` to get a float32 output array where all values lie in the range [0, 1.0].\n    If `max_value` is None the transform will try to infer the maximum value by inspecting the data type of the input\n    image.\n\n    See Also:\n        :class:`~albumentations.augmentations.transforms.FromFloat`\n\n    Args:\n        max_value: maximum possible input value. Default: None.\n        p: probability of applying the transform. Default: 1.0.\n\n    Targets:\n        image\n\n    Image types:\n        any type\n\n    \"\"\"\n\n    class InitSchema(BaseTransformInitSchema):\n        max_value: float | None = Field(default=None, description=\"Maximum possible input value.\")\n        p: ProbabilityType = 1\n\n    def __init__(self, max_value: float | None = None, p: float = 1.0, always_apply: bool | None = None):\n        super().__init__(p, always_apply)\n        self.max_value = max_value\n\n    def apply(self, img: np.ndarray, **params: Any) -&gt; np.ndarray:\n        return fmain.to_float(img, self.max_value)\n\n    def get_transform_init_args_names(self) -&gt; tuple[str]:\n        return (\"max_value\",)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.ToFloat.apply","title":"<code>apply (self, img, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def apply(self, img: np.ndarray, **params: Any) -&gt; np.ndarray:\n    return fmain.to_float(img, self.max_value)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.ToFloat.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str]:\n    return (\"max_value\",)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.ToGray","title":"<code>class  ToGray</code> <code>     (num_output_channels=3, method='weighted_average', always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Convert an image to grayscale and optionally replicate the grayscale channel.</p> <p>This transform first converts a color image to a single-channel grayscale image using various methods, then replicates the grayscale channel if num_output_channels is greater than 1.</p> <p>Parameters:</p> Name Type Description <code>num_output_channels</code> <code>int</code> <p>The number of channels in the output image. If greater than 1, the grayscale channel will be replicated. Default: 3.</p> <code>method</code> <code>Literal[\"weighted_average\", \"from_lab\", \"desaturation\", \"average\", \"max\", \"pca\"]</code> <p>The method used for grayscale conversion: - \"weighted_average\": Uses a weighted sum of RGB channels (0.299R + 0.587G + 0.114B).   Works only with 3-channel images. Provides realistic results based on human perception. - \"from_lab\": Extracts the L channel from the LAB color space.   Works only with 3-channel images. Gives perceptually uniform results. - \"desaturation\": Averages the maximum and minimum values across channels.   Works with any number of channels. Fast but may not preserve perceived brightness well. - \"average\": Simple average of all channels.   Works with any number of channels. Fast but may not give realistic results. - \"max\": Takes the maximum value across all channels.   Works with any number of channels. Tends to produce brighter results. - \"pca\": Applies Principal Component Analysis to reduce channels.   Works with any number of channels. Can preserve more information but is computationally intensive.</p> <code>p</code> <code>float</code> <p>Probability of applying the transform. Default: 0.5.</p> <p>Exceptions:</p> Type Description <code>TypeError</code> <p>If the input image doesn't have 3 channels for methods that require it.</p> <p>Note</p> <ul> <li>The transform first converts the input image to single-channel grayscale, then replicates   this channel if num_output_channels &gt; 1.</li> <li>\"weighted_average\" and \"from_lab\" are typically used in image processing and computer vision   applications where accurate representation of human perception is important.</li> <li>\"desaturation\" and \"average\" are often used in simple image manipulation tools or when   computational speed is a priority.</li> <li>\"max\" method can be useful in scenarios where preserving bright features is important,   such as in some medical imaging applications.</li> <li>\"pca\" might be used in advanced image analysis tasks or when dealing with hyperspectral images.</li> </ul> <p>Image types:     uint8, float32</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Grayscale image with the specified number of channels.</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>class ToGray(ImageOnlyTransform):\n    \"\"\"Convert an image to grayscale and optionally replicate the grayscale channel.\n\n    This transform first converts a color image to a single-channel grayscale image using various methods,\n    then replicates the grayscale channel if num_output_channels is greater than 1.\n\n    Args:\n        num_output_channels (int): The number of channels in the output image. If greater than 1,\n            the grayscale channel will be replicated. Default: 3.\n        method (Literal[\"weighted_average\", \"from_lab\", \"desaturation\", \"average\", \"max\", \"pca\"]):\n            The method used for grayscale conversion:\n            - \"weighted_average\": Uses a weighted sum of RGB channels (0.299R + 0.587G + 0.114B).\n              Works only with 3-channel images. Provides realistic results based on human perception.\n            - \"from_lab\": Extracts the L channel from the LAB color space.\n              Works only with 3-channel images. Gives perceptually uniform results.\n            - \"desaturation\": Averages the maximum and minimum values across channels.\n              Works with any number of channels. Fast but may not preserve perceived brightness well.\n            - \"average\": Simple average of all channels.\n              Works with any number of channels. Fast but may not give realistic results.\n            - \"max\": Takes the maximum value across all channels.\n              Works with any number of channels. Tends to produce brighter results.\n            - \"pca\": Applies Principal Component Analysis to reduce channels.\n              Works with any number of channels. Can preserve more information but is computationally intensive.\n        p (float): Probability of applying the transform. Default: 0.5.\n\n    Raises:\n        TypeError: If the input image doesn't have 3 channels for methods that require it.\n\n    Note:\n        - The transform first converts the input image to single-channel grayscale, then replicates\n          this channel if num_output_channels &gt; 1.\n        - \"weighted_average\" and \"from_lab\" are typically used in image processing and computer vision\n          applications where accurate representation of human perception is important.\n        - \"desaturation\" and \"average\" are often used in simple image manipulation tools or when\n          computational speed is a priority.\n        - \"max\" method can be useful in scenarios where preserving bright features is important,\n          such as in some medical imaging applications.\n        - \"pca\" might be used in advanced image analysis tasks or when dealing with hyperspectral images.\n\n    Image types:\n        uint8, float32\n\n    Returns:\n        np.ndarray: Grayscale image with the specified number of channels.\n    \"\"\"\n\n    class InitSchema(BaseTransformInitSchema):\n        num_output_channels: int = Field(default=3, description=\"The number of output channels.\", ge=1)\n        method: Literal[\"weighted_average\", \"from_lab\", \"desaturation\", \"average\", \"max\", \"pca\"]\n\n    def __init__(\n        self,\n        num_output_channels: int = 3,\n        method: Literal[\"weighted_average\", \"from_lab\", \"desaturation\", \"average\", \"max\", \"pca\"] = \"weighted_average\",\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p=p, always_apply=always_apply)\n        self.num_output_channels = num_output_channels\n        self.method = method\n\n    def apply(self, img: np.ndarray, **params: Any) -&gt; np.ndarray:\n        if is_grayscale_image(img):\n            warnings.warn(\"The image is already gray.\", stacklevel=2)\n            return img\n\n        num_channels = get_num_channels(img)\n\n        if num_channels != NUM_RGB_CHANNELS and self.method not in {\"desaturation\", \"average\", \"max\", \"pca\"}:\n            msg = \"ToGray transformation expects 3-channel images.\"\n            raise TypeError(msg)\n\n        return fmain.to_gray(img, self.num_output_channels, self.method)\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return \"num_output_channels\", \"method\"\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.ToGray.apply","title":"<code>apply (self, img, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def apply(self, img: np.ndarray, **params: Any) -&gt; np.ndarray:\n    if is_grayscale_image(img):\n        warnings.warn(\"The image is already gray.\", stacklevel=2)\n        return img\n\n    num_channels = get_num_channels(img)\n\n    if num_channels != NUM_RGB_CHANNELS and self.method not in {\"desaturation\", \"average\", \"max\", \"pca\"}:\n        msg = \"ToGray transformation expects 3-channel images.\"\n        raise TypeError(msg)\n\n    return fmain.to_gray(img, self.num_output_channels, self.method)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.ToGray.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return \"num_output_channels\", \"method\"\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.ToRGB","title":"<code>class  ToRGB</code> <code>     (p=1.0, always_apply=None)                 </code>  [view source on GitHub]","text":"<p>Convert the input grayscale image to RGB.</p> <p>Parameters:</p> Name Type Description <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 1.</p> <p>Targets</p> <p>image</p> <p>Image types:     uint8, float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>class ToRGB(ImageOnlyTransform):\n    \"\"\"Convert the input grayscale image to RGB.\n\n    Args:\n        p: probability of applying the transform. Default: 1.\n\n    Targets:\n        image\n\n    Image types:\n        uint8, float32\n\n    \"\"\"\n\n    def __init__(self, p: float = 1.0, always_apply: bool | None = None):\n        super().__init__(p=p, always_apply=always_apply)\n\n    def apply(self, img: np.ndarray, **params: Any) -&gt; np.ndarray:\n        if is_rgb_image(img):\n            warnings.warn(\"The image is already an RGB.\", stacklevel=2)\n            return np.ascontiguousarray(img)\n        if not is_grayscale_image(img):\n            msg = \"ToRGB transformation expects 2-dim images or 3-dim with the last dimension equal to 1.\"\n            raise TypeError(msg)\n\n        return fmain.grayscale_to_multichannel(img, num_output_channels=3)\n\n    def get_transform_init_args_names(self) -&gt; tuple[()]:\n        return ()\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.ToRGB.__init__","title":"<code>__init__ (self, p=1.0, always_apply=None)</code>  <code>special</code>","text":"<p>Initialize self.  See help(type(self)) for accurate signature.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def __init__(self, p: float = 1.0, always_apply: bool | None = None):\n    super().__init__(p=p, always_apply=always_apply)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.ToRGB.apply","title":"<code>apply (self, img, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def apply(self, img: np.ndarray, **params: Any) -&gt; np.ndarray:\n    if is_rgb_image(img):\n        warnings.warn(\"The image is already an RGB.\", stacklevel=2)\n        return np.ascontiguousarray(img)\n    if not is_grayscale_image(img):\n        msg = \"ToRGB transformation expects 2-dim images or 3-dim with the last dimension equal to 1.\"\n        raise TypeError(msg)\n\n    return fmain.grayscale_to_multichannel(img, num_output_channels=3)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.ToRGB.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[()]:\n    return ()\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.ToSepia","title":"<code>class  ToSepia</code> <code>     (p=0.5, always_apply=None)                 </code>  [view source on GitHub]","text":"<p>Applies sepia filter to the input RGB image</p> <p>Parameters:</p> Name Type Description <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 0.5.</p> <p>Targets</p> <p>image</p> <p>Image types:     uint8, float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>class ToSepia(ImageOnlyTransform):\n    \"\"\"Applies sepia filter to the input RGB image\n\n    Args:\n        p: probability of applying the transform. Default: 0.5.\n\n    Targets:\n        image\n\n    Image types:\n        uint8, float32\n\n    \"\"\"\n\n    def __init__(self, p: float = 0.5, always_apply: bool | None = None):\n        super().__init__(p, always_apply)\n        self.sepia_transformation_matrix = np.array(\n            [[0.393, 0.769, 0.189], [0.349, 0.686, 0.168], [0.272, 0.534, 0.131]],\n        )\n\n    def apply(self, img: np.ndarray, **params: Any) -&gt; np.ndarray:\n        if not is_rgb_image(img):\n            msg = \"ToSepia transformation expects 3-channel images.\"\n            raise TypeError(msg)\n        return fmain.linear_transformation_rgb(img, self.sepia_transformation_matrix)\n\n    def get_transform_init_args_names(self) -&gt; tuple[()]:\n        return ()\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.ToSepia.__init__","title":"<code>__init__ (self, p=0.5, always_apply=None)</code>  <code>special</code>","text":"<p>Initialize self.  See help(type(self)) for accurate signature.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def __init__(self, p: float = 0.5, always_apply: bool | None = None):\n    super().__init__(p, always_apply)\n    self.sepia_transformation_matrix = np.array(\n        [[0.393, 0.769, 0.189], [0.349, 0.686, 0.168], [0.272, 0.534, 0.131]],\n    )\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.ToSepia.apply","title":"<code>apply (self, img, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def apply(self, img: np.ndarray, **params: Any) -&gt; np.ndarray:\n    if not is_rgb_image(img):\n        msg = \"ToSepia transformation expects 3-channel images.\"\n        raise TypeError(msg)\n    return fmain.linear_transformation_rgb(img, self.sepia_transformation_matrix)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.ToSepia.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[()]:\n    return ()\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.UnsharpMask","title":"<code>class  UnsharpMask</code> <code>     (blur_limit=(3, 7), sigma_limit=0.0, alpha=(0.2, 0.5), threshold=10, always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Sharpen the input image using Unsharp Masking processing and overlays the result with the original image.</p> <p>Parameters:</p> Name Type Description <code>blur_limit</code> <code>ScaleIntType</code> <p>maximum Gaussian kernel size for blurring the input image. Must be zero or odd and in range [0, inf). If set to 0 it will be computed from sigma as <code>round(sigma * (3 if img.dtype == np.uint8 else 4) * 2 + 1) + 1</code>. If set single value <code>blur_limit</code> will be in range (0, blur_limit). Default: (3, 7).</p> <code>sigma_limit</code> <code>ScaleFloatType</code> <p>Gaussian kernel standard deviation. Must be in range [0, inf). If set single value <code>sigma_limit</code> will be in range (0, sigma_limit). If set to 0 sigma will be computed as <code>sigma = 0.3*((ksize-1)*0.5 - 1) + 0.8</code>. Default: 0.</p> <code>alpha</code> <code>ScaleFloatType</code> <p>range to choose the visibility of the sharpened image. At 0, only the original image is visible, at 1.0 only its sharpened version is visible. Default: (0.2, 0.5).</p> <code>threshold</code> <code>int</code> <p>Value to limit sharpening only for areas with high pixel difference between original image and it's smoothed version. Higher threshold means less sharpening on flat areas. Must be in range [0, 255]. Default: 10.</p> <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 0.5.</p> <p>Reference</p> <p>arxiv.org/pdf/2107.10833.pdf</p> <p>Targets</p> <p>image</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>class UnsharpMask(ImageOnlyTransform):\n    \"\"\"Sharpen the input image using Unsharp Masking processing and overlays the result with the original image.\n\n    Args:\n        blur_limit: maximum Gaussian kernel size for blurring the input image.\n            Must be zero or odd and in range [0, inf). If set to 0 it will be computed from sigma\n            as `round(sigma * (3 if img.dtype == np.uint8 else 4) * 2 + 1) + 1`.\n            If set single value `blur_limit` will be in range (0, blur_limit).\n            Default: (3, 7).\n        sigma_limit: Gaussian kernel standard deviation. Must be in range [0, inf).\n            If set single value `sigma_limit` will be in range (0, sigma_limit).\n            If set to 0 sigma will be computed as `sigma = 0.3*((ksize-1)*0.5 - 1) + 0.8`. Default: 0.\n        alpha: range to choose the visibility of the sharpened image.\n            At 0, only the original image is visible, at 1.0 only its sharpened version is visible.\n            Default: (0.2, 0.5).\n        threshold: Value to limit sharpening only for areas with high pixel difference between original image\n            and it's smoothed version. Higher threshold means less sharpening on flat areas.\n            Must be in range [0, 255]. Default: 10.\n        p: probability of applying the transform. Default: 0.5.\n\n    Reference:\n        arxiv.org/pdf/2107.10833.pdf\n\n    Targets:\n        image\n\n    \"\"\"\n\n    class InitSchema(BaseTransformInitSchema):\n        sigma_limit: NonNegativeFloatRangeType = 0\n        alpha: ZeroOneRangeType = (0.2, 0.5)\n        threshold: int = Field(default=10, ge=0, le=255, description=\"Threshold for limiting sharpening.\")\n\n        blur_limit: ScaleIntType = Field(\n            default=(3, 7),\n            description=\"Maximum kernel size for blurring the input image.\",\n        )\n\n        @field_validator(\"blur_limit\")\n        @classmethod\n        def process_blur(cls, value: ScaleIntType, info: ValidationInfo) -&gt; tuple[int, int]:\n            return process_blur_limit(value, info, min_value=3)\n\n    def __init__(\n        self,\n        blur_limit: ScaleIntType = (3, 7),\n        sigma_limit: ScaleFloatType = 0.0,\n        alpha: ScaleFloatType = (0.2, 0.5),\n        threshold: int = 10,\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p=p, always_apply=always_apply)\n        self.blur_limit = cast(Tuple[int, int], blur_limit)\n        self.sigma_limit = cast(Tuple[float, float], sigma_limit)\n        self.alpha = cast(Tuple[float, float], alpha)\n        self.threshold = threshold\n\n    def get_params(self) -&gt; dict[str, Any]:\n        return {\n            \"ksize\": random.randrange(self.blur_limit[0], self.blur_limit[1] + 1, 2),\n            \"sigma\": random.uniform(*self.sigma_limit),\n            \"alpha\": random.uniform(*self.alpha),\n        }\n\n    def apply(self, img: np.ndarray, ksize: int, sigma: int, alpha: float, **params: Any) -&gt; np.ndarray:\n        return fmain.unsharp_mask(img, ksize, sigma=sigma, alpha=alpha, threshold=self.threshold)\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return \"blur_limit\", \"sigma_limit\", \"alpha\", \"threshold\"\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.UnsharpMask.apply","title":"<code>apply (self, img, ksize, sigma, alpha, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def apply(self, img: np.ndarray, ksize: int, sigma: int, alpha: float, **params: Any) -&gt; np.ndarray:\n    return fmain.unsharp_mask(img, ksize, sigma=sigma, alpha=alpha, threshold=self.threshold)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.UnsharpMask.get_params","title":"<code>get_params (self)</code>","text":"<p>Returns parameters independent of input.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_params(self) -&gt; dict[str, Any]:\n    return {\n        \"ksize\": random.randrange(self.blur_limit[0], self.blur_limit[1] + 1, 2),\n        \"sigma\": random.uniform(*self.sigma_limit),\n        \"alpha\": random.uniform(*self.alpha),\n    }\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.transforms.UnsharpMask.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return \"blur_limit\", \"sigma_limit\", \"alpha\", \"threshold\"\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.augmentations.utils","title":"<code>utils</code>","text":""},{"location":"api_reference/full_reference/#albumentations.augmentations.utils.check_range","title":"<code>def check_range    (value, lower_bound, upper_bound, name)    </code> [view source on GitHub]","text":"<p>Checks if the given value is within the specified bounds</p> <p>Parameters:</p> Name Type Description <code>value</code> <code>tuple[float, float]</code> <p>The value to check and convert. Can be a single float or a tuple of floats.</p> <code>lower_bound</code> <code>float</code> <p>The lower bound for the range check.</p> <code>upper_bound</code> <code>float</code> <p>The upper bound for the range check.</p> <code>name</code> <code>str | None</code> <p>The name of the parameter being checked. Used for error messages.</p> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If the value is outside the bounds or if the tuple values are not ordered correctly.</p> Source code in <code>albumentations/augmentations/utils.py</code> Python<pre><code>def check_range(value: tuple[float, float], lower_bound: float, upper_bound: float, name: str | None) -&gt; None:\n    \"\"\"Checks if the given value is within the specified bounds\n\n    Args:\n        value: The value to check and convert. Can be a single float or a tuple of floats.\n        lower_bound: The lower bound for the range check.\n        upper_bound: The upper bound for the range check.\n        name: The name of the parameter being checked. Used for error messages.\n\n    Raises:\n        ValueError: If the value is outside the bounds or if the tuple values are not ordered correctly.\n    \"\"\"\n    if not all(lower_bound &lt;= x &lt;= upper_bound for x in value):\n        raise ValueError(f\"All values in {name} must be within [{lower_bound}, {upper_bound}] for tuple inputs.\")\n    if not value[0] &lt;= value[1]:\n        raise ValueError(f\"{name!s} tuple values must be ordered as (min, max). Got: {value}\")\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.check_version","title":"<code>check_version</code>","text":""},{"location":"api_reference/full_reference/#albumentations.check_version.parse_version","title":"<code>def parse_version    (data)    </code> [view source on GitHub]","text":"<p>Parses the version from the given JSON data.</p> Source code in <code>albumentations/check_version.py</code> Python<pre><code>def parse_version(data: str) -&gt; str:\n    \"\"\"Parses the version from the given JSON data.\"\"\"\n    if data:\n        try:\n            json_data = json.loads(data)\n            # Use .get() to avoid KeyError if 'version' is not present\n            return json_data.get(\"info\", {}).get(\"version\", \"\")\n        except json.JSONDecodeError:\n            # This will handle malformed JSON data\n            return \"\"\n    return \"\"\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.core","title":"<code>core</code>  <code>special</code>","text":""},{"location":"api_reference/full_reference/#albumentations.core.bbox_utils","title":"<code>bbox_utils</code>","text":""},{"location":"api_reference/full_reference/#albumentations.core.bbox_utils.BboxParams","title":"<code>class  BboxParams</code> <code>     (format, label_fields=None, min_area=0.0, min_visibility=0.0, min_width=0.0, min_height=0.0, check_each_transform=True, clip=False)                 </code>  [view source on GitHub]","text":"<p>Parameters of bounding boxes</p> <p>Parameters:</p> Name Type Description <code>format</code> <code>str</code> <p>format of bounding boxes. Should be <code>coco</code>, <code>pascal_voc</code>, <code>albumentations</code> or <code>yolo</code>.</p> <p>The <code>coco</code> format     <code>[x_min, y_min, width, height]</code>, e.g. [97, 12, 150, 200]. The <code>pascal_voc</code> format     <code>[x_min, y_min, x_max, y_max]</code>, e.g. [97, 12, 247, 212]. The <code>albumentations</code> format     is like <code>pascal_voc</code>, but normalized,     in other words: <code>[x_min, y_min, x_max, y_max]</code>, e.g. [0.2, 0.3, 0.4, 0.5]. The <code>yolo</code> format     <code>[x, y, width, height]</code>, e.g. [0.1, 0.2, 0.3, 0.4];     <code>x</code>, <code>y</code> - normalized bbox center; <code>width</code>, <code>height</code> - normalized bbox width and height.</p> <code>label_fields</code> <code>list</code> <p>List of fields joined with boxes, e.g., labels.</p> <code>min_area</code> <code>float</code> <p>Minimum area of a bounding box in pixels or normalized units. Bounding boxes with an area less than this value will be removed. Default: 0.0.</p> <code>min_visibility</code> <code>float</code> <p>Minimum fraction of area for a bounding box to remain in the list. Bounding boxes with a visible area less than this fraction will be removed. Default: 0.0.</p> <code>min_width</code> <code>float</code> <p>Minimum width of a bounding box in pixels or normalized units. Bounding boxes with a width less than this value will be removed. Default: 0.0.</p> <code>min_height</code> <code>float</code> <p>Minimum height of a bounding box in pixels or normalized units. Bounding boxes with a height less than this value will be removed. Default: 0.0.</p> <code>check_each_transform</code> <code>bool</code> <p>If True, bounding boxes will be checked after each dual transform. Default: True.</p> <code>clip</code> <code>bool</code> <p>If True, bounding boxes will be clipped to the image borders before applying any transform. Default: False.</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/core/bbox_utils.py</code> Python<pre><code>class BboxParams(Params):\n    \"\"\"Parameters of bounding boxes\n\n    Args:\n        format (str): format of bounding boxes. Should be `coco`, `pascal_voc`, `albumentations` or `yolo`.\n\n            The `coco` format\n                `[x_min, y_min, width, height]`, e.g. [97, 12, 150, 200].\n            The `pascal_voc` format\n                `[x_min, y_min, x_max, y_max]`, e.g. [97, 12, 247, 212].\n            The `albumentations` format\n                is like `pascal_voc`, but normalized,\n                in other words: `[x_min, y_min, x_max, y_max]`, e.g. [0.2, 0.3, 0.4, 0.5].\n            The `yolo` format\n                `[x, y, width, height]`, e.g. [0.1, 0.2, 0.3, 0.4];\n                `x`, `y` - normalized bbox center; `width`, `height` - normalized bbox width and height.\n\n        label_fields (list): List of fields joined with boxes, e.g., labels.\n        min_area (float): Minimum area of a bounding box in pixels or normalized units.\n            Bounding boxes with an area less than this value will be removed. Default: 0.0.\n        min_visibility (float): Minimum fraction of area for a bounding box to remain in the list.\n            Bounding boxes with a visible area less than this fraction will be removed. Default: 0.0.\n        min_width (float): Minimum width of a bounding box in pixels or normalized units.\n            Bounding boxes with a width less than this value will be removed. Default: 0.0.\n        min_height (float): Minimum height of a bounding box in pixels or normalized units.\n            Bounding boxes with a height less than this value will be removed. Default: 0.0.\n        check_each_transform (bool): If True, bounding boxes will be checked after each dual transform. Default: True.\n        clip (bool): If True, bounding boxes will be clipped to the image borders before applying any transform.\n            Default: False.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        format: str,  # noqa: A002\n        label_fields: Sequence[Any] | None = None,\n        min_area: float = 0.0,\n        min_visibility: float = 0.0,\n        min_width: float = 0.0,\n        min_height: float = 0.0,\n        check_each_transform: bool = True,\n        clip: bool = False,\n    ):\n        super().__init__(format, label_fields)\n        self.min_area = min_area\n        self.min_visibility = min_visibility\n        self.min_width = min_width\n        self.min_height = min_height\n        self.check_each_transform = check_each_transform\n        self.clip = clip\n\n    def to_dict_private(self) -&gt; dict[str, Any]:\n        data = super().to_dict_private()\n        data.update(\n            {\n                \"min_area\": self.min_area,\n                \"min_visibility\": self.min_visibility,\n                \"min_width\": self.min_width,\n                \"min_height\": self.min_height,\n                \"check_each_transform\": self.check_each_transform,\n                \"clip\": self.clip,\n            },\n        )\n        return data\n\n    @classmethod\n    def is_serializable(cls) -&gt; bool:\n        return True\n\n    @classmethod\n    def get_class_fullname(cls) -&gt; str:\n        return \"BboxParams\"\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.core.bbox_utils.calculate_bbox_area","title":"<code>def calculate_bbox_area    (bbox, image_shape)    </code> [view source on GitHub]","text":"<p>Calculate the area of a bounding box in (fractional) pixels.</p> <p>Parameters:</p> Name Type Description <code>bbox</code> <code>BoxType</code> <p>A bounding box <code>(x_min, y_min, x_max, y_max)</code>.</p> <code>image_shape</code> <code>Sequence[int]</code> <p>Image shape <code>(height, width)</code>.</p> <p>Returns:</p> Type Description <code>float</code> <p>Area in (fractional) pixels of the (denormalized) bounding box.</p> Source code in <code>albumentations/core/bbox_utils.py</code> Python<pre><code>def calculate_bbox_area(bbox: BoxType, image_shape: Sequence[int]) -&gt; float:\n    \"\"\"Calculate the area of a bounding box in (fractional) pixels.\n\n    Args:\n        bbox: A bounding box `(x_min, y_min, x_max, y_max)`.\n        image_shape: Image shape `(height, width)`.\n\n    Return:\n        Area in (fractional) pixels of the (denormalized) bounding box.\n\n    \"\"\"\n    bbox = denormalize_bbox(bbox, image_shape)\n    x_min, y_min, x_max, y_max = bbox[:4]\n    return (x_max - x_min) * (y_max - y_min)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.core.bbox_utils.check_bbox","title":"<code>def check_bbox    (bbox)    </code> [view source on GitHub]","text":"<p>Check if bbox boundaries are in range 0, 1 and minimums are lesser then maximums</p> Source code in <code>albumentations/core/bbox_utils.py</code> Python<pre><code>def check_bbox(bbox: BoxType) -&gt; None:\n    \"\"\"Check if bbox boundaries are in range 0, 1 and minimums are lesser then maximums\"\"\"\n    for name, value in zip([\"x_min\", \"y_min\", \"x_max\", \"y_max\"], bbox[:4]):\n        if not 0 &lt;= value &lt;= 1 and not np.isclose(value, 0) and not np.isclose(value, 1):\n            raise ValueError(f\"Expected {name} for bbox {bbox} to be in the range [0.0, 1.0], got {value}.\")\n    x_min, y_min, x_max, y_max = bbox[:4]\n    if x_max &lt;= x_min:\n        raise ValueError(f\"x_max is less than or equal to x_min for bbox {bbox}.\")\n    if y_max &lt;= y_min:\n        raise ValueError(f\"y_max is less than or equal to y_min for bbox {bbox}.\")\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.core.bbox_utils.check_bboxes","title":"<code>def check_bboxes    (bboxes)    </code> [view source on GitHub]","text":"<p>Check if bboxes boundaries are in range 0, 1 and minimums are lesser then maximums</p> Source code in <code>albumentations/core/bbox_utils.py</code> Python<pre><code>def check_bboxes(bboxes: Sequence[BoxType]) -&gt; None:\n    \"\"\"Check if bboxes boundaries are in range 0, 1 and minimums are lesser then maximums\"\"\"\n    for bbox in bboxes:\n        check_bbox(bbox)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.core.bbox_utils.clip_bbox","title":"<code>def clip_bbox    (bbox, image_shape)    </code> [view source on GitHub]","text":"<p>Clips the bounding box coordinates to ensure they fit within the boundaries of an image.</p> <p>The function first denormalizes the bounding box coordinates from relative to absolute (pixel) values. Each coordinate is then clipped to the respective dimension of the image to ensure that the bounding box does not exceed the image's boundaries. Finally, the bounding box is normalized back to relative values.</p> <p>Parameters:</p> Name Type Description <code>bbox</code> <code>BoxInternalType</code> <p>The bounding box in normalized format (relative to image dimensions).</p> <code>image_shape</code> <code>Sequence[int]</code> <p>Image shape <code>(height, width)</code>.</p> <p>Returns:</p> Type Description <code>BoxInternalType</code> <p>The clipped bounding box, normalized to the image dimensions.</p> Source code in <code>albumentations/core/bbox_utils.py</code> Python<pre><code>def clip_bbox(bbox: BoxType, image_shape: Sequence[int]) -&gt; BoxType:\n    \"\"\"Clips the bounding box coordinates to ensure they fit within the boundaries of an image.\n\n    The function first denormalizes the bounding box coordinates from relative to absolute (pixel) values.\n    Each coordinate is then clipped to the respective dimension of the image to ensure that the bounding box\n    does not exceed the image's boundaries. Finally, the bounding box is normalized back to relative values.\n\n    Parameters:\n        bbox (BoxInternalType): The bounding box in normalized format (relative to image dimensions).\n        image_shape (Sequence[int]): Image shape `(height, width)`.\n\n    Returns:\n        BoxInternalType: The clipped bounding box, normalized to the image dimensions.\n    \"\"\"\n    x_min, y_min, x_max, y_max = denormalize_bbox(bbox, image_shape)[:4]\n\n    ## Note:\n    # It could be tempting to use cols - 1 and rows - 1 as the upper bounds for the clipping\n\n    # But this would cause the bounding box to be clipped to the image dimensions - 1 which is not what we want.\n    # Bounding box lives not in the middle of pixels but between them.\n\n    # Example: for image with height 100, width 100, the pixel values are in the range [0, 99]\n    # but if we want bounding box to be 1 pixel width and height and lie on the boundary of the image\n    # it will be described as [99, 99, 100, 100] =&gt; clip by image_size - 1 will lead to [99, 99, 99, 99]\n    # which is incorrect\n\n    # It could be also tempting to clip `x_min`` to `cols - 1`` and `y_min` to `rows - 1`, but this also leads\n    # to another error. If image fully lies outside of the visible area and min_area is set to 0, then\n    # the bounding box will be clipped to the image size - 1 and will be 1 pixel in size and fully visible,\n    # but it should be completely removed.\n\n    rows, cols = image_shape[:2]\n\n    x_min = np.clip(x_min, 0, cols)\n    x_max = np.clip(x_max, 0, cols)\n    y_min = np.clip(y_min, 0, rows)\n    y_max = np.clip(y_max, 0, rows)\n    return cast(BoxType, normalize_bbox((x_min, y_min, x_max, y_max), image_shape) + tuple(bbox[4:]))\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.core.bbox_utils.convert_bbox_from_albumentations","title":"<code>def convert_bbox_from_albumentations    (bbox, target_format, image_shape, check_validity=False)    </code> [view source on GitHub]","text":"<p>Convert a bounding box from the format used by albumentations to a format, specified in <code>target_format</code>.</p> <p>Parameters:</p> Name Type Description <code>bbox</code> <code>BoxType</code> <p>An albumentations bounding box <code>(x_min, y_min, x_max, y_max)</code>.</p> <code>target_format</code> <code>str</code> <p>required format of the output bounding box. Should be 'coco', 'pascal_voc' or 'yolo'.</p> <code>image_shape</code> <code>Sequence[int]</code> <p>Image shape <code>(height, width)</code>.</p> <code>check_validity</code> <code>bool</code> <p>Check if all boxes are valid boxes.</p> <p>Returns:</p> Type Description <code>tuple</code> <p>A bounding box.</p> <p>Note</p> <p>The <code>coco</code> format of a bounding box looks like <code>[x_min, y_min, width, height]</code>, e.g. [97, 12, 150, 200]. The <code>pascal_voc</code> format of a bounding box looks like <code>[x_min, y_min, x_max, y_max]</code>, e.g. [97, 12, 247, 212]. The <code>yolo</code> format of a bounding box looks like <code>[x, y, width, height]</code>, e.g. [0.3, 0.1, 0.05, 0.07].</p> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>if <code>target_format</code> is not equal to <code>coco</code>, <code>pascal_voc</code> or <code>yolo</code>.</p> Source code in <code>albumentations/core/bbox_utils.py</code> Python<pre><code>def convert_bbox_from_albumentations(\n    bbox: BoxType,\n    target_format: str,\n    image_shape: Sequence[int],\n    check_validity: bool = False,\n) -&gt; BoxType:\n    \"\"\"Convert a bounding box from the format used by albumentations to a format, specified in `target_format`.\n\n    Args:\n        bbox: An albumentations bounding box `(x_min, y_min, x_max, y_max)`.\n        target_format: required format of the output bounding box. Should be 'coco', 'pascal_voc' or 'yolo'.\n        image_shape: Image shape `(height, width)`.\n        check_validity: Check if all boxes are valid boxes.\n\n    Returns:\n        tuple: A bounding box.\n\n    Note:\n        The `coco` format of a bounding box looks like `[x_min, y_min, width, height]`, e.g. [97, 12, 150, 200].\n        The `pascal_voc` format of a bounding box looks like `[x_min, y_min, x_max, y_max]`, e.g. [97, 12, 247, 212].\n        The `yolo` format of a bounding box looks like `[x, y, width, height]`, e.g. [0.3, 0.1, 0.05, 0.07].\n\n    Raises:\n        ValueError: if `target_format` is not equal to `coco`, `pascal_voc` or `yolo`.\n\n    \"\"\"\n    if target_format not in {\"coco\", \"pascal_voc\", \"yolo\"}:\n        raise ValueError(\n            f\"Unknown target_format {target_format}. Supported formats are: 'coco', 'pascal_voc' and 'yolo'\",\n        )\n    if check_validity:\n        check_bbox(bbox)\n\n    if target_format != \"yolo\":\n        bbox = denormalize_bbox(bbox, image_shape)\n    if target_format == \"coco\":\n        (x_min, y_min, x_max, y_max), tail = bbox[:4], tuple(bbox[4:])\n        width = x_max - x_min\n        height = y_max - y_min\n        bbox = cast(BoxType, (x_min, y_min, width, height, *tail))\n    elif target_format == \"yolo\":\n        (x_min, y_min, x_max, y_max), tail = bbox[:4], bbox[4:]\n        x = (x_min + x_max) / 2.0\n        y = (y_min + y_max) / 2.0\n        width = x_max - x_min\n        height = y_max - y_min\n        bbox = cast(BoxType, (x, y, width, height, *tail))\n    return bbox\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.core.bbox_utils.convert_bbox_to_albumentations","title":"<code>def convert_bbox_to_albumentations    (bbox, source_format, image_shape, check_validity=False)    </code> [view source on GitHub]","text":"<p>Convert a bounding box from a format specified in <code>source_format</code> to the format used by albumentations: normalized coordinates of top-left and bottom-right corners of the bounding box in a form of <code>(x_min, y_min, x_max, y_max)</code> e.g. <code>(0.15, 0.27, 0.67, 0.5)</code>.</p> <p>Parameters:</p> Name Type Description <code>bbox</code> <code>BoxType</code> <p>A bounding box tuple.</p> <code>source_format</code> <code>str</code> <p>format of the bounding box. Should be 'coco', 'pascal_voc', or 'yolo'.</p> <code>image_shape</code> <code>Sequence[int]</code> <p>Image shape <code>(height, width)</code>.</p> <code>check_validity</code> <code>bool</code> <p>Check if all boxes are valid boxes.</p> <p>Returns:</p> Type Description <code>tuple</code> <p>A bounding box <code>(x_min, y_min, x_max, y_max)</code>.</p> <p>Note</p> <p>The <code>coco</code> format of a bounding box looks like <code>(x_min, y_min, width, height)</code>, e.g. (97, 12, 150, 200). The <code>pascal_voc</code> format of a bounding box looks like <code>(x_min, y_min, x_max, y_max)</code>, e.g. (97, 12, 247, 212). The <code>yolo</code> format of a bounding box looks like <code>(x, y, width, height)</code>, e.g. (0.3, 0.1, 0.05, 0.07); where <code>x</code>, <code>y</code> coordinates of the center of the box, all values normalized to 1 by image height and width.</p> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>if <code>target_format</code> is not equal to <code>coco</code> or <code>pascal_voc</code>, or <code>yolo</code>.</p> <code>ValueError</code> <p>If in YOLO format all labels not in range (0, 1).</p> Source code in <code>albumentations/core/bbox_utils.py</code> Python<pre><code>def convert_bbox_to_albumentations(\n    bbox: BoxType,\n    source_format: str,\n    image_shape: Sequence[int],\n    check_validity: bool = False,\n) -&gt; BoxType:\n    \"\"\"Convert a bounding box from a format specified in `source_format` to the format used by albumentations:\n    normalized coordinates of top-left and bottom-right corners of the bounding box in a form of\n    `(x_min, y_min, x_max, y_max)` e.g. `(0.15, 0.27, 0.67, 0.5)`.\n\n    Args:\n        bbox: A bounding box tuple.\n        source_format: format of the bounding box. Should be 'coco', 'pascal_voc', or 'yolo'.\n        image_shape: Image shape `(height, width)`.\n        check_validity: Check if all boxes are valid boxes.\n\n\n    Returns:\n        tuple: A bounding box `(x_min, y_min, x_max, y_max)`.\n\n    Note:\n        The `coco` format of a bounding box looks like `(x_min, y_min, width, height)`, e.g. (97, 12, 150, 200).\n        The `pascal_voc` format of a bounding box looks like `(x_min, y_min, x_max, y_max)`, e.g. (97, 12, 247, 212).\n        The `yolo` format of a bounding box looks like `(x, y, width, height)`, e.g. (0.3, 0.1, 0.05, 0.07);\n        where `x`, `y` coordinates of the center of the box, all values normalized to 1 by image height and width.\n\n    Raises:\n        ValueError: if `target_format` is not equal to `coco` or `pascal_voc`, or `yolo`.\n        ValueError: If in YOLO format all labels not in range (0, 1).\n\n    \"\"\"\n    if source_format not in {\"coco\", \"pascal_voc\", \"yolo\"}:\n        raise ValueError(\n            f\"Unknown source_format {source_format}. Supported formats are: 'coco', 'pascal_voc' and 'yolo'\",\n        )\n\n    if source_format == \"coco\":\n        (x_min, y_min, width, height), tail = bbox[:4], bbox[4:]\n        x_max = x_min + width\n        y_max = y_min + height\n    elif source_format == \"yolo\":\n        # https://github.com/pjreddie/darknet/blob/f6d861736038da22c9eb0739dca84003c5a5e275/scripts/voc_label.py#L12\n        _bbox = np.array(bbox[:4])\n        if check_validity and np.any((_bbox &lt;= 0) | (_bbox &gt; 1)):\n            msg = \"In YOLO format all coordinates must be float and in range (0, 1]\"\n            raise ValueError(msg)\n\n        (x, y, width, height), tail = bbox[:4], bbox[4:]\n\n        w_half, h_half = width / 2, height / 2\n        x_min = x - w_half\n        y_min = y - h_half\n        x_max = x_min + width\n        y_max = y_min + height\n    else:\n        (x_min, y_min, x_max, y_max), tail = bbox[:4], bbox[4:]\n\n    bbox = (x_min, y_min, x_max, y_max, *tuple(tail))\n\n    if source_format != \"yolo\":\n        bbox = normalize_bbox(bbox, image_shape)\n    if check_validity:\n        check_bbox(bbox)\n    return bbox\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.core.bbox_utils.convert_bboxes_from_albumentations","title":"<code>def convert_bboxes_from_albumentations    (bboxes, target_format, image_shape, check_validity=False)    </code> [view source on GitHub]","text":"<p>Convert a list of bounding boxes from the format used by albumentations to a format, specified in <code>target_format</code>.</p> <p>Parameters:</p> Name Type Description <code>bboxes</code> <code>Sequence[BoxType]</code> <p>list of albumentations bounding box <code>(x_min, y_min, x_max, y_max)</code>.</p> <code>target_format</code> <code>str</code> <p>required format of the output bounding box. Should be 'coco', 'pascal_voc' or 'yolo'.</p> <code>image_shape</code> <code>Sequence[int]</code> <p>Image shape <code>(height, width)</code>.</p> <code>check_validity</code> <code>bool</code> <p>Check if all boxes are valid boxes.</p> <p>Returns:</p> Type Description <code>list[BoxType]</code> <p>list of bounding boxes.</p> Source code in <code>albumentations/core/bbox_utils.py</code> Python<pre><code>def convert_bboxes_from_albumentations(\n    bboxes: Sequence[BoxType],\n    target_format: str,\n    image_shape: Sequence[int],\n    check_validity: bool = False,\n) -&gt; list[BoxType]:\n    \"\"\"Convert a list of bounding boxes from the format used by albumentations to a format, specified\n    in `target_format`.\n\n    Args:\n        bboxes: list of albumentations bounding box `(x_min, y_min, x_max, y_max)`.\n        target_format: required format of the output bounding box. Should be 'coco', 'pascal_voc' or 'yolo'.\n        image_shape: Image shape `(height, width)`.\n        check_validity: Check if all boxes are valid boxes.\n\n    Returns:\n        list of bounding boxes.\n\n    \"\"\"\n    return [convert_bbox_from_albumentations(bbox, target_format, image_shape, check_validity) for bbox in bboxes]\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.core.bbox_utils.convert_bboxes_to_albumentations","title":"<code>def convert_bboxes_to_albumentations    (bboxes, source_format, image_shape, check_validity=False)    </code> [view source on GitHub]","text":"<p>Convert a list bounding boxes from a format specified in <code>source_format</code> to the format used by albumentations</p> Source code in <code>albumentations/core/bbox_utils.py</code> Python<pre><code>def convert_bboxes_to_albumentations(\n    bboxes: Sequence[BoxType],\n    source_format: str,\n    image_shape: Sequence[int],\n    check_validity: bool = False,\n) -&gt; list[BoxType]:\n    \"\"\"Convert a list bounding boxes from a format specified in `source_format` to the format used by albumentations\"\"\"\n    return [convert_bbox_to_albumentations(bbox, source_format, image_shape, check_validity) for bbox in bboxes]\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.core.bbox_utils.denormalize_bbox","title":"<code>def denormalize_bbox    (bbox, image_shape)    </code> [view source on GitHub]","text":"<p>Denormalize bounding box coordinates from relative to absolute pixel values.</p> <p>This function converts normalized bounding box coordinates (ranging from 0 to 1) to absolute pixel coordinates based on the given image shape.</p> <p>Parameters:</p> Name Type Description <code>bbox</code> <code>BoxType</code> <p>A bounding box in normalized coordinates (x_min, y_min, x_max, y_max, ...). Additional elements after the first four are preserved and returned unchanged.</p> <code>image_shape</code> <code>Sequence[int]</code> <p>The shape of the image (height, width, ...). Only the first two elements (height and width) are used.</p> <p>Returns:</p> Type Description <code>BoxType</code> <p>A bounding box with denormalized coordinates (x_min, y_min, x_max, y_max, ...).     The coordinates are in absolute pixel values.     Any additional elements from the input bbox are appended unchanged.</p> <p>Note</p> <ul> <li>Input bbox coordinates should be in the range [0, 1].</li> <li>The function assumes the first four elements of bbox are x_min, y_min, x_max, y_max.</li> <li>Any elements in bbox after the first four are returned as-is.</li> <li>The returned bbox type is cast to BoxType to maintain type consistency.</li> </ul> <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; denormalize_bbox((0.1, 0.2, 0.3, 0.4), (100, 200))\n(20.0, 20.0, 60.0, 40.0)\n&gt;&gt;&gt; denormalize_bbox((0.1, 0.2, 0.3, 0.4, 'label'), (100, 200))\n(20.0, 20.0, 60.0, 40.0, 'label')\n</code></pre> Source code in <code>albumentations/core/bbox_utils.py</code> Python<pre><code>def denormalize_bbox(bbox: BoxType, image_shape: Sequence[int]) -&gt; BoxType:\n    \"\"\"Denormalize bounding box coordinates from relative to absolute pixel values.\n\n    This function converts normalized bounding box coordinates (ranging from 0 to 1)\n    to absolute pixel coordinates based on the given image shape.\n\n    Args:\n        bbox (BoxType): A bounding box in normalized coordinates (x_min, y_min, x_max, y_max, ...).\n            Additional elements after the first four are preserved and returned unchanged.\n        image_shape (Sequence[int]): The shape of the image (height, width, ...).\n            Only the first two elements (height and width) are used.\n\n    Returns:\n        BoxType: A bounding box with denormalized coordinates (x_min, y_min, x_max, y_max, ...).\n            The coordinates are in absolute pixel values.\n            Any additional elements from the input bbox are appended unchanged.\n\n    Note:\n        - Input bbox coordinates should be in the range [0, 1].\n        - The function assumes the first four elements of bbox are x_min, y_min, x_max, y_max.\n        - Any elements in bbox after the first four are returned as-is.\n        - The returned bbox type is cast to BoxType to maintain type consistency.\n\n    Example:\n        &gt;&gt;&gt; denormalize_bbox((0.1, 0.2, 0.3, 0.4), (100, 200))\n        (20.0, 20.0, 60.0, 40.0)\n        &gt;&gt;&gt; denormalize_bbox((0.1, 0.2, 0.3, 0.4, 'label'), (100, 200))\n        (20.0, 20.0, 60.0, 40.0, 'label')\n    \"\"\"\n    rows, cols = image_shape[:2]\n\n    tail: tuple[Any, ...]\n    (x_min, y_min, x_max, y_max), tail = bbox[:4], tuple(bbox[4:])\n\n    x_min, x_max = x_min * cols, x_max * cols\n    y_min, y_max = y_min * rows, y_max * rows\n\n    return cast(BoxType, (x_min, y_min, x_max, y_max, *tail))\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.core.bbox_utils.denormalize_bboxes","title":"<code>def denormalize_bboxes    (bboxes, image_shape)    </code> [view source on GitHub]","text":"<p>Denormalize a list or array of bounding boxes.</p> <p>Parameters:</p> Name Type Description <code>bboxes</code> <code>Sequence[BoxType] | np.ndarray</code> <p>Normalized bounding boxes <code>[(x_min, y_min, x_max, y_max, ...)]</code>.</p> <code>image_shape</code> <code>Sequence[int]</code> <p>Image shape <code>(height, width)</code>.</p> <p>Returns:</p> Type Description <code>list[BoxType] | np.ndarray</code> <p>Denormalized bounding boxes <code>[(x_min, y_min, x_max, y_max, ...)]</code>.</p> Source code in <code>albumentations/core/bbox_utils.py</code> Python<pre><code>def denormalize_bboxes(\n    bboxes: Sequence[BoxType] | np.ndarray,\n    image_shape: Sequence[int],\n) -&gt; list[BoxType] | np.ndarray:\n    \"\"\"Denormalize a list or array of bounding boxes.\n\n    Args:\n        bboxes: Normalized bounding boxes `[(x_min, y_min, x_max, y_max, ...)]`.\n        image_shape: Image shape `(height, width)`.\n\n    Returns:\n        Denormalized bounding boxes `[(x_min, y_min, x_max, y_max, ...)]`.\n\n    \"\"\"\n    rows, cols = image_shape[:2]\n    if isinstance(bboxes, np.ndarray):\n        denormalized = bboxes.astype(float)\n        denormalized[:, [0, 2]] *= cols\n        denormalized[:, [1, 3]] *= rows\n        return denormalized\n\n    return [denormalize_bbox(bbox, image_shape) for bbox in bboxes]\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.core.bbox_utils.filter_bboxes","title":"<code>def filter_bboxes    (bboxes, image_shape, min_area=0.0, min_visibility=0.0, min_width=0.0, min_height=0.0)    </code> [view source on GitHub]","text":"<p>Remove bounding boxes that either lie outside of the visible area by more then min_visibility or whose area in pixels is under the threshold set by <code>min_area</code>. Also it crops boxes to final image size.</p> <p>Parameters:</p> Name Type Description <code>bboxes</code> <code>Sequence[BoxType]</code> <p>list of albumentations bounding box <code>(x_min, y_min, x_max, y_max)</code>.</p> <code>image_shape</code> <code>Sequence[int]</code> <p>Image shape <code>(height, width)</code>.</p> <code>min_area</code> <code>float</code> <p>Minimum area of a bounding box. All bounding boxes whose visible area in pixels. is less than this value will be removed. Default: 0.0.</p> <code>min_visibility</code> <code>float</code> <p>Minimum fraction of area for a bounding box to remain this box in list. Default: 0.0.</p> <code>min_width</code> <code>float</code> <p>Minimum width of a bounding box. All bounding boxes whose width is less than this value will be removed. Default: 0.0.</p> <code>min_height</code> <code>float</code> <p>Minimum height of a bounding box. All bounding boxes whose height is less than this value will be removed. Default: 0.0.</p> <p>Returns:</p> Type Description <code>list[BoxType]</code> <p>list of bounding boxes.</p> Source code in <code>albumentations/core/bbox_utils.py</code> Python<pre><code>def filter_bboxes(\n    bboxes: Sequence[BoxType],\n    image_shape: Sequence[int],\n    min_area: float = 0.0,\n    min_visibility: float = 0.0,\n    min_width: float = 0.0,\n    min_height: float = 0.0,\n) -&gt; list[BoxType]:\n    \"\"\"Remove bounding boxes that either lie outside of the visible area by more then min_visibility\n    or whose area in pixels is under the threshold set by `min_area`. Also it crops boxes to final image size.\n\n    Args:\n        bboxes: list of albumentations bounding box `(x_min, y_min, x_max, y_max)`.\n        image_shape: Image shape `(height, width)`.\n        min_area: Minimum area of a bounding box. All bounding boxes whose visible area in pixels.\n            is less than this value will be removed. Default: 0.0.\n        min_visibility: Minimum fraction of area for a bounding box to remain this box in list. Default: 0.0.\n        min_width: Minimum width of a bounding box. All bounding boxes whose width is\n            less than this value will be removed. Default: 0.0.\n        min_height: Minimum height of a bounding box. All bounding boxes whose height is\n            less than this value will be removed. Default: 0.0.\n\n    Returns:\n        list of bounding boxes.\n\n    \"\"\"\n    resulting_boxes: list[BoxType] = []\n    for i in range(len(bboxes)):\n        bbox = bboxes[i]\n        # Calculate areas of bounding box before and after clipping.\n        transformed_box_area = calculate_bbox_area(bbox, image_shape)\n        clipped_bbox = clip_bbox(bbox, image_shape)\n\n        bbox, tail = clipped_bbox[:4], clipped_bbox[4:]\n\n        clipped_box_area = calculate_bbox_area(bbox, image_shape)\n\n        # Calculate width and height of the clipped bounding box.\n        x_min, y_min, x_max, y_max = denormalize_bbox(bbox, image_shape)[:4]\n        clipped_width, clipped_height = x_max - x_min, y_max - y_min\n\n        if (\n            clipped_box_area != 0  # to ensure transformed_box_area!=0 and to handle min_area=0 or min_visibility=0\n            and clipped_box_area &gt;= min_area\n            and clipped_box_area / transformed_box_area &gt;= min_visibility\n            and clipped_width &gt;= min_width\n            and clipped_height &gt;= min_height\n        ):\n            resulting_boxes.append(cast(BoxType, bbox + tail))\n    return resulting_boxes\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.core.bbox_utils.filter_bboxes_by_visibility","title":"<code>def filter_bboxes_by_visibility    (original_shape, bboxes, transformed_shape, transformed_bboxes, threshold=0.0, min_area=0.0)    </code> [view source on GitHub]","text":"<p>Filter bounding boxes and return only those boxes whose visibility after transformation is above the threshold and minimal area of bounding box in pixels is more then min_area.</p> <p>Parameters:</p> Name Type Description <code>original_shape</code> <code>Sequence[int]</code> <p>Original image shape <code>(height, width, ...)</code>.</p> <code>bboxes</code> <code>Sequence[BoxType]</code> <p>Original bounding boxes <code>[(x_min, y_min, x_max, y_max)]</code>.</p> <code>transformed_shape</code> <code>Sequence[int]</code> <p>Transformed image shape <code>(height, width)</code>.</p> <code>transformed_bboxes</code> <code>Sequence[BoxType]</code> <p>Transformed bounding boxes <code>[(x_min, y_min, x_max, y_max)]</code>.</p> <code>threshold</code> <code>float</code> <p>visibility threshold. Should be a value in the range [0.0, 1.0].</p> <code>min_area</code> <code>float</code> <p>Minimal area threshold.</p> <p>Returns:</p> Type Description <code>list[BoxType]</code> <p>Filtered bounding boxes <code>[(x_min, y_min, x_max, y_max)]</code>.</p> Source code in <code>albumentations/core/bbox_utils.py</code> Python<pre><code>def filter_bboxes_by_visibility(\n    original_shape: Sequence[int],\n    bboxes: Sequence[BoxType],\n    transformed_shape: Sequence[int],\n    transformed_bboxes: Sequence[BoxType],\n    threshold: float = 0.0,\n    min_area: float = 0.0,\n) -&gt; list[BoxType]:\n    \"\"\"Filter bounding boxes and return only those boxes whose visibility after transformation is above\n    the threshold and minimal area of bounding box in pixels is more then min_area.\n\n    Args:\n        original_shape: Original image shape `(height, width, ...)`.\n        bboxes: Original bounding boxes `[(x_min, y_min, x_max, y_max)]`.\n        transformed_shape: Transformed image shape `(height, width)`.\n        transformed_bboxes: Transformed bounding boxes `[(x_min, y_min, x_max, y_max)]`.\n        threshold: visibility threshold. Should be a value in the range [0.0, 1.0].\n        min_area: Minimal area threshold.\n\n    Returns:\n        Filtered bounding boxes `[(x_min, y_min, x_max, y_max)]`.\n\n    \"\"\"\n    img_height, img_width = original_shape[:2]\n    transformed_img_height, transformed_img_width = transformed_shape[:2]\n\n    visible_bboxes = []\n    for bbox, transformed_bbox in zip(bboxes, transformed_bboxes):\n        if not all(0.0 &lt;= value &lt;= 1.0 for value in transformed_bbox[:4]):\n            continue\n        bbox_area = calculate_bbox_area(bbox, (img_height, img_width))\n        transformed_bbox_area = calculate_bbox_area(transformed_bbox, (transformed_img_height, transformed_img_width))\n        if transformed_bbox_area &lt; min_area:\n            continue\n        visibility = transformed_bbox_area / bbox_area\n        if visibility &gt;= threshold:\n            visible_bboxes.append(transformed_bbox)\n    return visible_bboxes\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.core.bbox_utils.normalize_bbox","title":"<code>def normalize_bbox    (bbox, image_shape)    </code> [view source on GitHub]","text":"<p>Normalize bounding box coordinates from absolute pixel values to relative values.</p> <p>This function converts absolute pixel coordinates of a bounding box to normalized coordinates (ranging from 0 to 1) based on the given image shape.</p> <p>Parameters:</p> Name Type Description <code>bbox</code> <code>BoxType</code> <p>A bounding box in absolute pixel coordinates (x_min, y_min, x_max, y_max, ...). Additional elements after the first four are preserved and returned unchanged.</p> <code>image_shape</code> <code>Sequence[int]</code> <p>The shape of the image (height, width, ...). Only the first two elements (height and width) are used.</p> <p>Returns:</p> Type Description <code>BoxType</code> <p>A bounding box with normalized coordinates (x_min, y_min, x_max, y_max, ...).     The coordinates are relative values in the range [0, 1].     Any additional elements from the input bbox are appended unchanged.</p> <p>Note</p> <ul> <li>Input bbox coordinates should be in pixel values, not exceeding image dimensions.</li> <li>The function assumes the first four elements of bbox are x_min, y_min, x_max, y_max.</li> <li>Any elements in bbox after the first four are returned as-is.</li> <li>The returned bbox type is cast to BoxType to maintain type consistency.</li> </ul> <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; normalize_bbox((20, 30, 60, 80), (100, 200))\n(0.1, 0.3, 0.3, 0.8)\n&gt;&gt;&gt; normalize_bbox((20, 30, 60, 80, 'label'), (100, 200))\n(0.1, 0.3, 0.3, 0.8, 'label')\n</code></pre> Source code in <code>albumentations/core/bbox_utils.py</code> Python<pre><code>def normalize_bbox(bbox: BoxType, image_shape: Sequence[int]) -&gt; BoxType:\n    \"\"\"Normalize bounding box coordinates from absolute pixel values to relative values.\n\n    This function converts absolute pixel coordinates of a bounding box to normalized coordinates\n    (ranging from 0 to 1) based on the given image shape.\n\n    Args:\n        bbox (BoxType): A bounding box in absolute pixel coordinates (x_min, y_min, x_max, y_max, ...).\n            Additional elements after the first four are preserved and returned unchanged.\n        image_shape (Sequence[int]): The shape of the image (height, width, ...).\n            Only the first two elements (height and width) are used.\n\n    Returns:\n        BoxType: A bounding box with normalized coordinates (x_min, y_min, x_max, y_max, ...).\n            The coordinates are relative values in the range [0, 1].\n            Any additional elements from the input bbox are appended unchanged.\n\n    Note:\n        - Input bbox coordinates should be in pixel values, not exceeding image dimensions.\n        - The function assumes the first four elements of bbox are x_min, y_min, x_max, y_max.\n        - Any elements in bbox after the first four are returned as-is.\n        - The returned bbox type is cast to BoxType to maintain type consistency.\n\n    Example:\n        &gt;&gt;&gt; normalize_bbox((20, 30, 60, 80), (100, 200))\n        (0.1, 0.3, 0.3, 0.8)\n        &gt;&gt;&gt; normalize_bbox((20, 30, 60, 80, 'label'), (100, 200))\n        (0.1, 0.3, 0.3, 0.8, 'label')\n    \"\"\"\n    rows, cols = image_shape[:2]\n\n    tail: tuple[Any, ...]\n    (x_min, y_min, x_max, y_max), tail = bbox[:4], tuple(bbox[4:])\n    x_min /= cols\n    x_max /= cols\n    y_min /= rows\n    y_max /= rows\n\n    return cast(BoxType, (x_min, y_min, x_max, y_max, *tail))\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.core.bbox_utils.normalize_bboxes","title":"<code>def normalize_bboxes    (bboxes, image_shape)    </code> [view source on GitHub]","text":"<p>Normalize a list or array of bounding boxes.</p> <p>Parameters:</p> Name Type Description <code>bboxes</code> <code>Sequence[BoxType] | np.ndarray</code> <p>Denormalized bounding boxes <code>[(x_min, y_min, x_max, y_max, ...)]</code>.</p> <code>image_shape</code> <code>Sequence[int]</code> <p>Image shape <code>(height, width)</code>.</p> <p>Returns:</p> Type Description <code>list[BoxType] | np.ndarray</code> <p>Normalized bounding boxes <code>[(x_min, y_min, x_max, y_max, ...)]</code>.</p> Source code in <code>albumentations/core/bbox_utils.py</code> Python<pre><code>def normalize_bboxes(bboxes: Sequence[BoxType] | np.ndarray, image_shape: Sequence[int]) -&gt; list[BoxType] | np.ndarray:\n    \"\"\"Normalize a list or array of bounding boxes.\n\n    Args:\n        bboxes: Denormalized bounding boxes `[(x_min, y_min, x_max, y_max, ...)]`.\n        image_shape: Image shape `(height, width)`.\n\n    Returns:\n        Normalized bounding boxes `[(x_min, y_min, x_max, y_max, ...)]`.\n\n    \"\"\"\n    rows, cols = image_shape[:2]\n    if isinstance(bboxes, np.ndarray):\n        normalized = bboxes.astype(float)\n        normalized[:, [0, 2]] /= cols\n        normalized[:, [1, 3]] /= rows\n        return normalized\n\n    return [normalize_bbox(bbox, image_shape) for bbox in bboxes]\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.core.bbox_utils.union_of_bboxes","title":"<code>def union_of_bboxes    (bboxes, erosion_rate)    </code> [view source on GitHub]","text":"<p>Calculate union of bounding boxes. Boxes could be in albumentations or Pascal Voc format.</p> <p>Parameters:</p> Name Type Description <code>bboxes</code> <code>list[tuple]</code> <p>List of bounding boxes</p> <code>erosion_rate</code> <code>float</code> <p>How much each bounding box can be shrunk, useful for erosive cropping. Set this in range [0, 1]. 0 will not be erosive at all, 1.0 can make any bbox lose its volume.</p> <p>Returns:</p> Type Description <code>Optional[tuple]</code> <p>A bounding box <code>(x_min, y_min, x_max, y_max)</code> or None if no bboxes are given or if                  the bounding boxes become invalid after erosion.</p> Source code in <code>albumentations/core/bbox_utils.py</code> Python<pre><code>def union_of_bboxes(bboxes: Sequence[BoxType], erosion_rate: float) -&gt; BoxInternalType | None:\n    \"\"\"Calculate union of bounding boxes. Boxes could be in albumentations or Pascal Voc format.\n\n    Args:\n        bboxes (list[tuple]): List of bounding boxes\n        erosion_rate (float): How much each bounding box can be shrunk, useful for erosive cropping.\n            Set this in range [0, 1]. 0 will not be erosive at all, 1.0 can make any bbox lose its volume.\n\n    Returns:\n        Optional[tuple]: A bounding box `(x_min, y_min, x_max, y_max)` or None if no bboxes are given or if\n                         the bounding boxes become invalid after erosion.\n    \"\"\"\n    if not bboxes:\n        return None\n\n    if len(bboxes) == 1:\n        if erosion_rate == 1:\n            return None\n        if erosion_rate == 0:\n            return bboxes[0][:4]\n\n    bboxes_np = np.array([bbox[:4] for bbox in bboxes])\n    x_min = bboxes_np[:, 0]\n    y_min = bboxes_np[:, 1]\n    x_max = bboxes_np[:, 2]\n    y_max = bboxes_np[:, 3]\n\n    bbox_width = x_max - x_min\n    bbox_height = y_max - y_min\n\n    # Adjust erosion rate to shrink bounding boxes accordingly\n    lim_x1 = x_min + erosion_rate * 0.5 * bbox_width\n    lim_y1 = y_min + erosion_rate * 0.5 * bbox_height\n    lim_x2 = x_max - erosion_rate * 0.5 * bbox_width\n    lim_y2 = y_max - erosion_rate * 0.5 * bbox_height\n\n    x1 = np.min(lim_x1)\n    y1 = np.min(lim_y1)\n    x2 = np.max(lim_x2)\n    y2 = np.max(lim_y2)\n\n    if x1 == x2 or y1 == y2:\n        return None\n\n    return x1, y1, x2, y2\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.core.composition","title":"<code>composition</code>","text":""},{"location":"api_reference/full_reference/#albumentations.core.composition.Compose","title":"<code>class  Compose</code> <code>     (transforms, bbox_params=None, keypoint_params=None, additional_targets=None, p=1.0, is_check_shapes=True, strict=True, return_params=False, save_key='applied_params')                 </code>  [view source on GitHub]","text":"<p>Compose transforms and handle all transformations regarding bounding boxes</p> <p>Parameters:</p> Name Type Description <code>transforms</code> <code>list</code> <p>list of transformations to compose.</p> <code>bbox_params</code> <code>BboxParams</code> <p>Parameters for bounding boxes transforms</p> <code>keypoint_params</code> <code>KeypointParams</code> <p>Parameters for keypoints transforms</p> <code>additional_targets</code> <code>dict</code> <p>Dict with keys - new target name, values - old target name. ex: {'image2': 'image'}</p> <code>p</code> <code>float</code> <p>probability of applying all list of transforms. Default: 1.0.</p> <code>is_check_shapes</code> <code>bool</code> <p>If True shapes consistency of images/mask/masks would be checked on each call. If you would like to disable this check - pass False (do it only if you are sure in your data consistency).</p> <code>strict</code> <code>bool</code> <p>If True, unknown keys will raise an error. If False, unknown keys will be ignored. Default: True.</p> <code>return_params</code> <code>bool</code> <p>if True returns params of each applied transform</p> <code>save_key</code> <code>str</code> <p>key to save applied params, default is 'applied_params'</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/core/composition.py</code> Python<pre><code>class Compose(BaseCompose, HubMixin):\n    \"\"\"Compose transforms and handle all transformations regarding bounding boxes\n\n    Args:\n        transforms (list): list of transformations to compose.\n        bbox_params (BboxParams): Parameters for bounding boxes transforms\n        keypoint_params (KeypointParams): Parameters for keypoints transforms\n        additional_targets (dict): Dict with keys - new target name, values - old target name. ex: {'image2': 'image'}\n        p (float): probability of applying all list of transforms. Default: 1.0.\n        is_check_shapes (bool): If True shapes consistency of images/mask/masks would be checked on each call. If you\n            would like to disable this check - pass False (do it only if you are sure in your data consistency).\n        strict (bool): If True, unknown keys will raise an error. If False, unknown keys will be ignored. Default: True.\n        return_params (bool): if True returns params of each applied transform\n        save_key (str): key to save applied params, default is 'applied_params'\n\n    \"\"\"\n\n    def __init__(\n        self,\n        transforms: TransformsSeqType,\n        bbox_params: dict[str, Any] | BboxParams | None = None,\n        keypoint_params: dict[str, Any] | KeypointParams | None = None,\n        additional_targets: dict[str, str] | None = None,\n        p: float = 1.0,\n        is_check_shapes: bool = True,\n        strict: bool = True,\n        return_params: bool = False,\n        save_key: str = \"applied_params\",\n    ):\n        super().__init__(transforms, p)\n\n        if bbox_params:\n            if isinstance(bbox_params, dict):\n                b_params = BboxParams(**bbox_params)\n            elif isinstance(bbox_params, BboxParams):\n                b_params = bbox_params\n            else:\n                msg = \"unknown format of bbox_params, please use `dict` or `BboxParams`\"\n                raise ValueError(msg)\n            self.processors[\"bboxes\"] = BboxProcessor(b_params)\n\n        if keypoint_params:\n            if isinstance(keypoint_params, dict):\n                k_params = KeypointParams(**keypoint_params)\n            elif isinstance(keypoint_params, KeypointParams):\n                k_params = keypoint_params\n            else:\n                msg = \"unknown format of keypoint_params, please use `dict` or `KeypointParams`\"\n                raise ValueError(msg)\n            self.processors[\"keypoints\"] = KeypointsProcessor(k_params)\n\n        for proc in self.processors.values():\n            proc.ensure_transforms_valid(self.transforms)\n\n        self.add_targets(additional_targets)\n        if not self.transforms:  # if no transforms -&gt; do nothing, all keys will be available\n            self._available_keys.update(AVAILABLE_KEYS)\n\n        self.is_check_args = True\n        self.strict = strict\n\n        self.is_check_shapes = is_check_shapes\n        self.check_each_transform = tuple(  # processors that checks after each transform\n            proc for proc in self.processors.values() if getattr(proc.params, \"check_each_transform\", False)\n        )\n        self._set_check_args_for_transforms(self.transforms)\n\n        self.return_params = return_params\n        if return_params:\n            self.save_key = save_key\n            self._available_keys.add(save_key)\n            self._transforms_dict = get_transforms_dict(self.transforms)\n            self.set_deterministic(True, save_key=save_key)\n\n    def _set_check_args_for_transforms(self, transforms: TransformsSeqType) -&gt; None:\n        for transform in transforms:\n            if isinstance(transform, BaseCompose):\n                self._set_check_args_for_transforms(transform.transforms)\n                transform.check_each_transform = self.check_each_transform\n                transform.processors = self.processors\n            if isinstance(transform, Compose):\n                transform.disable_check_args_private()\n\n    def disable_check_args_private(self) -&gt; None:\n        self.is_check_args = False\n        self.strict = False\n        self.main_compose = False\n\n    def __call__(self, *args: Any, force_apply: bool = False, **data: Any) -&gt; dict[str, Any]:\n        if args:\n            msg = \"You have to pass data to augmentations as named arguments, for example: aug(image=image)\"\n            raise KeyError(msg)\n\n        if not isinstance(force_apply, (bool, int)):\n            msg = \"force_apply must have bool or int type\"\n            raise TypeError(msg)\n\n        if self.return_params and self.main_compose:\n            data[self.save_key] = OrderedDict()\n\n        need_to_run = force_apply or random.random() &lt; self.p\n        if not need_to_run:\n            return data\n\n        self.preprocess(data)\n\n        for t in self.transforms:\n            data = t(**data)\n            data = self.check_data_post_transform(data)\n\n        return self.postprocess(data)\n\n    def run_with_params(self, *, params: dict[int, dict[str, Any]], **data: Any) -&gt; dict[str, Any]:\n        \"\"\"Run transforms with given parameters. Available only for Compose with `return_params=True`.\"\"\"\n        if self._transforms_dict is None:\n            raise RuntimeError(\"`run_with_params` is not available for Compose with `return_params=False`.\")\n\n        self.preprocess(data)\n\n        for tr_id, param in params.items():\n            tr = self._transforms_dict[tr_id]\n            data = tr.apply_with_params(param, **data)\n            data = self.check_data_post_transform(data)\n\n        return self.postprocess(data)\n\n    def preprocess(self, data: Any) -&gt; None:\n        if self.strict:\n            for data_name in data:\n                if data_name not in self._available_keys and data_name not in MASK_KEYS and data_name not in IMAGE_KEYS:\n                    msg = f\"Key {data_name} is not in available keys.\"\n                    raise ValueError(msg)\n        if self.is_check_args:\n            self._check_args(**data)\n        if self.main_compose:\n            for p in self.processors.values():\n                p.ensure_data_valid(data)\n            for p in self.processors.values():\n                p.preprocess(data)\n\n    def postprocess(self, data: dict[str, Any]) -&gt; dict[str, Any]:\n        if self.main_compose:\n            for p in self.processors.values():\n                p.postprocess(data)\n        return data\n\n    def to_dict_private(self) -&gt; dict[str, Any]:\n        dictionary = super().to_dict_private()\n        bbox_processor = self.processors.get(\"bboxes\")\n        keypoints_processor = self.processors.get(\"keypoints\")\n        dictionary.update(\n            {\n                \"bbox_params\": bbox_processor.params.to_dict_private() if bbox_processor else None,\n                \"keypoint_params\": (keypoints_processor.params.to_dict_private() if keypoints_processor else None),\n                \"additional_targets\": self.additional_targets,\n                \"is_check_shapes\": self.is_check_shapes,\n            },\n        )\n        return dictionary\n\n    def get_dict_with_id(self) -&gt; dict[str, Any]:\n        dictionary = super().get_dict_with_id()\n        bbox_processor = self.processors.get(\"bboxes\")\n        keypoints_processor = self.processors.get(\"keypoints\")\n        dictionary.update(\n            {\n                \"bbox_params\": bbox_processor.params.to_dict_private() if bbox_processor else None,\n                \"keypoint_params\": (keypoints_processor.params.to_dict_private() if keypoints_processor else None),\n                \"additional_targets\": self.additional_targets,\n                \"params\": None,\n                \"is_check_shapes\": self.is_check_shapes,\n            },\n        )\n        return dictionary\n\n    def _check_args(self, **kwargs: Any) -&gt; None:\n        shapes = []\n\n        for data_name, data in kwargs.items():\n            internal_data_name = self._additional_targets.get(data_name, data_name)\n            if internal_data_name in CHECKED_SINGLE:\n                if not isinstance(data, np.ndarray):\n                    raise TypeError(f\"{data_name} must be numpy array type\")\n                shapes.append(data.shape[:2])\n            if internal_data_name in CHECKED_MULTI and data is not None and len(data):\n                if not isinstance(data, Sequence) or not isinstance(data[0], np.ndarray):\n                    raise TypeError(f\"{data_name} must be list of numpy arrays\")\n                shapes.append(data[0].shape[:2])\n            if internal_data_name in CHECK_BBOX_PARAM and self.processors.get(\"bboxes\") is None:\n                msg = \"bbox_params must be specified for bbox transformations\"\n                raise ValueError(msg)\n\n            if internal_data_name in CHECK_KEYPOINTS_PARAM and self.processors.get(\"keypoints\") is None:\n                msg = \"keypoints_params must be specified for keypoint transformations\"\n                raise ValueError(msg)\n\n        if self.is_check_shapes and shapes and shapes.count(shapes[0]) != len(shapes):\n            msg = (\n                \"Height and Width of image, mask or masks should be equal. You can disable shapes check \"\n                \"by setting a parameter is_check_shapes=False of Compose class (do it only if you are sure \"\n                \"about your data consistency).\"\n            )\n            raise ValueError(msg)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.core.composition.Compose.run_with_params","title":"<code>run_with_params (self, *, params, **data)</code>","text":"<p>Run transforms with given parameters. Available only for Compose with <code>return_params=True</code>.</p> Source code in <code>albumentations/core/composition.py</code> Python<pre><code>def run_with_params(self, *, params: dict[int, dict[str, Any]], **data: Any) -&gt; dict[str, Any]:\n    \"\"\"Run transforms with given parameters. Available only for Compose with `return_params=True`.\"\"\"\n    if self._transforms_dict is None:\n        raise RuntimeError(\"`run_with_params` is not available for Compose with `return_params=False`.\")\n\n    self.preprocess(data)\n\n    for tr_id, param in params.items():\n        tr = self._transforms_dict[tr_id]\n        data = tr.apply_with_params(param, **data)\n        data = self.check_data_post_transform(data)\n\n    return self.postprocess(data)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.core.composition.OneOf","title":"<code>class  OneOf</code> <code>     (transforms, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Select one of transforms to apply. Selected transform will be called with <code>force_apply=True</code>. Transforms probabilities will be normalized to one 1, so in this case transforms probabilities works as weights.</p> <p>Parameters:</p> Name Type Description <code>transforms</code> <code>list</code> <p>list of transformations to compose.</p> <code>p</code> <code>float</code> <p>probability of applying selected transform. Default: 0.5.</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/core/composition.py</code> Python<pre><code>class OneOf(BaseCompose):\n    \"\"\"Select one of transforms to apply. Selected transform will be called with `force_apply=True`.\n    Transforms probabilities will be normalized to one 1, so in this case transforms probabilities works as weights.\n\n    Args:\n        transforms (list): list of transformations to compose.\n        p (float): probability of applying selected transform. Default: 0.5.\n\n    \"\"\"\n\n    def __init__(self, transforms: TransformsSeqType, p: float = 0.5):\n        super().__init__(transforms, p)\n        transforms_ps = [t.p for t in self.transforms]\n        s = sum(transforms_ps)\n        self.transforms_ps = [t / s for t in transforms_ps]\n\n    def __call__(self, *args: Any, force_apply: bool = False, **data: Any) -&gt; dict[str, Any]:\n        if self.replay_mode:\n            for t in self.transforms:\n                data = t(**data)\n            return data\n\n        if self.transforms_ps and (force_apply or random.random() &lt; self.p):\n            idx: int = random_utils.choice(len(self.transforms), p=self.transforms_ps)\n            t = self.transforms[idx]\n            data = t(force_apply=True, **data)\n        return data\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.core.composition.OneOrOther","title":"<code>class  OneOrOther</code> <code>     (first=None, second=None, transforms=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Select one or another transform to apply. Selected transform will be called with <code>force_apply=True</code>.</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/core/composition.py</code> Python<pre><code>class OneOrOther(BaseCompose):\n    \"\"\"Select one or another transform to apply. Selected transform will be called with `force_apply=True`.\"\"\"\n\n    def __init__(\n        self,\n        first: TransformType | None = None,\n        second: TransformType | None = None,\n        transforms: TransformsSeqType | None = None,\n        p: float = 0.5,\n    ):\n        if transforms is None:\n            if first is None or second is None:\n                msg = \"You must set both first and second or set transforms argument.\"\n                raise ValueError(msg)\n            transforms = [first, second]\n        super().__init__(transforms, p)\n        if len(self.transforms) != NUM_ONEOF_TRANSFORMS:\n            warnings.warn(\"Length of transforms is not equal to 2.\", stacklevel=2)\n\n    def __call__(self, *args: Any, force_apply: bool = False, **data: Any) -&gt; dict[str, Any]:\n        if self.replay_mode:\n            for t in self.transforms:\n                data = t(**data)\n            return data\n\n        if random.random() &lt; self.p:\n            return self.transforms[0](force_apply=True, **data)\n\n        return self.transforms[-1](force_apply=True, **data)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.core.composition.SelectiveChannelTransform","title":"<code>class  SelectiveChannelTransform</code> <code>     (transforms, channels=(0, 1, 2), p=1.0)                 </code>  [view source on GitHub]","text":"<p>A transformation class to apply specified transforms to selected channels of an image.</p> <p>This class extends BaseCompose to allow selective application of transformations to specified image channels. It extracts the selected channels, applies the transformations, and then reinserts the transformed channels back into their original positions in the image.</p> <p>Parameters:</p> Name Type Description <code>transforms</code> <code>TransformsSeqType</code> <p>A sequence of transformations (from Albumentations) to be applied to the specified channels.</p> <code>channels</code> <code>Sequence[int]</code> <p>A sequence of integers specifying the indices of the channels to which the transforms should be applied.</p> <code>p</code> <code>float</code> <p>Probability that the transform will be applied; the default is 1.0 (always apply).</p> <p>Methods</p> <p>call(args, *kwargs):     Applies the transforms to the image according to the specified channels.     The input data should include 'image' key with the image array.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>The transformed data dictionary, which includes the transformed 'image' key.</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/core/composition.py</code> Python<pre><code>class SelectiveChannelTransform(BaseCompose):\n    \"\"\"A transformation class to apply specified transforms to selected channels of an image.\n\n    This class extends BaseCompose to allow selective application of transformations to\n    specified image channels. It extracts the selected channels, applies the transformations,\n    and then reinserts the transformed channels back into their original positions in the image.\n\n    Parameters:\n        transforms (TransformsSeqType):\n            A sequence of transformations (from Albumentations) to be applied to the specified channels.\n        channels (Sequence[int]):\n            A sequence of integers specifying the indices of the channels to which the transforms should be applied.\n        p (float):\n            Probability that the transform will be applied; the default is 1.0 (always apply).\n\n    Methods:\n        __call__(*args, **kwargs):\n            Applies the transforms to the image according to the specified channels.\n            The input data should include 'image' key with the image array.\n\n    Returns:\n        dict[str, Any]: The transformed data dictionary, which includes the transformed 'image' key.\n    \"\"\"\n\n    def __init__(\n        self,\n        transforms: TransformsSeqType,\n        channels: Sequence[int] = (0, 1, 2),\n        p: float = 1.0,\n    ) -&gt; None:\n        super().__init__(transforms, p)\n        self.channels = channels\n\n    def __call__(self, *args: Any, force_apply: bool = False, **data: Any) -&gt; dict[str, Any]:\n        if force_apply or random.random() &lt; self.p:\n            image = data[\"image\"]\n\n            selected_channels = image[:, :, self.channels]\n            sub_image = np.ascontiguousarray(selected_channels)\n\n            for t in self.transforms:\n                sub_image = t(image=sub_image)[\"image\"]\n\n            transformed_channels = cv2.split(sub_image)\n            output_img = image.copy()\n\n            for idx, channel in zip(self.channels, transformed_channels):\n                output_img[:, :, idx] = channel\n\n            data[\"image\"] = np.ascontiguousarray(output_img)\n\n        return data\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.core.composition.Sequential","title":"<code>class  Sequential</code> <code>     (transforms, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Sequentially applies all transforms to targets.</p> <p>Note</p> <p>This transform is not intended to be a replacement for <code>Compose</code>. Instead, it should be used inside <code>Compose</code> the same way <code>OneOf</code> or <code>OneOrOther</code> are used. For instance, you can combine <code>OneOf</code> with <code>Sequential</code> to create an augmentation pipeline that contains multiple sequences of augmentations and applies one randomly chose sequence to input data (see the <code>Example</code> section for an example definition of such pipeline).</p> <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; import albumentations as A\n&gt;&gt;&gt; transform = A.Compose([\n&gt;&gt;&gt;    A.OneOf([\n&gt;&gt;&gt;        A.Sequential([\n&gt;&gt;&gt;            A.HorizontalFlip(p=0.5),\n&gt;&gt;&gt;            A.ShiftScaleRotate(p=0.5),\n&gt;&gt;&gt;        ]),\n&gt;&gt;&gt;        A.Sequential([\n&gt;&gt;&gt;            A.VerticalFlip(p=0.5),\n&gt;&gt;&gt;            A.RandomBrightnessContrast(p=0.5),\n&gt;&gt;&gt;        ]),\n&gt;&gt;&gt;    ], p=1)\n&gt;&gt;&gt; ])\n</code></pre> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/core/composition.py</code> Python<pre><code>class Sequential(BaseCompose):\n    \"\"\"Sequentially applies all transforms to targets.\n\n    Note:\n        This transform is not intended to be a replacement for `Compose`. Instead, it should be used inside `Compose`\n        the same way `OneOf` or `OneOrOther` are used. For instance, you can combine `OneOf` with `Sequential` to\n        create an augmentation pipeline that contains multiple sequences of augmentations and applies one randomly\n        chose sequence to input data (see the `Example` section for an example definition of such pipeline).\n\n    Example:\n        &gt;&gt;&gt; import albumentations as A\n        &gt;&gt;&gt; transform = A.Compose([\n        &gt;&gt;&gt;    A.OneOf([\n        &gt;&gt;&gt;        A.Sequential([\n        &gt;&gt;&gt;            A.HorizontalFlip(p=0.5),\n        &gt;&gt;&gt;            A.ShiftScaleRotate(p=0.5),\n        &gt;&gt;&gt;        ]),\n        &gt;&gt;&gt;        A.Sequential([\n        &gt;&gt;&gt;            A.VerticalFlip(p=0.5),\n        &gt;&gt;&gt;            A.RandomBrightnessContrast(p=0.5),\n        &gt;&gt;&gt;        ]),\n        &gt;&gt;&gt;    ], p=1)\n        &gt;&gt;&gt; ])\n\n    \"\"\"\n\n    def __init__(self, transforms: TransformsSeqType, p: float = 0.5):\n        super().__init__(transforms, p)\n\n    def __call__(self, *args: Any, force_apply: bool = False, **data: Any) -&gt; dict[str, Any]:\n        if self.replay_mode or force_apply or random.random() &lt; self.p:\n            for t in self.transforms:\n                data = t(**data)\n                data = self.check_data_post_transform(data)\n        return data\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.core.composition.SomeOf","title":"<code>class  SomeOf</code> <code>     (transforms, n, replace=True, p=1)                 </code>  [view source on GitHub]","text":"<p>Select N transforms to apply. Selected transforms will be called with <code>force_apply=True</code>. Transforms probabilities will be normalized to one 1, so in this case transforms probabilities works as weights.</p> <p>Parameters:</p> Name Type Description <code>transforms</code> <code>list</code> <p>list of transformations to compose.</p> <code>n</code> <code>int</code> <p>number of transforms to apply.</p> <code>replace</code> <code>bool</code> <p>Whether the sampled transforms are with or without replacement. Default: True.</p> <code>p</code> <code>float</code> <p>probability of applying selected transform. Default: 1.</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/core/composition.py</code> Python<pre><code>class SomeOf(BaseCompose):\n    \"\"\"Select N transforms to apply. Selected transforms will be called with `force_apply=True`.\n    Transforms probabilities will be normalized to one 1, so in this case transforms probabilities works as weights.\n\n    Args:\n        transforms (list): list of transformations to compose.\n        n (int): number of transforms to apply.\n        replace (bool): Whether the sampled transforms are with or without replacement. Default: True.\n        p (float): probability of applying selected transform. Default: 1.\n\n    \"\"\"\n\n    def __init__(self, transforms: TransformsSeqType, n: int, replace: bool = True, p: float = 1):\n        super().__init__(transforms, p)\n        self.n = n\n        self.replace = replace\n        transforms_ps = [t.p for t in self.transforms]\n        s = sum(transforms_ps)\n        self.transforms_ps = [t / s for t in transforms_ps]\n\n    def __call__(self, *arg: Any, force_apply: bool = False, **data: Any) -&gt; dict[str, Any]:\n        if self.replay_mode:\n            for t in self.transforms:\n                data = t(**data)\n                data = self.check_data_post_transform(data)\n            return data\n\n        if self.transforms_ps and (force_apply or random.random() &lt; self.p):\n            idx = random_utils.choice(len(self.transforms), size=self.n, replace=self.replace, p=self.transforms_ps)\n            for i in idx:\n                t = self.transforms[i]\n                data = t(force_apply=True, **data)\n                data = self.check_data_post_transform(data)\n        return data\n\n    def to_dict_private(self) -&gt; dict[str, Any]:\n        dictionary = super().to_dict_private()\n        dictionary.update({\"n\": self.n, \"replace\": self.replace})\n        return dictionary\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.core.hub_mixin","title":"<code>hub_mixin</code>","text":"<p>This module provides mixin functionality for the Albumentations library. It includes utility functions and classes to enhance the core capabilities.</p>"},{"location":"api_reference/full_reference/#albumentations.core.hub_mixin.HubMixin","title":"<code>class  HubMixin</code> <code> </code>  [view source on GitHub]","text":"<p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/core/hub_mixin.py</code> Python<pre><code>class HubMixin:\n    _CONFIG_KEYS = (\"train\", \"eval\")\n    _CONFIG_FILE_NAME_TEMPLATE = \"albumentations_config_{}.json\"\n\n    def _save_pretrained(self, save_directory: str | Path, filename: str) -&gt; Path:\n        \"\"\"Save the transform to a specified directory.\n\n        Args:\n            save_directory (Union[str, Path]):\n                Directory where the transform will be saved.\n            filename (str):\n                Name of the file to save the transform.\n\n        Returns:\n            Path: Path to the saved transform file.\n        \"\"\"\n        # create save directory and path\n        save_directory = Path(save_directory)\n        save_directory.mkdir(parents=True, exist_ok=True)\n        save_path = save_directory / filename\n\n        # save transforms\n        save_transform(self, save_path, data_format=\"json\")  # type: ignore[arg-type]\n\n        return save_path\n\n    @classmethod\n    def _from_pretrained(cls, save_directory: str | Path, filename: str) -&gt; object:\n        \"\"\"Load a transform from a specified directory.\n\n        Args:\n            save_directory (Union[str, Path]):\n                Directory from where the transform will be loaded.\n            filename (str):\n                Name of the file to load the transform from.\n\n        Returns:\n            A.Compose: Loaded transform.\n        \"\"\"\n        save_path = Path(save_directory) / filename\n        return load_transform(save_path, data_format=\"json\")\n\n    def save_pretrained(\n        self,\n        save_directory: str | Path,\n        *,\n        key: str = \"eval\",\n        allow_custom_keys: bool = False,\n        repo_id: str | None = None,\n        push_to_hub: bool = False,\n        **push_to_hub_kwargs: Any,\n    ) -&gt; str | None:\n        \"\"\"Save the transform and optionally push it to the Huggingface Hub.\n\n        Args:\n            save_directory (`str` or `Path`):\n                Path to directory in which the transform configuration will be saved.\n            key (`str`, *optional*):\n                Key to identify the configuration type, one of [\"train\", \"eval\"]. Defaults to \"eval\".\n            allow_custom_keys (`bool`, *optional*):\n                Allow custom keys for the configuration. Defaults to False.\n            push_to_hub (`bool`, *optional*, defaults to `False`):\n                Whether or not to push your transform to the Huggingface Hub after saving it.\n            repo_id (`str`, *optional*):\n                ID of your repository on the Hub. Used only if `push_to_hub=True`. Will default to the folder name if\n                not provided.\n            push_to_hub_kwargs:\n                Additional key word arguments passed along to the [`push_to_hub`] method.\n\n        Returns:\n            `str` or `None`: url of the commit on the Hub if `push_to_hub=True`, `None` otherwise.\n        \"\"\"\n        if not allow_custom_keys and key not in self._CONFIG_KEYS:\n            raise ValueError(\n                f\"Invalid key: `{key}`. Please use key from {self._CONFIG_KEYS} keys for upload. \"\n                \"If you want to use a custom key, set `allow_custom_keys=True`.\",\n            )\n\n        # save model transforms\n        filename = self._CONFIG_FILE_NAME_TEMPLATE.format(key)\n        self._save_pretrained(save_directory, filename)\n\n        # push to the Hub if required\n        if push_to_hub:\n            kwargs = push_to_hub_kwargs.copy()  # soft-copy to avoid mutating input\n            if repo_id is None:\n                repo_id = Path(save_directory).name  # Defaults to `save_directory` name\n            return self.push_to_hub(repo_id=repo_id, key=key, **kwargs)\n        return None\n\n    @classmethod\n    def from_pretrained(\n        cls: Any,\n        directory_or_repo_id: str | Path,\n        *,\n        key: str = \"eval\",\n        force_download: bool = False,\n        proxies: dict[str, str] | None = None,\n        token: str | bool | None = None,\n        cache_dir: str | Path | None = None,\n        local_files_only: bool = False,\n        revision: str | None = None,\n    ) -&gt; object:\n        \"\"\"Load a transform from the Huggingface Hub or a local directory.\n\n        Args:\n            directory_or_repo_id (`str`, `Path`):\n                - Either the `repo_id` (string) of a repo with hosted transform on the Hub, e.g. `qubvel-hf/albu`.\n                - Or a path to a `directory` containing transform config saved using\n                    [`~albumentations.Compose.save_pretrained`], e.g., `../path/to/my_directory/`.\n            key (`str`, *optional*):\n                Key to identify the configuration type, one of [\"train\", \"eval\"]. Defaults to \"eval\".\n            revision (`str`, *optional*):\n                Revision of the repo on the Hub. Can be a branch name, a git tag or any commit id.\n                Defaults to the latest commit on `main` branch.\n            force_download (`bool`, *optional*, defaults to `False`):\n                Whether to force (re-)downloading the transform configuration files from the Hub, overriding\n                the existing cache.\n            proxies (`dict[str, str]`, *optional*):\n                A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n                'http://hostname': 'foo.bar:4012'}`. The proxies are used on every request.\n            token (`str` or `bool`, *optional*):\n                The token to use as HTTP bearer authorization for remote files. By default, it will use the token\n                cached when running `huggingface-cli login`.\n            cache_dir (`str`, `Path`, *optional*):\n                Path to the folder where cached files are stored.\n            local_files_only (`bool`, *optional*, defaults to `False`):\n                If `True`, avoid downloading the file and return the path to the local cached file if it exists.\n        \"\"\"\n        filename = cls._CONFIG_FILE_NAME_TEMPLATE.format(key)\n        directory_or_repo_id = Path(directory_or_repo_id)\n        transform = None\n\n        # check if the file is already present locally\n        if directory_or_repo_id.is_dir():\n            if filename in os.listdir(directory_or_repo_id):\n                transform = cls._from_pretrained(save_directory=directory_or_repo_id, filename=filename)\n            elif is_huggingface_hub_available:\n                logging.info(\n                    f\"{filename} not found in {Path(directory_or_repo_id).resolve()}, trying to load from the Hub.\",\n                )\n            else:\n                raise FileNotFoundError(\n                    f\"{filename} not found in {Path(directory_or_repo_id).resolve()}.\"\n                    \" Please install `huggingface_hub` to load from the Hub.\",\n                )\n        if transform is not None:\n            return transform\n\n        # download the file from the Hub\n        try:\n            config_file = hf_hub_download(\n                repo_id=directory_or_repo_id,\n                filename=filename,\n                revision=revision,\n                cache_dir=cache_dir,\n                force_download=force_download,\n                proxies=proxies,\n                token=token,\n                local_files_only=local_files_only,\n            )\n            directory, filename = Path(config_file).parent, Path(config_file).name\n            return cls._from_pretrained(save_directory=directory, filename=filename)\n\n        except HfHubHTTPError as e:\n            raise HfHubHTTPError(f\"{filename} not found on the HuggingFace Hub\") from e\n\n    @require_huggingface_hub\n    def push_to_hub(\n        self,\n        repo_id: str,\n        *,\n        key: str = \"eval\",\n        allow_custom_keys: bool = False,\n        commit_message: str = \"Push transform using huggingface_hub.\",\n        private: bool = False,\n        token: str | None = None,\n        branch: str | None = None,\n        create_pr: bool | None = None,\n    ) -&gt; str:\n        \"\"\"Push the transform to the Huggingface Hub.\n\n        Use `allow_patterns` and `ignore_patterns` to precisely filter which files should be pushed to the hub. Use\n        `delete_patterns` to delete existing remote files in the same commit. See [`upload_folder`] reference for more\n        details.\n\n        Args:\n            repo_id (`str`):\n                ID of the repository to push to (example: `\"username/my-model\"`).\n            key (`str`, *optional*):\n                Key to identify the configuration type, one of [\"train\", \"eval\"]. Defaults to \"eval\".\n            allow_custom_keys (`bool`, *optional*):\n                Allow custom keys for the configuration. Defaults to False.\n            commit_message (`str`, *optional*):\n                Message to commit while pushing.\n            private (`bool`, *optional*, defaults to `False`):\n                Whether the repository created should be private.\n            token (`str`, *optional*):\n                The token to use as HTTP bearer authorization for remote files. By default, it will use the token\n                cached when running `huggingface-cli login`.\n            branch (`str`, *optional*):\n                The git branch on which to push the transform. This defaults to `\"main\"`.\n            create_pr (`boolean`, *optional*):\n                Whether or not to create a Pull Request from `branch` with that commit. Defaults to `False`.\n\n        Returns:\n            The url of the commit of your transform in the given repository.\n        \"\"\"\n        if not allow_custom_keys and key not in self._CONFIG_KEYS:\n            raise ValueError(\n                f\"Invalid key: `{key}`. Please use key from {self._CONFIG_KEYS} keys for upload. \"\n                \"If you still want to use a custom key, set `allow_custom_keys=True`.\",\n            )\n\n        api = HfApi(token=token)\n        repo_id = api.create_repo(repo_id=repo_id, private=private, exist_ok=True).repo_id\n\n        # Push the files to the repo in a single commit\n        with SoftTemporaryDirectory() as tmp:\n            save_directory = Path(tmp) / repo_id\n            filename = self._CONFIG_FILE_NAME_TEMPLATE.format(key)\n            save_path = self._save_pretrained(save_directory, filename=filename)\n            return api.upload_file(\n                path_or_fileobj=save_path,\n                path_in_repo=filename,\n                repo_id=repo_id,\n                commit_message=commit_message,\n                revision=branch,\n                create_pr=create_pr,\n            )\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.core.hub_mixin.HubMixin.from_pretrained","title":"<code>from_pretrained (directory_or_repo_id, *, key='eval', force_download=False, proxies=None, token=None, cache_dir=None, local_files_only=False, revision=None)</code>  <code>classmethod</code>","text":"<p>Load a transform from the Huggingface Hub or a local directory.</p> <p>Parameters:</p> Name Type Description <code>directory_or_repo_id</code> <code>`str`, `Path`</code> <ul> <li>Either the <code>repo_id</code> (string) of a repo with hosted transform on the Hub, e.g. <code>qubvel-hf/albu</code>.</li> <li>Or a path to a <code>directory</code> containing transform config saved using     [<code>~albumentations.Compose.save_pretrained</code>], e.g., <code>../path/to/my_directory/</code>.</li> </ul> <code>key</code> <code>`str`, *optional*</code> <p>Key to identify the configuration type, one of [\"train\", \"eval\"]. Defaults to \"eval\".</p> <code>revision</code> <code>`str`, *optional*</code> <p>Revision of the repo on the Hub. Can be a branch name, a git tag or any commit id. Defaults to the latest commit on <code>main</code> branch.</p> <code>force_download</code> <code>`bool`, *optional*, defaults to `False`</code> <p>Whether to force (re-)downloading the transform configuration files from the Hub, overriding the existing cache.</p> <code>proxies</code> <code>`dict[str, str]`, *optional*</code> <p>A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}</code>. The proxies are used on every request.</p> <code>token</code> <code>`str` or `bool`, *optional*</code> <p>The token to use as HTTP bearer authorization for remote files. By default, it will use the token cached when running <code>huggingface-cli login</code>.</p> <code>cache_dir</code> <code>`str`, `Path`, *optional*</code> <p>Path to the folder where cached files are stored.</p> <code>local_files_only</code> <code>`bool`, *optional*, defaults to `False`</code> <p>If <code>True</code>, avoid downloading the file and return the path to the local cached file if it exists.</p> Source code in <code>albumentations/core/hub_mixin.py</code> Python<pre><code>@classmethod\ndef from_pretrained(\n    cls: Any,\n    directory_or_repo_id: str | Path,\n    *,\n    key: str = \"eval\",\n    force_download: bool = False,\n    proxies: dict[str, str] | None = None,\n    token: str | bool | None = None,\n    cache_dir: str | Path | None = None,\n    local_files_only: bool = False,\n    revision: str | None = None,\n) -&gt; object:\n    \"\"\"Load a transform from the Huggingface Hub or a local directory.\n\n    Args:\n        directory_or_repo_id (`str`, `Path`):\n            - Either the `repo_id` (string) of a repo with hosted transform on the Hub, e.g. `qubvel-hf/albu`.\n            - Or a path to a `directory` containing transform config saved using\n                [`~albumentations.Compose.save_pretrained`], e.g., `../path/to/my_directory/`.\n        key (`str`, *optional*):\n            Key to identify the configuration type, one of [\"train\", \"eval\"]. Defaults to \"eval\".\n        revision (`str`, *optional*):\n            Revision of the repo on the Hub. Can be a branch name, a git tag or any commit id.\n            Defaults to the latest commit on `main` branch.\n        force_download (`bool`, *optional*, defaults to `False`):\n            Whether to force (re-)downloading the transform configuration files from the Hub, overriding\n            the existing cache.\n        proxies (`dict[str, str]`, *optional*):\n            A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n            'http://hostname': 'foo.bar:4012'}`. The proxies are used on every request.\n        token (`str` or `bool`, *optional*):\n            The token to use as HTTP bearer authorization for remote files. By default, it will use the token\n            cached when running `huggingface-cli login`.\n        cache_dir (`str`, `Path`, *optional*):\n            Path to the folder where cached files are stored.\n        local_files_only (`bool`, *optional*, defaults to `False`):\n            If `True`, avoid downloading the file and return the path to the local cached file if it exists.\n    \"\"\"\n    filename = cls._CONFIG_FILE_NAME_TEMPLATE.format(key)\n    directory_or_repo_id = Path(directory_or_repo_id)\n    transform = None\n\n    # check if the file is already present locally\n    if directory_or_repo_id.is_dir():\n        if filename in os.listdir(directory_or_repo_id):\n            transform = cls._from_pretrained(save_directory=directory_or_repo_id, filename=filename)\n        elif is_huggingface_hub_available:\n            logging.info(\n                f\"{filename} not found in {Path(directory_or_repo_id).resolve()}, trying to load from the Hub.\",\n            )\n        else:\n            raise FileNotFoundError(\n                f\"{filename} not found in {Path(directory_or_repo_id).resolve()}.\"\n                \" Please install `huggingface_hub` to load from the Hub.\",\n            )\n    if transform is not None:\n        return transform\n\n    # download the file from the Hub\n    try:\n        config_file = hf_hub_download(\n            repo_id=directory_or_repo_id,\n            filename=filename,\n            revision=revision,\n            cache_dir=cache_dir,\n            force_download=force_download,\n            proxies=proxies,\n            token=token,\n            local_files_only=local_files_only,\n        )\n        directory, filename = Path(config_file).parent, Path(config_file).name\n        return cls._from_pretrained(save_directory=directory, filename=filename)\n\n    except HfHubHTTPError as e:\n        raise HfHubHTTPError(f\"{filename} not found on the HuggingFace Hub\") from e\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.core.hub_mixin.HubMixin.push_to_hub","title":"<code>push_to_hub (self, repo_id, *, key='eval', allow_custom_keys=False, commit_message='Push transform using huggingface_hub.', private=False, token=None, branch=None, create_pr=None)</code>","text":"<p>Push the transform to the Huggingface Hub.</p> <p>Use <code>allow_patterns</code> and <code>ignore_patterns</code> to precisely filter which files should be pushed to the hub. Use <code>delete_patterns</code> to delete existing remote files in the same commit. See [<code>upload_folder</code>] reference for more details.</p> <p>Parameters:</p> Name Type Description <code>repo_id</code> <code>`str`</code> <p>ID of the repository to push to (example: <code>\"username/my-model\"</code>).</p> <code>key</code> <code>`str`, *optional*</code> <p>Key to identify the configuration type, one of [\"train\", \"eval\"]. Defaults to \"eval\".</p> <code>allow_custom_keys</code> <code>`bool`, *optional*</code> <p>Allow custom keys for the configuration. Defaults to False.</p> <code>commit_message</code> <code>`str`, *optional*</code> <p>Message to commit while pushing.</p> <code>private</code> <code>`bool`, *optional*, defaults to `False`</code> <p>Whether the repository created should be private.</p> <code>token</code> <code>`str`, *optional*</code> <p>The token to use as HTTP bearer authorization for remote files. By default, it will use the token cached when running <code>huggingface-cli login</code>.</p> <code>branch</code> <code>`str`, *optional*</code> <p>The git branch on which to push the transform. This defaults to <code>\"main\"</code>.</p> <code>create_pr</code> <code>`boolean`, *optional*</code> <p>Whether or not to create a Pull Request from <code>branch</code> with that commit. Defaults to <code>False</code>.</p> <p>Returns:</p> Type Description <code>str</code> <p>The url of the commit of your transform in the given repository.</p> Source code in <code>albumentations/core/hub_mixin.py</code> Python<pre><code>@require_huggingface_hub\ndef push_to_hub(\n    self,\n    repo_id: str,\n    *,\n    key: str = \"eval\",\n    allow_custom_keys: bool = False,\n    commit_message: str = \"Push transform using huggingface_hub.\",\n    private: bool = False,\n    token: str | None = None,\n    branch: str | None = None,\n    create_pr: bool | None = None,\n) -&gt; str:\n    \"\"\"Push the transform to the Huggingface Hub.\n\n    Use `allow_patterns` and `ignore_patterns` to precisely filter which files should be pushed to the hub. Use\n    `delete_patterns` to delete existing remote files in the same commit. See [`upload_folder`] reference for more\n    details.\n\n    Args:\n        repo_id (`str`):\n            ID of the repository to push to (example: `\"username/my-model\"`).\n        key (`str`, *optional*):\n            Key to identify the configuration type, one of [\"train\", \"eval\"]. Defaults to \"eval\".\n        allow_custom_keys (`bool`, *optional*):\n            Allow custom keys for the configuration. Defaults to False.\n        commit_message (`str`, *optional*):\n            Message to commit while pushing.\n        private (`bool`, *optional*, defaults to `False`):\n            Whether the repository created should be private.\n        token (`str`, *optional*):\n            The token to use as HTTP bearer authorization for remote files. By default, it will use the token\n            cached when running `huggingface-cli login`.\n        branch (`str`, *optional*):\n            The git branch on which to push the transform. This defaults to `\"main\"`.\n        create_pr (`boolean`, *optional*):\n            Whether or not to create a Pull Request from `branch` with that commit. Defaults to `False`.\n\n    Returns:\n        The url of the commit of your transform in the given repository.\n    \"\"\"\n    if not allow_custom_keys and key not in self._CONFIG_KEYS:\n        raise ValueError(\n            f\"Invalid key: `{key}`. Please use key from {self._CONFIG_KEYS} keys for upload. \"\n            \"If you still want to use a custom key, set `allow_custom_keys=True`.\",\n        )\n\n    api = HfApi(token=token)\n    repo_id = api.create_repo(repo_id=repo_id, private=private, exist_ok=True).repo_id\n\n    # Push the files to the repo in a single commit\n    with SoftTemporaryDirectory() as tmp:\n        save_directory = Path(tmp) / repo_id\n        filename = self._CONFIG_FILE_NAME_TEMPLATE.format(key)\n        save_path = self._save_pretrained(save_directory, filename=filename)\n        return api.upload_file(\n            path_or_fileobj=save_path,\n            path_in_repo=filename,\n            repo_id=repo_id,\n            commit_message=commit_message,\n            revision=branch,\n            create_pr=create_pr,\n        )\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.core.hub_mixin.HubMixin.save_pretrained","title":"<code>save_pretrained (self, save_directory, *, key='eval', allow_custom_keys=False, repo_id=None, push_to_hub=False, **push_to_hub_kwargs)</code>","text":"<p>Save the transform and optionally push it to the Huggingface Hub.</p> <p>Parameters:</p> Name Type Description <code>save_directory</code> <code>`str` or `Path`</code> <p>Path to directory in which the transform configuration will be saved.</p> <code>key</code> <code>`str`, *optional*</code> <p>Key to identify the configuration type, one of [\"train\", \"eval\"]. Defaults to \"eval\".</p> <code>allow_custom_keys</code> <code>`bool`, *optional*</code> <p>Allow custom keys for the configuration. Defaults to False.</p> <code>push_to_hub</code> <code>`bool`, *optional*, defaults to `False`</code> <p>Whether or not to push your transform to the Huggingface Hub after saving it.</p> <code>repo_id</code> <code>`str`, *optional*</code> <p>ID of your repository on the Hub. Used only if <code>push_to_hub=True</code>. Will default to the folder name if not provided.</p> <code>push_to_hub_kwargs</code> <code>Any</code> <p>Additional key word arguments passed along to the [<code>push_to_hub</code>] method.</p> <p>Returns:</p> Type Description <code>`str` or `None`</code> <p>url of the commit on the Hub if <code>push_to_hub=True</code>, <code>None</code> otherwise.</p> Source code in <code>albumentations/core/hub_mixin.py</code> Python<pre><code>def save_pretrained(\n    self,\n    save_directory: str | Path,\n    *,\n    key: str = \"eval\",\n    allow_custom_keys: bool = False,\n    repo_id: str | None = None,\n    push_to_hub: bool = False,\n    **push_to_hub_kwargs: Any,\n) -&gt; str | None:\n    \"\"\"Save the transform and optionally push it to the Huggingface Hub.\n\n    Args:\n        save_directory (`str` or `Path`):\n            Path to directory in which the transform configuration will be saved.\n        key (`str`, *optional*):\n            Key to identify the configuration type, one of [\"train\", \"eval\"]. Defaults to \"eval\".\n        allow_custom_keys (`bool`, *optional*):\n            Allow custom keys for the configuration. Defaults to False.\n        push_to_hub (`bool`, *optional*, defaults to `False`):\n            Whether or not to push your transform to the Huggingface Hub after saving it.\n        repo_id (`str`, *optional*):\n            ID of your repository on the Hub. Used only if `push_to_hub=True`. Will default to the folder name if\n            not provided.\n        push_to_hub_kwargs:\n            Additional key word arguments passed along to the [`push_to_hub`] method.\n\n    Returns:\n        `str` or `None`: url of the commit on the Hub if `push_to_hub=True`, `None` otherwise.\n    \"\"\"\n    if not allow_custom_keys and key not in self._CONFIG_KEYS:\n        raise ValueError(\n            f\"Invalid key: `{key}`. Please use key from {self._CONFIG_KEYS} keys for upload. \"\n            \"If you want to use a custom key, set `allow_custom_keys=True`.\",\n        )\n\n    # save model transforms\n    filename = self._CONFIG_FILE_NAME_TEMPLATE.format(key)\n    self._save_pretrained(save_directory, filename)\n\n    # push to the Hub if required\n    if push_to_hub:\n        kwargs = push_to_hub_kwargs.copy()  # soft-copy to avoid mutating input\n        if repo_id is None:\n            repo_id = Path(save_directory).name  # Defaults to `save_directory` name\n        return self.push_to_hub(repo_id=repo_id, key=key, **kwargs)\n    return None\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.core.keypoints_utils","title":"<code>keypoints_utils</code>","text":""},{"location":"api_reference/full_reference/#albumentations.core.keypoints_utils.KeypointParams","title":"<code>class  KeypointParams</code> <code>     (format, label_fields=None, remove_invisible=True, angle_in_degrees=True, check_each_transform=True)                 </code>  [view source on GitHub]","text":"<p>Parameters of keypoints</p> <p>Parameters:</p> Name Type Description <code>format</code> <code>str</code> <p>format of keypoints. Should be 'xy', 'yx', 'xya', 'xys', 'xyas', 'xysa'.</p> <p>x - X coordinate,</p> <p>y - Y coordinate</p> <p>s - Keypoint scale</p> <p>a - Keypoint orientation in radians or degrees (depending on KeypointParams.angle_in_degrees)</p> <code>label_fields</code> <code>list</code> <p>list of fields that are joined with keypoints, e.g labels. Should be same type as keypoints.</p> <code>remove_invisible</code> <code>bool</code> <p>to remove invisible points after transform or not</p> <code>angle_in_degrees</code> <code>bool</code> <p>angle in degrees or radians in 'xya', 'xyas', 'xysa' keypoints</p> <code>check_each_transform</code> <code>bool</code> <p>if <code>True</code>, then keypoints will be checked after each dual transform. Default: <code>True</code></p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/core/keypoints_utils.py</code> Python<pre><code>class KeypointParams(Params):\n    \"\"\"Parameters of keypoints\n\n    Args:\n        format (str): format of keypoints. Should be 'xy', 'yx', 'xya', 'xys', 'xyas', 'xysa'.\n\n            x - X coordinate,\n\n            y - Y coordinate\n\n            s - Keypoint scale\n\n            a - Keypoint orientation in radians or degrees (depending on KeypointParams.angle_in_degrees)\n        label_fields (list): list of fields that are joined with keypoints, e.g labels.\n            Should be same type as keypoints.\n        remove_invisible (bool): to remove invisible points after transform or not\n        angle_in_degrees (bool): angle in degrees or radians in 'xya', 'xyas', 'xysa' keypoints\n        check_each_transform (bool): if `True`, then keypoints will be checked after each dual transform.\n            Default: `True`\n\n    \"\"\"\n\n    def __init__(\n        self,\n        format: str,  # noqa: A002\n        label_fields: Sequence[str] | None = None,\n        remove_invisible: bool = True,\n        angle_in_degrees: bool = True,\n        check_each_transform: bool = True,\n    ):\n        super().__init__(format, label_fields)\n        self.remove_invisible = remove_invisible\n        self.angle_in_degrees = angle_in_degrees\n        self.check_each_transform = check_each_transform\n\n    def to_dict_private(self) -&gt; dict[str, Any]:\n        data = super().to_dict_private()\n        data.update(\n            {\n                \"remove_invisible\": self.remove_invisible,\n                \"angle_in_degrees\": self.angle_in_degrees,\n                \"check_each_transform\": self.check_each_transform,\n            },\n        )\n        return data\n\n    @classmethod\n    def is_serializable(cls) -&gt; bool:\n        return True\n\n    @classmethod\n    def get_class_fullname(cls) -&gt; str:\n        return \"KeypointParams\"\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.core.keypoints_utils.check_keypoint","title":"<code>def check_keypoint    (kp, image_shape)    </code> [view source on GitHub]","text":"<p>Check if keypoint coordinates are less than image shapes</p> Source code in <code>albumentations/core/keypoints_utils.py</code> Python<pre><code>def check_keypoint(kp: KeypointType, image_shape: Sequence[int]) -&gt; None:\n    \"\"\"Check if keypoint coordinates are less than image shapes\"\"\"\n    rows, cols = image_shape[:2]\n    for name, value, size in zip([\"x\", \"y\"], kp[:2], [cols, rows]):\n        if not 0 &lt;= value &lt; size:\n            raise ValueError(f\"Expected {name} for keypoint {kp} to be in the range [0.0, {size}], got {value}.\")\n\n    angle = kp[2]\n    if not (0 &lt;= angle &lt; 2 * math.pi):\n        raise ValueError(f\"Keypoint angle must be in range [0, 2 * PI). Got: {angle}\")\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.core.keypoints_utils.check_keypoints","title":"<code>def check_keypoints    (keypoints, image_shape)    </code> [view source on GitHub]","text":"<p>Check if keypoints boundaries are less than image shapes</p> Source code in <code>albumentations/core/keypoints_utils.py</code> Python<pre><code>def check_keypoints(keypoints: Sequence[KeypointType], image_shape: Sequence[int]) -&gt; None:\n    \"\"\"Check if keypoints boundaries are less than image shapes\"\"\"\n    for kp in keypoints:\n        check_keypoint(kp, image_shape)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.core.serialization","title":"<code>serialization</code>","text":""},{"location":"api_reference/full_reference/#albumentations.core.serialization.Serializable","title":"<code>class  Serializable</code> <code> </code>  [view source on GitHub]","text":"<p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/core/serialization.py</code> Python<pre><code>class Serializable(metaclass=SerializableMeta):\n    @classmethod\n    @abstractmethod\n    def is_serializable(cls) -&gt; bool:\n        raise NotImplementedError\n\n    @classmethod\n    @abstractmethod\n    def get_class_fullname(cls) -&gt; str:\n        raise NotImplementedError\n\n    @abstractmethod\n    def to_dict_private(self) -&gt; dict[str, Any]:\n        raise NotImplementedError\n\n    def to_dict(self, on_not_implemented_error: str = \"raise\") -&gt; dict[str, Any]:\n        \"\"\"Take a transform pipeline and convert it to a serializable representation that uses only standard\n        python data types: dictionaries, lists, strings, integers, and floats.\n\n        Args:\n            self: A transform that should be serialized. If the transform doesn't implement the `to_dict`\n                method and `on_not_implemented_error` equals to 'raise' then `NotImplementedError` is raised.\n                If `on_not_implemented_error` equals to 'warn' then `NotImplementedError` will be ignored\n                but no transform parameters will be serialized.\n            on_not_implemented_error (str): `raise` or `warn`.\n\n        \"\"\"\n        if on_not_implemented_error not in {\"raise\", \"warn\"}:\n            msg = f\"Unknown on_not_implemented_error value: {on_not_implemented_error}. Supported values are: 'raise' \"\n            \"and 'warn'\"\n            raise ValueError(msg)\n        try:\n            transform_dict = self.to_dict_private()\n        except NotImplementedError:\n            if on_not_implemented_error == \"raise\":\n                raise\n\n            transform_dict = {}\n            warnings.warn(\n                f\"Got NotImplementedError while trying to serialize {self}. Object arguments are not preserved. \"\n                f\"Implement either '{self.__class__.__name__}.get_transform_init_args_names' \"\n                f\"or '{self.__class__.__name__}.get_transform_init_args' \"\n                \"method to make the transform serializable\",\n                stacklevel=2,\n            )\n        return {\"__version__\": __version__, \"transform\": transform_dict}\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.core.serialization.Serializable.to_dict","title":"<code>to_dict (self, on_not_implemented_error='raise')</code>","text":"<p>Take a transform pipeline and convert it to a serializable representation that uses only standard python data types: dictionaries, lists, strings, integers, and floats.</p> <p>Parameters:</p> Name Type Description <code>self</code> <p>A transform that should be serialized. If the transform doesn't implement the <code>to_dict</code> method and <code>on_not_implemented_error</code> equals to 'raise' then <code>NotImplementedError</code> is raised. If <code>on_not_implemented_error</code> equals to 'warn' then <code>NotImplementedError</code> will be ignored but no transform parameters will be serialized.</p> <code>on_not_implemented_error</code> <code>str</code> <p><code>raise</code> or <code>warn</code>.</p> Source code in <code>albumentations/core/serialization.py</code> Python<pre><code>def to_dict(self, on_not_implemented_error: str = \"raise\") -&gt; dict[str, Any]:\n    \"\"\"Take a transform pipeline and convert it to a serializable representation that uses only standard\n    python data types: dictionaries, lists, strings, integers, and floats.\n\n    Args:\n        self: A transform that should be serialized. If the transform doesn't implement the `to_dict`\n            method and `on_not_implemented_error` equals to 'raise' then `NotImplementedError` is raised.\n            If `on_not_implemented_error` equals to 'warn' then `NotImplementedError` will be ignored\n            but no transform parameters will be serialized.\n        on_not_implemented_error (str): `raise` or `warn`.\n\n    \"\"\"\n    if on_not_implemented_error not in {\"raise\", \"warn\"}:\n        msg = f\"Unknown on_not_implemented_error value: {on_not_implemented_error}. Supported values are: 'raise' \"\n        \"and 'warn'\"\n        raise ValueError(msg)\n    try:\n        transform_dict = self.to_dict_private()\n    except NotImplementedError:\n        if on_not_implemented_error == \"raise\":\n            raise\n\n        transform_dict = {}\n        warnings.warn(\n            f\"Got NotImplementedError while trying to serialize {self}. Object arguments are not preserved. \"\n            f\"Implement either '{self.__class__.__name__}.get_transform_init_args_names' \"\n            f\"or '{self.__class__.__name__}.get_transform_init_args' \"\n            \"method to make the transform serializable\",\n            stacklevel=2,\n        )\n    return {\"__version__\": __version__, \"transform\": transform_dict}\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.core.serialization.SerializableMeta","title":"<code>class  SerializableMeta</code> <code> </code>  [view source on GitHub]","text":"<p>A metaclass that is used to register classes in <code>SERIALIZABLE_REGISTRY</code> or <code>NON_SERIALIZABLE_REGISTRY</code> so they can be found later while deserializing transformation pipeline using classes full names.</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/core/serialization.py</code> Python<pre><code>class SerializableMeta(ABCMeta):\n    \"\"\"A metaclass that is used to register classes in `SERIALIZABLE_REGISTRY` or `NON_SERIALIZABLE_REGISTRY`\n    so they can be found later while deserializing transformation pipeline using classes full names.\n    \"\"\"\n\n    def __new__(cls, name: str, bases: tuple[type, ...], *args: Any, **kwargs: Any) -&gt; SerializableMeta:\n        cls_obj = super().__new__(cls, name, bases, *args, **kwargs)\n        if name != \"Serializable\" and ABC not in bases:\n            if cls_obj.is_serializable():\n                SERIALIZABLE_REGISTRY[cls_obj.get_class_fullname()] = cls_obj\n            else:\n                NON_SERIALIZABLE_REGISTRY[cls_obj.get_class_fullname()] = cls_obj\n        return cls_obj\n\n    @classmethod\n    def is_serializable(cls) -&gt; bool:\n        return False\n\n    @classmethod\n    def get_class_fullname(cls) -&gt; str:\n        return get_shortest_class_fullname(cls)\n\n    @classmethod\n    def _to_dict(cls) -&gt; dict[str, Any]:\n        return {}\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.core.serialization.SerializableMeta.__new__","title":"<code>__new__ (cls, name, bases, *args, **kwargs)</code>  <code>special</code> <code>staticmethod</code>","text":"<p>Create and return a new object.  See help(type) for accurate signature.</p> Source code in <code>albumentations/core/serialization.py</code> Python<pre><code>def __new__(cls, name: str, bases: tuple[type, ...], *args: Any, **kwargs: Any) -&gt; SerializableMeta:\n    cls_obj = super().__new__(cls, name, bases, *args, **kwargs)\n    if name != \"Serializable\" and ABC not in bases:\n        if cls_obj.is_serializable():\n            SERIALIZABLE_REGISTRY[cls_obj.get_class_fullname()] = cls_obj\n        else:\n            NON_SERIALIZABLE_REGISTRY[cls_obj.get_class_fullname()] = cls_obj\n    return cls_obj\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.core.serialization.from_dict","title":"<code>def from_dict    (transform_dict, nonserializable=None)    </code> [view source on GitHub]","text":"<p>transform_dict: A dictionary with serialized transform pipeline. nonserializable (dict): A dictionary that contains non-serializable transforms.     This dictionary is required when you are restoring a pipeline that contains non-serializable transforms.     Keys in that dictionary should be named same as <code>name</code> arguments in respective transforms from     a serialized pipeline.</p> Source code in <code>albumentations/core/serialization.py</code> Python<pre><code>def from_dict(\n    transform_dict: dict[str, Any],\n    nonserializable: dict[str, Any] | None = None,\n) -&gt; Serializable | None:\n    \"\"\"Args:\n    transform_dict: A dictionary with serialized transform pipeline.\n    nonserializable (dict): A dictionary that contains non-serializable transforms.\n        This dictionary is required when you are restoring a pipeline that contains non-serializable transforms.\n        Keys in that dictionary should be named same as `name` arguments in respective transforms from\n        a serialized pipeline.\n\n    \"\"\"\n    register_additional_transforms()\n    transform = transform_dict[\"transform\"]\n    lmbd = instantiate_nonserializable(transform, nonserializable)\n    if lmbd:\n        return lmbd\n    name = transform[\"__class_fullname__\"]\n    args = {k: v for k, v in transform.items() if k != \"__class_fullname__\"}\n    cls = SERIALIZABLE_REGISTRY[shorten_class_name(name)]\n    if \"transforms\" in args:\n        args[\"transforms\"] = [from_dict({\"transform\": t}, nonserializable=nonserializable) for t in args[\"transforms\"]]\n    return cls(**args)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.core.serialization.get_shortest_class_fullname","title":"<code>def get_shortest_class_fullname    (cls)    </code> [view source on GitHub]","text":"<p>The function <code>get_shortest_class_fullname</code> takes a class object as input and returns its shortened full name.</p> <p>:param cls: The parameter <code>cls</code> is of type <code>Type[BasicCompose]</code>, which means it expects a class that is a subclass of <code>BasicCompose</code> :type cls: Type[BasicCompose] :return: a string, which is the shortened version of the full class name.</p> Source code in <code>albumentations/core/serialization.py</code> Python<pre><code>def get_shortest_class_fullname(cls: type[Any]) -&gt; str:\n    \"\"\"The function `get_shortest_class_fullname` takes a class object as input and returns its shortened\n    full name.\n\n    :param cls: The parameter `cls` is of type `Type[BasicCompose]`, which means it expects a class that\n    is a subclass of `BasicCompose`\n    :type cls: Type[BasicCompose]\n    :return: a string, which is the shortened version of the full class name.\n    \"\"\"\n    class_fullname = f\"{cls.__module__}.{cls.__name__}\"\n    return shorten_class_name(class_fullname)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.core.serialization.load","title":"<code>def load    (filepath_or_buffer, data_format='json', nonserializable=None)    </code> [view source on GitHub]","text":"<p>Load a serialized pipeline from a file or file-like object and construct a transform pipeline.</p> <p>Parameters:</p> Name Type Description <code>filepath_or_buffer</code> <code>Union[str, Path, TextIO]</code> <p>The file path or file-like object to read the serialized data from. If a string is provided, it is interpreted as a path to a file. If a file-like object is provided, the serialized data will be read from it directly.</p> <code>data_format</code> <code>str</code> <p>The format of the serialized data. Valid options are 'json' and 'yaml'. Defaults to 'json'.</p> <code>nonserializable</code> <code>Optional[dict[str, Any]]</code> <p>A dictionary that contains non-serializable transforms. This dictionary is required when restoring a pipeline that contains non-serializable transforms. Keys in the dictionary should be named the same as the <code>name</code> arguments in respective transforms from the serialized pipeline. Defaults to None.</p> <p>Returns:</p> Type Description <code>object</code> <p>The deserialized transform pipeline.</p> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If <code>data_format</code> is 'yaml' but PyYAML is not installed.</p> Source code in <code>albumentations/core/serialization.py</code> Python<pre><code>def load(\n    filepath_or_buffer: str | Path | TextIO,\n    data_format: str = \"json\",\n    nonserializable: dict[str, Any] | None = None,\n) -&gt; object:\n    \"\"\"Load a serialized pipeline from a file or file-like object and construct a transform pipeline.\n\n    Args:\n        filepath_or_buffer (Union[str, Path, TextIO]): The file path or file-like object to read the serialized\n            data from.\n            If a string is provided, it is interpreted as a path to a file. If a file-like object is provided,\n            the serialized data will be read from it directly.\n        data_format (str): The format of the serialized data. Valid options are 'json' and 'yaml'.\n            Defaults to 'json'.\n        nonserializable (Optional[dict[str, Any]]): A dictionary that contains non-serializable transforms.\n            This dictionary is required when restoring a pipeline that contains non-serializable transforms.\n            Keys in the dictionary should be named the same as the `name` arguments in respective transforms\n            from the serialized pipeline. Defaults to None.\n\n    Returns:\n        object: The deserialized transform pipeline.\n\n    Raises:\n        ValueError: If `data_format` is 'yaml' but PyYAML is not installed.\n\n    \"\"\"\n    check_data_format(data_format)\n\n    if isinstance(filepath_or_buffer, (str, Path)):  # Assume it's a filepath\n        with open(filepath_or_buffer) as f:\n            if data_format == \"json\":\n                transform_dict = json.load(f)\n            else:\n                if not yaml_available:\n                    msg = \"You need to install PyYAML to load a pipeline in yaml format\"\n                    raise ValueError(msg)\n                transform_dict = yaml.safe_load(f)\n    elif data_format == \"json\":\n        transform_dict = json.load(filepath_or_buffer)\n    else:\n        if not yaml_available:\n            msg = \"You need to install PyYAML to load a pipeline in yaml format\"\n            raise ValueError(msg)\n        transform_dict = yaml.safe_load(filepath_or_buffer)\n\n    return from_dict(transform_dict, nonserializable=nonserializable)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.core.serialization.register_additional_transforms","title":"<code>def register_additional_transforms    ()    </code> [view source on GitHub]","text":"<p>Register transforms that are not imported directly into the <code>albumentations</code> module by checking the availability of optional dependencies.</p> Source code in <code>albumentations/core/serialization.py</code> Python<pre><code>def register_additional_transforms() -&gt; None:\n    \"\"\"Register transforms that are not imported directly into the `albumentations` module by checking\n    the availability of optional dependencies.\n    \"\"\"\n    if importlib.util.find_spec(\"torch\") is not None:\n        try:\n            # Import `albumentations.pytorch` only if `torch` is installed.\n            import albumentations.pytorch\n\n            # Use a dummy operation to acknowledge the use of the imported module and avoid linting errors.\n            _ = albumentations.pytorch.ToTensorV2\n        except ImportError:\n            pass\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.core.serialization.save","title":"<code>def save    (transform, filepath_or_buffer, data_format='json', on_not_implemented_error='raise')    </code> [view source on GitHub]","text":"<p>Serialize a transform pipeline and save it to either a file specified by a path or a file-like object in either JSON or YAML format.</p> <p>Parameters:</p> Name Type Description <code>transform</code> <code>Serializable</code> <p>The transform pipeline to serialize.</p> <code>filepath_or_buffer</code> <code>Union[str, Path, TextIO]</code> <p>The file path or file-like object to write the serialized data to. If a string is provided, it is interpreted as a path to a file. If a file-like object is provided, the serialized data will be written to it directly.</p> <code>data_format</code> <code>str</code> <p>The format to serialize the data in. Valid options are 'json' and 'yaml'. Defaults to 'json'.</p> <code>on_not_implemented_error</code> <code>str</code> <p>Determines the behavior if a transform does not implement the <code>to_dict</code> method. If set to 'raise', a <code>NotImplementedError</code> is raised. If set to 'warn', the exception is ignored, and no transform arguments are saved. Defaults to 'raise'.</p> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If <code>data_format</code> is 'yaml' but PyYAML is not installed.</p> Source code in <code>albumentations/core/serialization.py</code> Python<pre><code>def save(\n    transform: Serializable,\n    filepath_or_buffer: str | Path | TextIO,\n    data_format: str = \"json\",\n    on_not_implemented_error: str = \"raise\",\n) -&gt; None:\n    \"\"\"Serialize a transform pipeline and save it to either a file specified by a path or a file-like object\n    in either JSON or YAML format.\n\n    Args:\n        transform (Serializable): The transform pipeline to serialize.\n        filepath_or_buffer (Union[str, Path, TextIO]): The file path or file-like object to write the serialized\n            data to.\n            If a string is provided, it is interpreted as a path to a file. If a file-like object is provided,\n            the serialized data will be written to it directly.\n        data_format (str): The format to serialize the data in. Valid options are 'json' and 'yaml'.\n            Defaults to 'json'.\n        on_not_implemented_error (str): Determines the behavior if a transform does not implement the `to_dict` method.\n            If set to 'raise', a `NotImplementedError` is raised. If set to 'warn', the exception is ignored, and\n            no transform arguments are saved. Defaults to 'raise'.\n\n    Raises:\n        ValueError: If `data_format` is 'yaml' but PyYAML is not installed.\n\n    \"\"\"\n    check_data_format(data_format)\n    transform_dict = transform.to_dict(on_not_implemented_error=on_not_implemented_error)\n    transform_dict = serialize_enum(transform_dict)\n\n    # Determine whether to write to a file or a file-like object\n    if isinstance(filepath_or_buffer, (str, Path)):  # It's a filepath\n        with open(filepath_or_buffer, \"w\") as f:\n            if data_format == \"yaml\":\n                if not yaml_available:\n                    msg = \"You need to install PyYAML to save a pipeline in YAML format\"\n                    raise ValueError(msg)\n                yaml.safe_dump(transform_dict, f, default_flow_style=False)\n            elif data_format == \"json\":\n                json.dump(transform_dict, f)\n    elif data_format == \"yaml\":\n        if not yaml_available:\n            msg = \"You need to install PyYAML to save a pipeline in YAML format\"\n            raise ValueError(msg)\n        yaml.safe_dump(transform_dict, filepath_or_buffer, default_flow_style=False)\n    elif data_format == \"json\":\n        json.dump(transform_dict, filepath_or_buffer, indent=2)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.core.serialization.serialize_enum","title":"<code>def serialize_enum    (obj)    </code> [view source on GitHub]","text":"<p>Recursively search for Enum objects and convert them to their value. Also handle any Mapping or Sequence types.</p> Source code in <code>albumentations/core/serialization.py</code> Python<pre><code>def serialize_enum(obj: Any) -&gt; Any:\n    \"\"\"Recursively search for Enum objects and convert them to their value.\n    Also handle any Mapping or Sequence types.\n    \"\"\"\n    if isinstance(obj, Mapping):\n        return {k: serialize_enum(v) for k, v in obj.items()}\n    if isinstance(obj, Sequence) and not isinstance(obj, str):  # exclude strings since they're also sequences\n        return [serialize_enum(v) for v in obj]\n    return obj.value if isinstance(obj, Enum) else obj\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.core.serialization.to_dict","title":"<code>def to_dict    (transform, on_not_implemented_error='raise')    </code> [view source on GitHub]","text":"<p>Take a transform pipeline and convert it to a serializable representation that uses only standard python data types: dictionaries, lists, strings, integers, and floats.</p> <p>Parameters:</p> Name Type Description <code>transform</code> <code>Serializable</code> <p>A transform that should be serialized. If the transform doesn't implement the <code>to_dict</code> method and <code>on_not_implemented_error</code> equals to 'raise' then <code>NotImplementedError</code> is raised. If <code>on_not_implemented_error</code> equals to 'warn' then <code>NotImplementedError</code> will be ignored but no transform parameters will be serialized.</p> <code>on_not_implemented_error</code> <code>str</code> <p><code>raise</code> or <code>warn</code>.</p> Source code in <code>albumentations/core/serialization.py</code> Python<pre><code>def to_dict(transform: Serializable, on_not_implemented_error: str = \"raise\") -&gt; dict[str, Any]:\n    \"\"\"Take a transform pipeline and convert it to a serializable representation that uses only standard\n    python data types: dictionaries, lists, strings, integers, and floats.\n\n    Args:\n        transform: A transform that should be serialized. If the transform doesn't implement the `to_dict`\n            method and `on_not_implemented_error` equals to 'raise' then `NotImplementedError` is raised.\n            If `on_not_implemented_error` equals to 'warn' then `NotImplementedError` will be ignored\n            but no transform parameters will be serialized.\n        on_not_implemented_error (str): `raise` or `warn`.\n\n    \"\"\"\n    return transform.to_dict(on_not_implemented_error)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.core.transforms_interface","title":"<code>transforms_interface</code>","text":""},{"location":"api_reference/full_reference/#albumentations.core.transforms_interface.BasicTransform","title":"<code>class  BasicTransform</code> <code>     (p=0.5, always_apply=None)                 </code>  [view source on GitHub]","text":"<p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/core/transforms_interface.py</code> Python<pre><code>class BasicTransform(Serializable, metaclass=CombinedMeta):\n    _targets: tuple[Targets, ...] | Targets  # targets that this transform can work on\n    _available_keys: set[str]  # targets that this transform, as string, lower-cased\n    _key2func: dict[\n        str,\n        Callable[..., Any],\n    ]  # mapping for targets (plus additional targets) and methods for which they depend\n    call_backup = None\n    interpolation: int\n    fill_value: ColorType\n    mask_fill_value: ColorType | None\n    # replay mode params\n    deterministic: bool = False\n    save_key = \"replay\"\n    replay_mode = False\n    applied_in_replay = False\n\n    class InitSchema(BaseTransformInitSchema):\n        pass\n\n    def __init__(self, p: float = 0.5, always_apply: bool | None = None):\n        self.p = p\n        if always_apply is not None:\n            if always_apply:\n                warn(\n                    \"always_apply is deprecated. Use `p=1` if you want to always apply the transform.\"\n                    \" self.p will be set to 1.\",\n                    DeprecationWarning,\n                    stacklevel=2,\n                )\n                self.p = 1.0\n            else:\n                warn(\n                    \"always_apply is deprecated.\",\n                    DeprecationWarning,\n                    stacklevel=2,\n                )\n        self._additional_targets: dict[str, str] = {}\n        # replay mode params\n        self.params: dict[Any, Any] = {}\n        self._key2func = {}\n        self._set_keys()\n\n    def __call__(self, *args: Any, force_apply: bool = False, **kwargs: Any) -&gt; Any:\n        if args:\n            msg = \"You have to pass data to augmentations as named arguments, for example: aug(image=image)\"\n            raise KeyError(msg)\n        if self.replay_mode:\n            if self.applied_in_replay:\n                return self.apply_with_params(self.params, **kwargs)\n\n            return kwargs\n\n        if force_apply or (random.random() &lt; self.p):\n            params = self.get_params()\n            params = self.update_params_shape(params=params, data=kwargs)\n\n            if self.targets_as_params:  # check if all required targets are in kwargs.\n                missing_keys = set(self.targets_as_params).difference(kwargs.keys())\n                if missing_keys and not (missing_keys == {\"image\"} and \"images\" in kwargs):\n                    msg = f\"{self.__class__.__name__} requires {self.targets_as_params} missing keys: {missing_keys}\"\n                    raise ValueError(msg)\n\n            params_dependent_on_data = self.get_params_dependent_on_data(params=params, data=kwargs)\n            params.update(params_dependent_on_data)\n\n            if self.targets_as_params:  # this block will be removed after removing `get_params_dependent_on_targets`\n                targets_as_params = {k: kwargs.get(k, None) for k in self.targets_as_params}\n                if missing_keys:  # here we expecting case when missing_keys == {\"image\"} and \"images\" in kwargs\n                    targets_as_params[\"image\"] = kwargs[\"images\"][0]\n                params_dependent_on_targets = self.get_params_dependent_on_targets(targets_as_params)\n                params.update(params_dependent_on_targets)\n            if self.deterministic:\n                kwargs[self.save_key][id(self)] = deepcopy(params)\n            return self.apply_with_params(params, **kwargs)\n\n        return kwargs\n\n    def apply_with_params(self, params: dict[str, Any], *args: Any, **kwargs: Any) -&gt; dict[str, Any]:\n        \"\"\"Apply transforms with parameters.\"\"\"\n        params = self.update_params(params, **kwargs)  # remove after move parameters like interpolation\n        res = {}\n        for key, arg in kwargs.items():\n            if key in self._key2func and arg is not None:\n                target_function = self._key2func[key]\n                if isinstance(arg, np.ndarray):\n                    result = target_function(np.require(arg, requirements=[\"C_CONTIGUOUS\"]), **params)\n                    if isinstance(result, np.ndarray):\n                        res[key] = np.require(result, requirements=[\"C_CONTIGUOUS\"])\n                    else:\n                        res[key] = result\n                else:\n                    res[key] = target_function(arg, **params)\n            else:\n                res[key] = arg\n        return res\n\n    def set_deterministic(self, flag: bool, save_key: str = \"replay\") -&gt; BasicTransform:\n        \"\"\"Set transform to be deterministic.\"\"\"\n        if save_key == \"params\":\n            msg = \"params save_key is reserved\"\n            raise KeyError(msg)\n\n        self.deterministic = flag\n        if self.deterministic and self.targets_as_params:\n            warn(\n                self.get_class_fullname() + \" could work incorrectly in ReplayMode for other input data\"\n                \" because its' params depend on targets.\",\n                stacklevel=2,\n            )\n        self.save_key = save_key\n        return self\n\n    def __repr__(self) -&gt; str:\n        state = self.get_base_init_args()\n        state.update(self.get_transform_init_args())\n        return f\"{self.__class__.__name__}({format_args(state)})\"\n\n    def apply(self, img: np.ndarray, *args: Any, **params: Any) -&gt; np.ndarray:\n        \"\"\"Apply transform on image.\"\"\"\n        raise NotImplementedError\n\n    def apply_to_images(self, images: np.ndarray, **params: Any) -&gt; list[np.ndarray]:\n        \"\"\"Apply transform on images.\"\"\"\n        return [self.apply(image, **params) for image in images]\n\n    def get_params(self) -&gt; dict[str, Any]:\n        \"\"\"Returns parameters independent of input.\"\"\"\n        return {}\n\n    def update_params_shape(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n        \"\"\"Updates parameters with input image shape.\"\"\"\n        # here we expects `image` or `images` in kwargs. it's checked at Compose._check_args\n        shape = data[\"image\"].shape if \"image\" in data else data[\"images\"][0].shape\n        params[\"shape\"] = shape\n        params.update({\"cols\": shape[1], \"rows\": shape[0]})\n        return params\n\n    def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n        \"\"\"Returns parameters dependent on input.\"\"\"\n        return params\n\n    @property\n    def targets(self) -&gt; dict[str, Callable[..., Any]]:\n        # mapping for targets and methods for which they depend\n        # for example:\n        # &gt;&gt;  {\"image\": self.apply}\n        # &gt;&gt;  {\"masks\": self.apply_to_masks}\n        raise NotImplementedError\n\n    def _set_keys(self) -&gt; None:\n        \"\"\"Set _available_keys.\"\"\"\n        if not hasattr(self, \"_targets\"):\n            self._available_keys = set()\n        else:\n            self._available_keys = {\n                target.value.lower()\n                for target in (self._targets if isinstance(self._targets, tuple) else [self._targets])\n            }\n        self._available_keys.update(self.targets.keys())\n        self._key2func = {key: self.targets[key] for key in self._available_keys if key in self.targets}\n\n    @property\n    def available_keys(self) -&gt; set[str]:\n        \"\"\"Returns set of available keys.\"\"\"\n        return self._available_keys\n\n    def update_params(self, params: dict[str, Any], **kwargs: Any) -&gt; dict[str, Any]:\n        \"\"\"Update parameters with transform specific params.\n        This method is deprecated, use:\n        - `get_params` for transform specific params like interpolation and\n        - `update_params_shape` for data like shape.\n        \"\"\"\n        if hasattr(self, \"interpolation\"):\n            params[\"interpolation\"] = self.interpolation\n        if hasattr(self, \"fill_value\"):\n            params[\"fill_value\"] = self.fill_value\n        if hasattr(self, \"mask_fill_value\"):\n            params[\"mask_fill_value\"] = self.mask_fill_value\n\n        # here we expects `image` or `images` in kwargs. it's checked at Compose._check_args\n        shape = kwargs[\"image\"].shape if \"image\" in kwargs else kwargs[\"images\"][0].shape\n        params[\"shape\"] = shape\n        params.update({\"cols\": shape[1], \"rows\": shape[0]})\n        return params\n\n    def add_targets(self, additional_targets: dict[str, str]) -&gt; None:\n        \"\"\"Add targets to transform them the same way as one of existing targets.\n        ex: {'target_image': 'image'}\n        ex: {'obj1_mask': 'mask', 'obj2_mask': 'mask'}\n        by the way you must have at least one object with key 'image'\n\n        Args:\n            additional_targets (dict): keys - new target name, values - old target name. ex: {'image2': 'image'}\n\n        \"\"\"\n        for k, v in additional_targets.items():\n            if k in self._additional_targets and v != self._additional_targets[k]:\n                raise ValueError(\n                    f\"Trying to overwrite existed additional targets. \"\n                    f\"Key={k} Exists={self._additional_targets[k]} New value: {v}\",\n                )\n            if v in self._available_keys:\n                self._additional_targets[k] = v\n                self._key2func[k] = self.targets[v]\n                self._available_keys.add(k)\n\n    @property\n    def targets_as_params(self) -&gt; list[str]:\n        \"\"\"Targets used to get params dependent on targets.\n        This is used to check input has all required targets.\n        \"\"\"\n        return []\n\n    def get_params_dependent_on_targets(self, params: dict[str, Any]) -&gt; dict[str, Any]:\n        \"\"\"This method is deprecated.\n        Use `get_params_dependent_on_data` instead.\n        Returns parameters dependent on targets.\n        Dependent target is defined in `self.targets_as_params`\n        \"\"\"\n        return {}\n\n    @classmethod\n    def get_class_fullname(cls) -&gt; str:\n        return get_shortest_class_fullname(cls)\n\n    @classmethod\n    def is_serializable(cls) -&gt; bool:\n        return True\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        \"\"\"Returns names of arguments that are used in __init__ method of the transform.\"\"\"\n        msg = (\n            f\"Class {self.get_class_fullname()} is not serializable because the `get_transform_init_args_names` \"\n            \"method is not implemented\"\n        )\n        raise NotImplementedError(msg)\n\n    def get_base_init_args(self) -&gt; dict[str, Any]:\n        \"\"\"Returns base init args - p\"\"\"\n        return {\"p\": self.p}\n\n    def get_transform_init_args(self) -&gt; dict[str, Any]:\n        return {k: getattr(self, k) for k in self.get_transform_init_args_names()}\n\n    def to_dict_private(self) -&gt; dict[str, Any]:\n        state = {\"__class_fullname__\": self.get_class_fullname()}\n        state.update(self.get_base_init_args())\n        state.update(self.get_transform_init_args())\n\n        return state\n\n    def get_dict_with_id(self) -&gt; dict[str, Any]:\n        d = self.to_dict_private()\n        d[\"id\"] = id(self)\n        return d\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.core.transforms_interface.BasicTransform.available_keys","title":"<code>available_keys: set[str]</code>  <code>property</code> <code>readonly</code>","text":"<p>Returns set of available keys.</p>"},{"location":"api_reference/full_reference/#albumentations.core.transforms_interface.BasicTransform.targets_as_params","title":"<code>targets_as_params: list[str]</code>  <code>property</code> <code>readonly</code>","text":"<p>Targets used to get params dependent on targets. This is used to check input has all required targets.</p>"},{"location":"api_reference/full_reference/#albumentations.core.transforms_interface.BasicTransform.add_targets","title":"<code>add_targets (self, additional_targets)</code>","text":"<p>Add targets to transform them the same way as one of existing targets. ex: {'target_image': 'image'} ex: {'obj1_mask': 'mask', 'obj2_mask': 'mask'} by the way you must have at least one object with key 'image'</p> <p>Parameters:</p> Name Type Description <code>additional_targets</code> <code>dict</code> <p>keys - new target name, values - old target name. ex: {'image2': 'image'}</p> Source code in <code>albumentations/core/transforms_interface.py</code> Python<pre><code>def add_targets(self, additional_targets: dict[str, str]) -&gt; None:\n    \"\"\"Add targets to transform them the same way as one of existing targets.\n    ex: {'target_image': 'image'}\n    ex: {'obj1_mask': 'mask', 'obj2_mask': 'mask'}\n    by the way you must have at least one object with key 'image'\n\n    Args:\n        additional_targets (dict): keys - new target name, values - old target name. ex: {'image2': 'image'}\n\n    \"\"\"\n    for k, v in additional_targets.items():\n        if k in self._additional_targets and v != self._additional_targets[k]:\n            raise ValueError(\n                f\"Trying to overwrite existed additional targets. \"\n                f\"Key={k} Exists={self._additional_targets[k]} New value: {v}\",\n            )\n        if v in self._available_keys:\n            self._additional_targets[k] = v\n            self._key2func[k] = self.targets[v]\n            self._available_keys.add(k)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.core.transforms_interface.BasicTransform.apply","title":"<code>apply (self, img, *args, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/core/transforms_interface.py</code> Python<pre><code>def apply(self, img: np.ndarray, *args: Any, **params: Any) -&gt; np.ndarray:\n    \"\"\"Apply transform on image.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.core.transforms_interface.BasicTransform.apply_to_images","title":"<code>apply_to_images (self, images, **params)</code>","text":"<p>Apply transform on images.</p> Source code in <code>albumentations/core/transforms_interface.py</code> Python<pre><code>def apply_to_images(self, images: np.ndarray, **params: Any) -&gt; list[np.ndarray]:\n    \"\"\"Apply transform on images.\"\"\"\n    return [self.apply(image, **params) for image in images]\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.core.transforms_interface.BasicTransform.apply_with_params","title":"<code>apply_with_params (self, params, *args, **kwargs)</code>","text":"<p>Apply transforms with parameters.</p> Source code in <code>albumentations/core/transforms_interface.py</code> Python<pre><code>def apply_with_params(self, params: dict[str, Any], *args: Any, **kwargs: Any) -&gt; dict[str, Any]:\n    \"\"\"Apply transforms with parameters.\"\"\"\n    params = self.update_params(params, **kwargs)  # remove after move parameters like interpolation\n    res = {}\n    for key, arg in kwargs.items():\n        if key in self._key2func and arg is not None:\n            target_function = self._key2func[key]\n            if isinstance(arg, np.ndarray):\n                result = target_function(np.require(arg, requirements=[\"C_CONTIGUOUS\"]), **params)\n                if isinstance(result, np.ndarray):\n                    res[key] = np.require(result, requirements=[\"C_CONTIGUOUS\"])\n                else:\n                    res[key] = result\n            else:\n                res[key] = target_function(arg, **params)\n        else:\n            res[key] = arg\n    return res\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.core.transforms_interface.BasicTransform.get_base_init_args","title":"<code>get_base_init_args (self)</code>","text":"<p>Returns base init args - p</p> Source code in <code>albumentations/core/transforms_interface.py</code> Python<pre><code>def get_base_init_args(self) -&gt; dict[str, Any]:\n    \"\"\"Returns base init args - p\"\"\"\n    return {\"p\": self.p}\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.core.transforms_interface.BasicTransform.get_params","title":"<code>get_params (self)</code>","text":"<p>Returns parameters independent of input.</p> Source code in <code>albumentations/core/transforms_interface.py</code> Python<pre><code>def get_params(self) -&gt; dict[str, Any]:\n    \"\"\"Returns parameters independent of input.\"\"\"\n    return {}\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.core.transforms_interface.BasicTransform.get_params_dependent_on_data","title":"<code>get_params_dependent_on_data (self, params, data)</code>","text":"<p>Returns parameters dependent on input.</p> Source code in <code>albumentations/core/transforms_interface.py</code> Python<pre><code>def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n    \"\"\"Returns parameters dependent on input.\"\"\"\n    return params\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.core.transforms_interface.BasicTransform.get_params_dependent_on_targets","title":"<code>get_params_dependent_on_targets (self, params)</code>","text":"<p>This method is deprecated. Use <code>get_params_dependent_on_data</code> instead. Returns parameters dependent on targets. Dependent target is defined in <code>self.targets_as_params</code></p> Source code in <code>albumentations/core/transforms_interface.py</code> Python<pre><code>def get_params_dependent_on_targets(self, params: dict[str, Any]) -&gt; dict[str, Any]:\n    \"\"\"This method is deprecated.\n    Use `get_params_dependent_on_data` instead.\n    Returns parameters dependent on targets.\n    Dependent target is defined in `self.targets_as_params`\n    \"\"\"\n    return {}\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.core.transforms_interface.BasicTransform.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/core/transforms_interface.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    \"\"\"Returns names of arguments that are used in __init__ method of the transform.\"\"\"\n    msg = (\n        f\"Class {self.get_class_fullname()} is not serializable because the `get_transform_init_args_names` \"\n        \"method is not implemented\"\n    )\n    raise NotImplementedError(msg)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.core.transforms_interface.BasicTransform.set_deterministic","title":"<code>set_deterministic (self, flag, save_key='replay')</code>","text":"<p>Set transform to be deterministic.</p> Source code in <code>albumentations/core/transforms_interface.py</code> Python<pre><code>def set_deterministic(self, flag: bool, save_key: str = \"replay\") -&gt; BasicTransform:\n    \"\"\"Set transform to be deterministic.\"\"\"\n    if save_key == \"params\":\n        msg = \"params save_key is reserved\"\n        raise KeyError(msg)\n\n    self.deterministic = flag\n    if self.deterministic and self.targets_as_params:\n        warn(\n            self.get_class_fullname() + \" could work incorrectly in ReplayMode for other input data\"\n            \" because its' params depend on targets.\",\n            stacklevel=2,\n        )\n    self.save_key = save_key\n    return self\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.core.transforms_interface.BasicTransform.update_params","title":"<code>update_params (self, params, **kwargs)</code>","text":"<p>Update parameters with transform specific params. This method is deprecated, use: - <code>get_params</code> for transform specific params like interpolation and - <code>update_params_shape</code> for data like shape.</p> Source code in <code>albumentations/core/transforms_interface.py</code> Python<pre><code>def update_params(self, params: dict[str, Any], **kwargs: Any) -&gt; dict[str, Any]:\n    \"\"\"Update parameters with transform specific params.\n    This method is deprecated, use:\n    - `get_params` for transform specific params like interpolation and\n    - `update_params_shape` for data like shape.\n    \"\"\"\n    if hasattr(self, \"interpolation\"):\n        params[\"interpolation\"] = self.interpolation\n    if hasattr(self, \"fill_value\"):\n        params[\"fill_value\"] = self.fill_value\n    if hasattr(self, \"mask_fill_value\"):\n        params[\"mask_fill_value\"] = self.mask_fill_value\n\n    # here we expects `image` or `images` in kwargs. it's checked at Compose._check_args\n    shape = kwargs[\"image\"].shape if \"image\" in kwargs else kwargs[\"images\"][0].shape\n    params[\"shape\"] = shape\n    params.update({\"cols\": shape[1], \"rows\": shape[0]})\n    return params\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.core.transforms_interface.BasicTransform.update_params_shape","title":"<code>update_params_shape (self, params, data)</code>","text":"<p>Updates parameters with input image shape.</p> Source code in <code>albumentations/core/transforms_interface.py</code> Python<pre><code>def update_params_shape(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n    \"\"\"Updates parameters with input image shape.\"\"\"\n    # here we expects `image` or `images` in kwargs. it's checked at Compose._check_args\n    shape = data[\"image\"].shape if \"image\" in data else data[\"images\"][0].shape\n    params[\"shape\"] = shape\n    params.update({\"cols\": shape[1], \"rows\": shape[0]})\n    return params\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.core.transforms_interface.DualTransform","title":"<code>class  DualTransform</code> <code> </code>  [view source on GitHub]","text":"<p>A base class for transformations that should be applied both to an image and its corresponding properties such as masks, bounding boxes, and keypoints. This class ensures that when a transform is applied to an image, all associated entities are transformed accordingly to maintain consistency between the image and its annotations.</p> <p>Properties</p> <p>targets (dict[str, Callable[..., Any]]): Defines the types of targets (e.g., image, mask, bboxes, keypoints)     that the transform should be applied to and maps them to the corresponding methods.</p> <p>Methods</p> <p>apply_to_bbox(bbox: BoxInternalType, args: Any, *params: Any) -&gt; BoxInternalType:     Applies the transform to a single bounding box. Should be implemented in the subclass.</p> <p>apply_to_keypoint(keypoint: KeypointInternalType, args: Any, *params: Any) -&gt; KeypointInternalType:     Applies the transform to a single keypoint. Should be implemented in the subclass.</p> <p>apply_to_bboxes(bboxes: Sequence[BoxType], args: Any, *params: Any) -&gt; Sequence[BoxType]:     Applies the transform to a list of bounding boxes. Delegates to <code>apply_to_bbox</code> for each bounding box.</p> <p>apply_to_keypoints(keypoints: Sequence[KeypointType], args: Any, *params: Any) -&gt; Sequence[KeypointType]:     Applies the transform to a list of keypoints. Delegates to <code>apply_to_keypoint</code> for each keypoint.</p> <p>apply_to_mask(mask: np.ndarray, args: Any, *params: Any) -&gt; np.ndarray:     Applies the transform specifically to a single mask.</p> <p>apply_to_masks(masks: Sequence[np.ndarray], **params: Any) -&gt; list[np.ndarray]:     Applies the transform to a list of masks. Delegates to <code>apply_to_mask</code> for each mask.</p> <p>Note</p> <p>This class is intended to be subclassed and should not be used directly. Subclasses are expected to implement the specific logic for each type of target (e.g., image, mask, bboxes, keypoints) in the corresponding <code>apply_to_*</code> methods.</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/core/transforms_interface.py</code> Python<pre><code>class DualTransform(BasicTransform):\n    \"\"\"A base class for transformations that should be applied both to an image and its corresponding properties\n    such as masks, bounding boxes, and keypoints. This class ensures that when a transform is applied to an image,\n    all associated entities are transformed accordingly to maintain consistency between the image and its annotations.\n\n    Properties:\n        targets (dict[str, Callable[..., Any]]): Defines the types of targets (e.g., image, mask, bboxes, keypoints)\n            that the transform should be applied to and maps them to the corresponding methods.\n\n    Methods:\n        apply_to_bbox(bbox: BoxInternalType, *args: Any, **params: Any) -&gt; BoxInternalType:\n            Applies the transform to a single bounding box. Should be implemented in the subclass.\n\n        apply_to_keypoint(keypoint: KeypointInternalType, *args: Any, **params: Any) -&gt; KeypointInternalType:\n            Applies the transform to a single keypoint. Should be implemented in the subclass.\n\n        apply_to_bboxes(bboxes: Sequence[BoxType], *args: Any, **params: Any) -&gt; Sequence[BoxType]:\n            Applies the transform to a list of bounding boxes. Delegates to `apply_to_bbox` for each bounding box.\n\n        apply_to_keypoints(keypoints: Sequence[KeypointType], *args: Any, **params: Any) -&gt; Sequence[KeypointType]:\n            Applies the transform to a list of keypoints. Delegates to `apply_to_keypoint` for each keypoint.\n\n        apply_to_mask(mask: np.ndarray, *args: Any, **params: Any) -&gt; np.ndarray:\n            Applies the transform specifically to a single mask.\n\n        apply_to_masks(masks: Sequence[np.ndarray], **params: Any) -&gt; list[np.ndarray]:\n            Applies the transform to a list of masks. Delegates to `apply_to_mask` for each mask.\n\n    Note:\n        This class is intended to be subclassed and should not be used directly. Subclasses are expected to\n        implement the specific logic for each type of target (e.g., image, mask, bboxes, keypoints) in the\n        corresponding `apply_to_*` methods.\n\n    \"\"\"\n\n    @property\n    def targets(self) -&gt; dict[str, Callable[..., Any]]:\n        return {\n            \"image\": self.apply,\n            \"images\": self.apply_to_images,\n            \"mask\": self.apply_to_mask,\n            \"masks\": self.apply_to_masks,\n            \"bboxes\": self.apply_to_bboxes,\n            \"keypoints\": self.apply_to_keypoints,\n        }\n\n    def apply_to_bbox(self, bbox: BoxInternalType, *args: Any, **params: Any) -&gt; BoxInternalType:\n        msg = f\"Method apply_to_bbox is not implemented in class {self.__class__.__name__}\"\n        raise NotImplementedError(msg)\n\n    def apply_to_keypoint(self, keypoint: KeypointInternalType, *args: Any, **params: Any) -&gt; KeypointInternalType:\n        msg = f\"Method apply_to_keypoint is not implemented in class {self.__class__.__name__}\"\n        raise NotImplementedError(msg)\n\n    def apply_to_global_label(self, label: np.ndarray, *args: Any, **params: Any) -&gt; np.ndarray:\n        msg = f\"Method apply_to_global_label is not implemented in class {self.__class__.__name__}\"\n        raise NotImplementedError(msg)\n\n    def apply_to_bboxes(self, bboxes: Sequence[BoxType], *args: Any, **params: Any) -&gt; Sequence[BoxType]:\n        return [\n            self.apply_to_bbox(cast(BoxInternalType, tuple(cast(BoxInternalType, bbox[:4]))), **params)\n            + tuple(bbox[4:])\n            for bbox in bboxes\n        ]\n\n    def apply_to_keypoints(\n        self,\n        keypoints: Sequence[KeypointType],\n        *args: Any,\n        **params: Any,\n    ) -&gt; Sequence[KeypointType]:\n        return [\n            self.apply_to_keypoint(cast(KeypointInternalType, tuple(keypoint[:4])), **params) + tuple(keypoint[4:])\n            for keypoint in keypoints\n        ]\n\n    def apply_to_mask(self, mask: np.ndarray, *args: Any, **params: Any) -&gt; np.ndarray:\n        return self.apply(mask, **{k: cv2.INTER_NEAREST if k == \"interpolation\" else v for k, v in params.items()})\n\n    def apply_to_masks(self, masks: Sequence[np.ndarray], **params: Any) -&gt; list[np.ndarray]:\n        return [self.apply_to_mask(mask, **params) for mask in masks]\n\n    def apply_to_global_labels(self, labels: Sequence[np.ndarray], **params: Any) -&gt; list[np.ndarray]:\n        return [self.apply_to_global_label(label, **params) for label in labels]\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.core.transforms_interface.ImageOnlyTransform","title":"<code>class  ImageOnlyTransform</code> <code> </code>  [view source on GitHub]","text":"<p>Transform applied to image only.</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/core/transforms_interface.py</code> Python<pre><code>class ImageOnlyTransform(BasicTransform):\n    \"\"\"Transform applied to image only.\"\"\"\n\n    _targets = Targets.IMAGE\n\n    @property\n    def targets(self) -&gt; dict[str, Callable[..., Any]]:\n        return {\"image\": self.apply, \"images\": self.apply_to_images}\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.core.transforms_interface.NoOp","title":"<code>class  NoOp</code> <code> </code>  [view source on GitHub]","text":"<p>Identity transform (does nothing).</p> <p>Targets</p> <p>image, mask, bboxes, keypoints, global_label</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/core/transforms_interface.py</code> Python<pre><code>class NoOp(DualTransform):\n    \"\"\"Identity transform (does nothing).\n\n    Targets:\n        image, mask, bboxes, keypoints, global_label\n    \"\"\"\n\n    _targets = (Targets.IMAGE, Targets.MASK, Targets.BBOXES, Targets.KEYPOINTS, Targets.GLOBAL_LABEL)\n\n    def apply_to_keypoint(self, keypoint: KeypointInternalType, **params: Any) -&gt; KeypointInternalType:\n        return keypoint\n\n    def apply_to_bbox(self, bbox: BoxInternalType, **params: Any) -&gt; BoxInternalType:\n        return bbox\n\n    def apply(self, img: np.ndarray, **params: Any) -&gt; np.ndarray:\n        return img\n\n    def apply_to_mask(self, mask: np.ndarray, **params: Any) -&gt; np.ndarray:\n        return mask\n\n    def apply_to_global_label(self, label: np.ndarray, **params: Any) -&gt; np.ndarray:\n        return label\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return ()\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.core.transforms_interface.NoOp.apply","title":"<code>apply (self, img, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/core/transforms_interface.py</code> Python<pre><code>def apply(self, img: np.ndarray, **params: Any) -&gt; np.ndarray:\n    return img\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.core.transforms_interface.NoOp.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/core/transforms_interface.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return ()\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.core.types","title":"<code>types</code>","text":""},{"location":"api_reference/full_reference/#albumentations.core.types.ImageCompressionType","title":"<code>class  ImageCompressionType</code> <code> </code>  [view source on GitHub]","text":"<p>Defines the types of image compression.</p> <p>This Enum class is used to specify the image compression format.</p> <p>Attributes:</p> Name Type Description <code>JPEG</code> <code>int</code> <p>Represents the JPEG image compression format.</p> <code>WEBP</code> <code>int</code> <p>Represents the WEBP image compression format.</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/core/types.py</code> Python<pre><code>class ImageCompressionType(IntEnum):\n    \"\"\"Defines the types of image compression.\n\n    This Enum class is used to specify the image compression format.\n\n    Attributes:\n        JPEG (int): Represents the JPEG image compression format.\n        WEBP (int): Represents the WEBP image compression format.\n\n    \"\"\"\n\n    JPEG = 0\n    WEBP = 1\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.core.utils","title":"<code>utils</code>","text":""},{"location":"api_reference/full_reference/#albumentations.core.utils.DataProcessor","title":"<code>class  DataProcessor</code> <code>     (params, additional_targets=None)                 </code>  [view source on GitHub]","text":"<p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/core/utils.py</code> Python<pre><code>class DataProcessor(ABC):\n    def __init__(self, params: Params, additional_targets: dict[str, str] | None = None):\n        self.params = params\n        self.data_fields = [self.default_data_name]\n        if additional_targets is not None:\n            self.add_targets(additional_targets)\n\n    @property\n    @abstractmethod\n    def default_data_name(self) -&gt; str:\n        raise NotImplementedError\n\n    def add_targets(self, additional_targets: dict[str, str]) -&gt; None:\n        \"\"\"Add targets to transform them the same way as one of existing targets.\"\"\"\n        for k, v in additional_targets.items():\n            if v == self.default_data_name and k not in self.data_fields:\n                self.data_fields.append(k)\n\n    def ensure_data_valid(self, data: dict[str, Any]) -&gt; None:\n        pass\n\n    def ensure_transforms_valid(self, transforms: Sequence[object]) -&gt; None:\n        pass\n\n    def postprocess(self, data: dict[str, Any]) -&gt; dict[str, Any]:\n        image_shape = get_shape(data[\"image\"])\n\n        for data_name in self.data_fields:\n            if data_name in data:\n                data[data_name] = self.filter(data[data_name], image_shape)\n                data[data_name] = self.check_and_convert(data[data_name], image_shape, direction=\"from\")\n\n        return self.remove_label_fields_from_data(data)\n\n    def preprocess(self, data: dict[str, Any]) -&gt; None:\n        data = self.add_label_fields_to_data(data)\n\n        image_shape = get_shape(data[\"image\"])\n\n        for data_name in self.data_fields:\n            if data_name in data:\n                data[data_name] = self.check_and_convert(data[data_name], image_shape, direction=\"to\")\n\n    def check_and_convert(\n        self,\n        data: list[BoxOrKeypointType],\n        image_shape: Sequence[int],\n        direction: Literal[\"to\", \"from\"] = \"to\",\n    ) -&gt; list[BoxOrKeypointType]:\n        if self.params.format == \"albumentations\":\n            self.check(data, image_shape)\n            return data\n\n        if direction == \"to\":\n            return self.convert_to_albumentations(data, image_shape)\n\n        if direction == \"from\":\n            return self.convert_from_albumentations(data, image_shape)\n\n        raise ValueError(f\"Invalid direction. Must be `to` or `from`. Got `{direction}`\")\n\n    @abstractmethod\n    def filter(self, data: Sequence[BoxOrKeypointType], image_shape: Sequence[int]) -&gt; Sequence[BoxOrKeypointType]:\n        pass\n\n    @abstractmethod\n    def check(self, data: list[BoxOrKeypointType], image_shape: Sequence[int]) -&gt; None:\n        pass\n\n    @abstractmethod\n    def convert_to_albumentations(\n        self,\n        data: list[BoxOrKeypointType],\n        image_shape: Sequence[int],\n    ) -&gt; list[BoxOrKeypointType]:\n        pass\n\n    @abstractmethod\n    def convert_from_albumentations(\n        self,\n        data: list[BoxOrKeypointType],\n        image_shape: Sequence[int],\n    ) -&gt; list[BoxOrKeypointType]:\n        pass\n\n    def add_label_fields_to_data(self, data: dict[str, Any]) -&gt; dict[str, Any]:\n        if self.params.label_fields is None:\n            return data\n        for data_name in self.data_fields:\n            if data_name in data:\n                for field in self.params.label_fields:\n                    if not len(data[data_name]) == len(data[field]):\n                        raise ValueError(\n                            f\"The lengths of bboxes and labels do not match. Got {len(data[data_name])} \"\n                            f\"and {len(data[field])} respectively.\",\n                        )\n\n                    data_with_added_field = []\n                    for d, field_value in zip(data[data_name], data[field]):\n                        data_with_added_field.append([*list(d), field_value])\n                    data[data_name] = data_with_added_field\n        return data\n\n    def remove_label_fields_from_data(self, data: dict[str, Any]) -&gt; dict[str, Any]:\n        if not self.params.label_fields:\n            return data\n        label_fields_len = len(self.params.label_fields)\n        for data_name in self.data_fields:\n            if data_name in data:\n                for idx, field in enumerate(self.params.label_fields):\n                    data[field] = [bbox[-label_fields_len + idx] for bbox in data[data_name]]\n                data[data_name] = [d[:-label_fields_len] for d in data[data_name]]\n        return data\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.core.utils.DataProcessor.add_targets","title":"<code>add_targets (self, additional_targets)</code>","text":"<p>Add targets to transform them the same way as one of existing targets.</p> Source code in <code>albumentations/core/utils.py</code> Python<pre><code>def add_targets(self, additional_targets: dict[str, str]) -&gt; None:\n    \"\"\"Add targets to transform them the same way as one of existing targets.\"\"\"\n    for k, v in additional_targets.items():\n        if v == self.default_data_name and k not in self.data_fields:\n            self.data_fields.append(k)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.core.utils.to_tuple","title":"<code>def to_tuple    (param, low=None, bias=None)    </code> [view source on GitHub]","text":"<p>Convert input argument to a min-max tuple.</p> <p>Parameters:</p> Name Type Description <code>param</code> <code>ScaleType</code> <p>Input value which could be a scalar or a sequence of exactly 2 scalars.</p> <code>low</code> <code>ScaleType | None</code> <p>Second element of the tuple, provided as an optional argument for when <code>param</code> is a scalar.</p> <code>bias</code> <code>ScalarType | None</code> <p>An offset added to both elements of the tuple.</p> <p>Returns:</p> Type Description <code>tuple[int, int] | tuple[float, float]</code> <p>A tuple of two scalars, optionally adjusted by <code>bias</code>. Raises ValueError for invalid combinations or types of arguments.</p> Source code in <code>albumentations/core/utils.py</code> Python<pre><code>def to_tuple(\n    param: ScaleType,\n    low: ScaleType | None = None,\n    bias: ScalarType | None = None,\n) -&gt; tuple[int, int] | tuple[float, float]:\n    \"\"\"Convert input argument to a min-max tuple.\n\n    Args:\n        param: Input value which could be a scalar or a sequence of exactly 2 scalars.\n        low: Second element of the tuple, provided as an optional argument for when `param` is a scalar.\n        bias: An offset added to both elements of the tuple.\n\n    Returns:\n        A tuple of two scalars, optionally adjusted by `bias`.\n        Raises ValueError for invalid combinations or types of arguments.\n\n    \"\"\"\n    # Validate mutually exclusive arguments\n    if low is not None and bias is not None:\n        msg = \"Arguments 'low' and 'bias' cannot be used together.\"\n        raise ValueError(msg)\n\n    if isinstance(param, Sequence) and len(param) == PAIR:\n        min_val, max_val = min(param), max(param)\n\n    # Handle scalar input\n    elif isinstance(param, (int, float)):\n        if isinstance(low, (int, float)):\n            # Use low and param to create a tuple\n            min_val, max_val = (low, param) if low &lt; param else (param, low)\n        else:\n            # Create a symmetric tuple around 0\n            min_val, max_val = -param, param\n    else:\n        msg = \"Argument 'param' must be either a scalar or a sequence of 2 elements.\"\n        raise ValueError(msg)\n\n    # Apply bias if provided\n    if bias is not None:\n        return (bias + min_val, bias + max_val)\n\n    return min_val, max_val\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.core.validation","title":"<code>validation</code>","text":""},{"location":"api_reference/full_reference/#albumentations.core.validation.ValidatedTransformMeta","title":"<code>class  ValidatedTransformMeta</code> <code> </code>  [view source on GitHub]","text":"<p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/core/validation.py</code> Python<pre><code>class ValidatedTransformMeta(type):\n    def __new__(cls: type[Any], name: str, bases: tuple[type, ...], dct: dict[str, Any]) -&gt; type[Any]:\n        if \"InitSchema\" in dct and issubclass(dct[\"InitSchema\"], BaseModel):\n            original_init: Callable[..., Any] | None = dct.get(\"__init__\")\n            if original_init is None:\n                msg = \"__init__ not found in class definition\"\n                raise ValueError(msg)\n\n            original_sig = signature(original_init)\n\n            def custom_init(self: Any, *args: Any, **kwargs: Any) -&gt; None:\n                init_params = signature(original_init).parameters\n                param_names = list(init_params.keys())[1:]  # Exclude 'self'\n                full_kwargs: dict[str, Any] = dict(zip(param_names, args))\n                full_kwargs.update(kwargs)\n\n                for parameter_name, parameter in init_params.items():\n                    if (\n                        parameter_name != \"self\"\n                        and parameter_name not in full_kwargs\n                        and parameter.default is not Parameter.empty\n                    ):\n                        full_kwargs[parameter_name] = parameter.default\n\n                # No try-except block needed as we want the exception to propagate naturally\n                config = dct[\"InitSchema\"](**full_kwargs)\n\n                validated_kwargs = config.model_dump()\n                for name_arg in kwargs:\n                    if name_arg not in validated_kwargs:\n                        warn(\n                            f\"Argument '{name_arg}' is not valid and will be ignored.\",\n                            stacklevel=2,\n                        )\n\n                original_init(self, **validated_kwargs)\n\n            # Preserve the original signature and docstring\n            custom_init.__signature__ = original_sig  # type: ignore[attr-defined]\n            custom_init.__doc__ = original_init.__doc__\n\n            # Rename __init__ to custom_init to avoid the N807 warning\n            dct[\"__init__\"] = custom_init\n\n        return super().__new__(cls, name, bases, dct)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.core.validation.ValidatedTransformMeta.__new__","title":"<code>__new__ (cls, name, bases, dct)</code>  <code>special</code> <code>staticmethod</code>","text":"<p>Create and return a new object.  See help(type) for accurate signature.</p> Source code in <code>albumentations/core/validation.py</code> Python<pre><code>def __new__(cls: type[Any], name: str, bases: tuple[type, ...], dct: dict[str, Any]) -&gt; type[Any]:\n    if \"InitSchema\" in dct and issubclass(dct[\"InitSchema\"], BaseModel):\n        original_init: Callable[..., Any] | None = dct.get(\"__init__\")\n        if original_init is None:\n            msg = \"__init__ not found in class definition\"\n            raise ValueError(msg)\n\n        original_sig = signature(original_init)\n\n        def custom_init(self: Any, *args: Any, **kwargs: Any) -&gt; None:\n            init_params = signature(original_init).parameters\n            param_names = list(init_params.keys())[1:]  # Exclude 'self'\n            full_kwargs: dict[str, Any] = dict(zip(param_names, args))\n            full_kwargs.update(kwargs)\n\n            for parameter_name, parameter in init_params.items():\n                if (\n                    parameter_name != \"self\"\n                    and parameter_name not in full_kwargs\n                    and parameter.default is not Parameter.empty\n                ):\n                    full_kwargs[parameter_name] = parameter.default\n\n            # No try-except block needed as we want the exception to propagate naturally\n            config = dct[\"InitSchema\"](**full_kwargs)\n\n            validated_kwargs = config.model_dump()\n            for name_arg in kwargs:\n                if name_arg not in validated_kwargs:\n                    warn(\n                        f\"Argument '{name_arg}' is not valid and will be ignored.\",\n                        stacklevel=2,\n                    )\n\n            original_init(self, **validated_kwargs)\n\n        # Preserve the original signature and docstring\n        custom_init.__signature__ = original_sig  # type: ignore[attr-defined]\n        custom_init.__doc__ = original_init.__doc__\n\n        # Rename __init__ to custom_init to avoid the N807 warning\n        dct[\"__init__\"] = custom_init\n\n    return super().__new__(cls, name, bases, dct)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.pytorch","title":"<code>pytorch</code>  <code>special</code>","text":""},{"location":"api_reference/full_reference/#albumentations.pytorch.transforms","title":"<code>transforms</code>","text":""},{"location":"api_reference/full_reference/#albumentations.pytorch.transforms.ToTensorV2","title":"<code>class  ToTensorV2</code> <code>     (transpose_mask=False, p=1.0, always_apply=None)                 </code>  [view source on GitHub]","text":"<p>Converts images/masks to PyTorch Tensors, inheriting from BasicTransform. Supports images in numpy <code>HWC</code> format and converts them to PyTorch <code>CHW</code> format. If the image is in <code>HW</code> format, it will be converted to PyTorch <code>HW</code>.</p> <p>Attributes:</p> Name Type Description <code>transpose_mask</code> <code>bool</code> <p>If True, transposes 3D input mask dimensions from <code>[height, width, num_channels]</code> to <code>[num_channels, height, width]</code>.</p> <code>always_apply</code> <code>bool</code> <p>Deprecated. Default: None.</p> <code>p</code> <code>float</code> <p>Probability of applying the transform. Default: 1.0.</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/pytorch/transforms.py</code> Python<pre><code>class ToTensorV2(BasicTransform):\n    \"\"\"Converts images/masks to PyTorch Tensors, inheriting from BasicTransform. Supports images in numpy `HWC` format\n    and converts them to PyTorch `CHW` format. If the image is in `HW` format, it will be converted to PyTorch `HW`.\n\n    Attributes:\n        transpose_mask (bool): If True, transposes 3D input mask dimensions from `[height, width, num_channels]` to\n            `[num_channels, height, width]`.\n        always_apply (bool): Deprecated. Default: None.\n        p (float): Probability of applying the transform. Default: 1.0.\n\n    \"\"\"\n\n    _targets = (Targets.IMAGE, Targets.MASK)\n\n    def __init__(self, transpose_mask: bool = False, p: float = 1.0, always_apply: bool | None = None):\n        super().__init__(p=p, always_apply=always_apply)\n        self.transpose_mask = transpose_mask\n\n    @property\n    def targets(self) -&gt; dict[str, Any]:\n        return {\"image\": self.apply, \"mask\": self.apply_to_mask, \"masks\": self.apply_to_masks}\n\n    def apply(self, img: np.ndarray, **params: Any) -&gt; torch.Tensor:\n        if len(img.shape) not in [2, 3]:\n            msg = \"Albumentations only supports images in HW or HWC format\"\n            raise ValueError(msg)\n\n        if len(img.shape) == MONO_CHANNEL_DIMENSIONS:\n            img = np.expand_dims(img, 2)\n\n        return torch.from_numpy(img.transpose(2, 0, 1))\n\n    def apply_to_mask(self, mask: np.ndarray, **params: Any) -&gt; torch.Tensor:\n        if self.transpose_mask and mask.ndim == NUM_MULTI_CHANNEL_DIMENSIONS:\n            mask = mask.transpose(2, 0, 1)\n        return torch.from_numpy(mask)\n\n    def apply_to_masks(self, masks: list[np.ndarray], **params: Any) -&gt; list[torch.Tensor]:\n        return [self.apply_to_mask(mask, **params) for mask in masks]\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return (\"transpose_mask\",)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.pytorch.transforms.ToTensorV2.__init__","title":"<code>__init__ (self, transpose_mask=False, p=1.0, always_apply=None)</code>  <code>special</code>","text":"<p>Initialize self.  See help(type(self)) for accurate signature.</p> Source code in <code>albumentations/pytorch/transforms.py</code> Python<pre><code>def __init__(self, transpose_mask: bool = False, p: float = 1.0, always_apply: bool | None = None):\n    super().__init__(p=p, always_apply=always_apply)\n    self.transpose_mask = transpose_mask\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.pytorch.transforms.ToTensorV2.apply","title":"<code>apply (self, img, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/pytorch/transforms.py</code> Python<pre><code>def apply(self, img: np.ndarray, **params: Any) -&gt; torch.Tensor:\n    if len(img.shape) not in [2, 3]:\n        msg = \"Albumentations only supports images in HW or HWC format\"\n        raise ValueError(msg)\n\n    if len(img.shape) == MONO_CHANNEL_DIMENSIONS:\n        img = np.expand_dims(img, 2)\n\n    return torch.from_numpy(img.transpose(2, 0, 1))\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.pytorch.transforms.ToTensorV2.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/pytorch/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return (\"transpose_mask\",)\n</code></pre>"},{"location":"api_reference/full_reference/#albumentations.random_utils","title":"<code>random_utils</code>","text":""},{"location":"api_reference/full_reference/#albumentations.random_utils.shuffle","title":"<code>def shuffle    (a, random_state=None)    </code> [view source on GitHub]","text":"<p>Shuffles an array in-place, using a specified random state or creating a new one if not provided.</p> <p>Parameters:</p> Name Type Description <code>a</code> <code>np.ndarray</code> <p>The array to be shuffled.</p> <code>random_state</code> <code>Optional[np.random.RandomState]</code> <p>The random state used for shuffling. Defaults to None.</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>The shuffled array (note: the shuffle is in-place, so the original array is modified).</p> Source code in <code>albumentations/random_utils.py</code> Python<pre><code>def shuffle(\n    a: np.ndarray,\n    random_state: np.random.RandomState | None = None,\n) -&gt; np.ndarray:\n    \"\"\"Shuffles an array in-place, using a specified random state or creating a new one if not provided.\n\n    Args:\n        a (np.ndarray): The array to be shuffled.\n        random_state (Optional[np.random.RandomState], optional): The random state used for shuffling. Defaults to None.\n\n    Returns:\n        np.ndarray: The shuffled array (note: the shuffle is in-place, so the original array is modified).\n    \"\"\"\n    if random_state is None:\n        random_state = get_random_state()\n    random_state.shuffle(a)\n    return a\n</code></pre>"},{"location":"api_reference/augmentations/","title":"Index","text":"<ul> <li>Transforms (albumentations.augmentations.transforms)</li> <li>Blur transforms (albumentations.augmentations.blur)</li> <li>Crop transforms (albumentations.augmentations.crops)</li> <li>Dropout transforms (albumentations.augmentations.dropout)</li> <li>Geometric transforms (albumentations.augmentations.geometric)</li> <li>Mixing transforms (albumentations.augmentations.mixing)</li> <li>Domain adaptation transforms (albumentations.augmentations.domain_adaptation)</li> <li>Functional transforms (albumentations.augmentations.functional)</li> </ul>"},{"location":"api_reference/augmentations/domain_adaptation/","title":"Domain adaptation transforms (augmentations.domain_adaptation)","text":""},{"location":"api_reference/augmentations/domain_adaptation/#albumentations.augmentations.domain_adaptation.FDA","title":"<code>class  FDA</code> <code>     (reference_images, beta_limit=(0, 0.1), read_fn=&lt;function read_rgb_image at 0x7f4afc5b3740&gt;, always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Fourier Domain Adaptation (FDA) for simple \"style transfer\" in the context of unsupervised domain adaptation (UDA). FDA manipulates the frequency components of images to reduce the domain gap between source and target datasets, effectively adapting images from one domain to closely resemble those from another without altering their semantic content.</p> <p>This transform is particularly beneficial in scenarios where the training (source) and testing (target) images come from different distributions, such as synthetic versus real images, or day versus night scenes. Unlike traditional domain adaptation methods that may require complex adversarial training, FDA achieves domain alignment by swapping low-frequency components of the Fourier transform between the source and target images. This technique has shown to improve the performance of models on the target domain, particularly for tasks like semantic segmentation, without additional training for domain invariance.</p> <p>The 'beta_limit' parameter controls the extent of frequency component swapping, with lower values preserving more of the original image's characteristics and higher values leading to more pronounced adaptation effects. It is recommended to use beta values less than 0.3 to avoid introducing artifacts.</p> <p>Parameters:</p> Name Type Description <code>reference_images</code> <code>Sequence[Any]</code> <p>Sequence of objects to be converted into images by <code>read_fn</code>. This typically involves paths to images that serve as target domain examples for adaptation.</p> <code>beta_limit</code> <code>float or tuple of float</code> <p>Coefficient beta from the paper, controlling the swapping extent of frequency components. Values should be less than 0.5.</p> <code>read_fn</code> <code>Callable</code> <p>User-defined function for reading images. It takes an element from <code>reference_images</code> and returns a numpy array of image pixels. By default, it is expected to take a path to an image and return a numpy array.</p> <p>Targets</p> <p>image</p> <p>Image types:     uint8, float32</p> <p>Reference</p> <ul> <li>https://github.com/YanchaoYang/FDA</li> <li>https://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_FDA_Fourier_Domain_Adaptation_for_Semantic_Segmentation_CVPR_2020_paper.pdf</li> </ul> <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import albumentations as A\n&gt;&gt;&gt; image = np.random.randint(0, 256, [100, 100, 3], dtype=np.uint8)\n&gt;&gt;&gt; target_image = np.random.randint(0, 256, [100, 100, 3], dtype=np.uint8)\n&gt;&gt;&gt; aug = A.Compose([A.FDA([target_image], p=1, read_fn=lambda x: x)])\n&gt;&gt;&gt; result = aug(image=image)\n</code></pre> <p>Note</p> <p>FDA is a powerful tool for domain adaptation, particularly in unsupervised settings where annotated target domain samples are unavailable. It enables significant improvements in model generalization by aligning the low-level statistics of source and target images through a simple yet effective Fourier-based method.</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/domain_adaptation.py</code> Python<pre><code>class FDA(ImageOnlyTransform):\n    \"\"\"Fourier Domain Adaptation (FDA) for simple \"style transfer\" in the context of unsupervised domain adaptation\n    (UDA). FDA manipulates the frequency components of images to reduce the domain gap between source\n    and target datasets, effectively adapting images from one domain to closely resemble those from another without\n    altering their semantic content.\n\n    This transform is particularly beneficial in scenarios where the training (source) and testing (target) images\n    come from different distributions, such as synthetic versus real images, or day versus night scenes.\n    Unlike traditional domain adaptation methods that may require complex adversarial training, FDA achieves domain\n    alignment by swapping low-frequency components of the Fourier transform between the source and target images.\n    This technique has shown to improve the performance of models on the target domain, particularly for tasks\n    like semantic segmentation, without additional training for domain invariance.\n\n    The 'beta_limit' parameter controls the extent of frequency component swapping, with lower values preserving more\n    of the original image's characteristics and higher values leading to more pronounced adaptation effects.\n    It is recommended to use beta values less than 0.3 to avoid introducing artifacts.\n\n    Args:\n        reference_images (Sequence[Any]): Sequence of objects to be converted into images by `read_fn`. This typically\n            involves paths to images that serve as target domain examples for adaptation.\n        beta_limit (float or tuple of float): Coefficient beta from the paper, controlling the swapping extent of\n            frequency components. Values should be less than 0.5.\n        read_fn (Callable): User-defined function for reading images. It takes an element from `reference_images` and\n            returns a numpy array of image pixels. By default, it is expected to take a path to an image and return a\n            numpy array.\n\n    Targets:\n        image\n\n    Image types:\n        uint8, float32\n\n    Reference:\n        - https://github.com/YanchaoYang/FDA\n        - https://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_FDA_Fourier_Domain_Adaptation_for_Semantic_Segmentation_CVPR_2020_paper.pdf\n\n    Example:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; import albumentations as A\n        &gt;&gt;&gt; image = np.random.randint(0, 256, [100, 100, 3], dtype=np.uint8)\n        &gt;&gt;&gt; target_image = np.random.randint(0, 256, [100, 100, 3], dtype=np.uint8)\n        &gt;&gt;&gt; aug = A.Compose([A.FDA([target_image], p=1, read_fn=lambda x: x)])\n        &gt;&gt;&gt; result = aug(image=image)\n\n    Note:\n        FDA is a powerful tool for domain adaptation, particularly in unsupervised settings where annotated target\n        domain samples are unavailable. It enables significant improvements in model generalization by aligning\n        the low-level statistics of source and target images through a simple yet effective Fourier-based method.\n    \"\"\"\n\n    class InitSchema(BaseTransformInitSchema):\n        reference_images: Sequence[Any]\n        read_fn: Callable[[Any], np.ndarray]\n        beta_limit: NonNegativeFloatRangeType = (0, 0.1)\n\n        @field_validator(\"beta_limit\")\n        @classmethod\n        def check_ranges(cls, value: tuple[float, float]) -&gt; tuple[float, float]:\n            bounds = 0, MAX_BETA_LIMIT\n            if not bounds[0] &lt;= value[0] &lt;= value[1] &lt;= bounds[1]:\n                raise ValueError(f\"Values should be in the range {bounds} got {value} \")\n            return value\n\n    def __init__(\n        self,\n        reference_images: Sequence[Any],\n        beta_limit: ScaleFloatType = (0, 0.1),\n        read_fn: Callable[[Any], np.ndarray] = read_rgb_image,\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p=p, always_apply=always_apply)\n        self.reference_images = reference_images\n        self.read_fn = read_fn\n        self.beta_limit = cast(Tuple[float, float], beta_limit)\n\n    def apply(\n        self,\n        img: np.ndarray,\n        target_image: np.ndarray,\n        beta: float,\n        **params: Any,\n    ) -&gt; np.ndarray:\n        return fourier_domain_adaptation(img, target_image, beta)\n\n    def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, np.ndarray]:\n        target_img = self.read_fn(random.choice(self.reference_images))\n        target_img = cv2.resize(target_img, dsize=(params[\"cols\"], params[\"rows\"]))\n\n        return {\"target_image\": target_img}\n\n    def get_params(self) -&gt; dict[str, float]:\n        return {\"beta\": random.uniform(self.beta_limit[0], self.beta_limit[1])}\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, str, str]:\n        return \"reference_images\", \"beta_limit\", \"read_fn\"\n\n    def to_dict_private(self) -&gt; dict[str, Any]:\n        msg = \"FDA can not be serialized.\"\n        raise NotImplementedError(msg)\n</code></pre>"},{"location":"api_reference/augmentations/domain_adaptation/#albumentations.augmentations.domain_adaptation.FDA.apply","title":"<code>apply (self, img, target_image, beta, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/domain_adaptation.py</code> Python<pre><code>def apply(\n    self,\n    img: np.ndarray,\n    target_image: np.ndarray,\n    beta: float,\n    **params: Any,\n) -&gt; np.ndarray:\n    return fourier_domain_adaptation(img, target_image, beta)\n</code></pre>"},{"location":"api_reference/augmentations/domain_adaptation/#albumentations.augmentations.domain_adaptation.FDA.get_params","title":"<code>get_params (self)</code>","text":"<p>Returns parameters independent of input.</p> Source code in <code>albumentations/augmentations/domain_adaptation.py</code> Python<pre><code>def get_params(self) -&gt; dict[str, float]:\n    return {\"beta\": random.uniform(self.beta_limit[0], self.beta_limit[1])}\n</code></pre>"},{"location":"api_reference/augmentations/domain_adaptation/#albumentations.augmentations.domain_adaptation.FDA.get_params_dependent_on_data","title":"<code>get_params_dependent_on_data (self, params, data)</code>","text":"<p>Returns parameters dependent on input.</p> Source code in <code>albumentations/augmentations/domain_adaptation.py</code> Python<pre><code>def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, np.ndarray]:\n    target_img = self.read_fn(random.choice(self.reference_images))\n    target_img = cv2.resize(target_img, dsize=(params[\"cols\"], params[\"rows\"]))\n\n    return {\"target_image\": target_img}\n</code></pre>"},{"location":"api_reference/augmentations/domain_adaptation/#albumentations.augmentations.domain_adaptation.FDA.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/domain_adaptation.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, str, str]:\n    return \"reference_images\", \"beta_limit\", \"read_fn\"\n</code></pre>"},{"location":"api_reference/augmentations/domain_adaptation/#albumentations.augmentations.domain_adaptation.HistogramMatching","title":"<code>class  HistogramMatching</code> <code>     (reference_images, blend_ratio=(0.5, 1.0), read_fn=&lt;function read_rgb_image at 0x7f4afc5b3740&gt;, always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Implements histogram matching, a technique that adjusts the pixel values of an input image to match the histogram of a reference image. This adjustment ensures that the output image has a similar tone and contrast to the reference. The process is applied independently to each channel of multi-channel images, provided both the input and reference images have the same number of channels.</p> <p>Histogram matching serves as an effective normalization method in image processing tasks such as feature matching. It is particularly useful when images originate from varied sources or are captured under different lighting conditions, helping to standardize the images' appearance before further processing.</p> <p>Parameters:</p> Name Type Description <code>reference_images</code> <code>Sequence[Any]</code> <p>A sequence of objects to be converted into images by <code>read_fn</code>. Typically, this is a sequence of image paths.</p> <code>blend_ratio</code> <code>tuple[float, float]</code> <p>Specifies the minimum and maximum blend ratio for blending the matched image with the original image. A random blend factor within this range is chosen for each image to increase the diversity of the output images.</p> <code>read_fn</code> <code>Callable[[Any], np.ndarray]</code> <p>A user-defined function for reading images, which accepts an element from <code>reference_images</code> and returns a numpy array of image pixels. By default, this is expected to take a file path and return an image as a numpy array.</p> <code>p</code> <code>float</code> <p>The probability of applying the transform to any given image. Defaults to 0.5.</p> <p>Targets</p> <p>image</p> <p>Image types:     uint8, float32</p> <p>Note</p> <p>This class cannot be serialized directly due to its dynamic nature and dependency on external image data. An attempt to serialize it will raise a NotImplementedError.</p> <p>Reference</p> <p>https://scikit-image.org/docs/dev/auto_examples/color_exposure/plot_histogram_matching.html</p> <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import albumentations as A\n&gt;&gt;&gt; image = np.random.randint(0, 256, [100, 100, 3], dtype=np.uint8)\n&gt;&gt;&gt; target_image = np.random.randint(0, 256, [100, 100, 3], dtype=np.uint8)\n&gt;&gt;&gt; aug = A.Compose([A.HistogramMatching([target_image], p=1, read_fn=lambda x: x)])\n&gt;&gt;&gt; result = aug(image=image)\n</code></pre> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/domain_adaptation.py</code> Python<pre><code>class HistogramMatching(ImageOnlyTransform):\n    \"\"\"Implements histogram matching, a technique that adjusts the pixel values of an input image\n    to match the histogram of a reference image. This adjustment ensures that the output image\n    has a similar tone and contrast to the reference. The process is applied independently to\n    each channel of multi-channel images, provided both the input and reference images have the\n    same number of channels.\n\n    Histogram matching serves as an effective normalization method in image processing tasks such\n    as feature matching. It is particularly useful when images originate from varied sources or are\n    captured under different lighting conditions, helping to standardize the images' appearance\n    before further processing.\n\n    Args:\n        reference_images (Sequence[Any]): A sequence of objects to be converted into images by `read_fn`.\n            Typically, this is a sequence of image paths.\n        blend_ratio (tuple[float, float]): Specifies the minimum and maximum blend ratio for blending the matched\n            image with the original image. A random blend factor within this range is chosen for each image to\n            increase the diversity of the output images.\n        read_fn (Callable[[Any], np.ndarray]): A user-defined function for reading images, which accepts an\n            element from `reference_images` and returns a numpy array of image pixels. By default, this is expected\n            to take a file path and return an image as a numpy array.\n        p (float): The probability of applying the transform to any given image. Defaults to 0.5.\n\n    Targets:\n        image\n\n    Image types:\n        uint8, float32\n\n    Note:\n        This class cannot be serialized directly due to its dynamic nature and dependency on external image data.\n        An attempt to serialize it will raise a NotImplementedError.\n\n    Reference:\n        https://scikit-image.org/docs/dev/auto_examples/color_exposure/plot_histogram_matching.html\n\n    Example:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; import albumentations as A\n        &gt;&gt;&gt; image = np.random.randint(0, 256, [100, 100, 3], dtype=np.uint8)\n        &gt;&gt;&gt; target_image = np.random.randint(0, 256, [100, 100, 3], dtype=np.uint8)\n        &gt;&gt;&gt; aug = A.Compose([A.HistogramMatching([target_image], p=1, read_fn=lambda x: x)])\n        &gt;&gt;&gt; result = aug(image=image)\n    \"\"\"\n\n    class InitSchema(BaseTransformInitSchema):\n        reference_images: Sequence[Any]\n        blend_ratio: Annotated[tuple[float, float], AfterValidator(nondecreasing), AfterValidator(check_01)] = (\n            0.5,\n            1.0,\n        )\n        read_fn: Callable[[Any], np.ndarray]\n\n    def __init__(\n        self,\n        reference_images: Sequence[Any],\n        blend_ratio: tuple[float, float] = (0.5, 1.0),\n        read_fn: Callable[[Any], np.ndarray] = read_rgb_image,\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p=p, always_apply=always_apply)\n        self.reference_images = reference_images\n        self.read_fn = read_fn\n        self.blend_ratio = blend_ratio\n\n    def apply(\n        self: np.ndarray,\n        img: np.ndarray,\n        reference_image: np.ndarray,\n        blend_ratio: float,\n        **params: Any,\n    ) -&gt; np.ndarray:\n        return apply_histogram(img, reference_image, blend_ratio)\n\n    def get_params(self) -&gt; dict[str, np.ndarray]:\n        return {\n            \"reference_image\": self.read_fn(random.choice(self.reference_images)),\n            \"blend_ratio\": random.uniform(self.blend_ratio[0], self.blend_ratio[1]),\n        }\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return \"reference_images\", \"blend_ratio\", \"read_fn\"\n\n    def to_dict_private(self) -&gt; dict[str, Any]:\n        msg = \"HistogramMatching can not be serialized.\"\n        raise NotImplementedError(msg)\n</code></pre>"},{"location":"api_reference/augmentations/domain_adaptation/#albumentations.augmentations.domain_adaptation.HistogramMatching.apply","title":"<code>apply (self, img, reference_image, blend_ratio, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/domain_adaptation.py</code> Python<pre><code>def apply(\n    self: np.ndarray,\n    img: np.ndarray,\n    reference_image: np.ndarray,\n    blend_ratio: float,\n    **params: Any,\n) -&gt; np.ndarray:\n    return apply_histogram(img, reference_image, blend_ratio)\n</code></pre>"},{"location":"api_reference/augmentations/domain_adaptation/#albumentations.augmentations.domain_adaptation.HistogramMatching.get_params","title":"<code>get_params (self)</code>","text":"<p>Returns parameters independent of input.</p> Source code in <code>albumentations/augmentations/domain_adaptation.py</code> Python<pre><code>def get_params(self) -&gt; dict[str, np.ndarray]:\n    return {\n        \"reference_image\": self.read_fn(random.choice(self.reference_images)),\n        \"blend_ratio\": random.uniform(self.blend_ratio[0], self.blend_ratio[1]),\n    }\n</code></pre>"},{"location":"api_reference/augmentations/domain_adaptation/#albumentations.augmentations.domain_adaptation.HistogramMatching.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/domain_adaptation.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return \"reference_images\", \"blend_ratio\", \"read_fn\"\n</code></pre>"},{"location":"api_reference/augmentations/domain_adaptation/#albumentations.augmentations.domain_adaptation.PixelDistributionAdaptation","title":"<code>class  PixelDistributionAdaptation</code> <code>     (reference_images, blend_ratio=(0.25, 1.0), read_fn=&lt;function read_rgb_image at 0x7f4afc5b3740&gt;, transform_type='pca', always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Performs pixel-level domain adaptation by aligning the pixel value distribution of an input image with that of a reference image. This process involves fitting a simple statistical transformation (such as PCA, StandardScaler, or MinMaxScaler) to both the original and the reference images, transforming the original image with the transformation trained on it, and then applying the inverse transformation using the transform fitted on the reference image. The result is an adapted image that retains the original content while mimicking the pixel value distribution of the reference domain.</p> <p>The process can be visualized as two main steps: 1. Adjusting the original image to a standard distribution space using a selected transform. 2. Moving the adjusted image into the distribution space of the reference image by applying the inverse    of the transform fitted on the reference image.</p> <p>This technique is especially useful in scenarios where images from different domains (e.g., synthetic vs. real images, day vs. night scenes) need to be harmonized for better consistency or performance in image processing tasks.</p> <p>Parameters:</p> Name Type Description <code>reference_images</code> <code>Sequence[Any]</code> <p>A sequence of objects (typically image paths) that will be converted into images by <code>read_fn</code>. These images serve as references for the domain adaptation.</p> <code>blend_ratio</code> <code>tuple[float, float]</code> <p>Specifies the minimum and maximum blend ratio for mixing the adapted image with the original, enhancing the diversity of the output images.</p> <code>read_fn</code> <code>Callable</code> <p>A user-defined function for reading and converting the objects in <code>reference_images</code> into numpy arrays. By default, it assumes these objects are image paths.</p> <code>transform_type</code> <code>str</code> <p>Specifies the type of statistical transformation to apply. Supported values are \"pca\" for Principal Component Analysis, \"standard\" for StandardScaler, and \"minmax\" for MinMaxScaler.</p> <code>p</code> <code>float</code> <p>The probability of applying the transform to any given image. Default is 1.0.</p> <p>Targets</p> <p>image</p> <p>Image types:     uint8, float32</p> <p>Reference</p> <p>For more information on the underlying approach, see: https://github.com/arsenyinfo/qudida</p> <p>Note</p> <p>The PixelDistributionAdaptation transform is a novel way to perform domain adaptation at the pixel level, suitable for adjusting images across different conditions without complex modeling. It is effective for preparing images before more advanced processing or analysis.</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/domain_adaptation.py</code> Python<pre><code>class PixelDistributionAdaptation(ImageOnlyTransform):\n    \"\"\"Performs pixel-level domain adaptation by aligning the pixel value distribution of an input image\n    with that of a reference image. This process involves fitting a simple statistical transformation\n    (such as PCA, StandardScaler, or MinMaxScaler) to both the original and the reference images,\n    transforming the original image with the transformation trained on it, and then applying the inverse\n    transformation using the transform fitted on the reference image. The result is an adapted image\n    that retains the original content while mimicking the pixel value distribution of the reference domain.\n\n    The process can be visualized as two main steps:\n    1. Adjusting the original image to a standard distribution space using a selected transform.\n    2. Moving the adjusted image into the distribution space of the reference image by applying the inverse\n       of the transform fitted on the reference image.\n\n    This technique is especially useful in scenarios where images from different domains (e.g., synthetic\n    vs. real images, day vs. night scenes) need to be harmonized for better consistency or performance in\n    image processing tasks.\n\n    Args:\n        reference_images (Sequence[Any]): A sequence of objects (typically image paths) that will be\n            converted into images by `read_fn`. These images serve as references for the domain adaptation.\n        blend_ratio (tuple[float, float]): Specifies the minimum and maximum blend ratio for mixing\n            the adapted image with the original, enhancing the diversity of the output images.\n        read_fn (Callable): A user-defined function for reading and converting the objects in\n            `reference_images` into numpy arrays. By default, it assumes these objects are image paths.\n        transform_type (str): Specifies the type of statistical transformation to apply. Supported values\n            are \"pca\" for Principal Component Analysis, \"standard\" for StandardScaler, and \"minmax\" for\n            MinMaxScaler.\n        p (float): The probability of applying the transform to any given image. Default is 1.0.\n\n    Targets:\n        image\n\n    Image types:\n        uint8, float32\n\n    Reference:\n        For more information on the underlying approach, see: https://github.com/arsenyinfo/qudida\n\n    Note:\n        The PixelDistributionAdaptation transform is a novel way to perform domain adaptation at the pixel level,\n        suitable for adjusting images across different conditions without complex modeling. It is effective\n        for preparing images before more advanced processing or analysis.\n    \"\"\"\n\n    class InitSchema(BaseTransformInitSchema):\n        reference_images: Sequence[Any]\n        blend_ratio: Annotated[tuple[float, float], AfterValidator(nondecreasing), AfterValidator(check_01)] = (\n            0.25,\n            1.0,\n        )\n        read_fn: Callable[[Any], np.ndarray]\n        transform_type: Literal[\"pca\", \"standard\", \"minmax\"]\n\n    def __init__(\n        self,\n        reference_images: Sequence[Any],\n        blend_ratio: tuple[float, float] = (0.25, 1.0),\n        read_fn: Callable[[Any], np.ndarray] = read_rgb_image,\n        transform_type: Literal[\"pca\", \"standard\", \"minmax\"] = \"pca\",\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p=p, always_apply=always_apply)\n        self.reference_images = reference_images\n        self.read_fn = read_fn\n        self.blend_ratio = blend_ratio\n        self.transform_type = transform_type\n\n    @staticmethod\n    def _validate_shape(img: np.ndarray) -&gt; None:\n        if is_grayscale_image(img) or is_multispectral_image(img):\n            raise ValueError(\n                f\"Unexpected image shape: expected 3 dimensions, got {len(img.shape)}.\"\n                f\"Is it a grayscale or multispectral image? It's not supported for now.\",\n            )\n\n    def ensure_uint8(self, img: np.ndarray) -&gt; tuple[np.ndarray, bool]:\n        if img.dtype == np.float32:\n            if img.min() &lt; 0 or img.max() &gt; 1:\n                message = (\n                    \"PixelDistributionAdaptation uses uint8 under the hood, so float32 should be converted,\"\n                    \"Can not do it automatically when the image is out of [0..1] range.\"\n                )\n                raise TypeError(message)\n            return clip(img * 255, np.uint8), True\n        return img, False\n\n    def apply(self, img: np.ndarray, reference_image: np.ndarray, blend_ratio: float, **params: Any) -&gt; np.ndarray:\n        self._validate_shape(img)\n        reference_image, _ = self.ensure_uint8(reference_image)\n        img, needs_reconvert = self.ensure_uint8(img)\n\n        adapted = adapt_pixel_distribution(\n            img,\n            ref=reference_image,\n            weight=blend_ratio,\n            transform_type=self.transform_type,\n        )\n\n        return fmain.to_float(adapted) if needs_reconvert else adapted\n\n    def get_params(self) -&gt; dict[str, Any]:\n        return {\n            \"reference_image\": self.read_fn(random.choice(self.reference_images)),\n            \"blend_ratio\": random.uniform(self.blend_ratio[0], self.blend_ratio[1]),\n        }\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, str, str, str]:\n        return \"reference_images\", \"blend_ratio\", \"read_fn\", \"transform_type\"\n\n    def to_dict_private(self) -&gt; dict[str, Any]:\n        msg = \"PixelDistributionAdaptation can not be serialized.\"\n        raise NotImplementedError(msg)\n</code></pre>"},{"location":"api_reference/augmentations/domain_adaptation/#albumentations.augmentations.domain_adaptation.PixelDistributionAdaptation.apply","title":"<code>apply (self, img, reference_image, blend_ratio, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/domain_adaptation.py</code> Python<pre><code>def apply(self, img: np.ndarray, reference_image: np.ndarray, blend_ratio: float, **params: Any) -&gt; np.ndarray:\n    self._validate_shape(img)\n    reference_image, _ = self.ensure_uint8(reference_image)\n    img, needs_reconvert = self.ensure_uint8(img)\n\n    adapted = adapt_pixel_distribution(\n        img,\n        ref=reference_image,\n        weight=blend_ratio,\n        transform_type=self.transform_type,\n    )\n\n    return fmain.to_float(adapted) if needs_reconvert else adapted\n</code></pre>"},{"location":"api_reference/augmentations/domain_adaptation/#albumentations.augmentations.domain_adaptation.PixelDistributionAdaptation.get_params","title":"<code>get_params (self)</code>","text":"<p>Returns parameters independent of input.</p> Source code in <code>albumentations/augmentations/domain_adaptation.py</code> Python<pre><code>def get_params(self) -&gt; dict[str, Any]:\n    return {\n        \"reference_image\": self.read_fn(random.choice(self.reference_images)),\n        \"blend_ratio\": random.uniform(self.blend_ratio[0], self.blend_ratio[1]),\n    }\n</code></pre>"},{"location":"api_reference/augmentations/domain_adaptation/#albumentations.augmentations.domain_adaptation.PixelDistributionAdaptation.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/domain_adaptation.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, str, str, str]:\n    return \"reference_images\", \"blend_ratio\", \"read_fn\", \"transform_type\"\n</code></pre>"},{"location":"api_reference/augmentations/functional/","title":"Functional transforms (augmentations.functional)","text":""},{"location":"api_reference/augmentations/functional/#albumentations.augmentations.functional.add_fog","title":"<code>def add_fog    (img, fog_coef, alpha_coef, haze_list)    </code> [view source on GitHub]","text":"<p>Add fog to an image using the provided coefficients and haze points.</p> <p>Parameters:</p> Name Type Description <code>img</code> <code>np.ndarray</code> <p>The input image, expected to be a numpy array.</p> <code>fog_coef</code> <code>float</code> <p>The fog coefficient, used to determine the intensity of the fog.</p> <code>alpha_coef</code> <code>float</code> <p>The alpha coefficient, used to determine the transparency of the fog.</p> <code>haze_list</code> <code>list[tuple[int, int]]</code> <p>A list of tuples, where each tuple represents the x and y coordinates of a haze point.</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>The output image with added fog, as a numpy array.</p> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If the input image's dtype is not uint8 or float32.</p> <p>Reference</p> <p>https://github.com/UjjwalSaxena/Automold--Road-Augmentation-Library</p> Source code in <code>albumentations/augmentations/functional.py</code> Python<pre><code>@preserve_channel_dim\ndef add_fog(img: np.ndarray, fog_coef: float, alpha_coef: float, haze_list: list[tuple[int, int]]) -&gt; np.ndarray:\n    \"\"\"Add fog to an image using the provided coefficients and haze points.\n\n    Args:\n        img (np.ndarray): The input image, expected to be a numpy array.\n        fog_coef (float): The fog coefficient, used to determine the intensity of the fog.\n        alpha_coef (float): The alpha coefficient, used to determine the transparency of the fog.\n        haze_list (list[tuple[int, int]]): A list of tuples, where each tuple represents the x and y\n            coordinates of a haze point.\n\n    Returns:\n        np.ndarray: The output image with added fog, as a numpy array.\n\n    Raises:\n        ValueError: If the input image's dtype is not uint8 or float32.\n\n    Reference:\n        https://github.com/UjjwalSaxena/Automold--Road-Augmentation-Library\n    \"\"\"\n    non_rgb_warning(img)\n\n    input_dtype = img.dtype\n    needs_float = False\n\n    if input_dtype == np.float32:\n        img = from_float(img, dtype=np.dtype(\"uint8\"))\n        needs_float = True\n    elif input_dtype not in (np.uint8, np.float32):\n        raise ValueError(f\"Unexpected dtype {input_dtype} for RandomFog augmentation\")\n\n    width = img.shape[1]\n\n    hw = max(int(width // 3 * fog_coef), 10)\n\n    for haze_points in haze_list:\n        x, y = haze_points\n        overlay = img.copy()\n        output = img.copy()\n        alpha = alpha_coef * fog_coef\n        rad = hw // 2\n        point = (x + hw // 2, y + hw // 2)\n        cv2.circle(overlay, point, int(rad), (255, 255, 255), -1)\n        output = add_weighted(overlay, alpha, output, 1 - alpha)\n\n        img = output.copy()\n\n    image_rgb = cv2.blur(img, (hw // 10, hw // 10))\n\n    return to_float(image_rgb, max_value=255) if needs_float else image_rgb\n</code></pre>"},{"location":"api_reference/augmentations/functional/#albumentations.augmentations.functional.add_gravel","title":"<code>def add_gravel    (img, gravels)    </code> [view source on GitHub]","text":"<p>Add gravel to the image.</p> <p>Parameters:</p> Name Type Description <code>img</code> <code>numpy.ndarray</code> <p>image to add gravel to</p> <code>gravels</code> <code>list</code> <p>list of gravel parameters. (float, float, float, float): (top-left x, top-left y, bottom-right x, bottom right y)</p> <p>Returns:</p> Type Description <code>numpy.ndarray</code> <p>Reference</p> <p>https://github.com/UjjwalSaxena/Automold--Road-Augmentation-Library</p> Source code in <code>albumentations/augmentations/functional.py</code> Python<pre><code>@preserve_channel_dim\ndef add_gravel(img: np.ndarray, gravels: list[Any]) -&gt; np.ndarray:\n    \"\"\"Add gravel to the image.\n\n    Args:\n        img (numpy.ndarray): image to add gravel to\n        gravels (list): list of gravel parameters. (float, float, float, float):\n            (top-left x, top-left y, bottom-right x, bottom right y)\n\n    Returns:\n        numpy.ndarray:\n\n    Reference:\n        https://github.com/UjjwalSaxena/Automold--Road-Augmentation-Library\n    \"\"\"\n    non_rgb_warning(img)\n    input_dtype = img.dtype\n    needs_float = False\n\n    if input_dtype == np.float32:\n        img = from_float(img, dtype=np.dtype(\"uint8\"))\n        needs_float = True\n    elif input_dtype not in (np.uint8, np.float32):\n        raise ValueError(f\"Unexpected dtype {input_dtype} for AddGravel augmentation\")\n\n    image_hls = cv2.cvtColor(img, cv2.COLOR_RGB2HLS)\n\n    for gravel in gravels:\n        y1, y2, x1, x2, sat = gravel\n        image_hls[x1:x2, y1:y2, 1] = sat\n\n    image_rgb = cv2.cvtColor(image_hls, cv2.COLOR_HLS2RGB)\n\n    return to_float(image_rgb, max_value=255) if needs_float else image_rgb\n</code></pre>"},{"location":"api_reference/augmentations/functional/#albumentations.augmentations.functional.add_rain","title":"<code>def add_rain    (img, slant, drop_length, drop_width, drop_color, blur_value, brightness_coefficient, rain_drops)    </code> [view source on GitHub]","text":"<p>Adds rain drops to the image.</p> <p>Parameters:</p> Name Type Description <code>img</code> <code>np.ndarray</code> <p>Input image.</p> <code>slant</code> <code>int</code> <p>The angle of the rain drops.</p> <code>drop_length</code> <code>int</code> <p>The length of each rain drop.</p> <code>drop_width</code> <code>int</code> <p>The width of each rain drop.</p> <code>drop_color</code> <code>tuple[int, int, int]</code> <p>The color of the rain drops in RGB format.</p> <code>blur_value</code> <code>int</code> <p>The size of the kernel used to blur the image. Rainy views are blurry.</p> <code>brightness_coefficient</code> <code>float</code> <p>Coefficient to adjust the brightness of the image. Rainy days are usually shady.</p> <code>rain_drops</code> <code>list[tuple[int, int]]</code> <p>A list of tuples where each tuple represents the (x, y) coordinates of the starting point of a rain drop.</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Image with rain effect added.</p> <p>Reference</p> <p>https://github.com/UjjwalSaxena/Automold--Road-Augmentation-Library</p> Source code in <code>albumentations/augmentations/functional.py</code> Python<pre><code>@preserve_channel_dim\ndef add_rain(\n    img: np.ndarray,\n    slant: int,\n    drop_length: int,\n    drop_width: int,\n    drop_color: tuple[int, int, int],\n    blur_value: int,\n    brightness_coefficient: float,\n    rain_drops: list[tuple[int, int]],\n) -&gt; np.ndarray:\n    \"\"\"Adds rain drops to the image.\n\n    Args:\n        img (np.ndarray): Input image.\n        slant (int): The angle of the rain drops.\n        drop_length (int): The length of each rain drop.\n        drop_width (int): The width of each rain drop.\n        drop_color (tuple[int, int, int]): The color of the rain drops in RGB format.\n        blur_value (int): The size of the kernel used to blur the image. Rainy views are blurry.\n        brightness_coefficient (float): Coefficient to adjust the brightness of the image. Rainy days are usually shady.\n        rain_drops (list[tuple[int, int]]): A list of tuples where each tuple represents the (x, y)\n            coordinates of the starting point of a rain drop.\n\n    Returns:\n        np.ndarray: Image with rain effect added.\n\n    Reference:\n        https://github.com/UjjwalSaxena/Automold--Road-Augmentation-Library\n    \"\"\"\n    non_rgb_warning(img)\n\n    input_dtype = img.dtype\n    needs_float = False\n\n    if input_dtype == np.float32:\n        img = from_float(img, dtype=np.dtype(\"uint8\"))\n        needs_float = True\n\n    image = img.copy()\n\n    for rain_drop_x0, rain_drop_y0 in rain_drops:\n        rain_drop_x1 = rain_drop_x0 + slant\n        rain_drop_y1 = rain_drop_y0 + drop_length\n\n        cv2.line(\n            image,\n            (rain_drop_x0, rain_drop_y0),\n            (rain_drop_x1, rain_drop_y1),\n            drop_color,\n            drop_width,\n        )\n\n    image = cv2.blur(image, (blur_value, blur_value))  # rainy view are blurry\n    image_hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV).astype(np.float32)\n    image_hsv[:, :, 2] *= brightness_coefficient\n\n    image_rgb = cv2.cvtColor(image_hsv.astype(np.uint8), cv2.COLOR_HSV2RGB)\n\n    return to_float(image_rgb, max_value=255) if needs_float else image_rgb\n</code></pre>"},{"location":"api_reference/augmentations/functional/#albumentations.augmentations.functional.add_shadow","title":"<code>def add_shadow    (img, vertices_list, intensities)    </code> [view source on GitHub]","text":"<p>Add shadows to the image by reducing the intensity of the pixel values in specified regions.</p> <p>Parameters:</p> Name Type Description <code>img</code> <code>np.ndarray</code> <p>Input image. Multichannel images are supported.</p> <code>vertices_list</code> <code>list[np.ndarray]</code> <p>List of vertices for shadow polygons.</p> <code>intensities</code> <code>np.ndarray</code> <p>Array of shadow intensities. Range is [0, 1].</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Image with shadows added.</p> <p>Reference</p> <p>https://github.com/UjjwalSaxena/Automold--Road-Augmentation-Library</p> Source code in <code>albumentations/augmentations/functional.py</code> Python<pre><code>@preserve_channel_dim\ndef add_shadow(img: np.ndarray, vertices_list: list[np.ndarray], intensities: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Add shadows to the image by reducing the intensity of the pixel values in specified regions.\n\n    Args:\n        img (np.ndarray): Input image. Multichannel images are supported.\n        vertices_list (list[np.ndarray]): List of vertices for shadow polygons.\n        intensities (np.ndarray): Array of shadow intensities. Range is [0, 1].\n\n    Returns:\n        np.ndarray: Image with shadows added.\n\n    Reference:\n        https://github.com/UjjwalSaxena/Automold--Road-Augmentation-Library\n    \"\"\"\n    input_dtype = img.dtype\n    needs_float = False\n    num_channels = get_num_channels(img)\n    max_value = MAX_VALUES_BY_DTYPE[np.uint8]\n\n    if input_dtype == np.float32:\n        img = from_float(img, dtype=np.dtype(\"uint8\"))\n        needs_float = True\n\n    img_shadowed = img.copy()\n\n    # Iterate over the vertices and intensity list\n    for vertices, shadow_intensity in zip(vertices_list, intensities):\n        # Create mask for the current shadow polygon\n        mask = np.zeros((img.shape[0], img.shape[1], 1), dtype=np.uint8)\n        cv2.fillPoly(mask, [vertices], (max_value,))\n\n        # Duplicate the mask to have the same number of channels as the image\n        mask = np.repeat(mask, num_channels, axis=2)\n\n        # Apply shadow to the channels directly\n        # It could be tempting to convert to HLS and apply the shadow to the L channel, but it creates artifacts\n        shadowed_indices = mask[:, :, 0] == max_value\n        img_shadowed[shadowed_indices] = clip(\n            img_shadowed[shadowed_indices] * shadow_intensity,\n            np.uint8,\n        )\n\n    if needs_float:\n        return to_float(img_shadowed, max_value=max_value)\n\n    return img_shadowed\n</code></pre>"},{"location":"api_reference/augmentations/functional/#albumentations.augmentations.functional.add_snow","title":"<code>def add_snow    (img, snow_point, brightness_coeff)    </code> [view source on GitHub]","text":"<p>Bleaches out pixels, imitating snow.</p> <p>Parameters:</p> Name Type Description <code>img</code> <code>np.ndarray</code> <p>Input image.</p> <code>snow_point</code> <code>float</code> <p>A float in the range [0, 1], scaled and adjusted to determine the threshold for pixel modification.</p> <code>brightness_coeff</code> <code>float</code> <p>Coefficient applied to increase the brightness of pixels below the snow_point threshold. Larger values lead to more pronounced snow effects.</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Image with simulated snow effect.</p> <p>Reference</p> <p>https://github.com/UjjwalSaxena/Automold--Road-Augmentation-Library</p> Source code in <code>albumentations/augmentations/functional.py</code> Python<pre><code>@preserve_channel_dim\ndef add_snow(img: np.ndarray, snow_point: float, brightness_coeff: float) -&gt; np.ndarray:\n    \"\"\"Bleaches out pixels, imitating snow.\n\n    Args:\n        img (np.ndarray): Input image.\n        snow_point (float): A float in the range [0, 1], scaled and adjusted to determine\n            the threshold for pixel modification.\n        brightness_coeff (float): Coefficient applied to increase the brightness of pixels below the snow_point\n            threshold. Larger values lead to more pronounced snow effects.\n\n    Returns:\n        np.ndarray: Image with simulated snow effect.\n\n    Reference:\n        https://github.com/UjjwalSaxena/Automold--Road-Augmentation-Library\n\n    \"\"\"\n    non_rgb_warning(img)\n\n    input_dtype = img.dtype\n    needs_float = False\n\n    snow_point *= 127.5  # = 255 / 2\n    snow_point += 85  # = 255 / 3\n\n    if input_dtype == np.float32:\n        img = from_float(img, dtype=np.dtype(\"uint8\"))\n        needs_float = True\n\n    image_hls = cv2.cvtColor(img, cv2.COLOR_RGB2HLS)\n    image_hls = np.array(image_hls, dtype=np.float32)\n\n    image_hls[:, :, 1][image_hls[:, :, 1] &lt; snow_point] *= brightness_coeff\n\n    image_hls[:, :, 1] = clip(image_hls[:, :, 1], np.uint8)\n\n    image_hls = np.array(image_hls, dtype=np.uint8)\n\n    image_rgb = cv2.cvtColor(image_hls, cv2.COLOR_HLS2RGB)\n\n    return to_float(image_rgb, max_value=255) if needs_float else image_rgb\n</code></pre>"},{"location":"api_reference/augmentations/functional/#albumentations.augmentations.functional.add_sun_flare","title":"<code>def add_sun_flare    (img, flare_center, src_radius, src_color, circles)    </code> [view source on GitHub]","text":"<p>Add a sun flare effect to an image.</p> <p>Parameters:</p> Name Type Description <code>img</code> <code>np.ndarray</code> <p>The input image.</p> <code>flare_center</code> <code>tuple[float, float]</code> <p>(x, y) coordinates of the flare center</p> <code>src_radius</code> <code>int</code> <p>The radius of the source of the flare.</p> <code>src_color</code> <code>ColorType</code> <p>The color of the flare, represented as a tuple of RGB values.</p> <code>circles</code> <code>list[Any]</code> <p>A list of tuples, each representing a circle that contributes to the flare effect. Each tuple contains the alpha value, the center coordinates, the radius, and the color of the circle.</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>The output image with the sun flare effect added.</p> <p>Reference</p> <p>https://github.com/UjjwalSaxena/Automold--Road-Augmentation-Library</p> Source code in <code>albumentations/augmentations/functional.py</code> Python<pre><code>@preserve_channel_dim\ndef add_sun_flare(\n    img: np.ndarray,\n    flare_center: tuple[float, float],\n    src_radius: int,\n    src_color: ColorType,\n    circles: list[Any],\n) -&gt; np.ndarray:\n    \"\"\"Add a sun flare effect to an image.\n\n    Args:\n        img (np.ndarray): The input image.\n        flare_center (tuple[float, float]): (x, y) coordinates of the flare center\n        src_radius (int): The radius of the source of the flare.\n        src_color (ColorType): The color of the flare, represented as a tuple of RGB values.\n        circles (list[Any]): A list of tuples, each representing a circle that contributes to the flare effect.\n            Each tuple contains the alpha value, the center coordinates, the radius, and the color of the circle.\n\n    Returns:\n        np.ndarray: The output image with the sun flare effect added.\n\n    Reference:\n        https://github.com/UjjwalSaxena/Automold--Road-Augmentation-Library\n    \"\"\"\n    non_rgb_warning(img)\n\n    input_dtype = img.dtype\n    needs_float = False\n\n    if input_dtype == np.float32:\n        img = from_float(img, dtype=np.dtype(\"uint8\"))\n        needs_float = True\n\n    overlay = img.copy()\n    output = img.copy()\n\n    for alpha, (x, y), rad3, (r_color, g_color, b_color) in circles:\n        cv2.circle(overlay, (x, y), rad3, (r_color, g_color, b_color), -1)\n        output = add_weighted(overlay, alpha, output, 1 - alpha)\n\n    point = [int(x) for x in flare_center]\n\n    overlay = output.copy()\n    num_times = src_radius // 10\n    alpha = np.linspace(0.0, 1, num=num_times)\n    rad = np.linspace(1, src_radius, num=num_times)\n    for i in range(num_times):\n        cv2.circle(overlay, point, int(rad[i]), src_color, -1)\n        alp = alpha[num_times - i - 1] * alpha[num_times - i - 1] * alpha[num_times - i - 1]\n        output = add_weighted(overlay, alp, output, 1 - alp)\n\n    return to_float(output, max_value=255) if needs_float else output\n</code></pre>"},{"location":"api_reference/augmentations/functional/#albumentations.augmentations.functional.almost_equal_intervals","title":"<code>def almost_equal_intervals    (n, parts)    </code> [view source on GitHub]","text":"<p>Generates an array of nearly equal integer intervals that sum up to <code>n</code>.</p> <p>This function divides the number <code>n</code> into <code>parts</code> nearly equal parts. It ensures that the sum of all parts equals <code>n</code>, and the difference between any two parts is at most one. This is useful for distributing a total amount into nearly equal discrete parts.</p> <p>Parameters:</p> Name Type Description <code>n</code> <code>int</code> <p>The total value to be split.</p> <code>parts</code> <code>int</code> <p>The number of parts to split into.</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>An array of integers where each integer represents the size of a part.</p> <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; almost_equal_intervals(20, 3)\narray([7, 7, 6])  # Splits 20 into three parts: 7, 7, and 6\n&gt;&gt;&gt; almost_equal_intervals(16, 4)\narray([4, 4, 4, 4])  # Splits 16 into four equal parts\n</code></pre> Source code in <code>albumentations/augmentations/functional.py</code> Python<pre><code>def almost_equal_intervals(n: int, parts: int) -&gt; np.ndarray:\n    \"\"\"Generates an array of nearly equal integer intervals that sum up to `n`.\n\n    This function divides the number `n` into `parts` nearly equal parts. It ensures that\n    the sum of all parts equals `n`, and the difference between any two parts is at most one.\n    This is useful for distributing a total amount into nearly equal discrete parts.\n\n    Args:\n        n (int): The total value to be split.\n        parts (int): The number of parts to split into.\n\n    Returns:\n        np.ndarray: An array of integers where each integer represents the size of a part.\n\n    Example:\n        &gt;&gt;&gt; almost_equal_intervals(20, 3)\n        array([7, 7, 6])  # Splits 20 into three parts: 7, 7, and 6\n        &gt;&gt;&gt; almost_equal_intervals(16, 4)\n        array([4, 4, 4, 4])  # Splits 16 into four equal parts\n    \"\"\"\n    part_size, remainder = divmod(n, parts)\n    # Create an array with the base part size and adjust the first `remainder` parts by adding 1\n    return np.array([part_size + 1 if i &lt; remainder else part_size for i in range(parts)])\n</code></pre>"},{"location":"api_reference/augmentations/functional/#albumentations.augmentations.functional.bbox_from_mask","title":"<code>def bbox_from_mask    (mask)    </code> [view source on GitHub]","text":"<p>Create bounding box from binary mask (fast version)</p> <p>Parameters:</p> Name Type Description <code>mask</code> <code>numpy.ndarray</code> <p>binary mask.</p> <p>Returns:</p> Type Description <code>tuple</code> <p>A bounding box tuple <code>(x_min, y_min, x_max, y_max)</code>.</p> Source code in <code>albumentations/augmentations/functional.py</code> Python<pre><code>def bbox_from_mask(mask: np.ndarray) -&gt; tuple[int, int, int, int]:\n    \"\"\"Create bounding box from binary mask (fast version)\n\n    Args:\n        mask (numpy.ndarray): binary mask.\n\n    Returns:\n        tuple: A bounding box tuple `(x_min, y_min, x_max, y_max)`.\n\n    \"\"\"\n    rows = np.any(mask, axis=1)\n    if not rows.any():\n        return -1, -1, -1, -1\n    cols = np.any(mask, axis=0)\n    y_min, y_max = np.where(rows)[0][[0, -1]]\n    x_min, x_max = np.where(cols)[0][[0, -1]]\n    return x_min, y_min, x_max + 1, y_max + 1\n</code></pre>"},{"location":"api_reference/augmentations/functional/#albumentations.augmentations.functional.center","title":"<code>def center    (image_shape)    </code> [view source on GitHub]","text":"<p>Calculate the center coordinates if image. Used by images, masks and keypoints.</p> <p>Parameters:</p> Name Type Description <code>image_shape</code> <code>tuple[int, int]</code> <p>The shape of the image.</p> <p>Returns:</p> Type Description <code>tuple[float, float]</code> <p>The center coordinates.</p> Source code in <code>albumentations/augmentations/functional.py</code> Python<pre><code>def center(image_shape: tuple[int, int]) -&gt; tuple[float, float]:\n    \"\"\"Calculate the center coordinates if image. Used by images, masks and keypoints.\n\n    Args:\n        image_shape (tuple[int, int]): The shape of the image.\n\n    Returns:\n        tuple[float, float]: The center coordinates.\n    \"\"\"\n    height, width = image_shape[:2]\n    return width / 2 - 0.5, height / 2 - 0.5\n</code></pre>"},{"location":"api_reference/augmentations/functional/#albumentations.augmentations.functional.center_bbox","title":"<code>def center_bbox    (image_shape)    </code> [view source on GitHub]","text":"<p>Calculate the center coordinates for of image for bounding boxes.</p> <p>Parameters:</p> Name Type Description <code>image_shape</code> <code>tuple[int, int]</code> <p>The shape of the image.</p> <p>Returns:</p> Type Description <code>tuple[float, float]</code> <p>The center coordinates.</p> Source code in <code>albumentations/augmentations/functional.py</code> Python<pre><code>def center_bbox(image_shape: tuple[int, int]) -&gt; tuple[float, float]:\n    \"\"\"Calculate the center coordinates for of image for bounding boxes.\n\n    Args:\n        image_shape (tuple[int, int]): The shape of the image.\n\n    Returns:\n        tuple[float, float]: The center coordinates.\n    \"\"\"\n    height, width = image_shape[:2]\n    return width / 2, height / 2\n</code></pre>"},{"location":"api_reference/augmentations/functional/#albumentations.augmentations.functional.create_shape_groups","title":"<code>def create_shape_groups    (tiles)    </code> [view source on GitHub]","text":"<p>Groups tiles by their shape and stores the indices for each shape.</p> Source code in <code>albumentations/augmentations/functional.py</code> Python<pre><code>def create_shape_groups(tiles: np.ndarray) -&gt; dict[tuple[int, int], list[int]]:\n    \"\"\"Groups tiles by their shape and stores the indices for each shape.\"\"\"\n    shape_groups = defaultdict(list)\n    for index, (start_y, start_x, end_y, end_x) in enumerate(tiles):\n        shape = (end_y - start_y, end_x - start_x)\n        shape_groups[shape].append(index)\n    return shape_groups\n</code></pre>"},{"location":"api_reference/augmentations/functional/#albumentations.augmentations.functional.fancy_pca","title":"<code>def fancy_pca    (img, alpha=0.1)    </code> [view source on GitHub]","text":"<p>Perform 'Fancy PCA' augmentation</p> <p>Parameters:</p> Name Type Description <code>img</code> <code>np.ndarray</code> <p>numpy array with (h, w, rgb) shape, as ints between 0-255</p> <code>alpha</code> <code>float</code> <p>how much to perturb/scale the eigen vectors and values     the paper used std=0.1</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>numpy image-like array as uint8 range(0, 255)</p> <p>Reference</p> <p>http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf</p> Source code in <code>albumentations/augmentations/functional.py</code> Python<pre><code>@clipped\ndef fancy_pca(img: np.ndarray, alpha: float = 0.1) -&gt; np.ndarray:\n    \"\"\"Perform 'Fancy PCA' augmentation\n\n    Args:\n        img: numpy array with (h, w, rgb) shape, as ints between 0-255\n        alpha: how much to perturb/scale the eigen vectors and values\n                the paper used std=0.1\n\n    Returns:\n        numpy image-like array as uint8 range(0, 255)\n\n    Reference:\n        http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf\n    \"\"\"\n    if not is_rgb_image(img) or img.dtype != np.uint8:\n        msg = \"Image must be RGB image in uint8 format.\"\n        raise TypeError(msg)\n\n    orig_img = img.astype(float).copy()\n\n    img = to_float(img)  # rescale to 0 to 1 range\n\n    # flatten image to columns of RGB\n    img_rs = img.reshape(-1, 3)\n    # img_rs shape (640000, 3)\n\n    # center mean\n    img_centered = img_rs - np.mean(img_rs, axis=0)\n\n    # paper says 3x3 covariance matrix\n    img_cov = np.cov(img_centered, rowvar=False)\n\n    # eigen values and eigen vectors\n    eig_vals, eig_vecs = np.linalg.eigh(img_cov)\n\n    # sort values and vector\n    sort_perm = eig_vals[::-1].argsort()\n    eig_vals[::-1].sort()\n    eig_vecs = eig_vecs[:, sort_perm]\n\n    # &gt; get [p1, p2, p3]\n    m1 = np.column_stack(eig_vecs)\n\n    # get 3x1 matrix of eigen values multiplied by random variable draw from normal\n    # distribution with mean of 0 and standard deviation of 0.1\n    m2 = np.zeros((3, 1))\n    # according to the paper alpha should only be draw once per augmentation (not once per channel)\n    # &gt; alpha = np.random.normal(0, alpha_std)\n\n    # broad cast to speed things up\n    m2[:, 0] = alpha * eig_vals[:]\n\n    # this is the vector that we're going to add to each pixel in a moment\n    add_vect = np.array(m1) @ np.array(m2)\n\n    for idx in range(3):  # RGB\n        orig_img[..., idx] += add_vect[idx] * 255\n\n    # for image processing it was found that working with float 0.0 to 1.0\n    # was easier than integers between 0-255\n    return orig_img\n</code></pre>"},{"location":"api_reference/augmentations/functional/#albumentations.augmentations.functional.generate_shuffled_splits","title":"<code>def generate_shuffled_splits    (size, divisions, random_state=None)    </code> [view source on GitHub]","text":"<p>Generate shuffled splits for a given dimension size and number of divisions.</p> <p>Parameters:</p> Name Type Description <code>size</code> <code>int</code> <p>Total size of the dimension (height or width).</p> <code>divisions</code> <code>int</code> <p>Number of divisions (rows or columns).</p> <code>random_state</code> <code>Optional[np.random.RandomState]</code> <p>Seed for the random number generator for reproducibility.</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Cumulative edges of the shuffled intervals.</p> Source code in <code>albumentations/augmentations/functional.py</code> Python<pre><code>def generate_shuffled_splits(\n    size: int,\n    divisions: int,\n    random_state: np.random.RandomState | None = None,\n) -&gt; np.ndarray:\n    \"\"\"Generate shuffled splits for a given dimension size and number of divisions.\n\n    Args:\n        size (int): Total size of the dimension (height or width).\n        divisions (int): Number of divisions (rows or columns).\n        random_state (Optional[np.random.RandomState]): Seed for the random number generator for reproducibility.\n\n    Returns:\n        np.ndarray: Cumulative edges of the shuffled intervals.\n    \"\"\"\n    intervals = almost_equal_intervals(size, divisions)\n    intervals = random_utils.shuffle(intervals, random_state=random_state)\n    return np.insert(np.cumsum(intervals), 0, 0)\n</code></pre>"},{"location":"api_reference/augmentations/functional/#albumentations.augmentations.functional.grayscale_to_multichannel","title":"<code>def grayscale_to_multichannel    (grayscale_image, num_output_channels=3)    </code> [view source on GitHub]","text":"<p>Convert a grayscale image to a multi-channel image.</p> <p>This function takes a 2D grayscale image or a 3D image with a single channel and converts it to a multi-channel image by repeating the grayscale data across the specified number of channels.</p> <p>Parameters:</p> Name Type Description <code>grayscale_image</code> <code>np.ndarray</code> <p>Input grayscale image. Can be 2D (height, width)                           or 3D (height, width, 1).</p> <code>num_output_channels</code> <code>int</code> <p>Number of channels in the output image. Defaults to 3.</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Multi-channel image with shape (height, width, num_channels).</p> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If the input is not a 2D grayscale image or 3D with shape (height, width, 1).</p> <p>Note</p> <p>If the input is already a multi-channel image with the desired number of channels, it will be returned unchanged.</p> Source code in <code>albumentations/augmentations/functional.py</code> Python<pre><code>def grayscale_to_multichannel(grayscale_image: np.ndarray, num_output_channels: int = 3) -&gt; np.ndarray:\n    \"\"\"Convert a grayscale image to a multi-channel image.\n\n    This function takes a 2D grayscale image or a 3D image with a single channel\n    and converts it to a multi-channel image by repeating the grayscale data\n    across the specified number of channels.\n\n    Args:\n        grayscale_image (np.ndarray): Input grayscale image. Can be 2D (height, width)\n                                      or 3D (height, width, 1).\n        num_output_channels (int, optional): Number of channels in the output image. Defaults to 3.\n\n    Returns:\n        np.ndarray: Multi-channel image with shape (height, width, num_channels).\n\n    Raises:\n        ValueError: If the input is not a 2D grayscale image or 3D with shape (height, width, 1).\n\n    Note:\n        If the input is already a multi-channel image with the desired number of channels,\n        it will be returned unchanged.\n    \"\"\"\n    grayscale_image = grayscale_image.copy().squeeze()\n    return np.stack([grayscale_image] * num_output_channels, axis=-1)\n</code></pre>"},{"location":"api_reference/augmentations/functional/#albumentations.augmentations.functional.iso_noise","title":"<code>def iso_noise    (image, color_shift=0.05, intensity=0.5, random_state=None)    </code> [view source on GitHub]","text":"<p>Apply poisson noise to an image to simulate camera sensor noise.</p> <p>Parameters:</p> Name Type Description <code>image</code> <code>np.ndarray</code> <p>Input image. Currently, only RGB images are supported.</p> <code>color_shift</code> <code>float</code> <p>The amount of color shift to apply. Default is 0.05.</p> <code>intensity</code> <code>float</code> <p>Multiplication factor for noise values. Values of ~0.5 produce a noticeable,                yet acceptable level of noise. Default is 0.5.</p> <code>random_state</code> <code>Optional[np.random.RandomState]</code> <p>If specified, this will be random state used for noise generation.</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>The noised image.</p> <p>Exceptions:</p> Type Description <code>TypeError</code> <p>If the input image's dtype is not RGB.</p> Source code in <code>albumentations/augmentations/functional.py</code> Python<pre><code>@clipped\ndef iso_noise(\n    image: np.ndarray,\n    color_shift: float = 0.05,\n    intensity: float = 0.5,\n    random_state: np.random.RandomState | None = None,\n) -&gt; np.ndarray:\n    \"\"\"Apply poisson noise to an image to simulate camera sensor noise.\n\n    Args:\n        image (np.ndarray): Input image. Currently, only RGB images are supported.\n        color_shift (float): The amount of color shift to apply. Default is 0.05.\n        intensity (float): Multiplication factor for noise values. Values of ~0.5 produce a noticeable,\n                           yet acceptable level of noise. Default is 0.5.\n        random_state (Optional[np.random.RandomState]): If specified, this will be random state used\n            for noise generation.\n\n    Returns:\n        np.ndarray: The noised image.\n\n    Raises:\n        TypeError: If the input image's dtype is not RGB.\n    \"\"\"\n    if not is_rgb_image(image):\n        msg = \"Image must be RGB\"\n        raise TypeError(msg)\n\n    input_dtype = image.dtype\n    factor = 1\n\n    if input_dtype == np.uint8:\n        image = to_float(image)\n        factor = MAX_VALUES_BY_DTYPE[input_dtype]\n\n    hls = cv2.cvtColor(image, cv2.COLOR_RGB2HLS)\n    _, stddev = cv2.meanStdDev(hls)\n\n    luminance_noise = random_utils.poisson(stddev[1] * intensity * 255, size=hls.shape[:2], random_state=random_state)\n    color_noise = random_utils.normal(0, color_shift * 360 * intensity, size=hls.shape[:2], random_state=random_state)\n\n    hue = hls[..., 0]\n    hue += color_noise\n    hue %= 360\n\n    luminance = hls[..., 1]\n    luminance += (luminance_noise / 255) * (1.0 - luminance)\n\n    return cv2.cvtColor(hls, cv2.COLOR_HLS2RGB) * factor\n</code></pre>"},{"location":"api_reference/augmentations/functional/#albumentations.augmentations.functional.mask_from_bbox","title":"<code>def mask_from_bbox    (img, bbox)    </code> [view source on GitHub]","text":"<p>Create binary mask from bounding box</p> <p>Parameters:</p> Name Type Description <code>img</code> <code>np.ndarray</code> <p>input image</p> <code>bbox</code> <code>tuple[int, int, int, int]</code> <p>A bounding box tuple <code>(x_min, y_min, x_max, y_max)</code></p> <p>Returns:</p> Type Description <code>mask</code> <p>binary mask</p> Source code in <code>albumentations/augmentations/functional.py</code> Python<pre><code>def mask_from_bbox(img: np.ndarray, bbox: tuple[int, int, int, int]) -&gt; np.ndarray:\n    \"\"\"Create binary mask from bounding box\n\n    Args:\n        img: input image\n        bbox: A bounding box tuple `(x_min, y_min, x_max, y_max)`\n\n    Returns:\n        mask: binary mask\n\n    \"\"\"\n    mask = np.zeros(img.shape[:2], dtype=np.uint8)\n    x_min, y_min, x_max, y_max = bbox\n    mask[y_min:y_max, x_min:x_max] = 1\n    return mask\n</code></pre>"},{"location":"api_reference/augmentations/functional/#albumentations.augmentations.functional.move_tone_curve","title":"<code>def move_tone_curve    (img, low_y, high_y)    </code> [view source on GitHub]","text":"<p>Rescales the relationship between bright and dark areas of the image by manipulating its tone curve.</p> <p>Parameters:</p> Name Type Description <code>img</code> <code>np.ndarray</code> <p>np.ndarray. Any number of channels</p> <code>low_y</code> <code>float | np.ndarray</code> <p>per-channel or single y-position of a Bezier control point used to adjust the tone curve, must be in range [0, 1]</p> <code>high_y</code> <code>float | np.ndarray</code> <p>per-channel or single y-position of a Bezier control point used to adjust image tone curve, must be in range [0, 1]</p> Source code in <code>albumentations/augmentations/functional.py</code> Python<pre><code>@preserve_channel_dim\ndef move_tone_curve(\n    img: np.ndarray,\n    low_y: float | np.ndarray,\n    high_y: float | np.ndarray,\n) -&gt; np.ndarray:\n    \"\"\"Rescales the relationship between bright and dark areas of the image by manipulating its tone curve.\n\n    Args:\n        img: np.ndarray. Any number of channels\n        low_y: per-channel or single y-position of a Bezier control point used\n            to adjust the tone curve, must be in range [0, 1]\n        high_y: per-channel or single y-position of a Bezier control point used\n            to adjust image tone curve, must be in range [0, 1]\n\n    \"\"\"\n    input_dtype = img.dtype\n    needs_float = False\n\n    if input_dtype in [np.float32, np.float64, np.float16]:\n        img = from_float(img, dtype=np.uint8)\n        needs_float = True\n\n    t = np.linspace(0.0, 1.0, 256)\n\n    def evaluate_bez(t: np.ndarray, low_y: float | np.ndarray, high_y: float | np.ndarray) -&gt; np.ndarray:\n        one_minus_t = 1 - t\n        return (3 * one_minus_t**2 * t * low_y + 3 * one_minus_t * t**2 * high_y + t**3) * 255\n\n    num_channels = get_num_channels(img)\n\n    if np.isscalar(low_y) and np.isscalar(high_y):\n        lut = clip(np.rint(evaluate_bez(t, low_y, high_y)), np.uint8)\n        output = cv2.LUT(img, lut)\n    elif isinstance(low_y, np.ndarray) and isinstance(high_y, np.ndarray):\n        luts = clip(np.rint(evaluate_bez(t[:, np.newaxis], low_y, high_y).T), np.uint8)\n        output = cv2.merge([cv2.LUT(img[:, :, i], luts[i]) for i in range(num_channels)])\n    else:\n        raise TypeError(\n            f\"low_y and high_y must both be of type float or np.ndarray. Got {type(low_y)} and {type(high_y)}\",\n        )\n\n    return to_float(output, max_value=255) if needs_float else output\n</code></pre>"},{"location":"api_reference/augmentations/functional/#albumentations.augmentations.functional.posterize","title":"<code>def posterize    (img, bits)    </code> [view source on GitHub]","text":"<p>Reduce the number of bits for each color channel.</p> <p>Parameters:</p> Name Type Description <code>img</code> <code>np.ndarray</code> <p>image to posterize.</p> <code>bits</code> <code>int</code> <p>number of high bits. Must be in range [0, 8]</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Image with reduced color channels.</p> Source code in <code>albumentations/augmentations/functional.py</code> Python<pre><code>@preserve_channel_dim\ndef posterize(img: np.ndarray, bits: int) -&gt; np.ndarray:\n    \"\"\"Reduce the number of bits for each color channel.\n\n    Args:\n        img: image to posterize.\n        bits: number of high bits. Must be in range [0, 8]\n\n    Returns:\n        Image with reduced color channels.\n\n    \"\"\"\n    bits_array = np.uint8(bits)\n\n    if img.dtype != np.uint8:\n        msg = \"Image must have uint8 channel type\"\n        raise TypeError(msg)\n    if np.any((bits_array &lt; 0) | (bits_array &gt; EIGHT)):\n        msg = \"bits must be in range [0, 8]\"\n        raise ValueError(msg)\n\n    if not bits_array.shape or len(bits_array) == 1:\n        if bits_array == 0:\n            return np.zeros_like(img)\n        if bits_array == EIGHT:\n            return img.copy()\n\n        lut = np.arange(0, 256, dtype=np.uint8)\n        mask = ~np.uint8(2 ** (8 - bits_array) - 1)\n        lut &amp;= mask\n\n        return cv2.LUT(img, lut)\n\n    if not is_rgb_image(img):\n        msg = \"If bits is iterable image must be RGB\"\n        raise TypeError(msg)\n\n    result_img = np.empty_like(img)\n    for i, channel_bits in enumerate(bits_array):\n        if channel_bits == 0:\n            result_img[..., i] = np.zeros_like(img[..., i])\n        elif channel_bits == EIGHT:\n            result_img[..., i] = img[..., i].copy()\n        else:\n            lut = np.arange(0, 256, dtype=np.uint8)\n            mask = ~np.uint8(2 ** (8 - channel_bits) - 1)\n            lut &amp;= mask\n\n            result_img[..., i] = cv2.LUT(img[..., i], lut)\n\n    return result_img\n</code></pre>"},{"location":"api_reference/augmentations/functional/#albumentations.augmentations.functional.shuffle_tiles_within_shape_groups","title":"<code>def shuffle_tiles_within_shape_groups    (shape_groups, random_state=None)    </code> [view source on GitHub]","text":"<p>Shuffles indices within each group of similar shapes and creates a list where each index points to the index of the tile it should be mapped to.</p> <p>Parameters:</p> Name Type Description <code>shape_groups</code> <code>dict[tuple[int, int], list[int]]</code> <p>Groups of tile indices categorized by shape.</p> <code>random_state</code> <code>Optional[np.random.RandomState]</code> <p>Seed for the random number generator for reproducibility.</p> <p>Returns:</p> Type Description <code>list[int]</code> <p>A list where each index is mapped to the new index of the tile after shuffling.</p> Source code in <code>albumentations/augmentations/functional.py</code> Python<pre><code>def shuffle_tiles_within_shape_groups(\n    shape_groups: dict[tuple[int, int], list[int]],\n    random_state: np.random.RandomState | None = None,\n) -&gt; list[int]:\n    \"\"\"Shuffles indices within each group of similar shapes and creates a list where each\n    index points to the index of the tile it should be mapped to.\n\n    Args:\n        shape_groups (dict[tuple[int, int], list[int]]): Groups of tile indices categorized by shape.\n        random_state (Optional[np.random.RandomState]): Seed for the random number generator for reproducibility.\n\n    Returns:\n        list[int]: A list where each index is mapped to the new index of the tile after shuffling.\n    \"\"\"\n    # Initialize the output list with the same size as the total number of tiles, filled with -1\n    num_tiles = sum(len(indices) for indices in shape_groups.values())\n    mapping = [-1] * num_tiles\n\n    # Prepare the random number generator\n\n    for indices in shape_groups.values():\n        shuffled_indices = random_utils.shuffle(indices.copy(), random_state=random_state)\n        for old, new in zip(indices, shuffled_indices):\n            mapping[old] = new\n\n    return mapping\n</code></pre>"},{"location":"api_reference/augmentations/functional/#albumentations.augmentations.functional.solarize","title":"<code>def solarize    (img, threshold=128)    </code> [view source on GitHub]","text":"<p>Invert all pixel values above a threshold.</p> <p>Parameters:</p> Name Type Description <code>img</code> <code>np.ndarray</code> <p>The image to solarize.</p> <code>threshold</code> <code>int</code> <p>All pixels above this grayscale level are inverted.</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Solarized image.</p> Source code in <code>albumentations/augmentations/functional.py</code> Python<pre><code>def solarize(img: np.ndarray, threshold: int = 128) -&gt; np.ndarray:\n    \"\"\"Invert all pixel values above a threshold.\n\n    Args:\n        img: The image to solarize.\n        threshold: All pixels above this grayscale level are inverted.\n\n    Returns:\n        Solarized image.\n\n    \"\"\"\n    dtype = img.dtype\n    max_val = MAX_VALUES_BY_DTYPE[dtype]\n\n    if dtype == np.uint8:\n        lut = [(i if i &lt; threshold else max_val - i) for i in range(int(max_val) + 1)]\n\n        prev_shape = img.shape\n        img = cv2.LUT(img, np.array(lut, dtype=dtype))\n\n        if len(prev_shape) != len(img.shape):\n            img = np.expand_dims(img, -1)\n        return img\n\n    result_img = img.copy()\n    cond = img &gt;= threshold\n    result_img[cond] = max_val - result_img[cond]\n    return result_img\n</code></pre>"},{"location":"api_reference/augmentations/functional/#albumentations.augmentations.functional.split_uniform_grid","title":"<code>def split_uniform_grid    (image_shape, grid, random_state=None)    </code> [view source on GitHub]","text":"<p>Splits an image shape into a uniform grid specified by the grid dimensions.</p> <p>Parameters:</p> Name Type Description <code>image_shape</code> <code>tuple[int, int]</code> <p>The shape of the image as (height, width).</p> <code>grid</code> <code>tuple[int, int]</code> <p>The grid size as (rows, columns).</p> <code>random_state</code> <code>Optional[np.random.RandomState]</code> <p>The random state to use for shuffling the splits. If None, the splits are not shuffled.</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>An array containing the tiles' coordinates in the format (start_y, start_x, end_y, end_x).</p> <p>Note</p> <p>The function uses <code>generate_shuffled_splits</code> to generate the splits for the height and width of the image. The splits are then used to calculate the coordinates of the tiles.</p> Source code in <code>albumentations/augmentations/functional.py</code> Python<pre><code>def split_uniform_grid(\n    image_shape: tuple[int, int],\n    grid: tuple[int, int],\n    random_state: np.random.RandomState | None = None,\n) -&gt; np.ndarray:\n    \"\"\"Splits an image shape into a uniform grid specified by the grid dimensions.\n\n    Args:\n        image_shape (tuple[int, int]): The shape of the image as (height, width).\n        grid (tuple[int, int]): The grid size as (rows, columns).\n        random_state (Optional[np.random.RandomState]): The random state to use for shuffling the splits.\n            If None, the splits are not shuffled.\n\n    Returns:\n        np.ndarray: An array containing the tiles' coordinates in the format (start_y, start_x, end_y, end_x).\n\n    Note:\n        The function uses `generate_shuffled_splits` to generate the splits for the height and width of the image.\n        The splits are then used to calculate the coordinates of the tiles.\n    \"\"\"\n    n_rows, n_cols = grid\n\n    height_splits = generate_shuffled_splits(image_shape[0], grid[0], random_state)\n    width_splits = generate_shuffled_splits(image_shape[1], grid[1], random_state)\n\n    # Calculate tiles coordinates\n    tiles = [\n        (height_splits[i], width_splits[j], height_splits[i + 1], width_splits[j + 1])\n        for i in range(n_rows)\n        for j in range(n_cols)\n    ]\n\n    return np.array(tiles)\n</code></pre>"},{"location":"api_reference/augmentations/functional/#albumentations.augmentations.functional.swap_tiles_on_image","title":"<code>def swap_tiles_on_image    (image, tiles, mapping=None)    </code> [view source on GitHub]","text":"<p>Swap tiles on the image according to the new format.</p> <p>Parameters:</p> Name Type Description <code>image</code> <code>np.ndarray</code> <p>Input image.</p> <code>tiles</code> <code>np.ndarray</code> <p>Array of tiles with each tile as [start_y, start_x, end_y, end_x].</p> <code>mapping</code> <code>list[int] | None</code> <p>list of new tile indices.</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Output image with tiles swapped according to the random shuffle.</p> Source code in <code>albumentations/augmentations/functional.py</code> Python<pre><code>def swap_tiles_on_image(image: np.ndarray, tiles: np.ndarray, mapping: list[int] | None = None) -&gt; np.ndarray:\n    \"\"\"Swap tiles on the image according to the new format.\n\n    Args:\n        image: Input image.\n        tiles: Array of tiles with each tile as [start_y, start_x, end_y, end_x].\n        mapping: list of new tile indices.\n\n    Returns:\n        np.ndarray: Output image with tiles swapped according to the random shuffle.\n    \"\"\"\n    # If no tiles are provided, return a copy of the original image\n    if tiles.size == 0 or mapping is None:\n        return image.copy()\n\n    # Create a copy of the image to retain original for reference\n    new_image = np.empty_like(image)\n    for num, new_index in enumerate(mapping):\n        start_y, start_x, end_y, end_x = tiles[new_index]\n        start_y_orig, start_x_orig, end_y_orig, end_x_orig = tiles[num]\n        # Assign the corresponding tile from the original image to the new image\n        new_image[start_y:end_y, start_x:end_x] = image[start_y_orig:end_y_orig, start_x_orig:end_x_orig]\n\n    return new_image\n</code></pre>"},{"location":"api_reference/augmentations/functional/#albumentations.augmentations.functional.to_gray_average","title":"<code>def to_gray_average    (img)    </code> [view source on GitHub]","text":"<p>Convert an image to grayscale using the average method.</p> <p>This function computes the arithmetic mean across all channels for each pixel, resulting in a grayscale representation of the image.</p> <p>Key aspects of this method: 1. It treats all channels equally, regardless of their perceptual importance. 2. Works with any number of channels, making it versatile for various image types. 3. Simple and fast to compute, but may not accurately represent perceived brightness. 4. For RGB images, the formula is: Gray = (R + G + B) / 3</p> <p>Note: This method may produce different results compared to weighted methods (like RGB weighted average) which account for human perception of color brightness. It may also produce unexpected results for images with alpha channels or non-color data in additional channels.</p> <p>Parameters:</p> Name Type Description <code>img</code> <code>np.ndarray</code> <p>Input image as a numpy array. Can be any number of channels.</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Grayscale image as a 2D numpy array. The output data type             matches the input data type.</p> <p>Image types:     uint8, float32</p> <p>Number of channels:     any</p> Source code in <code>albumentations/augmentations/functional.py</code> Python<pre><code>def to_gray_average(img: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Convert an image to grayscale using the average method.\n\n    This function computes the arithmetic mean across all channels for each pixel,\n    resulting in a grayscale representation of the image.\n\n    Key aspects of this method:\n    1. It treats all channels equally, regardless of their perceptual importance.\n    2. Works with any number of channels, making it versatile for various image types.\n    3. Simple and fast to compute, but may not accurately represent perceived brightness.\n    4. For RGB images, the formula is: Gray = (R + G + B) / 3\n\n    Note: This method may produce different results compared to weighted methods\n    (like RGB weighted average) which account for human perception of color brightness.\n    It may also produce unexpected results for images with alpha channels or\n    non-color data in additional channels.\n\n    Args:\n        img (np.ndarray): Input image as a numpy array. Can be any number of channels.\n\n    Returns:\n        np.ndarray: Grayscale image as a 2D numpy array. The output data type\n                    matches the input data type.\n\n    Image types:\n        uint8, float32\n\n    Number of channels:\n        any\n    \"\"\"\n    return np.mean(img, axis=-1).astype(img.dtype)\n</code></pre>"},{"location":"api_reference/augmentations/functional/#albumentations.augmentations.functional.to_gray_desaturation","title":"<code>def to_gray_desaturation    (img)    </code> [view source on GitHub]","text":"<p>Convert an image to grayscale using the desaturation method.</p> <p>Parameters:</p> Name Type Description <code>img</code> <code>np.ndarray</code> <p>Input image as a numpy array.</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Grayscale image as a 2D numpy array.</p> <p>Image types:     uint8, float32</p> <p>Number of channels:     any</p> Source code in <code>albumentations/augmentations/functional.py</code> Python<pre><code>@clipped\ndef to_gray_desaturation(img: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Convert an image to grayscale using the desaturation method.\n\n    Args:\n        img (np.ndarray): Input image as a numpy array.\n\n    Returns:\n        np.ndarray: Grayscale image as a 2D numpy array.\n\n    Image types:\n        uint8, float32\n\n    Number of channels:\n        any\n    \"\"\"\n    float_image = img.astype(np.float32)\n    return (np.max(float_image, axis=-1) + np.min(float_image, axis=-1)) / 2\n</code></pre>"},{"location":"api_reference/augmentations/functional/#albumentations.augmentations.functional.to_gray_from_lab","title":"<code>def to_gray_from_lab    (img)    </code> [view source on GitHub]","text":"<p>Convert an RGB image to grayscale using the L channel from the LAB color space.</p> <p>This function converts the RGB image to the LAB color space and extracts the L channel. The LAB color space is designed to approximate human vision, where L represents lightness.</p> <p>Key aspects of this method: 1. The L channel represents the lightness of each pixel, ranging from 0 (black) to 100 (white). 2. It's more perceptually uniform than RGB, meaning equal changes in L values correspond to    roughly equal changes in perceived lightness. 3. The L channel is independent of the color information (A and B channels), making it    suitable for grayscale conversion.</p> <p>This method can be particularly useful when you want a grayscale image that closely matches human perception of lightness, potentially preserving more perceived contrast than simple RGB-based methods.</p> <p>Parameters:</p> Name Type Description <code>img</code> <code>np.ndarray</code> <p>Input RGB image as a numpy array.</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Grayscale image as a 2D numpy array, representing the L (lightness) channel.             Values are scaled to match the input image's data type range.</p> <p>Image types:     uint8, float32</p> <p>Number of channels:     3</p> Source code in <code>albumentations/augmentations/functional.py</code> Python<pre><code>@clipped\ndef to_gray_from_lab(img: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Convert an RGB image to grayscale using the L channel from the LAB color space.\n\n    This function converts the RGB image to the LAB color space and extracts the L channel.\n    The LAB color space is designed to approximate human vision, where L represents lightness.\n\n    Key aspects of this method:\n    1. The L channel represents the lightness of each pixel, ranging from 0 (black) to 100 (white).\n    2. It's more perceptually uniform than RGB, meaning equal changes in L values correspond to\n       roughly equal changes in perceived lightness.\n    3. The L channel is independent of the color information (A and B channels), making it\n       suitable for grayscale conversion.\n\n    This method can be particularly useful when you want a grayscale image that closely\n    matches human perception of lightness, potentially preserving more perceived contrast\n    than simple RGB-based methods.\n\n    Args:\n        img (np.ndarray): Input RGB image as a numpy array.\n\n    Returns:\n        np.ndarray: Grayscale image as a 2D numpy array, representing the L (lightness) channel.\n                    Values are scaled to match the input image's data type range.\n\n    Image types:\n        uint8, float32\n\n    Number of channels:\n        3\n    \"\"\"\n    dtype = img.dtype\n    img_uint8 = from_float(img, dtype=np.uint8) if dtype == np.float32 else img\n    result = cv2.cvtColor(img_uint8, cv2.COLOR_RGB2LAB)[..., 0]\n\n    return to_float(result) if dtype == np.float32 else result\n</code></pre>"},{"location":"api_reference/augmentations/functional/#albumentations.augmentations.functional.to_gray_max","title":"<code>def to_gray_max    (img)    </code> [view source on GitHub]","text":"<p>Convert an image to grayscale using the maximum channel value method.</p> <p>This function takes the maximum value across all channels for each pixel, resulting in a grayscale image that preserves the brightest parts of the original image.</p> <p>Key aspects of this method: 1. Works with any number of channels, making it versatile for various image types. 2. For 3-channel (e.g., RGB) images, this method is equivalent to extracting the V (Value)    channel from the HSV color space. 3. Preserves the brightest parts of the image but may lose some color contrast information. 4. Simple and fast to compute.</p> <p>Note: - This method tends to produce brighter grayscale images compared to other conversion methods,   as it always selects the highest intensity value from the channels. - For RGB images, it may not accurately represent perceived brightness as it doesn't   account for human color perception.</p> <p>Parameters:</p> Name Type Description <code>img</code> <code>np.ndarray</code> <p>Input image as a numpy array. Can be any number of channels.</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Grayscale image as a 2D numpy array. The output data type             matches the input data type.</p> <p>Image types:     uint8, float32</p> <p>Number of channels:     any</p> Source code in <code>albumentations/augmentations/functional.py</code> Python<pre><code>def to_gray_max(img: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Convert an image to grayscale using the maximum channel value method.\n\n    This function takes the maximum value across all channels for each pixel,\n    resulting in a grayscale image that preserves the brightest parts of the original image.\n\n    Key aspects of this method:\n    1. Works with any number of channels, making it versatile for various image types.\n    2. For 3-channel (e.g., RGB) images, this method is equivalent to extracting the V (Value)\n       channel from the HSV color space.\n    3. Preserves the brightest parts of the image but may lose some color contrast information.\n    4. Simple and fast to compute.\n\n    Note:\n    - This method tends to produce brighter grayscale images compared to other conversion methods,\n      as it always selects the highest intensity value from the channels.\n    - For RGB images, it may not accurately represent perceived brightness as it doesn't\n      account for human color perception.\n\n    Args:\n        img (np.ndarray): Input image as a numpy array. Can be any number of channels.\n\n    Returns:\n        np.ndarray: Grayscale image as a 2D numpy array. The output data type\n                    matches the input data type.\n\n    Image types:\n        uint8, float32\n\n    Number of channels:\n        any\n    \"\"\"\n    return np.max(img, axis=-1)\n</code></pre>"},{"location":"api_reference/augmentations/functional/#albumentations.augmentations.functional.to_gray_pca","title":"<code>def to_gray_pca    (img)    </code> [view source on GitHub]","text":"<p>Convert an image to grayscale using Principal Component Analysis (PCA).</p> <p>This function applies PCA to reduce a multi-channel image to a single channel, effectively creating a grayscale representation that captures the maximum variance in the color data.</p> <p>Parameters:</p> Name Type Description <code>img</code> <code>np.ndarray</code> <p>Input image as a numpy array with shape (height, width, channels).</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Grayscale image as a 2D numpy array with shape (height, width).             If input is uint8, output is uint8 in range [0, 255].             If input is float32, output is float32 in range [0, 1].</p> <p>Note</p> <p>This method can potentially preserve more information from the original image compared to standard weighted average methods, as it accounts for the correlations between color channels.</p> <p>Image types:     uint8, float32</p> <p>Number of channels:     any</p> Source code in <code>albumentations/augmentations/functional.py</code> Python<pre><code>@clipped\ndef to_gray_pca(img: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Convert an image to grayscale using Principal Component Analysis (PCA).\n\n    This function applies PCA to reduce a multi-channel image to a single channel,\n    effectively creating a grayscale representation that captures the maximum variance\n    in the color data.\n\n    Args:\n        img (np.ndarray): Input image as a numpy array with shape (height, width, channels).\n\n    Returns:\n        np.ndarray: Grayscale image as a 2D numpy array with shape (height, width).\n                    If input is uint8, output is uint8 in range [0, 255].\n                    If input is float32, output is float32 in range [0, 1].\n\n    Note:\n        This method can potentially preserve more information from the original image\n        compared to standard weighted average methods, as it accounts for the\n        correlations between color channels.\n\n    Image types:\n        uint8, float32\n\n    Number of channels:\n        any\n    \"\"\"\n    dtype = img.dtype\n    # Reshape the image to a 2D array of pixels\n    pixels = img.reshape(-1, img.shape[2])\n\n    # Perform PCA\n    pca = PCA(n_components=1)\n    pca_result = pca.fit_transform(pixels)\n\n    # Reshape back to image dimensions and scale to 0-255\n    grayscale = pca_result.reshape(img.shape[:2])\n    grayscale = normalize_per_image(grayscale, \"min_max\")\n\n    return from_float(grayscale, dtype=np.uint8) if dtype == np.uint8 else grayscale\n</code></pre>"},{"location":"api_reference/augmentations/functional/#albumentations.augmentations.functional.to_gray_weighted_average","title":"<code>def to_gray_weighted_average    (img)    </code> [view source on GitHub]","text":"<p>Convert an RGB image to grayscale using the weighted average method.</p> <p>This function uses OpenCV's cvtColor function with COLOR_RGB2GRAY conversion, which applies the following formula: Y = 0.299R + 0.587G + 0.114*B</p> <p>Parameters:</p> Name Type Description <code>img</code> <code>np.ndarray</code> <p>Input RGB image as a numpy array.</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Grayscale image as a 2D numpy array.</p> <p>Image types:     uint8, float32</p> <p>Number of channels:     3</p> Source code in <code>albumentations/augmentations/functional.py</code> Python<pre><code>def to_gray_weighted_average(img: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Convert an RGB image to grayscale using the weighted average method.\n\n    This function uses OpenCV's cvtColor function with COLOR_RGB2GRAY conversion,\n    which applies the following formula:\n    Y = 0.299*R + 0.587*G + 0.114*B\n\n    Args:\n        img (np.ndarray): Input RGB image as a numpy array.\n\n    Returns:\n        np.ndarray: Grayscale image as a 2D numpy array.\n\n    Image types:\n        uint8, float32\n\n    Number of channels:\n        3\n    \"\"\"\n    return cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n</code></pre>"},{"location":"api_reference/augmentations/geometric/","title":"Geometric augmentations (augmentations.geometric)","text":""},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.functional","title":"<code>functional</code>","text":""},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.functional.bbox_d4","title":"<code>def bbox_d4    (bbox, group_member)    </code> [view source on GitHub]","text":"<p>Applies a <code>D_4</code> symmetry group transformation to a bounding box.</p> <p>The function transforms a bounding box according to the specified group member from the <code>D_4</code> group. These transformations include rotations and reflections, specified to work on an image's bounding box given its dimensions.</p> <ul> <li>bbox (BoxInternalType): The bounding box to transform. This should be a structure specifying coordinates     like (xmin, ymin, xmax, ymax).</li> <li>group_member (D4Type): A string identifier for the <code>D_4</code> group transformation to apply.     Valid values are 'e', 'r90', 'r180', 'r270', 'v', 'hvt', 'h', 't'.</li> </ul> <ul> <li>BoxInternalType: The transformed bounding box.</li> </ul> <ul> <li>ValueError: If an invalid group member is specified.</li> </ul> <p>Examples:</p> <ul> <li>Applying a 90-degree rotation:   <code>bbox_d4((10, 20, 110, 120), 'r90')</code>   This would rotate the bounding box 90 degrees within a 100x100 image.</li> </ul> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>def bbox_d4(\n    bbox: BoxInternalType,\n    group_member: D4Type,\n) -&gt; BoxInternalType:\n    \"\"\"Applies a `D_4` symmetry group transformation to a bounding box.\n\n    The function transforms a bounding box according to the specified group member from the `D_4` group.\n    These transformations include rotations and reflections, specified to work on an image's bounding box given\n    its dimensions.\n\n    Parameters:\n    - bbox (BoxInternalType): The bounding box to transform. This should be a structure specifying coordinates\n        like (xmin, ymin, xmax, ymax).\n    - group_member (D4Type): A string identifier for the `D_4` group transformation to apply.\n        Valid values are 'e', 'r90', 'r180', 'r270', 'v', 'hvt', 'h', 't'.\n\n    Returns:\n    - BoxInternalType: The transformed bounding box.\n\n    Raises:\n    - ValueError: If an invalid group member is specified.\n\n    Examples:\n    - Applying a 90-degree rotation:\n      `bbox_d4((10, 20, 110, 120), 'r90')`\n      This would rotate the bounding box 90 degrees within a 100x100 image.\n    \"\"\"\n    transformations = {\n        \"e\": lambda x: x,  # Identity transformation\n        \"r90\": lambda x: bbox_rot90(x, 1),  # Rotate 90 degrees\n        \"r180\": lambda x: bbox_rot90(x, 2),  # Rotate 180 degrees\n        \"r270\": lambda x: bbox_rot90(x, 3),  # Rotate 270 degrees\n        \"v\": lambda x: bbox_vflip(x),  # Vertical flip\n        \"hvt\": lambda x: bbox_transpose(bbox_rot90(x, 2)),  # Reflect over anti-diagonal\n        \"h\": lambda x: bbox_hflip(x),  # Horizontal flip\n        \"t\": lambda x: bbox_transpose(x),  # Transpose (reflect over main diagonal)\n    }\n\n    # Execute the appropriate transformation\n    if group_member in transformations:\n        return transformations[group_member](bbox)\n\n    raise ValueError(f\"Invalid group member: {group_member}\")\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.functional.bbox_flip","title":"<code>def bbox_flip    (bbox, d)    </code> [view source on GitHub]","text":"<p>Flip a bounding box either vertically, horizontally or both depending on the value of <code>d</code>.</p> <p>Parameters:</p> Name Type Description <code>bbox</code> <code>BoxInternalType</code> <p>A bounding box <code>(x_min, y_min, x_max, y_max)</code>.</p> <code>d</code> <code>int</code> <p>dimension. 0 for vertical flip, 1 for horizontal, -1 for transpose</p> <p>Returns:</p> Type Description <code>BoxInternalType</code> <p>A bounding box <code>(x_min, y_min, x_max, y_max)</code>.</p> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>if value of <code>d</code> is not -1, 0 or 1.</p> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>def bbox_flip(bbox: BoxInternalType, d: int) -&gt; BoxInternalType:\n    \"\"\"Flip a bounding box either vertically, horizontally or both depending on the value of `d`.\n\n    Args:\n        bbox: A bounding box `(x_min, y_min, x_max, y_max)`.\n        d: dimension. 0 for vertical flip, 1 for horizontal, -1 for transpose\n\n    Returns:\n        A bounding box `(x_min, y_min, x_max, y_max)`.\n\n    Raises:\n        ValueError: if value of `d` is not -1, 0 or 1.\n\n    \"\"\"\n    if d == 0:\n        bbox = bbox_vflip(bbox)\n    elif d == 1:\n        bbox = bbox_hflip(bbox)\n    elif d == -1:\n        bbox = bbox_hflip(bbox)\n        bbox = bbox_vflip(bbox)\n    else:\n        raise ValueError(f\"Invalid d value {d}. Valid values are -1, 0 and 1\")\n    return bbox\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.functional.bbox_hflip","title":"<code>def bbox_hflip    (bbox)    </code> [view source on GitHub]","text":"<p>Flip a bounding box horizontally around the y-axis.</p> <p>Parameters:</p> Name Type Description <code>bbox</code> <code>BoxInternalType</code> <p>A bounding box <code>(x_min, y_min, x_max, y_max)</code>.</p> <p>Returns:</p> Type Description <code>BoxInternalType</code> <p>A bounding box <code>(x_min, y_min, x_max, y_max)</code>.</p> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>def bbox_hflip(bbox: BoxInternalType) -&gt; BoxInternalType:\n    \"\"\"Flip a bounding box horizontally around the y-axis.\n\n    Args:\n        bbox: A bounding box `(x_min, y_min, x_max, y_max)`.\n\n    Returns:\n        A bounding box `(x_min, y_min, x_max, y_max)`.\n\n    \"\"\"\n    x_min, y_min, x_max, y_max = bbox[:4]\n    return 1 - x_max, y_min, 1 - x_min, y_max\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.functional.bbox_rot90","title":"<code>def bbox_rot90    (bbox, factor)    </code> [view source on GitHub]","text":"<p>Rotates a bounding box by 90 degrees CCW (see np.rot90)</p> <p>Parameters:</p> Name Type Description <code>bbox</code> <code>BoxInternalType</code> <p>A bounding box tuple (x_min, y_min, x_max, y_max).</p> <code>factor</code> <code>int</code> <p>Number of CCW rotations. Must be in set {0, 1, 2, 3} See np.rot90.</p> <p>Returns:</p> Type Description <code>tuple</code> <p>A bounding box tuple (x_min, y_min, x_max, y_max).</p> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>def bbox_rot90(bbox: BoxInternalType, factor: int) -&gt; BoxInternalType:\n    \"\"\"Rotates a bounding box by 90 degrees CCW (see np.rot90)\n\n    Args:\n        bbox: A bounding box tuple (x_min, y_min, x_max, y_max).\n        factor: Number of CCW rotations. Must be in set {0, 1, 2, 3} See np.rot90.\n\n    Returns:\n        tuple: A bounding box tuple (x_min, y_min, x_max, y_max).\n\n    \"\"\"\n    if factor not in {0, 1, 2, 3}:\n        msg = \"Parameter n must be in set {0, 1, 2, 3}\"\n        raise ValueError(msg)\n    x_min, y_min, x_max, y_max = bbox[:4]\n    if factor == 1:\n        bbox = y_min, 1 - x_max, y_max, 1 - x_min\n    elif factor == ROT90_180_FACTOR:\n        bbox = 1 - x_max, 1 - y_max, 1 - x_min, 1 - y_min\n    elif factor == ROT90_270_FACTOR:\n        bbox = 1 - y_max, x_min, 1 - y_min, x_max\n    return bbox\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.functional.bbox_rotate","title":"<code>def bbox_rotate    (bbox, angle, method, image_shape)    </code> [view source on GitHub]","text":"<p>Rotates a bounding box by angle degrees.</p> <p>Parameters:</p> Name Type Description <code>bbox</code> <code>BoxInternalType</code> <p>A bounding box <code>(x_min, y_min, x_max, y_max)</code>.</p> <code>angle</code> <code>float</code> <p>Angle of rotation in degrees.</p> <code>method</code> <code>str</code> <p>Rotation method used. Should be one of: \"largest_box\", \"ellipse\". Default: \"largest_box\".</p> <code>image_shape</code> <code>tuple[int, int]</code> <p>Image shape <code>(height, width)</code>.</p> <p>Returns:</p> Type Description <code>BoxInternalType</code> <p>A bounding box <code>(x_min, y_min, x_max, y_max)</code>.</p> <p>Reference</p> <p>https://arxiv.org/abs/2109.13488</p> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>def bbox_rotate(bbox: BoxInternalType, angle: float, method: str, image_shape: tuple[int, int]) -&gt; BoxInternalType:\n    \"\"\"Rotates a bounding box by angle degrees.\n\n    Args:\n        bbox: A bounding box `(x_min, y_min, x_max, y_max)`.\n        angle: Angle of rotation in degrees.\n        method: Rotation method used. Should be one of: \"largest_box\", \"ellipse\". Default: \"largest_box\".\n        image_shape: Image shape `(height, width)`.\n\n    Returns:\n        A bounding box `(x_min, y_min, x_max, y_max)`.\n\n    Reference:\n        https://arxiv.org/abs/2109.13488\n\n    \"\"\"\n    rows, cols = image_shape\n    x_min, y_min, x_max, y_max = bbox[:4]\n    scale = cols / float(rows)\n    if method == \"largest_box\":\n        x = np.array([x_min, x_max, x_max, x_min]) - 0.5\n        y = np.array([y_min, y_min, y_max, y_max]) - 0.5\n    elif method == \"ellipse\":\n        w = (x_max - x_min) / 2\n        h = (y_max - y_min) / 2\n        data = np.arange(0, 360, dtype=np.float32)\n        x = w * np.sin(np.radians(data)) + (w + x_min - 0.5)\n        y = h * np.cos(np.radians(data)) + (h + y_min - 0.5)\n    else:\n        raise ValueError(f\"Method {method} is not a valid rotation method.\")\n    angle = np.deg2rad(angle)\n    x_t = (np.cos(angle) * x * scale + np.sin(angle) * y) / scale\n    y_t = -np.sin(angle) * x * scale + np.cos(angle) * y\n    x_t = x_t + 0.5\n    y_t = y_t + 0.5\n\n    x_min, x_max = min(x_t), max(x_t)\n    y_min, y_max = min(y_t), max(y_t)\n\n    return x_min, y_min, x_max, y_max\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.functional.bbox_transpose","title":"<code>def bbox_transpose    (bbox)    </code> [view source on GitHub]","text":"<p>Transposes a bounding box along given axis.</p> <p>Parameters:</p> Name Type Description <code>bbox</code> <code>KeypointInternalType</code> <p>A bounding box <code>(x_min, y_min, x_max, y_max)</code>.</p> <p>Returns:</p> Type Description <code>KeypointInternalType</code> <p>A bounding box tuple <code>(x_min, y_min, x_max, y_max)</code>.</p> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If axis not equal to 0 or 1.</p> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>def bbox_transpose(\n    bbox: KeypointInternalType,\n) -&gt; KeypointInternalType:\n    \"\"\"Transposes a bounding box along given axis.\n\n    Args:\n        bbox: A bounding box `(x_min, y_min, x_max, y_max)`.\n\n    Returns:\n        A bounding box tuple `(x_min, y_min, x_max, y_max)`.\n\n    Raises:\n        ValueError: If axis not equal to 0 or 1.\n\n    \"\"\"\n    x_min, y_min, x_max, y_max = bbox[:4]\n    return (y_min, x_min, y_max, x_max)\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.functional.bbox_vflip","title":"<code>def bbox_vflip    (bbox)    </code> [view source on GitHub]","text":"<p>Flip a bounding box vertically around the x-axis.</p> <p>Parameters:</p> Name Type Description <code>bbox</code> <code>BoxInternalType</code> <p>A bounding box <code>(x_min, y_min, x_max, y_max)</code>.</p> <p>Returns:</p> Type Description <code>tuple</code> <p>A bounding box <code>(x_min, y_min, x_max, y_max)</code>.</p> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>def bbox_vflip(bbox: BoxInternalType) -&gt; BoxInternalType:\n    \"\"\"Flip a bounding box vertically around the x-axis.\n\n    Args:\n        bbox: A bounding box `(x_min, y_min, x_max, y_max)`.\n\n    Returns:\n        tuple: A bounding box `(x_min, y_min, x_max, y_max)`.\n\n    \"\"\"\n    x_min, y_min, x_max, y_max = bbox[:4]\n    return x_min, 1 - y_max, x_max, 1 - y_min\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.functional.bboxes_affine","title":"<code>def bboxes_affine    (bboxes, matrix, rotate_method, image_shape, border_mode, output_shape)    </code> [view source on GitHub]","text":"<p>Apply an affine transformation to bounding boxes.</p> <p>For reflection border modes (cv2.BORDER_REFLECT_101, cv2.BORDER_REFLECT), this function: 1. Calculates necessary padding to avoid information loss 2. Applies padding to the bounding boxes 3. Adjusts the transformation matrix to account for padding 4. Applies the affine transformation 5. Validates the transformed bounding boxes</p> <p>For other border modes, it directly applies the affine transformation without padding.</p> <p>Parameters:</p> Name Type Description <code>bboxes</code> <code>np.ndarray</code> <p>Input bounding boxes</p> <code>matrix</code> <code>skimage.transform.ProjectiveTransform</code> <p>Affine transformation matrix</p> <code>rotate_method</code> <code>str</code> <p>Method for rotating bounding boxes ('largest_box' or 'ellipse')</p> <code>image_shape</code> <code>Sequence[int]</code> <p>Shape of the input image</p> <code>border_mode</code> <code>int</code> <p>OpenCV border mode</p> <code>output_shape</code> <code>Sequence[int]</code> <p>Shape of the output image</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Transformed and normalized bounding boxes</p> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>def bboxes_affine(\n    bboxes: np.ndarray,\n    matrix: skimage.transform.ProjectiveTransform,\n    rotate_method: Literal[\"largest_box\", \"ellipse\"],\n    image_shape: tuple[int, int],\n    border_mode: int,\n    output_shape: Sequence[int],\n) -&gt; np.ndarray:\n    \"\"\"Apply an affine transformation to bounding boxes.\n\n    For reflection border modes (cv2.BORDER_REFLECT_101, cv2.BORDER_REFLECT), this function:\n    1. Calculates necessary padding to avoid information loss\n    2. Applies padding to the bounding boxes\n    3. Adjusts the transformation matrix to account for padding\n    4. Applies the affine transformation\n    5. Validates the transformed bounding boxes\n\n    For other border modes, it directly applies the affine transformation without padding.\n\n    Args:\n        bboxes (np.ndarray): Input bounding boxes\n        matrix (skimage.transform.ProjectiveTransform): Affine transformation matrix\n        rotate_method (str): Method for rotating bounding boxes ('largest_box' or 'ellipse')\n        image_shape (Sequence[int]): Shape of the input image\n        border_mode (int): OpenCV border mode\n        output_shape (Sequence[int]): Shape of the output image\n\n    Returns:\n        np.ndarray: Transformed and normalized bounding boxes\n    \"\"\"\n    if is_identity_matrix(matrix):\n        return bboxes\n\n    bboxes = denormalize_bboxes(bboxes, image_shape)\n\n    if border_mode in REFLECT_BORDER_MODES:\n        # Step 1: Compute affine transform padding\n        pad_left, pad_right, pad_top, pad_bottom = calculate_affine_transform_padding(matrix, image_shape)\n        grid_dimensions = get_pad_grid_dimensions(pad_top, pad_bottom, pad_left, pad_right, image_shape)\n        bboxes = generate_reflected_bboxes(bboxes, grid_dimensions, image_shape, center_in_origin=True)\n\n    # Apply affine transform\n    if rotate_method == \"largest_box\":\n        transformed_bboxes = bboxes_affine_largest_box(bboxes, matrix)\n    elif rotate_method == \"ellipse\":\n        transformed_bboxes = bboxes_affine_ellipse(bboxes, matrix)\n    else:\n        raise ValueError(f\"Method {rotate_method} is not a valid rotation method.\")\n\n    # Validate and normalize bboxes\n    validated_bboxes = validate_bboxes(transformed_bboxes, output_shape)\n\n    return normalize_bboxes(validated_bboxes, output_shape)\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.functional.bboxes_affine_ellipse","title":"<code>def bboxes_affine_ellipse    (bboxes, matrix)    </code> [view source on GitHub]","text":"<p>Apply an affine transformation to bounding boxes using an ellipse approximation method.</p> <p>This function transforms bounding boxes by approximating each box with an ellipse, transforming points along the ellipse's circumference, and then computing the new bounding box that encloses the transformed ellipse.</p> <p>Parameters:</p> Name Type Description <code>bboxes</code> <code>np.ndarray</code> <p>An array of bounding boxes with shape (N, 4+) where N is the number of                  bounding boxes. Each row should contain [x_min, y_min, x_max, y_max]                  followed by any additional attributes (e.g., class labels).</p> <code>matrix</code> <code>skimage.transform.ProjectiveTransform</code> <p>The affine transformation matrix to apply.</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>An array of transformed bounding boxes with the same shape as the input.             Each row contains [new_x_min, new_y_min, new_x_max, new_y_max] followed by             any additional attributes from the input bounding boxes.</p> <p>Note</p> <ul> <li>This function assumes that the input bounding boxes are in the format [x_min, y_min, x_max, y_max].</li> <li>The ellipse approximation method can provide a tighter bounding box compared to the   largest box method, especially for rotations.</li> <li>360 points are used to approximate each ellipse, which provides a good balance between   accuracy and computational efficiency.</li> <li>Any additional attributes beyond the first 4 coordinates are preserved unchanged.</li> <li>This method may be more suitable for objects that are roughly elliptical in shape.</li> </ul> <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; bboxes = np.array([[10, 10, 30, 20, 1], [40, 40, 60, 60, 2]])  # Two boxes with class labels\n&gt;&gt;&gt; matrix = skimage.transform.AffineTransform(rotation=np.pi/4)  # 45-degree rotation\n&gt;&gt;&gt; transformed_bboxes = bboxes_affine_ellipse(bboxes, matrix)\n&gt;&gt;&gt; print(transformed_bboxes)\n[[ 5.86  5.86 34.14 24.14  1.  ]\n [30.   30.   70.   70.    2.  ]]\n</code></pre> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>def bboxes_affine_ellipse(bboxes: np.ndarray, matrix: skimage.transform.ProjectiveTransform) -&gt; np.ndarray:\n    \"\"\"Apply an affine transformation to bounding boxes using an ellipse approximation method.\n\n    This function transforms bounding boxes by approximating each box with an ellipse,\n    transforming points along the ellipse's circumference, and then computing the\n    new bounding box that encloses the transformed ellipse.\n\n    Args:\n        bboxes (np.ndarray): An array of bounding boxes with shape (N, 4+) where N is the number of\n                             bounding boxes. Each row should contain [x_min, y_min, x_max, y_max]\n                             followed by any additional attributes (e.g., class labels).\n        matrix (skimage.transform.ProjectiveTransform): The affine transformation matrix to apply.\n\n    Returns:\n        np.ndarray: An array of transformed bounding boxes with the same shape as the input.\n                    Each row contains [new_x_min, new_y_min, new_x_max, new_y_max] followed by\n                    any additional attributes from the input bounding boxes.\n\n    Note:\n        - This function assumes that the input bounding boxes are in the format [x_min, y_min, x_max, y_max].\n        - The ellipse approximation method can provide a tighter bounding box compared to the\n          largest box method, especially for rotations.\n        - 360 points are used to approximate each ellipse, which provides a good balance between\n          accuracy and computational efficiency.\n        - Any additional attributes beyond the first 4 coordinates are preserved unchanged.\n        - This method may be more suitable for objects that are roughly elliptical in shape.\n\n    Example:\n        &gt;&gt;&gt; bboxes = np.array([[10, 10, 30, 20, 1], [40, 40, 60, 60, 2]])  # Two boxes with class labels\n        &gt;&gt;&gt; matrix = skimage.transform.AffineTransform(rotation=np.pi/4)  # 45-degree rotation\n        &gt;&gt;&gt; transformed_bboxes = bboxes_affine_ellipse(bboxes, matrix)\n        &gt;&gt;&gt; print(transformed_bboxes)\n        [[ 5.86  5.86 34.14 24.14  1.  ]\n         [30.   30.   70.   70.    2.  ]]\n    \"\"\"\n    x_min, y_min, x_max, y_max = bboxes[:, 0], bboxes[:, 1], bboxes[:, 2], bboxes[:, 3]\n    bbox_width = (x_max - x_min) / 2\n    bbox_height = (y_max - y_min) / 2\n    center_x = x_min + bbox_width\n    center_y = y_min + bbox_height\n\n    angles = np.arange(0, 360, dtype=np.float32)\n    cos_angles = np.cos(np.radians(angles))\n    sin_angles = np.sin(np.radians(angles))\n\n    # Generate points for all ellipses at once\n    x = bbox_width[:, np.newaxis] * sin_angles + center_x[:, np.newaxis]\n    y = bbox_height[:, np.newaxis] * cos_angles + center_y[:, np.newaxis]\n    points = np.stack([x, y], axis=-1).reshape(-1, 2)\n\n    # Transform all points at once\n    transformed_points = skimage.transform.matrix_transform(points, matrix.params)\n    transformed_points = transformed_points.reshape(len(bboxes), -1, 2)\n\n    # Compute new bounding boxes\n    new_x_min = np.min(transformed_points[:, :, 0], axis=1)\n    new_x_max = np.max(transformed_points[:, :, 0], axis=1)\n    new_y_min = np.min(transformed_points[:, :, 1], axis=1)\n    new_y_max = np.max(transformed_points[:, :, 1], axis=1)\n\n    return np.column_stack([new_x_min, new_y_min, new_x_max, new_y_max, bboxes[:, 4:]])\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.functional.bboxes_affine_largest_box","title":"<code>def bboxes_affine_largest_box    (bboxes, matrix)    </code> [view source on GitHub]","text":"<p>Apply an affine transformation to bounding boxes and return the largest enclosing boxes.</p> <p>This function transforms each corner of every bounding box using the given affine transformation matrix, then computes the new bounding boxes that fully enclose the transformed corners.</p> <p>Parameters:</p> Name Type Description <code>bboxes</code> <code>np.ndarray</code> <p>An array of bounding boxes with shape (N, 4+) where N is the number of                  bounding boxes. Each row should contain [x_min, y_min, x_max, y_max]                  followed by any additional attributes (e.g., class labels).</p> <code>matrix</code> <code>skimage.transform.ProjectiveTransform</code> <p>The affine transformation matrix to apply.</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>An array of transformed bounding boxes with the same shape as the input.             Each row contains [new_x_min, new_y_min, new_x_max, new_y_max] followed by             any additional attributes from the input bounding boxes.</p> <p>Note</p> <ul> <li>This function assumes that the input bounding boxes are in the format [x_min, y_min, x_max, y_max].</li> <li>The resulting bounding boxes are the smallest axis-aligned boxes that completely   enclose the transformed original boxes. They may be larger than the minimal possible   bounding box if the original box becomes rotated.</li> <li>Any additional attributes beyond the first 4 coordinates are preserved unchanged.</li> <li>This method is called \"largest box\" because it returns the largest axis-aligned box   that encloses all corners of the transformed bounding box.</li> </ul> <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; bboxes = np.array([[10, 10, 20, 20, 1], [30, 30, 40, 40, 2]])  # Two boxes with class labels\n&gt;&gt;&gt; matrix = skimage.transform.AffineTransform(scale=(2, 2), translation=(5, 5))\n&gt;&gt;&gt; transformed_bboxes = bboxes_affine_largest_box(bboxes, matrix)\n&gt;&gt;&gt; print(transformed_bboxes)\n[[ 25.  25.  45.  45.   1.]\n [ 65.  65.  85.  85.   2.]]\n</code></pre> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>def bboxes_affine_largest_box(bboxes: np.ndarray, matrix: skimage.transform.ProjectiveTransform) -&gt; np.ndarray:\n    \"\"\"Apply an affine transformation to bounding boxes and return the largest enclosing boxes.\n\n    This function transforms each corner of every bounding box using the given affine transformation\n    matrix, then computes the new bounding boxes that fully enclose the transformed corners.\n\n    Args:\n        bboxes (np.ndarray): An array of bounding boxes with shape (N, 4+) where N is the number of\n                             bounding boxes. Each row should contain [x_min, y_min, x_max, y_max]\n                             followed by any additional attributes (e.g., class labels).\n        matrix (skimage.transform.ProjectiveTransform): The affine transformation matrix to apply.\n\n    Returns:\n        np.ndarray: An array of transformed bounding boxes with the same shape as the input.\n                    Each row contains [new_x_min, new_y_min, new_x_max, new_y_max] followed by\n                    any additional attributes from the input bounding boxes.\n\n    Note:\n        - This function assumes that the input bounding boxes are in the format [x_min, y_min, x_max, y_max].\n        - The resulting bounding boxes are the smallest axis-aligned boxes that completely\n          enclose the transformed original boxes. They may be larger than the minimal possible\n          bounding box if the original box becomes rotated.\n        - Any additional attributes beyond the first 4 coordinates are preserved unchanged.\n        - This method is called \"largest box\" because it returns the largest axis-aligned box\n          that encloses all corners of the transformed bounding box.\n\n    Example:\n        &gt;&gt;&gt; bboxes = np.array([[10, 10, 20, 20, 1], [30, 30, 40, 40, 2]])  # Two boxes with class labels\n        &gt;&gt;&gt; matrix = skimage.transform.AffineTransform(scale=(2, 2), translation=(5, 5))\n        &gt;&gt;&gt; transformed_bboxes = bboxes_affine_largest_box(bboxes, matrix)\n        &gt;&gt;&gt; print(transformed_bboxes)\n        [[ 25.  25.  45.  45.   1.]\n         [ 65.  65.  85.  85.   2.]]\n    \"\"\"\n    # Extract corners of all bboxes\n    x_min, y_min, x_max, y_max = bboxes[:, 0], bboxes[:, 1], bboxes[:, 2], bboxes[:, 3]\n    corners = np.array([[x_min, y_min], [x_max, y_min], [x_max, y_max], [x_min, y_max]]).transpose(\n        2,\n        0,\n        1,\n    )  # Shape: (num_bboxes, 4, 2)\n\n    # Transform all corners at once\n    transformed_corners = skimage.transform.matrix_transform(corners.reshape(-1, 2), matrix.params)\n    transformed_corners = transformed_corners.reshape(-1, 4, 2)\n\n    # Compute new bounding boxes\n    new_x_min = np.min(transformed_corners[:, :, 0], axis=1)\n    new_x_max = np.max(transformed_corners[:, :, 0], axis=1)\n    new_y_min = np.min(transformed_corners[:, :, 1], axis=1)\n    new_y_max = np.max(transformed_corners[:, :, 1], axis=1)\n\n    return np.column_stack([new_x_min, new_y_min, new_x_max, new_y_max, bboxes[:, 4:]])\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.functional.calculate_affine_transform_padding","title":"<code>def calculate_affine_transform_padding    (matrix, image_shape)    </code> [view source on GitHub]","text":"<p>Calculate the necessary padding for an affine transformation to avoid empty spaces.</p> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>def calculate_affine_transform_padding(\n    matrix: skimage.transform.ProjectiveTransform,\n    image_shape: Sequence[int],\n) -&gt; tuple[int, int, int, int]:\n    \"\"\"Calculate the necessary padding for an affine transformation to avoid empty spaces.\"\"\"\n    height, width = image_shape[:2]\n\n    # Check for identity transform\n    if is_identity_matrix(matrix):\n        return (0, 0, 0, 0)\n\n    # Original corners\n    corners = np.array([[0, 0], [width, 0], [width, height], [0, height]])\n\n    # Transform corners\n    transformed_corners = matrix(corners)\n\n    # Find box that includes both original and transformed corners\n    all_corners = np.vstack((corners, transformed_corners))\n    min_x, min_y = all_corners.min(axis=0)\n    max_x, max_y = all_corners.max(axis=0)\n    # Compute the inverse transform\n    inverse_matrix = matrix.inverse\n\n    # Apply inverse transform to all corners of the bounding box\n    bbox_corners = np.array([[min_x, min_y], [max_x, min_y], [max_x, max_y], [min_x, max_y]])\n\n    inverse_corners = inverse_matrix(bbox_corners)\n\n    min_x, min_y = inverse_corners.min(axis=0)\n    max_x, max_y = inverse_corners.max(axis=0)\n\n    pad_left = max(0, math.ceil(0 - min_x))\n    pad_right = max(0, math.ceil(max_x - width))\n    pad_top = max(0, math.ceil(0 - min_y))\n    pad_bottom = max(0, math.ceil(max_y - height))\n\n    return pad_left, pad_right, pad_top, pad_bottom\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.functional.calculate_grid_dimensions","title":"<code>def calculate_grid_dimensions    (image_shape, num_grid_xy)    </code> [view source on GitHub]","text":"<p>Calculate the dimensions of a grid overlay on an image using vectorized operations.</p> <p>This function divides an image into a grid and calculates the dimensions (x_min, y_min, x_max, y_max) for each cell in the grid without using loops.</p> <p>Parameters:</p> Name Type Description <code>image_shape</code> <code>tuple[int, int]</code> <p>The shape of the image (height, width).</p> <code>num_grid_xy</code> <code>tuple[int, int]</code> <p>The number of grid cells in (x, y) directions.</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>A 3D array of shape (grid_height, grid_width, 4) where each element             is [x_min, y_min, x_max, y_max] for a grid cell.</p> <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; image_shape = (100, 150)\n&gt;&gt;&gt; num_grid_xy = (3, 2)\n&gt;&gt;&gt; dimensions = calculate_grid_dimensions(image_shape, num_grid_xy)\n&gt;&gt;&gt; print(dimensions.shape)\n(2, 3, 4)\n&gt;&gt;&gt; print(dimensions[0, 0])  # First cell\n[  0   0  50  50]\n</code></pre> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>def calculate_grid_dimensions(\n    image_shape: tuple[int, int],\n    num_grid_xy: tuple[int, int],\n) -&gt; np.ndarray:\n    \"\"\"Calculate the dimensions of a grid overlay on an image using vectorized operations.\n\n    This function divides an image into a grid and calculates the dimensions\n    (x_min, y_min, x_max, y_max) for each cell in the grid without using loops.\n\n    Args:\n        image_shape (tuple[int, int]): The shape of the image (height, width).\n        num_grid_xy (tuple[int, int]): The number of grid cells in (x, y) directions.\n\n    Returns:\n        np.ndarray: A 3D array of shape (grid_height, grid_width, 4) where each element\n                    is [x_min, y_min, x_max, y_max] for a grid cell.\n\n    Example:\n        &gt;&gt;&gt; image_shape = (100, 150)\n        &gt;&gt;&gt; num_grid_xy = (3, 2)\n        &gt;&gt;&gt; dimensions = calculate_grid_dimensions(image_shape, num_grid_xy)\n        &gt;&gt;&gt; print(dimensions.shape)\n        (2, 3, 4)\n        &gt;&gt;&gt; print(dimensions[0, 0])  # First cell\n        [  0   0  50  50]\n    \"\"\"\n    num_grid_yx = np.array(num_grid_xy[::-1])  # Reverse to match image_shape order\n    image_shape = np.array(image_shape)\n\n    square_shape = image_shape // num_grid_yx\n    last_square_shape = image_shape - (square_shape * (num_grid_yx - 1))\n\n    grid_width, grid_height = num_grid_xy\n\n    # Create meshgrid for row and column indices\n    col_indices, row_indices = np.meshgrid(np.arange(grid_width), np.arange(grid_height))\n\n    # Calculate x_min and y_min\n    x_min = col_indices * square_shape[1]\n    y_min = row_indices * square_shape[0]\n\n    # Calculate x_max and y_max\n    x_max = np.where(col_indices == grid_width - 1, x_min + last_square_shape[1], x_min + square_shape[1])\n    y_max = np.where(row_indices == grid_height - 1, y_min + last_square_shape[0], y_min + square_shape[0])\n\n    # Stack the dimensions\n    return np.stack([x_min, y_min, x_max, y_max], axis=-1).astype(np.int16)\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.functional.compute_transformed_image_bounds","title":"<code>def compute_transformed_image_bounds    (matrix, image_shape)    </code> [view source on GitHub]","text":"<p>Compute the bounds of an image after applying an affine transformation.</p> <p>Parameters:</p> Name Type Description <code>matrix</code> <code>skimage.transform.ProjectiveTransform</code> <p>The affine transformation matrix.</p> <code>image_shape</code> <code>tuple[int, int]</code> <p>The shape of the image as (height, width).</p> <p>Returns:</p> Type Description <code>tuple[np.ndarray, np.ndarray]</code> <p>A tuple containing:     - min_coords: An array with the minimum x and y coordinates.     - max_coords: An array with the maximum x and y coordinates.</p> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>def compute_transformed_image_bounds(\n    matrix: skimage.transform.ProjectiveTransform,\n    image_shape: tuple[int, int],\n) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"Compute the bounds of an image after applying an affine transformation.\n\n    Args:\n        matrix (skimage.transform.ProjectiveTransform): The affine transformation matrix.\n        image_shape (tuple[int, int]): The shape of the image as (height, width).\n\n    Returns:\n        tuple[np.ndarray, np.ndarray]: A tuple containing:\n            - min_coords: An array with the minimum x and y coordinates.\n            - max_coords: An array with the maximum x and y coordinates.\n    \"\"\"\n    height, width = image_shape[:2]\n\n    # Define the corners of the image\n    corners = np.array([[0, 0], [width, 0], [width, height], [0, height]])\n\n    # Transform the corners\n    transformed_corners = matrix(corners)\n\n    # Calculate the bounding box of the transformed corners\n    min_coords = np.floor(transformed_corners.min(axis=0)).astype(int)\n    max_coords = np.ceil(transformed_corners.max(axis=0)).astype(int)\n\n    return min_coords, max_coords\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.functional.create_affine_transformation_matrix","title":"<code>def create_affine_transformation_matrix    (translate, shear, scale, rotate, shift)    </code> [view source on GitHub]","text":"<p>Create an affine transformation matrix combining translation, shear, scale, and rotation.</p> <p>This function creates a complex affine transformation by combining multiple transformations in a specific order. The transformations are applied as follows: 1. Shift to top-left: Moves the center of transformation to (0, 0) 2. Apply main transformations: scale, rotation, shear, and translation 3. Shift back to center: Moves the center of transformation back to its original position</p> <p>The order of these transformations is crucial as matrix multiplications are not commutative.</p> <p>Parameters:</p> Name Type Description <code>translate</code> <code>TranslateDict</code> <p>Translation in x and y directions.                        Keys: 'x', 'y'. Values: translation amounts in pixels.</p> <code>shear</code> <code>ShearDict</code> <p>Shear in x and y directions.                Keys: 'x', 'y'. Values: shear angles in degrees.</p> <code>scale</code> <code>ScaleDict</code> <p>Scale factors for x and y directions.                Keys: 'x', 'y'. Values: scale factors (1.0 means no scaling).</p> <code>rotate</code> <code>float</code> <p>Rotation angle in degrees. Positive values rotate counter-clockwise.</p> <code>shift</code> <code>tuple[float, float]</code> <p>Shift to apply before and after transformations.                          Typically the image center (width/2, height/2).</p> <p>Returns:</p> Type Description <code>skimage.transform.ProjectiveTransform</code> <p>The resulting affine transformation matrix.</p> <p>Note</p> <ul> <li>All angle inputs (rotate, shear) are in degrees and are converted to radians internally.</li> <li>The order of transformations in the AffineTransform is: scale, rotation, shear, translation.</li> <li>The resulting transformation can be applied to coordinates using the call method.</li> </ul> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>def create_affine_transformation_matrix(\n    translate: TranslateDict,\n    shear: ShearDict,\n    scale: ScaleDict,\n    rotate: float,\n    shift: tuple[float, float],\n) -&gt; skimage.transform.ProjectiveTransform:\n    \"\"\"Create an affine transformation matrix combining translation, shear, scale, and rotation.\n\n    This function creates a complex affine transformation by combining multiple transformations\n    in a specific order. The transformations are applied as follows:\n    1. Shift to top-left: Moves the center of transformation to (0, 0)\n    2. Apply main transformations: scale, rotation, shear, and translation\n    3. Shift back to center: Moves the center of transformation back to its original position\n\n    The order of these transformations is crucial as matrix multiplications are not commutative.\n\n    Args:\n        translate (TranslateDict): Translation in x and y directions.\n                                   Keys: 'x', 'y'. Values: translation amounts in pixels.\n        shear (ShearDict): Shear in x and y directions.\n                           Keys: 'x', 'y'. Values: shear angles in degrees.\n        scale (ScaleDict): Scale factors for x and y directions.\n                           Keys: 'x', 'y'. Values: scale factors (1.0 means no scaling).\n        rotate (float): Rotation angle in degrees. Positive values rotate counter-clockwise.\n        shift (tuple[float, float]): Shift to apply before and after transformations.\n                                     Typically the image center (width/2, height/2).\n\n    Returns:\n        skimage.transform.ProjectiveTransform: The resulting affine transformation matrix.\n\n    Note:\n        - All angle inputs (rotate, shear) are in degrees and are converted to radians internally.\n        - The order of transformations in the AffineTransform is: scale, rotation, shear, translation.\n        - The resulting transformation can be applied to coordinates using the __call__ method.\n    \"\"\"\n    # Step 1: Create matrix to shift to top-left\n    # This moves the center of transformation to (0, 0)\n    matrix_to_topleft = skimage.transform.SimilarityTransform(translation=[shift[0], shift[1]])\n\n    # Step 2: Create matrix for main transformations\n    # This includes scaling, translation, rotation, and x-shear\n    matrix_transforms = skimage.transform.AffineTransform(\n        scale=(scale[\"x\"], scale[\"y\"]),\n        rotation=np.deg2rad(rotate),\n        shear=(np.deg2rad(shear[\"x\"]), np.deg2rad(shear[\"y\"])),  # Both x and y shear\n        translation=(translate[\"x\"], translate[\"y\"]),\n    )\n\n    # Step 3: Create matrix to shift back to center\n    # This is the inverse of the top-left shift\n    matrix_to_center = matrix_to_topleft.inverse\n\n    # Combine all transformations\n    # The order is important: transformations are applied from right to left\n    return (\n        matrix_to_center  # 3. Shift back to original center\n        + matrix_transforms  # 2. Apply main transformations\n        + matrix_to_topleft  # 1. Shift to top-left\n    )\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.functional.d4","title":"<code>def d4    (img, group_member)    </code> [view source on GitHub]","text":"<p>Applies a <code>D_4</code> symmetry group transformation to an image array.</p> <p>This function manipulates an image using transformations such as rotations and flips, corresponding to the <code>D_4</code> dihedral group symmetry operations. Each transformation is identified by a unique group member code.</p> <ul> <li>img (np.ndarray): The input image array to transform.</li> <li>group_member (D4Type): A string identifier indicating the specific transformation to apply. Valid codes include:</li> <li>'e': Identity (no transformation).</li> <li>'r90': Rotate 90 degrees counterclockwise.</li> <li>'r180': Rotate 180 degrees.</li> <li>'r270': Rotate 270 degrees counterclockwise.</li> <li>'v': Vertical flip.</li> <li>'hvt': Transpose over second diagonal</li> <li>'h': Horizontal flip.</li> <li>'t': Transpose (reflect over the main diagonal).</li> </ul> <ul> <li>np.ndarray: The transformed image array.</li> </ul> <ul> <li>ValueError: If an invalid group member is specified.</li> </ul> <p>Examples:</p> <ul> <li>Rotating an image by 90 degrees:   <code>transformed_image = d4(original_image, 'r90')</code></li> <li>Applying a horizontal flip to an image:   <code>transformed_image = d4(original_image, 'h')</code></li> </ul> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>def d4(img: np.ndarray, group_member: D4Type) -&gt; np.ndarray:\n    \"\"\"Applies a `D_4` symmetry group transformation to an image array.\n\n    This function manipulates an image using transformations such as rotations and flips,\n    corresponding to the `D_4` dihedral group symmetry operations.\n    Each transformation is identified by a unique group member code.\n\n    Parameters:\n    - img (np.ndarray): The input image array to transform.\n    - group_member (D4Type): A string identifier indicating the specific transformation to apply. Valid codes include:\n      - 'e': Identity (no transformation).\n      - 'r90': Rotate 90 degrees counterclockwise.\n      - 'r180': Rotate 180 degrees.\n      - 'r270': Rotate 270 degrees counterclockwise.\n      - 'v': Vertical flip.\n      - 'hvt': Transpose over second diagonal\n      - 'h': Horizontal flip.\n      - 't': Transpose (reflect over the main diagonal).\n\n    Returns:\n    - np.ndarray: The transformed image array.\n\n    Raises:\n    - ValueError: If an invalid group member is specified.\n\n    Examples:\n    - Rotating an image by 90 degrees:\n      `transformed_image = d4(original_image, 'r90')`\n    - Applying a horizontal flip to an image:\n      `transformed_image = d4(original_image, 'h')`\n    \"\"\"\n    transformations = {\n        \"e\": lambda x: x,  # Identity transformation\n        \"r90\": lambda x: rot90(x, 1),  # Rotate 90 degrees\n        \"r180\": lambda x: rot90(x, 2),  # Rotate 180 degrees\n        \"r270\": lambda x: rot90(x, 3),  # Rotate 270 degrees\n        \"v\": vflip,  # Vertical flip\n        \"hvt\": lambda x: transpose(rot90(x, 2)),  # Reflect over anti-diagonal\n        \"h\": hflip,  # Horizontal flip\n        \"t\": transpose,  # Transpose (reflect over main diagonal)\n    }\n\n    # Execute the appropriate transformation\n    if group_member in transformations:\n        return transformations[group_member](img)\n\n    raise ValueError(f\"Invalid group member: {group_member}\")\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.functional.distort_image","title":"<code>def distort_image    (image, generated_mesh, interpolation)    </code> [view source on GitHub]","text":"<p>Apply perspective distortion to an image based on a generated mesh.</p> <p>This function applies a perspective transformation to each cell of the image defined by the generated mesh. The distortion is applied using OpenCV's perspective transformation and blending techniques.</p> <p>Parameters:</p> Name Type Description <code>image</code> <code>np.ndarray</code> <p>The input image to be distorted. Can be a 2D grayscale image or a                 3D color image.</p> <code>generated_mesh</code> <code>np.ndarray</code> <p>A 2D array where each row represents a quadrilateral cell                         as [x1, y1, x2, y2, dst_x1, dst_y1, dst_x2, dst_y2, dst_x3, dst_y3, dst_x4, dst_y4].                         The first four values define the source rectangle, and the last eight values                         define the destination quadrilateral.</p> <code>interpolation</code> <code>int</code> <p>Interpolation method to be used in the perspective transformation.                  Should be one of the OpenCV interpolation flags (e.g., cv2.INTER_LINEAR).</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>The distorted image with the same shape and dtype as the input image.</p> <p>Note</p> <ul> <li>The function preserves the channel dimension of the input image.</li> <li>Each cell of the generated mesh is transformed independently and then blended into the output image.</li> <li>The distortion is applied using perspective transformation, which allows for more complex   distortions compared to affine transformations.</li> </ul> <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; image = np.random.randint(0, 255, (100, 100, 3), dtype=np.uint8)\n&gt;&gt;&gt; mesh = np.array([[0, 0, 50, 50, 5, 5, 45, 5, 45, 45, 5, 45]])\n&gt;&gt;&gt; distorted = distort_image(image, mesh, cv2.INTER_LINEAR)\n&gt;&gt;&gt; distorted.shape\n(100, 100, 3)\n</code></pre> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>@preserve_channel_dim\ndef distort_image(image: np.ndarray, generated_mesh: np.ndarray, interpolation: int) -&gt; np.ndarray:\n    \"\"\"Apply perspective distortion to an image based on a generated mesh.\n\n    This function applies a perspective transformation to each cell of the image defined by the\n    generated mesh. The distortion is applied using OpenCV's perspective transformation and\n    blending techniques.\n\n    Args:\n        image (np.ndarray): The input image to be distorted. Can be a 2D grayscale image or a\n                            3D color image.\n        generated_mesh (np.ndarray): A 2D array where each row represents a quadrilateral cell\n                                    as [x1, y1, x2, y2, dst_x1, dst_y1, dst_x2, dst_y2, dst_x3, dst_y3, dst_x4, dst_y4].\n                                    The first four values define the source rectangle, and the last eight values\n                                    define the destination quadrilateral.\n        interpolation (int): Interpolation method to be used in the perspective transformation.\n                             Should be one of the OpenCV interpolation flags (e.g., cv2.INTER_LINEAR).\n\n    Returns:\n        np.ndarray: The distorted image with the same shape and dtype as the input image.\n\n    Note:\n        - The function preserves the channel dimension of the input image.\n        - Each cell of the generated mesh is transformed independently and then blended into the output image.\n        - The distortion is applied using perspective transformation, which allows for more complex\n          distortions compared to affine transformations.\n\n    Example:\n        &gt;&gt;&gt; image = np.random.randint(0, 255, (100, 100, 3), dtype=np.uint8)\n        &gt;&gt;&gt; mesh = np.array([[0, 0, 50, 50, 5, 5, 45, 5, 45, 45, 5, 45]])\n        &gt;&gt;&gt; distorted = distort_image(image, mesh, cv2.INTER_LINEAR)\n        &gt;&gt;&gt; distorted.shape\n        (100, 100, 3)\n    \"\"\"\n    distorted_image = np.zeros_like(image)\n\n    for mesh in generated_mesh:\n        # Extract source rectangle and destination quadrilateral\n        x1, y1, x2, y2 = mesh[:4]  # Source rectangle\n        dst_quad = mesh[4:].reshape(4, 2)  # Destination quadrilateral\n\n        # Convert source rectangle to quadrilateral\n        src_quad = np.array(\n            [\n                [x1, y1],  # Top-left\n                [x2, y1],  # Top-right\n                [x2, y2],  # Bottom-right\n                [x1, y2],  # Bottom-left\n            ],\n            dtype=np.float32,\n        )\n\n        # Calculate Perspective transformation matrix\n        perspective_mat = cv2.getPerspectiveTransform(src_quad, dst_quad)\n\n        # Apply Perspective transformation\n        warped = cv2.warpPerspective(image, perspective_mat, (image.shape[1], image.shape[0]), flags=interpolation)\n\n        # Create mask for the transformed region\n        mask = np.zeros(image.shape[:2], dtype=np.uint8)\n        cv2.fillConvexPoly(mask, np.int32(dst_quad), 255)\n\n        # Copy only the warped quadrilateral area to the output image\n        distorted_image = cv2.copyTo(warped, mask, distorted_image)\n\n    return distorted_image\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.functional.elastic_transform","title":"<code>def elastic_transform    (img, alpha, sigma, interpolation, border_mode, value=None, random_state=None, approximate=False, same_dxdy=False)    </code> [view source on GitHub]","text":"<p>Apply an elastic transformation to an image.</p> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>@preserve_channel_dim\ndef elastic_transform(\n    img: np.ndarray,\n    alpha: float,\n    sigma: float,\n    interpolation: int,\n    border_mode: int,\n    value: ColorType | None = None,\n    random_state: np.random.RandomState | None = None,\n    approximate: bool = False,\n    same_dxdy: bool = False,\n) -&gt; np.ndarray:\n    \"\"\"Apply an elastic transformation to an image.\"\"\"\n    if approximate:\n        return elastic_transform_approximate(\n            img,\n            alpha,\n            sigma,\n            interpolation,\n            border_mode,\n            value,\n            random_state,\n            same_dxdy,\n        )\n    return elastic_transform_precise(\n        img,\n        alpha,\n        sigma,\n        interpolation,\n        border_mode,\n        value,\n        random_state,\n        same_dxdy,\n    )\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.functional.elastic_transform_approximate","title":"<code>def elastic_transform_approximate    (img, alpha, sigma, interpolation, border_mode, value, random_state, same_dxdy=False)    </code> [view source on GitHub]","text":"<p>Apply an approximate elastic transformation to an image.</p> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>def elastic_transform_approximate(\n    img: np.ndarray,\n    alpha: float,\n    sigma: float,\n    interpolation: int,\n    border_mode: int,\n    value: ColorType | None,\n    random_state: np.random.RandomState | None,\n    same_dxdy: bool = False,\n) -&gt; np.ndarray:\n    \"\"\"Apply an approximate elastic transformation to an image.\"\"\"\n    return elastic_transform_helper(\n        img,\n        alpha,\n        sigma,\n        interpolation,\n        border_mode,\n        value,\n        random_state,\n        same_dxdy,\n        kernel_size=(17, 17),\n    )\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.functional.elastic_transform_precise","title":"<code>def elastic_transform_precise    (img, alpha, sigma, interpolation, border_mode, value, random_state, same_dxdy=False)    </code> [view source on GitHub]","text":"<p>Apply a precise elastic transformation to an image.</p> <p>This function applies an elastic deformation to the input image using a precise method. The transformation involves creating random displacement fields, smoothing them using Gaussian blur with adaptive kernel size, and then remapping the image according to the smoothed displacement fields.</p> <p>Parameters:</p> Name Type Description <code>img</code> <code>np.ndarray</code> <p>Input image.</p> <code>alpha</code> <code>float</code> <p>Scaling factor for the random displacement fields.</p> <code>sigma</code> <code>float</code> <p>Standard deviation for Gaussian blur applied to the displacement fields.</p> <code>interpolation</code> <code>int</code> <p>Interpolation method to be used (e.g., cv2.INTER_LINEAR).</p> <code>border_mode</code> <code>int</code> <p>Pixel extrapolation method (e.g., cv2.BORDER_CONSTANT).</p> <code>value</code> <code>ColorType | None</code> <p>Border value if border_mode is cv2.BORDER_CONSTANT.</p> <code>random_state</code> <code>np.random.RandomState | None</code> <p>Random state for reproducibility.</p> <code>same_dxdy</code> <code>bool</code> <p>If True, use the same displacement field for both x and y directions.</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Transformed image with precise elastic deformation applied.</p> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>def elastic_transform_precise(\n    img: np.ndarray,\n    alpha: float,\n    sigma: float,\n    interpolation: int,\n    border_mode: int,\n    value: ColorType | None,\n    random_state: np.random.RandomState | None,\n    same_dxdy: bool = False,\n) -&gt; np.ndarray:\n    \"\"\"Apply a precise elastic transformation to an image.\n\n    This function applies an elastic deformation to the input image using a precise method.\n    The transformation involves creating random displacement fields, smoothing them using Gaussian\n    blur with adaptive kernel size, and then remapping the image according to the smoothed displacement fields.\n\n    Args:\n        img (np.ndarray): Input image.\n        alpha (float): Scaling factor for the random displacement fields.\n        sigma (float): Standard deviation for Gaussian blur applied to the displacement fields.\n        interpolation (int): Interpolation method to be used (e.g., cv2.INTER_LINEAR).\n        border_mode (int): Pixel extrapolation method (e.g., cv2.BORDER_CONSTANT).\n        value (ColorType | None): Border value if border_mode is cv2.BORDER_CONSTANT.\n        random_state (np.random.RandomState | None): Random state for reproducibility.\n        same_dxdy (bool, optional): If True, use the same displacement field for both x and y directions.\n\n    Returns:\n        np.ndarray: Transformed image with precise elastic deformation applied.\n    \"\"\"\n    return elastic_transform_helper(\n        img,\n        alpha,\n        sigma,\n        interpolation,\n        border_mode,\n        value,\n        random_state,\n        same_dxdy,\n        kernel_size=(0, 0),\n    )\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.functional.find_keypoint","title":"<code>def find_keypoint    (position, distance_map, threshold, inverted)    </code> [view source on GitHub]","text":"<p>Determine if a valid keypoint can be found at the given position.</p> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>def find_keypoint(\n    position: tuple[int, int],\n    distance_map: np.ndarray,\n    threshold: float | None,\n    inverted: bool,\n) -&gt; tuple[float, float] | None:\n    \"\"\"Determine if a valid keypoint can be found at the given position.\"\"\"\n    y, x = position\n    value = distance_map[y, x]\n    if not inverted and threshold is not None and value &gt;= threshold:\n        return None\n    if inverted and threshold is not None and value &lt; threshold:\n        return None\n    return float(x), float(y)\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.functional.flip_bboxes","title":"<code>def flip_bboxes    (bboxes, flip_horizontal=False, flip_vertical=False, image_shape=(0, 0))    </code> [view source on GitHub]","text":"<p>Flip bounding boxes horizontally and/or vertically.</p> <p>Parameters:</p> Name Type Description <code>bboxes</code> <code>np.ndarray</code> <p>Array of bounding boxes with shape (n, m) where each row is [x_min, y_min, x_max, y_max, ...].</p> <code>flip_horizontal</code> <code>bool</code> <p>Whether to flip horizontally.</p> <code>flip_vertical</code> <code>bool</code> <p>Whether to flip vertically.</p> <code>image_shape</code> <code>tuple[int, int]</code> <p>Shape of the image as (height, width).</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Flipped bounding boxes.</p> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>def flip_bboxes(\n    bboxes: np.ndarray,\n    flip_horizontal: bool = False,\n    flip_vertical: bool = False,\n    image_shape: tuple[int, int] = (0, 0),\n) -&gt; np.ndarray:\n    \"\"\"Flip bounding boxes horizontally and/or vertically.\n\n    Args:\n        bboxes (np.ndarray): Array of bounding boxes with shape (n, m) where each row is\n            [x_min, y_min, x_max, y_max, ...].\n        flip_horizontal (bool): Whether to flip horizontally.\n        flip_vertical (bool): Whether to flip vertically.\n        image_shape (tuple[int, int]): Shape of the image as (height, width).\n\n    Returns:\n        np.ndarray: Flipped bounding boxes.\n    \"\"\"\n    rows, cols = image_shape[:2]\n    flipped_bboxes = bboxes.copy()\n    if flip_horizontal:\n        flipped_bboxes[:, [0, 2]] = cols - flipped_bboxes[:, [2, 0]]\n    if flip_vertical:\n        flipped_bboxes[:, [1, 3]] = rows - flipped_bboxes[:, [3, 1]]\n    return flipped_bboxes\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.functional.from_distance_maps","title":"<code>def from_distance_maps    (distance_maps, inverted, if_not_found_coords, threshold)    </code> [view source on GitHub]","text":"<p>Convert outputs of <code>to_distance_maps</code> to <code>KeypointsOnImage</code>. This is the inverse of <code>to_distance_maps</code>.</p> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>def from_distance_maps(\n    distance_maps: np.ndarray,\n    inverted: bool,\n    if_not_found_coords: Sequence[int] | dict[str, Any] | None,\n    threshold: float | None,\n) -&gt; list[tuple[float, float]]:\n    \"\"\"Convert outputs of `to_distance_maps` to `KeypointsOnImage`.\n    This is the inverse of `to_distance_maps`.\n    \"\"\"\n    if distance_maps.ndim != NUM_MULTI_CHANNEL_DIMENSIONS:\n        msg = f\"Expected three-dimensional input, got {distance_maps.ndim} dimensions and shape {distance_maps.shape}.\"\n        raise ValueError(msg)\n    height, width, nb_keypoints = distance_maps.shape\n\n    drop_if_not_found, if_not_found_x, if_not_found_y = validate_if_not_found_coords(if_not_found_coords)\n\n    keypoints = []\n    for i in range(nb_keypoints):\n        hitidx_flat = np.argmax(distance_maps[..., i]) if inverted else np.argmin(distance_maps[..., i])\n        hitidx_ndim = np.unravel_index(hitidx_flat, (height, width))\n        keypoint = find_keypoint(hitidx_ndim, distance_maps[:, :, i], threshold, inverted)\n        if keypoint:\n            keypoints.append(keypoint)\n        elif not drop_if_not_found:\n            keypoints.append((if_not_found_x, if_not_found_y))\n\n    return keypoints\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.functional.generate_distorted_grid_polygons","title":"<code>def generate_distorted_grid_polygons    (dimensions, magnitude)    </code> [view source on GitHub]","text":"<p>Generate distorted grid polygons based on input dimensions and magnitude.</p> <p>This function creates a grid of polygons and applies random distortions to the internal vertices, while keeping the boundary vertices fixed. The distortion is applied consistently across shared vertices to avoid gaps or overlaps in the resulting grid.</p> <p>Parameters:</p> Name Type Description <code>dimensions</code> <code>np.ndarray</code> <p>A 3D array of shape (grid_height, grid_width, 4) where each element                      is [x_min, y_min, x_max, y_max] representing the dimensions of a grid cell.</p> <code>magnitude</code> <code>int</code> <p>Maximum pixel-wise displacement for distortion. The actual displacement              will be randomly chosen in the range [-magnitude, magnitude].</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>A 2D array of shape (total_cells, 8) where each row represents a distorted polygon             as [x1, y1, x2, y1, x2, y2, x1, y2]. The total_cells is equal to grid_height * grid_width.</p> <p>Note</p> <ul> <li>Only internal grid points are distorted; boundary points remain fixed.</li> <li>The function ensures consistent distortion across shared vertices of adjacent cells.</li> <li>The distortion is applied to the following points of each internal cell:<ul> <li>Bottom-right of the cell above and to the left</li> <li>Bottom-left of the cell above</li> <li>Top-right of the cell to the left</li> <li>Top-left of the current cell</li> </ul> </li> <li>Each square represents a cell, and the X marks indicate the coordinates where displacement occurs.     +--+--+--+--+     |  |  |  |  |     +--X--X--X--+     |  |  |  |  |     +--X--X--X--+     |  |  |  |  |     +--X--X--X--+     |  |  |  |  |     +--+--+--+--+</li> <li>For each X, the coordinates of the left, right, top, and bottom edges   in the four adjacent cells are displaced.</li> </ul> <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; dimensions = np.array([[[0, 0, 50, 50], [50, 0, 100, 50]],\n...                        [[0, 50, 50, 100], [50, 50, 100, 100]]])\n&gt;&gt;&gt; distorted = generate_distorted_grid_polygons(dimensions, magnitude=10)\n&gt;&gt;&gt; distorted.shape\n(4, 8)\n</code></pre> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>def generate_distorted_grid_polygons(\n    dimensions: np.ndarray,\n    magnitude: int,\n) -&gt; np.ndarray:\n    \"\"\"Generate distorted grid polygons based on input dimensions and magnitude.\n\n    This function creates a grid of polygons and applies random distortions to the internal vertices,\n    while keeping the boundary vertices fixed. The distortion is applied consistently across shared\n    vertices to avoid gaps or overlaps in the resulting grid.\n\n    Args:\n        dimensions (np.ndarray): A 3D array of shape (grid_height, grid_width, 4) where each element\n                                 is [x_min, y_min, x_max, y_max] representing the dimensions of a grid cell.\n        magnitude (int): Maximum pixel-wise displacement for distortion. The actual displacement\n                         will be randomly chosen in the range [-magnitude, magnitude].\n\n    Returns:\n        np.ndarray: A 2D array of shape (total_cells, 8) where each row represents a distorted polygon\n                    as [x1, y1, x2, y1, x2, y2, x1, y2]. The total_cells is equal to grid_height * grid_width.\n\n    Note:\n        - Only internal grid points are distorted; boundary points remain fixed.\n        - The function ensures consistent distortion across shared vertices of adjacent cells.\n        - The distortion is applied to the following points of each internal cell:\n            * Bottom-right of the cell above and to the left\n            * Bottom-left of the cell above\n            * Top-right of the cell to the left\n            * Top-left of the current cell\n        - Each square represents a cell, and the X marks indicate the coordinates where displacement occurs.\n            +--+--+--+--+\n            |  |  |  |  |\n            +--X--X--X--+\n            |  |  |  |  |\n            +--X--X--X--+\n            |  |  |  |  |\n            +--X--X--X--+\n            |  |  |  |  |\n            +--+--+--+--+\n        - For each X, the coordinates of the left, right, top, and bottom edges\n          in the four adjacent cells are displaced.\n\n    Example:\n        &gt;&gt;&gt; dimensions = np.array([[[0, 0, 50, 50], [50, 0, 100, 50]],\n        ...                        [[0, 50, 50, 100], [50, 50, 100, 100]]])\n        &gt;&gt;&gt; distorted = generate_distorted_grid_polygons(dimensions, magnitude=10)\n        &gt;&gt;&gt; distorted.shape\n        (4, 8)\n    \"\"\"\n    grid_height, grid_width = dimensions.shape[:2]\n    total_cells = grid_height * grid_width\n\n    # Initialize polygons\n    polygons = np.zeros((total_cells, 8), dtype=np.float32)\n    polygons[:, 0:2] = dimensions.reshape(-1, 4)[:, [0, 1]]  # x1, y1\n    polygons[:, 2:4] = dimensions.reshape(-1, 4)[:, [2, 1]]  # x2, y1\n    polygons[:, 4:6] = dimensions.reshape(-1, 4)[:, [2, 3]]  # x2, y2\n    polygons[:, 6:8] = dimensions.reshape(-1, 4)[:, [0, 3]]  # x1, y2\n\n    # Generate displacements for internal grid points only\n    internal_points_height, internal_points_width = grid_height - 1, grid_width - 1\n    displacements = random_utils.randint(\n        -magnitude,\n        magnitude + 1,\n        size=(internal_points_height, internal_points_width, 2),\n    ).astype(np.float32)\n\n    # Apply displacements to internal polygon vertices\n    for i in range(1, grid_height):\n        for j in range(1, grid_width):\n            dx, dy = displacements[i - 1, j - 1]\n\n            # Bottom-right of cell (i-1, j-1)\n            polygons[(i - 1) * grid_width + (j - 1), 4:6] += [dx, dy]\n\n            # Bottom-left of cell (i-1, j)\n            polygons[(i - 1) * grid_width + j, 6:8] += [dx, dy]\n\n            # Top-right of cell (i, j-1)\n            polygons[i * grid_width + (j - 1), 2:4] += [dx, dy]\n\n            # Top-left of cell (i, j)\n            polygons[i * grid_width + j, 0:2] += [dx, dy]\n\n    return polygons\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.functional.generate_reflected_bboxes","title":"<code>def generate_reflected_bboxes    (bboxes, grid_dims, image_shape, center_in_origin=False)    </code> [view source on GitHub]","text":"<p>Generate reflected bounding boxes for the entire reflection grid.</p> <p>Parameters:</p> Name Type Description <code>bboxes</code> <code>np.ndarray</code> <p>Original bounding boxes.</p> <code>grid_dims</code> <code>dict[str, tuple[int, int]]</code> <p>Grid dimensions and original position.</p> <code>image_shape</code> <code>tuple[int, int]</code> <p>Shape of the original image as (height, width).</p> <code>center_in_origin</code> <code>bool</code> <p>If True, center the grid at the origin. Default is False.</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Array of reflected and shifted bounding boxes for the entire grid.</p> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>def generate_reflected_bboxes(\n    bboxes: np.ndarray,\n    grid_dims: dict[str, tuple[int, int]],\n    image_shape: tuple[int, int],\n    center_in_origin: bool = False,\n) -&gt; np.ndarray:\n    \"\"\"Generate reflected bounding boxes for the entire reflection grid.\n\n    Args:\n        bboxes (np.ndarray): Original bounding boxes.\n        grid_dims (dict[str, tuple[int, int]]): Grid dimensions and original position.\n        image_shape (tuple[int, int]): Shape of the original image as (height, width).\n        center_in_origin (bool): If True, center the grid at the origin. Default is False.\n\n    Returns:\n        np.ndarray: Array of reflected and shifted bounding boxes for the entire grid.\n    \"\"\"\n    rows, cols = image_shape[:2]\n    grid_rows, grid_cols = grid_dims[\"grid_shape\"]\n    original_row, original_col = grid_dims[\"original_position\"]\n\n    # Prepare flipped versions of bboxes\n    bboxes_hflipped = flip_bboxes(bboxes, flip_horizontal=True, image_shape=image_shape)\n    bboxes_vflipped = flip_bboxes(bboxes, flip_vertical=True, image_shape=image_shape)\n    bboxes_hvflipped = flip_bboxes(bboxes, flip_horizontal=True, flip_vertical=True, image_shape=image_shape)\n\n    # Shift all versions to the original position\n    shift_vector = np.array([original_col * cols, original_row * rows, original_col * cols, original_row * rows])\n    bboxes = shift_bboxes(bboxes, shift_vector)\n    bboxes_hflipped = shift_bboxes(bboxes_hflipped, shift_vector)\n    bboxes_vflipped = shift_bboxes(bboxes_vflipped, shift_vector)\n    bboxes_hvflipped = shift_bboxes(bboxes_hvflipped, shift_vector)\n\n    new_bboxes = []\n\n    for grid_row in range(grid_rows):\n        for grid_col in range(grid_cols):\n            # Determine which version of bboxes to use based on grid position\n            if (grid_row - original_row) % 2 == 0 and (grid_col - original_col) % 2 == 0:\n                current_bboxes = bboxes\n            elif (grid_row - original_row) % 2 == 0:\n                current_bboxes = bboxes_hflipped\n            elif (grid_col - original_col) % 2 == 0:\n                current_bboxes = bboxes_vflipped\n            else:\n                current_bboxes = bboxes_hvflipped\n\n            # Shift to the current grid cell\n            cell_shift = np.array(\n                [\n                    (grid_col - original_col) * cols,\n                    (grid_row - original_row) * rows,\n                    (grid_col - original_col) * cols,\n                    (grid_row - original_row) * rows,\n                ],\n            )\n            shifted_bboxes = shift_bboxes(current_bboxes, cell_shift)\n\n            new_bboxes.append(shifted_bboxes)\n\n    result = np.vstack(new_bboxes)\n\n    return shift_bboxes(result, -shift_vector) if center_in_origin else result\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.functional.generate_reflected_keypoints","title":"<code>def generate_reflected_keypoints    (keypoints, grid_dims, image_shape, center_in_origin=False)    </code> [view source on GitHub]","text":"<p>Generate reflected keypoints for the entire reflection grid.</p> <p>This function creates a grid of keypoints by reflecting and shifting the original keypoints. It handles both centered and non-centered grids based on the <code>center_in_origin</code> parameter.</p> <p>Parameters:</p> Name Type Description <code>keypoints</code> <code>np.ndarray</code> <p>Original keypoints array of shape (N, 4+), where N is the number of keypoints,                     and each keypoint is represented by at least 4 values (x, y, angle, scale, ...).</p> <code>grid_dims</code> <code>dict[str, tuple[int, int]]</code> <p>A dictionary containing grid dimensions and original position. It should have the following keys: - \"grid_shape\": tuple[int, int] representing (grid_rows, grid_cols) - \"original_position\": tuple[int, int] representing (original_row, original_col)</p> <code>image_shape</code> <code>tuple[int, int]</code> <p>Shape of the original image as (height, width).</p> <code>center_in_origin</code> <code>bool</code> <p>If True, center the grid at the origin. Default is False.</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Array of reflected and shifted keypoints for the entire grid. The shape is             (N * grid_rows * grid_cols, 4+), where N is the number of original keypoints.</p> <p>Note</p> <ul> <li>The function handles keypoint flipping and shifting to create a grid of reflected keypoints.</li> <li>It preserves the angle and scale information of the keypoints during transformations.</li> <li>The resulting grid can be either centered at the origin or positioned based on the original grid.</li> </ul> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>def generate_reflected_keypoints(\n    keypoints: np.ndarray,\n    grid_dims: dict[str, tuple[int, int]],\n    image_shape: tuple[int, int],\n    center_in_origin: bool = False,\n) -&gt; np.ndarray:\n    \"\"\"Generate reflected keypoints for the entire reflection grid.\n\n    This function creates a grid of keypoints by reflecting and shifting the original keypoints.\n    It handles both centered and non-centered grids based on the `center_in_origin` parameter.\n\n    Args:\n        keypoints (np.ndarray): Original keypoints array of shape (N, 4+), where N is the number of keypoints,\n                                and each keypoint is represented by at least 4 values (x, y, angle, scale, ...).\n        grid_dims (dict[str, tuple[int, int]]): A dictionary containing grid dimensions and original position.\n            It should have the following keys:\n            - \"grid_shape\": tuple[int, int] representing (grid_rows, grid_cols)\n            - \"original_position\": tuple[int, int] representing (original_row, original_col)\n        image_shape (tuple[int, int]): Shape of the original image as (height, width).\n        center_in_origin (bool, optional): If True, center the grid at the origin. Default is False.\n\n    Returns:\n        np.ndarray: Array of reflected and shifted keypoints for the entire grid. The shape is\n                    (N * grid_rows * grid_cols, 4+), where N is the number of original keypoints.\n\n    Note:\n        - The function handles keypoint flipping and shifting to create a grid of reflected keypoints.\n        - It preserves the angle and scale information of the keypoints during transformations.\n        - The resulting grid can be either centered at the origin or positioned based on the original grid.\n    \"\"\"\n    grid_rows, grid_cols = grid_dims[\"grid_shape\"]\n    original_row, original_col = grid_dims[\"original_position\"]\n\n    # Prepare flipped versions of keypoints\n    keypoints_hflipped = flip_keypoints(keypoints, flip_horizontal=True, image_shape=image_shape)\n    keypoints_vflipped = flip_keypoints(keypoints, flip_vertical=True, image_shape=image_shape)\n    keypoints_hvflipped = flip_keypoints(keypoints, flip_horizontal=True, flip_vertical=True, image_shape=image_shape)\n\n    rows, cols = image_shape[:2]\n\n    # Shift all versions to the original position\n    shift_vector = np.array([original_col * cols, original_row * rows, 0, 0])  # Only shift x and y\n    keypoints = shift_keypoints(keypoints, shift_vector)\n    keypoints_hflipped = shift_keypoints(keypoints_hflipped, shift_vector)\n    keypoints_vflipped = shift_keypoints(keypoints_vflipped, shift_vector)\n    keypoints_hvflipped = shift_keypoints(keypoints_hvflipped, shift_vector)\n\n    new_keypoints = []\n\n    for grid_row in range(grid_rows):\n        for grid_col in range(grid_cols):\n            # Determine which version of keypoints to use based on grid position\n            if (grid_row - original_row) % 2 == 0 and (grid_col - original_col) % 2 == 0:\n                current_keypoints = keypoints\n            elif (grid_row - original_row) % 2 == 0:\n                current_keypoints = keypoints_hflipped\n            elif (grid_col - original_col) % 2 == 0:\n                current_keypoints = keypoints_vflipped\n            else:\n                current_keypoints = keypoints_hvflipped\n\n            # Shift to the current grid cell\n            cell_shift = np.array([(grid_col - original_col) * cols, (grid_row - original_row) * rows, 0, 0])\n            shifted_keypoints = shift_keypoints(current_keypoints, cell_shift)\n\n            new_keypoints.append(shifted_keypoints)\n\n    result = np.vstack(new_keypoints)\n\n    return shift_keypoints(result, -shift_vector) if center_in_origin else result\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.functional.get_pad_grid_dimensions","title":"<code>def get_pad_grid_dimensions    (pad_top, pad_bottom, pad_left, pad_right, image_shape)    </code> [view source on GitHub]","text":"<p>Calculate the dimensions of the grid needed for reflection padding and the position of the original image.</p> <p>Parameters:</p> Name Type Description <code>pad_top</code> <code>int</code> <p>Number of pixels to pad above the image.</p> <code>pad_bottom</code> <code>int</code> <p>Number of pixels to pad below the image.</p> <code>pad_left</code> <code>int</code> <p>Number of pixels to pad to the left of the image.</p> <code>pad_right</code> <code>int</code> <p>Number of pixels to pad to the right of the image.</p> <code>image_shape</code> <code>tuple[int, int]</code> <p>Shape of the original image as (height, width).</p> <p>Returns:</p> Type Description <code>dict[str, tuple[int, int]]</code> <p>A dictionary containing:     - 'grid_shape': A tuple (grid_rows, grid_cols) where:         - grid_rows (int): Number of times the image needs to be repeated vertically.         - grid_cols (int): Number of times the image needs to be repeated horizontally.     - 'original_position': A tuple (original_row, original_col) where:         - original_row (int): Row index of the original image in the grid.         - original_col (int): Column index of the original image in the grid.</p> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>def get_pad_grid_dimensions(\n    pad_top: int,\n    pad_bottom: int,\n    pad_left: int,\n    pad_right: int,\n    image_shape: tuple[int, int],\n) -&gt; dict[str, tuple[int, int]]:\n    \"\"\"Calculate the dimensions of the grid needed for reflection padding and the position of the original image.\n\n    Args:\n        pad_top (int): Number of pixels to pad above the image.\n        pad_bottom (int): Number of pixels to pad below the image.\n        pad_left (int): Number of pixels to pad to the left of the image.\n        pad_right (int): Number of pixels to pad to the right of the image.\n        image_shape (tuple[int, int]): Shape of the original image as (height, width).\n\n    Returns:\n        dict[str, tuple[int, int]]: A dictionary containing:\n            - 'grid_shape': A tuple (grid_rows, grid_cols) where:\n                - grid_rows (int): Number of times the image needs to be repeated vertically.\n                - grid_cols (int): Number of times the image needs to be repeated horizontally.\n            - 'original_position': A tuple (original_row, original_col) where:\n                - original_row (int): Row index of the original image in the grid.\n                - original_col (int): Column index of the original image in the grid.\n    \"\"\"\n    rows, cols = image_shape[:2]\n\n    grid_rows = 1 + math.ceil(pad_top / rows) + math.ceil(pad_bottom / rows)\n    grid_cols = 1 + math.ceil(pad_left / cols) + math.ceil(pad_right / cols)\n    original_row = math.ceil(pad_top / rows)\n    original_col = math.ceil(pad_left / cols)\n\n    return {\"grid_shape\": (grid_rows, grid_cols), \"original_position\": (original_row, original_col)}\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.functional.keypoint_d4","title":"<code>def keypoint_d4    (keypoint, group_member, image_shape, ** params)    </code> [view source on GitHub]","text":"<p>Applies a <code>D_4</code> symmetry group transformation to a keypoint.</p> <p>This function adjusts a keypoint's coordinates according to the specified <code>D_4</code> group transformation, which includes rotations and reflections suitable for image processing tasks. These transformations account for the dimensions of the image to ensure the keypoint remains within its boundaries.</p> <ul> <li>keypoint (KeypointInternalType): The keypoint to transform. T     his should be a structure or tuple specifying coordinates     like (x, y, [additional parameters]).</li> <li>group_member (D4Type): A string identifier for the <code>D_4</code> group transformation to apply.     Valid values are 'e', 'r90', 'r180', 'r270', 'v', 'hv', 'h', 't'.</li> <li>image_shape (tuple[int, int]): The shape of the image.</li> <li>params (Any): Not used</li> </ul> <ul> <li>KeypointInternalType: The transformed keypoint.</li> </ul> <ul> <li>ValueError: If an invalid group member is specified, indicating that the specified transformation does not exist.</li> </ul> <p>Examples:</p> <ul> <li>Rotating a keypoint by 90 degrees in a 100x100 image:   <code>keypoint_d4((50, 30), 'r90', 100, 100)</code>   This would move the keypoint from (50, 30) to (70, 50) assuming standard coordinate transformations.</li> </ul> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>def keypoint_d4(\n    keypoint: KeypointInternalType,\n    group_member: D4Type,\n    image_shape: tuple[int, int],\n    **params: Any,\n) -&gt; KeypointInternalType:\n    \"\"\"Applies a `D_4` symmetry group transformation to a keypoint.\n\n    This function adjusts a keypoint's coordinates according to the specified `D_4` group transformation,\n    which includes rotations and reflections suitable for image processing tasks. These transformations account\n    for the dimensions of the image to ensure the keypoint remains within its boundaries.\n\n    Parameters:\n    - keypoint (KeypointInternalType): The keypoint to transform. T\n        his should be a structure or tuple specifying coordinates\n        like (x, y, [additional parameters]).\n    - group_member (D4Type): A string identifier for the `D_4` group transformation to apply.\n        Valid values are 'e', 'r90', 'r180', 'r270', 'v', 'hv', 'h', 't'.\n    - image_shape (tuple[int, int]): The shape of the image.\n    - params (Any): Not used\n\n    Returns:\n    - KeypointInternalType: The transformed keypoint.\n\n    Raises:\n    - ValueError: If an invalid group member is specified, indicating that the specified transformation does not exist.\n\n    Examples:\n    - Rotating a keypoint by 90 degrees in a 100x100 image:\n      `keypoint_d4((50, 30), 'r90', 100, 100)`\n      This would move the keypoint from (50, 30) to (70, 50) assuming standard coordinate transformations.\n    \"\"\"\n    rows, cols = image_shape[:2]\n    transformations = {\n        \"e\": lambda x: x,  # Identity transformation\n        \"r90\": lambda x: keypoint_rot90(x, 1, image_shape),  # Rotate 90 degrees\n        \"r180\": lambda x: keypoint_rot90(x, 2, image_shape),  # Rotate 180 degrees\n        \"r270\": lambda x: keypoint_rot90(x, 3, image_shape),  # Rotate 270 degrees\n        \"v\": lambda x: keypoint_vflip(x, rows),  # Vertical flip\n        \"hvt\": lambda x: keypoint_transpose(keypoint_rot90(x, 2, image_shape)),  # Reflect over anti diagonal\n        \"h\": lambda x: keypoint_hflip(x, cols),  # Horizontal flip\n        \"t\": lambda x: keypoint_transpose(x),  # Transpose (reflect over main diagonal)\n    }\n    # Execute the appropriate transformation\n    if group_member in transformations:\n        return transformations[group_member](keypoint)\n\n    raise ValueError(f\"Invalid group member: {group_member}\")\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.functional.keypoint_flip","title":"<code>def keypoint_flip    (keypoint, d, image_shape)    </code> [view source on GitHub]","text":"<p>Flip a keypoint either vertically, horizontally or both depending on the value of <code>d</code>.</p> <p>Parameters:</p> Name Type Description <code>keypoint</code> <code>KeypointInternalType</code> <p>A keypoint <code>(x, y, angle, scale)</code>.</p> <code>d</code> <code>int</code> <p>Number of flip. Must be -1, 0 or 1: * 0 - vertical flip, * 1 - horizontal flip, * -1 - vertical and horizontal flip.</p> <code>image_shape</code> <code>tuple[int, int]</code> <p>A tuple of image shape <code>(height, width, channels)</code>.</p> <p>Returns:</p> Type Description <code>KeypointInternalType</code> <p>A keypoint <code>(x, y, angle, scale)</code>.</p> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>if value of <code>d</code> is not -1, 0 or 1.</p> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>@angle_2pi_range\ndef keypoint_flip(keypoint: KeypointInternalType, d: int, image_shape: tuple[int, int]) -&gt; KeypointInternalType:\n    \"\"\"Flip a keypoint either vertically, horizontally or both depending on the value of `d`.\n\n    Args:\n        keypoint: A keypoint `(x, y, angle, scale)`.\n        d: Number of flip. Must be -1, 0 or 1:\n            * 0 - vertical flip,\n            * 1 - horizontal flip,\n            * -1 - vertical and horizontal flip.\n        image_shape: A tuple of image shape `(height, width, channels)`.\n\n    Returns:\n        A keypoint `(x, y, angle, scale)`.\n\n    Raises:\n        ValueError: if value of `d` is not -1, 0 or 1.\n\n    \"\"\"\n    rows, cols = image_shape[:2]\n\n    if d == 0:\n        keypoint = keypoint_vflip(keypoint, rows)\n    elif d == 1:\n        keypoint = keypoint_hflip(keypoint, cols)\n    elif d == -1:\n        keypoint = keypoint_hflip(keypoint, cols)\n        keypoint = keypoint_vflip(keypoint, rows)\n    else:\n        raise ValueError(f\"Invalid d value {d}. Valid values are -1, 0 and 1\")\n    return keypoint\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.functional.keypoint_hflip","title":"<code>def keypoint_hflip    (keypoint, cols)    </code> [view source on GitHub]","text":"<p>Flip a keypoint horizontally around the y-axis.</p> <p>Parameters:</p> Name Type Description <code>keypoint</code> <code>KeypointInternalType</code> <p>A keypoint <code>(x, y, angle, scale)</code>.</p> <code>cols</code> <code>int</code> <p>Image width.</p> <p>Returns:</p> Type Description <code>KeypointInternalType</code> <p>A keypoint <code>(x, y, angle, scale)</code>.</p> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>@angle_2pi_range\ndef keypoint_hflip(keypoint: KeypointInternalType, cols: int) -&gt; KeypointInternalType:\n    \"\"\"Flip a keypoint horizontally around the y-axis.\n\n    Args:\n        keypoint: A keypoint `(x, y, angle, scale)`.\n        cols: Image width.\n\n    Returns:\n        A keypoint `(x, y, angle, scale)`.\n\n    \"\"\"\n    x, y, angle, scale = keypoint[:4]\n    angle = math.pi - angle\n    return (cols - 1) - x, y, angle, scale\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.functional.keypoint_rot90","title":"<code>def keypoint_rot90    (keypoint, factor, image_shape, ** params)    </code> [view source on GitHub]","text":"<p>Rotate a keypoint by 90 degrees counter-clockwise (CCW) a specified number of times.</p> <p>Parameters:</p> Name Type Description <code>keypoint</code> <code>KeypointInternalType</code> <p>A keypoint in the format <code>(x, y, angle, scale)</code>.</p> <code>factor</code> <code>int</code> <p>The number of 90 degree CCW rotations to apply. Must be in the range [0, 3].</p> <code>image_shape</code> <code>tuple[int, int]</code> <p>The shape of the image.</p> <code>**params</code> <code>Any</code> <p>Additional parameters.</p> <p>Returns:</p> Type Description <code>KeypointInternalType</code> <p>The rotated keypoint in the format <code>(x, y, angle, scale)</code>.</p> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If the factor is not in the set {0, 1, 2, 3}.</p> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>@angle_2pi_range\ndef keypoint_rot90(\n    keypoint: KeypointInternalType,\n    factor: int,\n    image_shape: tuple[int, int],\n    **params: Any,\n) -&gt; KeypointInternalType:\n    \"\"\"Rotate a keypoint by 90 degrees counter-clockwise (CCW) a specified number of times.\n\n    Args:\n        keypoint (KeypointInternalType): A keypoint in the format `(x, y, angle, scale)`.\n        factor (int): The number of 90 degree CCW rotations to apply. Must be in the range [0, 3].\n        image_shape (tuple[int, int]): The shape of the image.\n        **params: Additional parameters.\n\n    Returns:\n        KeypointInternalType: The rotated keypoint in the format `(x, y, angle, scale)`.\n\n    Raises:\n        ValueError: If the factor is not in the set {0, 1, 2, 3}.\n    \"\"\"\n    x, y, angle, scale = keypoint\n\n    if factor not in {0, 1, 2, 3}:\n        raise ValueError(\"Parameter factor must be in set {0, 1, 2, 3}\")\n\n    rows, cols = image_shape[:2]\n\n    if factor == 1:\n        x, y, angle = y, (cols - 1) - x, angle - math.pi / 2\n    elif factor == ROT90_180_FACTOR:\n        x, y, angle = (cols - 1) - x, (rows - 1) - y, angle - math.pi\n    elif factor == ROT90_270_FACTOR:\n        x, y, angle = (rows - 1) - y, x, angle + math.pi / 2\n\n    return x, y, angle, scale\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.functional.keypoint_rotate","title":"<code>def keypoint_rotate    (keypoint, angle, image_shape, ** params)    </code> [view source on GitHub]","text":"<p>Rotate a keypoint by a specified angle.</p> <p>Parameters:</p> Name Type Description <code>keypoint</code> <code>KeypointInternalType</code> <p>A keypoint in the format <code>(x, y, angle, scale)</code>.</p> <code>angle</code> <code>float</code> <p>The angle by which to rotate the keypoint, in degrees.</p> <code>image_shape</code> <code>tuple[int, int]</code> <p>The shape of the image the keypoint belongs to.</p> <code>**params</code> <code>Any</code> <p>Additional parameters.</p> <p>Returns:</p> Type Description <code>KeypointInternalType</code> <p>The rotated keypoint in the format <code>(x, y, angle, scale)</code>.</p> <p>Note</p> <p>The rotation is performed around the center of the image.</p> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>@angle_2pi_range\ndef keypoint_rotate(\n    keypoint: KeypointInternalType,\n    angle: float,\n    image_shape: tuple[int, int],\n    **params: Any,\n) -&gt; KeypointInternalType:\n    \"\"\"Rotate a keypoint by a specified angle.\n\n    Args:\n        keypoint (KeypointInternalType): A keypoint in the format `(x, y, angle, scale)`.\n        angle (float): The angle by which to rotate the keypoint, in degrees.\n        image_shape (tuple[int, int]): The shape of the image the keypoint belongs to.\n        **params: Additional parameters.\n\n    Returns:\n        KeypointInternalType: The rotated keypoint in the format `(x, y, angle, scale)`.\n\n    Note:\n        The rotation is performed around the center of the image.\n    \"\"\"\n    image_center = center(image_shape)\n    matrix = cv2.getRotationMatrix2D(image_center, angle, 1.0)\n    x, y, a, s = keypoint[:4]\n    x, y = cv2.transform(np.array([[[x, y]]]), matrix).squeeze()\n    return x, y, a + math.radians(angle), s\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.functional.keypoint_scale","title":"<code>def keypoint_scale    (keypoint, scale_x, scale_y)    </code> [view source on GitHub]","text":"<p>Scales a keypoint by scale_x and scale_y.</p> <p>Parameters:</p> Name Type Description <code>keypoint</code> <code>KeypointInternalType</code> <p>A keypoint <code>(x, y, angle, scale)</code>.</p> <code>scale_x</code> <code>float</code> <p>Scale coefficient x-axis.</p> <code>scale_y</code> <code>float</code> <p>Scale coefficient y-axis.</p> <p>Returns:</p> Type Description <code>KeypointInternalType</code> <p>A keypoint <code>(x, y, angle, scale)</code>.</p> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>def keypoint_scale(keypoint: KeypointInternalType, scale_x: float, scale_y: float) -&gt; KeypointInternalType:\n    \"\"\"Scales a keypoint by scale_x and scale_y.\n\n    Args:\n        keypoint: A keypoint `(x, y, angle, scale)`.\n        scale_x: Scale coefficient x-axis.\n        scale_y: Scale coefficient y-axis.\n\n    Returns:\n        A keypoint `(x, y, angle, scale)`.\n\n    \"\"\"\n    x, y, angle, scale = keypoint[:4]\n    return x * scale_x, y * scale_y, angle, scale * max(scale_x, scale_y)\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.functional.keypoint_transpose","title":"<code>def keypoint_transpose    (keypoint)    </code> [view source on GitHub]","text":"<p>Transposes a keypoint along a specified axis: main diagonal</p> <p>Parameters:</p> Name Type Description <code>keypoint</code> <code>KeypointInternalType</code> <p>A keypoint <code>(x, y, angle, scale)</code>.</p> <p>Returns:</p> Type Description <code>KeypointInternalType</code> <p>A transformed keypoint <code>(x, y, angle, scale)</code>.</p> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If axis is not 0 or 1.</p> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>@angle_2pi_range\ndef keypoint_transpose(keypoint: KeypointInternalType) -&gt; KeypointInternalType:\n    \"\"\"Transposes a keypoint along a specified axis: main diagonal\n\n    Args:\n        keypoint: A keypoint `(x, y, angle, scale)`.\n\n    Returns:\n        A transformed keypoint `(x, y, angle, scale)`.\n\n    Raises:\n        ValueError: If axis is not 0 or 1.\n\n    \"\"\"\n    x, y, angle, scale = keypoint[:4]\n\n    # Transpose over the main diagonal: swap x and y.\n    new_x, new_y = y, x\n    # Adjust angle to reflect the coordinate swap.\n    angle = np.pi / 2 - angle if angle &lt;= np.pi else 3 * np.pi / 2 - angle\n\n    return new_x, new_y, angle, scale\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.functional.keypoint_vflip","title":"<code>def keypoint_vflip    (keypoint, rows)    </code> [view source on GitHub]","text":"<p>Flip a keypoint vertically around the x-axis.</p> <p>Parameters:</p> Name Type Description <code>keypoint</code> <code>KeypointInternalType</code> <p>A keypoint <code>(x, y, angle, scale)</code>.</p> <code>rows</code> <code>int</code> <p>Image height.</p> <p>Returns:</p> Type Description <code>tuple</code> <p>A keypoint <code>(x, y, angle, scale)</code>.</p> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>@angle_2pi_range\ndef keypoint_vflip(keypoint: KeypointInternalType, rows: int) -&gt; KeypointInternalType:\n    \"\"\"Flip a keypoint vertically around the x-axis.\n\n    Args:\n        keypoint: A keypoint `(x, y, angle, scale)`.\n        rows: Image height.\n\n\n    Returns:\n        tuple: A keypoint `(x, y, angle, scale)`.\n\n    \"\"\"\n    x, y, angle, scale = keypoint[:4]\n    angle = -angle\n    return x, (rows - 1) - y, angle, scale\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.functional.keypoints_affine","title":"<code>def keypoints_affine    (keypoints, matrix, image_shape, scale, mode)    </code> [view source on GitHub]","text":"<p>Apply an affine transformation to keypoints.</p> <p>This function transforms keypoints using the given affine transformation matrix. It handles reflection padding if necessary, updates coordinates, angles, and scales.</p> <p>Parameters:</p> Name Type Description <code>keypoints</code> <code>np.ndarray</code> <p>Array of keypoints with shape (N, 4+) where N is the number of keypoints.                     Each keypoint is represented as [x, y, angle, scale, ...].</p> <code>matrix</code> <code>skimage.transform.ProjectiveTransform</code> <p>The affine transformation matrix.</p> <code>image_shape</code> <code>tuple[int, int]</code> <p>Shape of the image (height, width).</p> <code>scale</code> <code>dict[str, Any]</code> <p>Dictionary containing scale factors for x and y directions.                     Expected keys are 'x' and 'y'.</p> <code>mode</code> <code>int</code> <p>Border mode for handling keypoints near image edges.         Use cv2.BORDER_REFLECT_101, cv2.BORDER_REFLECT, etc.</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Transformed keypoints array with the same shape as input.</p> <p>Notes</p> <ul> <li>The function applies reflection padding if the mode is in REFLECT_BORDER_MODES.</li> <li>Coordinates (x, y) are transformed using the affine matrix.</li> <li>Angles are adjusted based on the rotation component of the affine transformation.</li> <li>Scales are multiplied by the maximum of x and y scale factors.</li> <li>The @angle_2pi_range decorator ensures angles remain in the [0, 2\u03c0] range.</li> </ul> <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; keypoints = np.array([[100, 100, 0, 1]])\n&gt;&gt;&gt; matrix = skimage.transform.ProjectiveTransform(...)\n&gt;&gt;&gt; scale = {'x': 1.5, 'y': 1.2}\n&gt;&gt;&gt; transformed_keypoints = keypoints_affine(keypoints, matrix, (480, 640), scale, cv2.BORDER_REFLECT_101)\n</code></pre> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>@angle_2pi_range\ndef keypoints_affine(\n    keypoints: np.ndarray,\n    matrix: skimage.transform.ProjectiveTransform,\n    image_shape: tuple[int, int],\n    scale: dict[str, Any],\n    mode: int,\n) -&gt; np.ndarray:\n    \"\"\"Apply an affine transformation to keypoints.\n\n    This function transforms keypoints using the given affine transformation matrix.\n    It handles reflection padding if necessary, updates coordinates, angles, and scales.\n\n    Args:\n        keypoints (np.ndarray): Array of keypoints with shape (N, 4+) where N is the number of keypoints.\n                                Each keypoint is represented as [x, y, angle, scale, ...].\n        matrix (skimage.transform.ProjectiveTransform): The affine transformation matrix.\n        image_shape (tuple[int, int]): Shape of the image (height, width).\n        scale (dict[str, Any]): Dictionary containing scale factors for x and y directions.\n                                Expected keys are 'x' and 'y'.\n        mode (int): Border mode for handling keypoints near image edges.\n                    Use cv2.BORDER_REFLECT_101, cv2.BORDER_REFLECT, etc.\n\n    Returns:\n        np.ndarray: Transformed keypoints array with the same shape as input.\n\n    Notes:\n        - The function applies reflection padding if the mode is in REFLECT_BORDER_MODES.\n        - Coordinates (x, y) are transformed using the affine matrix.\n        - Angles are adjusted based on the rotation component of the affine transformation.\n        - Scales are multiplied by the maximum of x and y scale factors.\n        - The @angle_2pi_range decorator ensures angles remain in the [0, 2\u03c0] range.\n\n    Example:\n        &gt;&gt;&gt; keypoints = np.array([[100, 100, 0, 1]])\n        &gt;&gt;&gt; matrix = skimage.transform.ProjectiveTransform(...)\n        &gt;&gt;&gt; scale = {'x': 1.5, 'y': 1.2}\n        &gt;&gt;&gt; transformed_keypoints = keypoints_affine(keypoints, matrix, (480, 640), scale, cv2.BORDER_REFLECT_101)\n    \"\"\"\n    keypoints = keypoints.copy().astype(np.float32)\n\n    if is_identity_matrix(matrix):\n        return keypoints\n\n    if mode in REFLECT_BORDER_MODES:\n        # Step 1: Compute affine transform padding\n        pad_left, pad_right, pad_top, pad_bottom = calculate_affine_transform_padding(matrix, image_shape)\n        grid_dimensions = get_pad_grid_dimensions(pad_top, pad_bottom, pad_left, pad_right, image_shape)\n        keypoints = generate_reflected_keypoints(keypoints, grid_dimensions, image_shape, center_in_origin=True)\n\n    # Extract x, y coordinates\n    xy = keypoints[:, :2]\n\n    # Transform x, y coordinates\n    xy_transformed = cv2.transform(xy.reshape(-1, 1, 2), matrix.params[:2]).squeeze()\n\n    # Calculate angle adjustment\n    angle_adjustment = rotation2d_matrix_to_euler_angles(matrix.params[:2], y_up=False)\n\n    # Update angles\n    keypoints[:, 2] = keypoints[:, 2] + angle_adjustment\n\n    # Update scales\n    max_scale = max(scale[\"x\"], scale[\"y\"])\n\n    keypoints[:, 3] *= max_scale\n\n    # Update x, y coordinates\n    keypoints[:, :2] = xy_transformed\n\n    return keypoints\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.functional.optical_distortion","title":"<code>def optical_distortion    (img, k, dx, dy, interpolation, border_mode, value=None)    </code> [view source on GitHub]","text":"<p>Barrel / pincushion distortion. Unconventional augment.</p> <p>Reference</p> <p>|  https://stackoverflow.com/questions/6199636/formulas-for-barrel-pincushion-distortion |  https://stackoverflow.com/questions/10364201/image-transformation-in-opencv |  https://stackoverflow.com/questions/2477774/correcting-fisheye-distortion-programmatically |  http://www.coldvision.io/2017/03/02/advanced-lane-finding-using-opencv/</p> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>@preserve_channel_dim\ndef optical_distortion(\n    img: np.ndarray,\n    k: int,\n    dx: int,\n    dy: int,\n    interpolation: int,\n    border_mode: int,\n    value: ColorType | None = None,\n) -&gt; np.ndarray:\n    \"\"\"Barrel / pincushion distortion. Unconventional augment.\n\n    Reference:\n        |  https://stackoverflow.com/questions/6199636/formulas-for-barrel-pincushion-distortion\n        |  https://stackoverflow.com/questions/10364201/image-transformation-in-opencv\n        |  https://stackoverflow.com/questions/2477774/correcting-fisheye-distortion-programmatically\n        |  http://www.coldvision.io/2017/03/02/advanced-lane-finding-using-opencv/\n    \"\"\"\n    height, width = img.shape[:2]\n\n    fx = width\n    fy = height\n\n    cx = width * 0.5 + dx\n    cy = height * 0.5 + dy\n\n    camera_matrix = np.array([[fx, 0, cx], [0, fy, cy], [0, 0, 1]], dtype=np.float32)\n\n    distortion = np.array([k, k, 0, 0, 0], dtype=np.float32)\n    map1, map2 = cv2.initUndistortRectifyMap(camera_matrix, distortion, None, None, (width, height), cv2.CV_32FC1)\n    return cv2.remap(img, map1, map2, interpolation=interpolation, borderMode=border_mode, borderValue=value)\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.functional.rotation2d_matrix_to_euler_angles","title":"<code>def rotation2d_matrix_to_euler_angles    (matrix, y_up)    </code> [view source on GitHub]","text":"<p>matrix (np.ndarray): Rotation matrix y_up (bool): is Y axis looks up or down</p> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>def rotation2d_matrix_to_euler_angles(matrix: np.ndarray, y_up: bool) -&gt; float:\n    \"\"\"Args:\n    matrix (np.ndarray): Rotation matrix\n    y_up (bool): is Y axis looks up or down\n\n    \"\"\"\n    if y_up:\n        return np.arctan2(matrix[1, 0], matrix[0, 0])\n    return np.arctan2(-matrix[1, 0], matrix[0, 0])\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.functional.shift_bboxes","title":"<code>def shift_bboxes    (bboxes, shift_vector)    </code> [view source on GitHub]","text":"<p>Shift bounding boxes by a given vector.</p> <p>Parameters:</p> Name Type Description <code>bboxes</code> <code>np.ndarray</code> <p>Array of bounding boxes with shape (n, m) where n is the number of bboxes                  and m &gt;= 4. The first 4 columns are [x_min, y_min, x_max, y_max].</p> <code>shift_vector</code> <code>np.ndarray</code> <p>Vector to shift the bounding boxes by, with shape (4,) for                        [shift_x, shift_y, shift_x, shift_y].</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Shifted bounding boxes with the same shape as input.</p> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>def shift_bboxes(bboxes: np.ndarray, shift_vector: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Shift bounding boxes by a given vector.\n\n    Args:\n        bboxes (np.ndarray): Array of bounding boxes with shape (n, m) where n is the number of bboxes\n                             and m &gt;= 4. The first 4 columns are [x_min, y_min, x_max, y_max].\n        shift_vector (np.ndarray): Vector to shift the bounding boxes by, with shape (4,) for\n                                   [shift_x, shift_y, shift_x, shift_y].\n\n    Returns:\n        np.ndarray: Shifted bounding boxes with the same shape as input.\n    \"\"\"\n    # Create a copy of the input array to avoid modifying it in-place\n    shifted_bboxes = bboxes.copy()\n\n    # Add the shift vector to the first 4 columns\n    shifted_bboxes[:, :4] += shift_vector\n\n    return shifted_bboxes\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.functional.to_distance_maps","title":"<code>def to_distance_maps    (keypoints, image_shape, inverted=False)    </code> [view source on GitHub]","text":"<p>Generate a <code>(H,W,N)</code> array of distance maps for <code>N</code> keypoints.</p> <p>The <code>n</code>-th distance map contains at every location <code>(y, x)</code> the euclidean distance to the <code>n</code>-th keypoint.</p> <p>This function can be used as a helper when augmenting keypoints with a method that only supports the augmentation of images.</p> <p>Parameters:</p> Name Type Description <code>keypoints</code> <code>Sequence[tuple[float, float]]</code> <p>keypoint coordinates</p> <code>image_shape</code> <code>tuple[int, int]</code> <p>tuple[int, int] shape of the image</p> <code>inverted</code> <code>bool</code> <p>If <code>True</code>, inverted distance maps are returned where each distance value d is replaced by <code>d/(d+1)</code>, i.e. the distance maps have values in the range <code>(0.0, 1.0]</code> with <code>1.0</code> denoting exactly the position of the respective keypoint.</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>(H, W, N) ndarray     A <code>float32</code> array containing <code>N</code> distance maps for <code>N</code>     keypoints. Each location <code>(y, x, n)</code> in the array denotes the     euclidean distance at <code>(y, x)</code> to the <code>n</code>-th keypoint.     If <code>inverted</code> is <code>True</code>, the distance <code>d</code> is replaced     by <code>d/(d+1)</code>. The height and width of the array match the     height and width in <code>KeypointsOnImage.shape</code>.</p> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>def to_distance_maps(\n    keypoints: Sequence[tuple[float, float]],\n    image_shape: tuple[int, int],\n    inverted: bool = False,\n) -&gt; np.ndarray:\n    \"\"\"Generate a ``(H,W,N)`` array of distance maps for ``N`` keypoints.\n\n    The ``n``-th distance map contains at every location ``(y, x)`` the\n    euclidean distance to the ``n``-th keypoint.\n\n    This function can be used as a helper when augmenting keypoints with a\n    method that only supports the augmentation of images.\n\n    Args:\n        keypoints: keypoint coordinates\n        image_shape: tuple[int, int] shape of the image\n        inverted (bool): If ``True``, inverted distance maps are returned where each\n            distance value d is replaced by ``d/(d+1)``, i.e. the distance\n            maps have values in the range ``(0.0, 1.0]`` with ``1.0`` denoting\n            exactly the position of the respective keypoint.\n\n    Returns:\n        (H, W, N) ndarray\n            A ``float32`` array containing ``N`` distance maps for ``N``\n            keypoints. Each location ``(y, x, n)`` in the array denotes the\n            euclidean distance at ``(y, x)`` to the ``n``-th keypoint.\n            If `inverted` is ``True``, the distance ``d`` is replaced\n            by ``d/(d+1)``. The height and width of the array match the\n            height and width in ``KeypointsOnImage.shape``.\n\n    \"\"\"\n    height, width = image_shape[:2]\n    distance_maps = np.zeros((height, width, len(keypoints)), dtype=np.float32)\n\n    yy = np.arange(0, height)\n    xx = np.arange(0, width)\n    grid_xx, grid_yy = np.meshgrid(xx, yy)\n\n    for i, (x, y) in enumerate(keypoints):\n        distance_maps[:, :, i] = (grid_xx - x) ** 2 + (grid_yy - y) ** 2\n\n    distance_maps = np.sqrt(distance_maps)\n    if inverted:\n        return 1 / (distance_maps + 1)\n    return distance_maps\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.functional.transpose","title":"<code>def transpose    (img)    </code> [view source on GitHub]","text":"<p>Transposes the first two dimensions of an array of any dimensionality. Retains the order of any additional dimensions.</p> <p>Parameters:</p> Name Type Description <code>img</code> <code>np.ndarray</code> <p>Input array.</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Transposed array.</p> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>def transpose(img: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Transposes the first two dimensions of an array of any dimensionality.\n    Retains the order of any additional dimensions.\n\n    Args:\n        img (np.ndarray): Input array.\n\n    Returns:\n        np.ndarray: Transposed array.\n    \"\"\"\n    # Generate the new axes order\n    new_axes = list(range(img.ndim))\n    new_axes[0], new_axes[1] = 1, 0  # Swap the first two dimensions\n\n    # Transpose the array using the new axes order\n    return img.transpose(new_axes)\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.functional.validate_bboxes","title":"<code>def validate_bboxes    (bboxes, image_shape)    </code> [view source on GitHub]","text":"<p>Validate bounding boxes and remove invalid ones.</p> <p>Parameters:</p> Name Type Description <code>bboxes</code> <code>np.ndarray</code> <p>Array of bounding boxes with shape (n, 4) where each row is [x_min, y_min, x_max, y_max].</p> <code>image_shape</code> <code>tuple[int, int]</code> <p>Shape of the image as (height, width).</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Array of valid bounding boxes, potentially with fewer boxes than the input.</p> <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; bboxes = np.array([[10, 20, 30, 40], [-10, -10, 5, 5], [100, 100, 120, 120]])\n&gt;&gt;&gt; valid_bboxes = validate_bboxes(bboxes, (100, 100))\n&gt;&gt;&gt; print(valid_bboxes)\n[[10 20 30 40]]\n</code></pre> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>def validate_bboxes(bboxes: np.ndarray, image_shape: Sequence[int]) -&gt; np.ndarray:\n    \"\"\"Validate bounding boxes and remove invalid ones.\n\n    Args:\n        bboxes (np.ndarray): Array of bounding boxes with shape (n, 4) where each row is [x_min, y_min, x_max, y_max].\n        image_shape (tuple[int, int]): Shape of the image as (height, width).\n\n    Returns:\n        np.ndarray: Array of valid bounding boxes, potentially with fewer boxes than the input.\n\n    Example:\n        &gt;&gt;&gt; bboxes = np.array([[10, 20, 30, 40], [-10, -10, 5, 5], [100, 100, 120, 120]])\n        &gt;&gt;&gt; valid_bboxes = validate_bboxes(bboxes, (100, 100))\n        &gt;&gt;&gt; print(valid_bboxes)\n        [[10 20 30 40]]\n    \"\"\"\n    rows, cols = image_shape[:2]\n\n    x_min, y_min, x_max, y_max = bboxes[:, 0], bboxes[:, 1], bboxes[:, 2], bboxes[:, 3]\n\n    valid_indices = (x_max &gt; 0) &amp; (y_max &gt; 0) &amp; (x_min &lt; cols) &amp; (y_min &lt; rows)\n\n    return bboxes[valid_indices]\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.functional.validate_if_not_found_coords","title":"<code>def validate_if_not_found_coords    (if_not_found_coords)    </code> [view source on GitHub]","text":"<p>Validate and process <code>if_not_found_coords</code> parameter.</p> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>def validate_if_not_found_coords(\n    if_not_found_coords: Sequence[int] | dict[str, Any] | None,\n) -&gt; tuple[bool, int, int]:\n    \"\"\"Validate and process `if_not_found_coords` parameter.\"\"\"\n    if if_not_found_coords is None:\n        return True, -1, -1\n    if isinstance(if_not_found_coords, (tuple, list)):\n        if len(if_not_found_coords) != PAIR:\n            msg = \"Expected tuple/list 'if_not_found_coords' to contain exactly two entries.\"\n            raise ValueError(msg)\n        return False, if_not_found_coords[0], if_not_found_coords[1]\n    if isinstance(if_not_found_coords, dict):\n        return False, if_not_found_coords[\"x\"], if_not_found_coords[\"y\"]\n\n    msg = \"Expected if_not_found_coords to be None, tuple, list, or dict.\"\n    raise ValueError(msg)\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.functional.validate_keypoints","title":"<code>def validate_keypoints    (keypoints, image_shape)    </code> [view source on GitHub]","text":"<p>Validate keypoints and remove those that fall outside the image boundaries.</p> <p>Parameters:</p> Name Type Description <code>keypoints</code> <code>np.ndarray</code> <p>Array of keypoints with shape (N, M) where N is the number of keypoints                     and M &gt;= 2. The first two columns represent x and y coordinates.</p> <code>image_shape</code> <code>tuple[int, int]</code> <p>Shape of the image as (height, width).</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Array of valid keypoints that fall within the image boundaries.</p> <p>Note</p> <p>This function only checks the x and y coordinates (first two columns) of the keypoints. Any additional columns (e.g., angle, scale) are preserved for valid keypoints.</p> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>def validate_keypoints(keypoints: np.ndarray, image_shape: tuple[int, int]) -&gt; np.ndarray:\n    \"\"\"Validate keypoints and remove those that fall outside the image boundaries.\n\n    Args:\n        keypoints (np.ndarray): Array of keypoints with shape (N, M) where N is the number of keypoints\n                                and M &gt;= 2. The first two columns represent x and y coordinates.\n        image_shape (tuple[int, int]): Shape of the image as (height, width).\n\n    Returns:\n        np.ndarray: Array of valid keypoints that fall within the image boundaries.\n\n    Note:\n        This function only checks the x and y coordinates (first two columns) of the keypoints.\n        Any additional columns (e.g., angle, scale) are preserved for valid keypoints.\n    \"\"\"\n    rows, cols = image_shape[:2]\n\n    x, y = keypoints[:, 0], keypoints[:, 1]\n\n    valid_indices = (x &gt;= 0) &amp; (x &lt; cols) &amp; (y &gt;= 0) &amp; (y &lt; rows)\n\n    return keypoints[valid_indices]\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.resize","title":"<code>resize</code>","text":""},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.resize.LongestMaxSize","title":"<code>class  LongestMaxSize</code> <code>     (max_size=1024, interpolation=1, always_apply=None, p=1)                 </code>  [view source on GitHub]","text":"<p>Rescale an image so that maximum side is equal to max_size, keeping the aspect ratio of the initial image.</p> <p>Parameters:</p> Name Type Description <code>max_size</code> <code>int, list of int</code> <p>maximum size of the image after the transformation. When using a list, max size will be randomly selected from the values in the list.</p> <code>interpolation</code> <code>OpenCV flag</code> <p>interpolation method. Default: cv2.INTER_LINEAR.</p> <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 1.</p> <p>Targets</p> <p>image, mask, bboxes, keypoints</p> <p>Image types:     uint8, float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/geometric/resize.py</code> Python<pre><code>class LongestMaxSize(DualTransform):\n    \"\"\"Rescale an image so that maximum side is equal to max_size, keeping the aspect ratio of the initial image.\n\n    Args:\n        max_size (int, list of int): maximum size of the image after the transformation. When using a list, max size\n            will be randomly selected from the values in the list.\n        interpolation (OpenCV flag): interpolation method. Default: cv2.INTER_LINEAR.\n        p (float): probability of applying the transform. Default: 1.\n\n    Targets:\n        image, mask, bboxes, keypoints\n\n    Image types:\n        uint8, float32\n\n    \"\"\"\n\n    _targets = (Targets.IMAGE, Targets.MASK, Targets.BBOXES, Targets.KEYPOINTS)\n\n    class InitSchema(MaxSizeInitSchema):\n        pass\n\n    def __init__(\n        self,\n        max_size: int | Sequence[int] = 1024,\n        interpolation: int = cv2.INTER_LINEAR,\n        always_apply: bool | None = None,\n        p: float = 1,\n    ):\n        super().__init__(p, always_apply)\n        self.interpolation = interpolation\n        self.max_size = max_size\n\n    def apply(\n        self,\n        img: np.ndarray,\n        max_size: int,\n        interpolation: int,\n        **params: Any,\n    ) -&gt; np.ndarray:\n        return fgeometric.longest_max_size(img, max_size=max_size, interpolation=interpolation)\n\n    def apply_to_bbox(self, bbox: BoxInternalType, **params: Any) -&gt; BoxInternalType:\n        # Bounding box coordinates are scale invariant\n        return bbox\n\n    def apply_to_keypoint(\n        self,\n        keypoint: KeypointInternalType,\n        max_size: int,\n        **params: Any,\n    ) -&gt; KeypointInternalType:\n        image_shape = params[\"shape\"][:2]\n\n        scale = max_size / max(image_shape)\n        return fgeometric.keypoint_scale(keypoint, scale, scale)\n\n    def get_params(self) -&gt; dict[str, int]:\n        return {\"max_size\": self.max_size if isinstance(self.max_size, int) else random.choice(self.max_size)}\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return \"max_size\", \"interpolation\"\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.resize.LongestMaxSize.apply","title":"<code>apply (self, img, max_size, interpolation, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/geometric/resize.py</code> Python<pre><code>def apply(\n    self,\n    img: np.ndarray,\n    max_size: int,\n    interpolation: int,\n    **params: Any,\n) -&gt; np.ndarray:\n    return fgeometric.longest_max_size(img, max_size=max_size, interpolation=interpolation)\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.resize.LongestMaxSize.get_params","title":"<code>get_params (self)</code>","text":"<p>Returns parameters independent of input.</p> Source code in <code>albumentations/augmentations/geometric/resize.py</code> Python<pre><code>def get_params(self) -&gt; dict[str, int]:\n    return {\"max_size\": self.max_size if isinstance(self.max_size, int) else random.choice(self.max_size)}\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.resize.LongestMaxSize.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/geometric/resize.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return \"max_size\", \"interpolation\"\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.resize.RandomScale","title":"<code>class  RandomScale</code> <code>     (scale_limit=0.1, interpolation=1, always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Randomly resize the input. Output image size is different from the input image size.</p> <p>Parameters:</p> Name Type Description <code>scale_limit</code> <code>float, float) or float</code> <p>scaling factor range. If scale_limit is a single float value, the range will be (-scale_limit, scale_limit). Note that the scale_limit will be biased by 1. If scale_limit is a tuple, like (low, high), sampling will be done from the range (1 + low, 1 + high). Default: (-0.1, 0.1).</p> <code>interpolation</code> <code>OpenCV flag</code> <p>flag that is used to specify the interpolation algorithm. Should be one of: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4. Default: cv2.INTER_LINEAR.</p> <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 0.5.</p> <p>Targets</p> <p>image, mask, bboxes, keypoints</p> <p>Image types:     uint8, float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/geometric/resize.py</code> Python<pre><code>class RandomScale(DualTransform):\n    \"\"\"Randomly resize the input. Output image size is different from the input image size.\n\n    Args:\n        scale_limit ((float, float) or float): scaling factor range. If scale_limit is a single float value, the\n            range will be (-scale_limit, scale_limit). Note that the scale_limit will be biased by 1.\n            If scale_limit is a tuple, like (low, high), sampling will be done from the range (1 + low, 1 + high).\n            Default: (-0.1, 0.1).\n        interpolation (OpenCV flag): flag that is used to specify the interpolation algorithm. Should be one of:\n            cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.\n            Default: cv2.INTER_LINEAR.\n        p (float): probability of applying the transform. Default: 0.5.\n\n    Targets:\n        image, mask, bboxes, keypoints\n\n    Image types:\n        uint8, float32\n\n    \"\"\"\n\n    _targets = (Targets.IMAGE, Targets.MASK, Targets.BBOXES, Targets.KEYPOINTS)\n\n    class InitSchema(BaseTransformInitSchema):\n        scale_limit: ScaleFloatType = Field(\n            default=0.1,\n            description=\"Scaling factor range. If a single float value =&gt; (1-scale_limit, 1 + scale_limit).\",\n        )\n        interpolation: InterpolationType = cv2.INTER_LINEAR\n\n        @field_validator(\"scale_limit\")\n        @classmethod\n        def check_scale_limit(cls, v: ScaleFloatType) -&gt; tuple[float, float]:\n            return to_tuple(v, bias=1.0)\n\n    def __init__(\n        self,\n        scale_limit: ScaleFloatType = 0.1,\n        interpolation: int = cv2.INTER_LINEAR,\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p, always_apply)\n        self.scale_limit = cast(Tuple[float, float], scale_limit)\n        self.interpolation = interpolation\n\n    def get_params(self) -&gt; dict[str, float]:\n        return {\"scale\": random.uniform(self.scale_limit[0], self.scale_limit[1])}\n\n    def apply(\n        self,\n        img: np.ndarray,\n        scale: float,\n        interpolation: int,\n        **params: Any,\n    ) -&gt; np.ndarray:\n        return fgeometric.scale(img, scale, interpolation)\n\n    def apply_to_bbox(self, bbox: BoxInternalType, **params: Any) -&gt; BoxInternalType:\n        # Bounding box coordinates are scale invariant\n        return bbox\n\n    def apply_to_keypoint(\n        self,\n        keypoint: KeypointInternalType,\n        scale: float,\n        **params: Any,\n    ) -&gt; KeypointInternalType:\n        return fgeometric.keypoint_scale(keypoint, scale, scale)\n\n    def get_transform_init_args(self) -&gt; dict[str, Any]:\n        return {\"interpolation\": self.interpolation, \"scale_limit\": to_tuple(self.scale_limit, bias=-1.0)}\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.resize.RandomScale.apply","title":"<code>apply (self, img, scale, interpolation, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/geometric/resize.py</code> Python<pre><code>def apply(\n    self,\n    img: np.ndarray,\n    scale: float,\n    interpolation: int,\n    **params: Any,\n) -&gt; np.ndarray:\n    return fgeometric.scale(img, scale, interpolation)\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.resize.RandomScale.get_params","title":"<code>get_params (self)</code>","text":"<p>Returns parameters independent of input.</p> Source code in <code>albumentations/augmentations/geometric/resize.py</code> Python<pre><code>def get_params(self) -&gt; dict[str, float]:\n    return {\"scale\": random.uniform(self.scale_limit[0], self.scale_limit[1])}\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.resize.Resize","title":"<code>class  Resize</code> <code>     (height, width, interpolation=1, always_apply=None, p=1)                 </code>  [view source on GitHub]","text":"<p>Resize the input to the given height and width.</p> <p>Parameters:</p> Name Type Description <code>height</code> <code>int</code> <p>desired height of the output.</p> <code>width</code> <code>int</code> <p>desired width of the output.</p> <code>interpolation</code> <code>OpenCV flag</code> <p>flag that is used to specify the interpolation algorithm. Should be one of: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4. Default: cv2.INTER_LINEAR.</p> <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 1.</p> <p>Targets</p> <p>image, mask, bboxes, keypoints</p> <p>Image types:     uint8, float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/geometric/resize.py</code> Python<pre><code>class Resize(DualTransform):\n    \"\"\"Resize the input to the given height and width.\n\n    Args:\n        height (int): desired height of the output.\n        width (int): desired width of the output.\n        interpolation (OpenCV flag): flag that is used to specify the interpolation algorithm. Should be one of:\n            cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.\n            Default: cv2.INTER_LINEAR.\n        p (float): probability of applying the transform. Default: 1.\n\n    Targets:\n        image, mask, bboxes, keypoints\n\n    Image types:\n        uint8, float32\n\n    \"\"\"\n\n    _targets = (Targets.IMAGE, Targets.MASK, Targets.KEYPOINTS, Targets.BBOXES)\n\n    class InitSchema(BaseTransformInitSchema):\n        height: int = Field(ge=1, description=\"Desired height of the output.\")\n        width: int = Field(ge=1, description=\"Desired width of the output.\")\n        interpolation: InterpolationType = cv2.INTER_LINEAR\n        p: ProbabilityType = 1\n\n    def __init__(\n        self,\n        height: int,\n        width: int,\n        interpolation: int = cv2.INTER_LINEAR,\n        always_apply: bool | None = None,\n        p: float = 1,\n    ):\n        super().__init__(p, always_apply)\n        self.height = height\n        self.width = width\n        self.interpolation = interpolation\n\n    def apply(self, img: np.ndarray, interpolation: int, **params: Any) -&gt; np.ndarray:\n        return fgeometric.resize(img, (self.height, self.width), interpolation=interpolation)\n\n    def apply_to_bbox(self, bbox: BoxInternalType, **params: Any) -&gt; BoxInternalType:\n        # Bounding box coordinates are scale invariant\n        return bbox\n\n    def apply_to_keypoint(self, keypoint: KeypointInternalType, **params: Any) -&gt; KeypointInternalType:\n        height, width = params[\"shape\"][:2]\n        scale_x = self.width / width\n        scale_y = self.height / height\n        return fgeometric.keypoint_scale(keypoint, scale_x, scale_y)\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return \"height\", \"width\", \"interpolation\"\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.resize.Resize.apply","title":"<code>apply (self, img, interpolation, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/geometric/resize.py</code> Python<pre><code>def apply(self, img: np.ndarray, interpolation: int, **params: Any) -&gt; np.ndarray:\n    return fgeometric.resize(img, (self.height, self.width), interpolation=interpolation)\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.resize.Resize.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/geometric/resize.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return \"height\", \"width\", \"interpolation\"\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.resize.SmallestMaxSize","title":"<code>class  SmallestMaxSize</code> <code>     (max_size=1024, interpolation=1, always_apply=None, p=1)                 </code>  [view source on GitHub]","text":"<p>Rescale an image so that minimum side is equal to max_size, keeping the aspect ratio of the initial image.</p> <p>Parameters:</p> Name Type Description <code>max_size</code> <code>int, list of int</code> <p>maximum size of smallest side of the image after the transformation. When using a list, max size will be randomly selected from the values in the list.</p> <code>interpolation</code> <code>OpenCV flag</code> <p>interpolation method. Default: cv2.INTER_LINEAR.</p> <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 1.</p> <p>Targets</p> <p>image, mask, bboxes, keypoints</p> <p>Image types:     uint8, float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/geometric/resize.py</code> Python<pre><code>class SmallestMaxSize(DualTransform):\n    \"\"\"Rescale an image so that minimum side is equal to max_size, keeping the aspect ratio of the initial image.\n\n    Args:\n        max_size (int, list of int): maximum size of smallest side of the image after the transformation. When using a\n            list, max size will be randomly selected from the values in the list.\n        interpolation (OpenCV flag): interpolation method. Default: cv2.INTER_LINEAR.\n        p (float): probability of applying the transform. Default: 1.\n\n    Targets:\n        image, mask, bboxes, keypoints\n\n    Image types:\n        uint8, float32\n\n    \"\"\"\n\n    _targets = (Targets.IMAGE, Targets.MASK, Targets.KEYPOINTS, Targets.BBOXES)\n\n    class InitSchema(MaxSizeInitSchema):\n        pass\n\n    def __init__(\n        self,\n        max_size: int | Sequence[int] = 1024,\n        interpolation: int = cv2.INTER_LINEAR,\n        always_apply: bool | None = None,\n        p: float = 1,\n    ):\n        super().__init__(p, always_apply)\n        self.interpolation = interpolation\n        self.max_size = max_size\n\n    def apply(\n        self,\n        img: np.ndarray,\n        max_size: int,\n        interpolation: int,\n        **params: Any,\n    ) -&gt; np.ndarray:\n        return fgeometric.smallest_max_size(img, max_size=max_size, interpolation=interpolation)\n\n    def apply_to_bbox(self, bbox: BoxInternalType, **params: Any) -&gt; BoxInternalType:\n        return bbox\n\n    def apply_to_keypoint(\n        self,\n        keypoint: KeypointInternalType,\n        max_size: int,\n        **params: Any,\n    ) -&gt; KeypointInternalType:\n        image_shape = params[\"shape\"][:2]\n        height, width = image_shape\n\n        scale = max_size / min(image_shape)\n        return fgeometric.keypoint_scale(keypoint, scale, scale)\n\n    def get_params(self) -&gt; dict[str, int]:\n        return {\"max_size\": self.max_size if isinstance(self.max_size, int) else random.choice(self.max_size)}\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return (\"max_size\", \"interpolation\")\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.resize.SmallestMaxSize.apply","title":"<code>apply (self, img, max_size, interpolation, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/geometric/resize.py</code> Python<pre><code>def apply(\n    self,\n    img: np.ndarray,\n    max_size: int,\n    interpolation: int,\n    **params: Any,\n) -&gt; np.ndarray:\n    return fgeometric.smallest_max_size(img, max_size=max_size, interpolation=interpolation)\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.resize.SmallestMaxSize.get_params","title":"<code>get_params (self)</code>","text":"<p>Returns parameters independent of input.</p> Source code in <code>albumentations/augmentations/geometric/resize.py</code> Python<pre><code>def get_params(self) -&gt; dict[str, int]:\n    return {\"max_size\": self.max_size if isinstance(self.max_size, int) else random.choice(self.max_size)}\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.resize.SmallestMaxSize.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/geometric/resize.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return (\"max_size\", \"interpolation\")\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.rotate","title":"<code>rotate</code>","text":""},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.rotate.RandomRotate90","title":"<code>class  RandomRotate90</code> <code> </code>  [view source on GitHub]","text":"<p>Randomly rotate the input by 90 degrees zero or more times.</p> <p>Parameters:</p> Name Type Description <code>p</code> <p>probability of applying the transform. Default: 0.5.</p> <p>Targets</p> <p>image, mask, bboxes, keypoints</p> <p>Image types:     uint8, float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/geometric/rotate.py</code> Python<pre><code>class RandomRotate90(DualTransform):\n    \"\"\"Randomly rotate the input by 90 degrees zero or more times.\n\n    Args:\n        p: probability of applying the transform. Default: 0.5.\n\n    Targets:\n        image, mask, bboxes, keypoints\n\n    Image types:\n        uint8, float32\n\n    \"\"\"\n\n    _targets = (Targets.IMAGE, Targets.MASK, Targets.BBOXES, Targets.KEYPOINTS)\n\n    def apply(self, img: np.ndarray, factor: int, **params: Any) -&gt; np.ndarray:\n        return fgeometric.rot90(img, factor)\n\n    def get_params(self) -&gt; dict[str, int]:\n        # Random int in the range [0, 3]\n        return {\"factor\": random.randint(0, 3)}\n\n    def apply_to_bbox(self, bbox: BoxInternalType, factor: int, **params: Any) -&gt; BoxInternalType:\n        return fgeometric.bbox_rot90(bbox, factor)\n\n    def apply_to_keypoint(self, keypoint: KeypointInternalType, factor: int, **params: Any) -&gt; BoxInternalType:\n        return fgeometric.keypoint_rot90(keypoint, factor, params[\"shape\"])\n\n    def get_transform_init_args_names(self) -&gt; tuple[()]:\n        return ()\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.rotate.RandomRotate90.apply","title":"<code>apply (self, img, factor, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/geometric/rotate.py</code> Python<pre><code>def apply(self, img: np.ndarray, factor: int, **params: Any) -&gt; np.ndarray:\n    return fgeometric.rot90(img, factor)\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.rotate.RandomRotate90.get_params","title":"<code>get_params (self)</code>","text":"<p>Returns parameters independent of input.</p> Source code in <code>albumentations/augmentations/geometric/rotate.py</code> Python<pre><code>def get_params(self) -&gt; dict[str, int]:\n    # Random int in the range [0, 3]\n    return {\"factor\": random.randint(0, 3)}\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.rotate.RandomRotate90.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/geometric/rotate.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[()]:\n    return ()\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.rotate.Rotate","title":"<code>class  Rotate</code> <code>     (limit=(-90, 90), interpolation=1, border_mode=4, value=None, mask_value=None, rotate_method='largest_box', crop_border=False, always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Rotate the input by an angle selected randomly from the uniform distribution.</p> <p>Parameters:</p> Name Type Description <code>limit</code> <code>ScaleFloatType</code> <p>range from which a random angle is picked. If limit is a single int an angle is picked from (-limit, limit). Default: (-90, 90)</p> <code>interpolation</code> <code>OpenCV flag</code> <p>flag that is used to specify the interpolation algorithm. Should be one of: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4. Default: cv2.INTER_LINEAR.</p> <code>border_mode</code> <code>OpenCV flag</code> <p>flag that is used to specify the pixel extrapolation method. Should be one of: cv2.BORDER_CONSTANT, cv2.BORDER_REPLICATE, cv2.BORDER_REFLECT, cv2.BORDER_WRAP, cv2.BORDER_REFLECT_101. Default: cv2.BORDER_REFLECT_101</p> <code>value</code> <code>int, float, list of ints, list of float</code> <p>padding value if border_mode is cv2.BORDER_CONSTANT.</p> <code>mask_value</code> <code>int, float,         list of ints,         list of float</code> <p>padding value if border_mode is cv2.BORDER_CONSTANT applied for masks.</p> <code>rotate_method</code> <code>str</code> <p>rotation method used for the bounding boxes. Should be one of \"largest_box\" or \"ellipse\". Default: \"largest_box\"</p> <code>crop_border</code> <code>bool</code> <p>If True would make a largest possible crop within rotated image</p> <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 0.5.</p> <p>Targets</p> <p>image, mask, bboxes, keypoints</p> <p>Image types:     uint8, float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/geometric/rotate.py</code> Python<pre><code>class Rotate(DualTransform):\n    \"\"\"Rotate the input by an angle selected randomly from the uniform distribution.\n\n    Args:\n        limit: range from which a random angle is picked. If limit is a single int\n            an angle is picked from (-limit, limit). Default: (-90, 90)\n        interpolation (OpenCV flag): flag that is used to specify the interpolation algorithm. Should be one of:\n            cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.\n            Default: cv2.INTER_LINEAR.\n        border_mode (OpenCV flag): flag that is used to specify the pixel extrapolation method. Should be one of:\n            cv2.BORDER_CONSTANT, cv2.BORDER_REPLICATE, cv2.BORDER_REFLECT, cv2.BORDER_WRAP, cv2.BORDER_REFLECT_101.\n            Default: cv2.BORDER_REFLECT_101\n        value (int, float, list of ints, list of float): padding value if border_mode is cv2.BORDER_CONSTANT.\n        mask_value (int, float,\n                    list of ints,\n                    list of float): padding value if border_mode is cv2.BORDER_CONSTANT applied for masks.\n        rotate_method (str): rotation method used for the bounding boxes. Should be one of \"largest_box\" or \"ellipse\".\n            Default: \"largest_box\"\n        crop_border (bool): If True would make a largest possible crop within rotated image\n        p (float): probability of applying the transform. Default: 0.5.\n\n    Targets:\n        image, mask, bboxes, keypoints\n\n    Image types:\n        uint8, float32\n\n    \"\"\"\n\n    _targets = (Targets.IMAGE, Targets.MASK, Targets.BBOXES, Targets.KEYPOINTS)\n\n    class InitSchema(RotateInitSchema):\n        rotate_method: Literal[\"largest_box\", \"ellipse\"] = \"largest_box\"\n        crop_border: bool = Field(\n            default=False,\n            description=\"If True, makes a largest possible crop within the rotated image.\",\n        )\n\n    def __init__(\n        self,\n        limit: ScaleFloatType = (-90, 90),\n        interpolation: int = cv2.INTER_LINEAR,\n        border_mode: int = cv2.BORDER_REFLECT_101,\n        value: ColorType | None = None,\n        mask_value: ColorType | None = None,\n        rotate_method: Literal[\"largest_box\", \"ellipse\"] = \"largest_box\",\n        crop_border: bool = False,\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p=p, always_apply=always_apply)\n        self.limit = cast(Tuple[float, float], limit)\n        self.interpolation = interpolation\n        self.border_mode = border_mode\n        self.value = value\n        self.mask_value = mask_value\n        self.rotate_method = rotate_method\n        self.crop_border = crop_border\n\n    def apply(\n        self,\n        img: np.ndarray,\n        angle: float,\n        interpolation: int,\n        x_min: int,\n        x_max: int,\n        y_min: int,\n        y_max: int,\n        **params: Any,\n    ) -&gt; np.ndarray:\n        img_out = fgeometric.rotate(img, angle, interpolation, self.border_mode, self.value)\n        if self.crop_border:\n            return fcrops.crop(img_out, x_min, y_min, x_max, y_max)\n        return img_out\n\n    def apply_to_mask(\n        self,\n        mask: np.ndarray,\n        angle: float,\n        x_min: int,\n        x_max: int,\n        y_min: int,\n        y_max: int,\n        **params: Any,\n    ) -&gt; np.ndarray:\n        img_out = fgeometric.rotate(mask, angle, cv2.INTER_NEAREST, self.border_mode, self.mask_value)\n        if self.crop_border:\n            return fcrops.crop(img_out, x_min, y_min, x_max, y_max)\n        return img_out\n\n    def apply_to_bbox(\n        self,\n        bbox: BoxInternalType,\n        angle: float,\n        x_min: int,\n        x_max: int,\n        y_min: int,\n        y_max: int,\n        **params: Any,\n    ) -&gt; np.ndarray:\n        image_shape = params[\"shape\"][:2]\n        bbox_out = fgeometric.bbox_rotate(bbox, angle, self.rotate_method, image_shape)\n        if self.crop_border:\n            return fcrops.crop_bbox_by_coords(bbox_out, (x_min, y_min, x_max, y_max), image_shape)\n        return bbox_out\n\n    def apply_to_keypoint(\n        self,\n        keypoint: KeypointInternalType,\n        angle: float,\n        x_min: int,\n        x_max: int,\n        y_min: int,\n        y_max: int,\n        **params: Any,\n    ) -&gt; KeypointInternalType:\n        keypoint_out = fgeometric.keypoint_rotate(keypoint, angle, params[\"shape\"][:2], **params)\n        if self.crop_border:\n            return fcrops.crop_keypoint_by_coords(keypoint_out, (x_min, y_min, x_max, y_max))\n        return keypoint_out\n\n    @staticmethod\n    def _rotated_rect_with_max_area(height: int, width: int, angle: float) -&gt; dict[str, int]:\n        \"\"\"Given a rectangle of size wxh that has been rotated by 'angle' (in\n        degrees), computes the width and height of the largest possible\n        axis-aligned rectangle (maximal area) within the rotated rectangle.\n\n        Reference:\n            https://stackoverflow.com/questions/16702966/rotate-image-and-crop-out-black-borders\n        \"\"\"\n        angle = math.radians(angle)\n        width_is_longer = width &gt;= height\n        side_long, side_short = (width, height) if width_is_longer else (height, width)\n\n        # since the solutions for angle, -angle and 180-angle are all the same,\n        # it is sufficient to look at the first quadrant and the absolute values of sin,cos:\n        sin_a, cos_a = abs(math.sin(angle)), abs(math.cos(angle))\n        if side_short &lt;= 2.0 * sin_a * cos_a * side_long or abs(sin_a - cos_a) &lt; SMALL_NUMBER:\n            # half constrained case: two crop corners touch the longer side,\n            # the other two corners are on the mid-line parallel to the longer line\n            x = 0.5 * side_short\n            wr, hr = (x / sin_a, x / cos_a) if width_is_longer else (x / cos_a, x / sin_a)\n        else:\n            # fully constrained case: crop touches all 4 sides\n            cos_2a = cos_a * cos_a - sin_a * sin_a\n            wr, hr = (width * cos_a - height * sin_a) / cos_2a, (height * cos_a - width * sin_a) / cos_2a\n\n        return {\n            \"x_min\": max(0, int(width / 2 - wr / 2)),\n            \"x_max\": min(width, int(width / 2 + wr / 2)),\n            \"y_min\": max(0, int(height / 2 - hr / 2)),\n            \"y_max\": min(height, int(height / 2 + hr / 2)),\n        }\n\n    def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n        out_params = {\"angle\": random.uniform(self.limit[0], self.limit[1])}\n        if self.crop_border:\n            height, width = params[\"shape\"][:2]\n            out_params.update(self._rotated_rect_with_max_area(height, width, out_params[\"angle\"]))\n        else:\n            out_params.update({\"x_min\": -1, \"x_max\": -1, \"y_min\": -1, \"y_max\": -1})\n\n        return out_params\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return \"limit\", \"interpolation\", \"border_mode\", \"value\", \"mask_value\", \"rotate_method\", \"crop_border\"\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.rotate.Rotate.apply","title":"<code>apply (self, img, angle, interpolation, x_min, x_max, y_min, y_max, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/geometric/rotate.py</code> Python<pre><code>def apply(\n    self,\n    img: np.ndarray,\n    angle: float,\n    interpolation: int,\n    x_min: int,\n    x_max: int,\n    y_min: int,\n    y_max: int,\n    **params: Any,\n) -&gt; np.ndarray:\n    img_out = fgeometric.rotate(img, angle, interpolation, self.border_mode, self.value)\n    if self.crop_border:\n        return fcrops.crop(img_out, x_min, y_min, x_max, y_max)\n    return img_out\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.rotate.Rotate.get_params_dependent_on_data","title":"<code>get_params_dependent_on_data (self, params, data)</code>","text":"<p>Returns parameters dependent on input.</p> Source code in <code>albumentations/augmentations/geometric/rotate.py</code> Python<pre><code>def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n    out_params = {\"angle\": random.uniform(self.limit[0], self.limit[1])}\n    if self.crop_border:\n        height, width = params[\"shape\"][:2]\n        out_params.update(self._rotated_rect_with_max_area(height, width, out_params[\"angle\"]))\n    else:\n        out_params.update({\"x_min\": -1, \"x_max\": -1, \"y_min\": -1, \"y_max\": -1})\n\n    return out_params\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.rotate.Rotate.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/geometric/rotate.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return \"limit\", \"interpolation\", \"border_mode\", \"value\", \"mask_value\", \"rotate_method\", \"crop_border\"\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.rotate.SafeRotate","title":"<code>class  SafeRotate</code> <code>     (limit=(-90, 90), interpolation=1, border_mode=4, value=None, mask_value=None, always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Rotate the input inside the input's frame by an angle selected randomly from the uniform distribution.</p> <p>The resulting image may have artifacts in it. After rotation, the image may have a different aspect ratio, and after resizing, it returns to its original shape with the original aspect ratio of the image. For these reason we may see some artifacts.</p> <p>Parameters:</p> Name Type Description <code>limit</code> <code>int, int) or int</code> <p>range from which a random angle is picked. If limit is a single int an angle is picked from (-limit, limit). Default: (-90, 90)</p> <code>interpolation</code> <code>OpenCV flag</code> <p>flag that is used to specify the interpolation algorithm. Should be one of: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4. Default: cv2.INTER_LINEAR.</p> <code>border_mode</code> <code>OpenCV flag</code> <p>flag that is used to specify the pixel extrapolation method. Should be one of: cv2.BORDER_CONSTANT, cv2.BORDER_REPLICATE, cv2.BORDER_REFLECT, cv2.BORDER_WRAP, cv2.BORDER_REFLECT_101. Default: cv2.BORDER_REFLECT_101</p> <code>value</code> <code>int, float, list of ints, list of float</code> <p>padding value if border_mode is cv2.BORDER_CONSTANT.</p> <code>mask_value</code> <code>int, float,         list of ints,         list of float</code> <p>padding value if border_mode is cv2.BORDER_CONSTANT applied for masks.</p> <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 0.5.</p> <p>Targets</p> <p>image, mask, bboxes, keypoints</p> <p>Image types:     uint8, float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/geometric/rotate.py</code> Python<pre><code>class SafeRotate(DualTransform):\n    \"\"\"Rotate the input inside the input's frame by an angle selected randomly from the uniform distribution.\n\n    The resulting image may have artifacts in it. After rotation, the image may have a different aspect ratio, and\n    after resizing, it returns to its original shape with the original aspect ratio of the image. For these reason we\n    may see some artifacts.\n\n    Args:\n        limit ((int, int) or int): range from which a random angle is picked. If limit is a single int\n            an angle is picked from (-limit, limit). Default: (-90, 90)\n        interpolation (OpenCV flag): flag that is used to specify the interpolation algorithm. Should be one of:\n            cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.\n            Default: cv2.INTER_LINEAR.\n        border_mode (OpenCV flag): flag that is used to specify the pixel extrapolation method. Should be one of:\n            cv2.BORDER_CONSTANT, cv2.BORDER_REPLICATE, cv2.BORDER_REFLECT, cv2.BORDER_WRAP, cv2.BORDER_REFLECT_101.\n            Default: cv2.BORDER_REFLECT_101\n        value (int, float, list of ints, list of float): padding value if border_mode is cv2.BORDER_CONSTANT.\n        mask_value (int, float,\n                    list of ints,\n                    list of float): padding value if border_mode is cv2.BORDER_CONSTANT applied for masks.\n        p (float): probability of applying the transform. Default: 0.5.\n\n    Targets:\n        image, mask, bboxes, keypoints\n\n    Image types:\n        uint8, float32\n\n    \"\"\"\n\n    _targets = (Targets.IMAGE, Targets.MASK, Targets.BBOXES, Targets.KEYPOINTS)\n\n    class InitSchema(RotateInitSchema):\n        pass\n\n    def __init__(\n        self,\n        limit: ScaleFloatType = (-90, 90),\n        interpolation: int = cv2.INTER_LINEAR,\n        border_mode: int = cv2.BORDER_REFLECT_101,\n        value: ColorType | None = None,\n        mask_value: ColorType | None = None,\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p=p, always_apply=always_apply)\n        self.limit = cast(Tuple[float, float], limit)\n        self.interpolation = interpolation\n        self.border_mode = border_mode\n        self.value = value\n        self.mask_value = mask_value\n\n    def apply(self, img: np.ndarray, matrix: np.ndarray, **params: Any) -&gt; np.ndarray:\n        return fgeometric.safe_rotate(img, matrix, self.interpolation, self.value, self.border_mode)\n\n    def apply_to_mask(self, mask: np.ndarray, matrix: np.ndarray, **params: Any) -&gt; np.ndarray:\n        return fgeometric.safe_rotate(mask, matrix, cv2.INTER_NEAREST, self.mask_value, self.border_mode)\n\n    def apply_to_bbox(self, bbox: BoxInternalType, **params: Any) -&gt; BoxInternalType:\n        return fgeometric.bbox_safe_rotate(bbox, params[\"matrix\"], params[\"shape\"])\n\n    def apply_to_keypoint(\n        self,\n        keypoint: KeypointInternalType,\n        angle: float,\n        scale_x: float,\n        scale_y: float,\n        **params: Any,\n    ) -&gt; KeypointInternalType:\n        return fgeometric.keypoint_safe_rotate(keypoint, params[\"matrix\"], angle, scale_x, scale_y, params[\"shape\"])\n\n    def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n        image_shape = params[\"shape\"]\n\n        angle = random.uniform(*self.limit)\n\n        # https://stackoverflow.com/questions/43892506/opencv-python-rotate-image-without-cropping-sides\n        image_center = center(image_shape)\n\n        # Rotation Matrix\n        rotation_mat = cv2.getRotationMatrix2D(image_center, angle, 1.0)\n\n        # rotation calculates the cos and sin, taking absolutes of those.\n        abs_cos = abs(rotation_mat[0, 0])\n        abs_sin = abs(rotation_mat[0, 1])\n\n        height, width = image_shape[:2]\n\n        # find the new width and height bounds\n        new_w = math.ceil(height * abs_sin + width * abs_cos)\n        new_h = math.ceil(height * abs_cos + width * abs_sin)\n\n        scale_x = width / new_w\n        scale_y = height / new_h\n\n        # Shift the image to create padding\n        rotation_mat[0, 2] += new_w / 2 - image_center[0]\n        rotation_mat[1, 2] += new_h / 2 - image_center[1]\n\n        # Rescale to original size\n        scale_mat = np.diag(np.ones(3))\n        scale_mat[0, 0] *= scale_x\n        scale_mat[1, 1] *= scale_y\n        _tmp = np.diag(np.ones(3))\n        _tmp[:2] = rotation_mat\n        _tmp = scale_mat @ _tmp\n        rotation_mat = _tmp[:2]\n\n        return {\"matrix\": rotation_mat, \"angle\": angle, \"scale_x\": scale_x, \"scale_y\": scale_y}\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return \"limit\", \"interpolation\", \"border_mode\", \"value\", \"mask_value\"\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.rotate.SafeRotate.apply","title":"<code>apply (self, img, matrix, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/geometric/rotate.py</code> Python<pre><code>def apply(self, img: np.ndarray, matrix: np.ndarray, **params: Any) -&gt; np.ndarray:\n    return fgeometric.safe_rotate(img, matrix, self.interpolation, self.value, self.border_mode)\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.rotate.SafeRotate.get_params_dependent_on_data","title":"<code>get_params_dependent_on_data (self, params, data)</code>","text":"<p>Returns parameters dependent on input.</p> Source code in <code>albumentations/augmentations/geometric/rotate.py</code> Python<pre><code>def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n    image_shape = params[\"shape\"]\n\n    angle = random.uniform(*self.limit)\n\n    # https://stackoverflow.com/questions/43892506/opencv-python-rotate-image-without-cropping-sides\n    image_center = center(image_shape)\n\n    # Rotation Matrix\n    rotation_mat = cv2.getRotationMatrix2D(image_center, angle, 1.0)\n\n    # rotation calculates the cos and sin, taking absolutes of those.\n    abs_cos = abs(rotation_mat[0, 0])\n    abs_sin = abs(rotation_mat[0, 1])\n\n    height, width = image_shape[:2]\n\n    # find the new width and height bounds\n    new_w = math.ceil(height * abs_sin + width * abs_cos)\n    new_h = math.ceil(height * abs_cos + width * abs_sin)\n\n    scale_x = width / new_w\n    scale_y = height / new_h\n\n    # Shift the image to create padding\n    rotation_mat[0, 2] += new_w / 2 - image_center[0]\n    rotation_mat[1, 2] += new_h / 2 - image_center[1]\n\n    # Rescale to original size\n    scale_mat = np.diag(np.ones(3))\n    scale_mat[0, 0] *= scale_x\n    scale_mat[1, 1] *= scale_y\n    _tmp = np.diag(np.ones(3))\n    _tmp[:2] = rotation_mat\n    _tmp = scale_mat @ _tmp\n    rotation_mat = _tmp[:2]\n\n    return {\"matrix\": rotation_mat, \"angle\": angle, \"scale_x\": scale_x, \"scale_y\": scale_y}\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.rotate.SafeRotate.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/geometric/rotate.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return \"limit\", \"interpolation\", \"border_mode\", \"value\", \"mask_value\"\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.transforms","title":"<code>transforms</code>","text":""},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.transforms.Affine","title":"<code>class  Affine</code> <code>     (scale=None, translate_percent=None, translate_px=None, rotate=None, shear=None, interpolation=1, mask_interpolation=0, cval=0, cval_mask=0, mode=0, fit_output=False, keep_ratio=False, rotate_method='largest_box', balanced_scale=False, always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Augmentation to apply affine transformations to images.</p> <p>Affine transformations involve:</p> <pre><code>- Translation (\"move\" image on the x-/y-axis)\n- Rotation\n- Scaling (\"zoom\" in/out)\n- Shear (move one side of the image, turning a square into a trapezoid)\n</code></pre> <p>All such transformations can create \"new\" pixels in the image without a defined content, e.g. if the image is translated to the left, pixels are created on the right. A method has to be defined to deal with these pixel values. The parameters <code>cval</code> and <code>mode</code> of this class deal with this.</p> <p>Some transformations involve interpolations between several pixels of the input image to generate output pixel values. The parameters <code>interpolation</code> and <code>mask_interpolation</code> deals with the method of interpolation used for this.</p> <p>Parameters:</p> Name Type Description <code>scale</code> <code>number, tuple of number or dict</code> <p>Scaling factor to use, where <code>1.0</code> denotes \"no change\" and <code>0.5</code> is zoomed out to <code>50</code> percent of the original size.     * If a single number, then that value will be used for all images.     * If a tuple <code>(a, b)</code>, then a value will be uniformly sampled per image from the interval <code>[a, b]</code>.       That the same range will be used for both x- and y-axis. To keep the aspect ratio, set       <code>keep_ratio=True</code>, then the same value will be used for both x- and y-axis.     * If a dictionary, then it is expected to have the keys <code>x</code> and/or <code>y</code>.       Each of these keys can have the same values as described above.       Using a dictionary allows to set different values for the two axis and sampling will then happen       independently per axis, resulting in samples that differ between the axes. Note that when       the <code>keep_ratio=True</code>, the x- and y-axis ranges should be the same.</p> <code>translate_percent</code> <code>None, number, tuple of number or dict</code> <p>Translation as a fraction of the image height/width (x-translation, y-translation), where <code>0</code> denotes \"no change\" and <code>0.5</code> denotes \"half of the axis size\".     * If <code>None</code> then equivalent to <code>0.0</code> unless <code>translate_px</code> has a value other than <code>None</code>.     * If a single number, then that value will be used for all images.     * If a tuple <code>(a, b)</code>, then a value will be uniformly sampled per image from the interval <code>[a, b]</code>.       That sampled fraction value will be used identically for both x- and y-axis.     * If a dictionary, then it is expected to have the keys <code>x</code> and/or <code>y</code>.       Each of these keys can have the same values as described above.       Using a dictionary allows to set different values for the two axis and sampling will then happen       independently per axis, resulting in samples that differ between the axes.</p> <code>translate_px</code> <code>None, int, tuple of int or dict</code> <p>Translation in pixels.     * If <code>None</code> then equivalent to <code>0</code> unless <code>translate_percent</code> has a value other than <code>None</code>.     * If a single int, then that value will be used for all images.     * If a tuple <code>(a, b)</code>, then a value will be uniformly sampled per image from       the discrete interval <code>[a..b]</code>. That number will be used identically for both x- and y-axis.     * If a dictionary, then it is expected to have the keys <code>x</code> and/or <code>y</code>.       Each of these keys can have the same values as described above.       Using a dictionary allows to set different values for the two axis and sampling will then happen       independently per axis, resulting in samples that differ between the axes.</p> <code>rotate</code> <code>number or tuple of number</code> <p>Rotation in degrees (NOT radians), i.e. expected value range is around <code>[-360, 360]</code>. Rotation happens around the center of the image, not the top left corner as in some other frameworks.     * If a number, then that value will be used for all images.     * If a tuple <code>(a, b)</code>, then a value will be uniformly sampled per image from the interval <code>[a, b]</code>       and used as the rotation value.</p> <code>shear</code> <code>number, tuple of number or dict</code> <p>Shear in degrees (NOT radians), i.e. expected value range is around <code>[-360, 360]</code>, with reasonable values being in the range of <code>[-45, 45]</code>.     * If a number, then that value will be used for all images as       the shear on the x-axis (no shear on the y-axis will be done).     * If a tuple <code>(a, b)</code>, then two value will be uniformly sampled per image       from the interval <code>[a, b]</code> and be used as the x- and y-shear value.     * If a dictionary, then it is expected to have the keys <code>x</code> and/or <code>y</code>.       Each of these keys can have the same values as described above.       Using a dictionary allows to set different values for the two axis and sampling will then happen       independently per axis, resulting in samples that differ between the axes.</p> <code>interpolation</code> <code>int</code> <p>OpenCV interpolation flag.</p> <code>mask_interpolation</code> <code>int</code> <p>OpenCV interpolation flag.</p> <code>cval</code> <code>number or sequence of number</code> <p>The constant value to use when filling in newly created pixels. (E.g. translating by 1px to the right will create a new 1px-wide column of pixels on the left of the image). The value is only used when <code>mode=constant</code>. The expected value range is <code>[0, 255]</code> for <code>uint8</code> images.</p> <code>cval_mask</code> <code>number or tuple of number</code> <p>Same as cval but only for masks.</p> <code>mode</code> <code>int</code> <p>OpenCV border flag.</p> <code>fit_output</code> <code>bool</code> <p>If True, the image plane size and position will be adjusted to tightly capture the whole image after affine transformation (<code>translate_percent</code> and <code>translate_px</code> are ignored). Otherwise (<code>False</code>),  parts of the transformed image may end up outside the image plane. Fitting the output shape can be useful to avoid corners of the image being outside the image plane after applying rotations. Default: False</p> <code>keep_ratio</code> <code>bool</code> <p>When True, the original aspect ratio will be kept when the random scale is applied. Default: False.</p> <code>rotate_method</code> <code>Literal[\"largest_box\", \"ellipse\"]</code> <p>rotation method used for the bounding boxes. Should be one of \"largest_box\" or \"ellipse\"[1]. Default: \"largest_box\"</p> <code>balanced_scale</code> <code>bool</code> <p>When True, scaling factors are chosen to be either entirely below or above 1, ensuring balanced scaling. Default: False.</p> <p>This is important because without it, scaling tends to lean towards upscaling. For example, if we want the image to zoom in and out by 2x, we may pick an interval [0.5, 2]. Since the interval [0.5, 1] is three times smaller than [1, 2], values above 1 are picked three times more often if sampled directly from [0.5, 2]. With <code>balanced_scale</code>, the  function ensures that half the time, the scaling factor is picked from below 1 (zooming out), and the other half from above 1 (zooming in). This makes the zooming in and out process more balanced.</p> <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 0.5.</p> <p>Targets</p> <p>image, mask, keypoints, bboxes</p> <p>Image types:     uint8, float32</p> <p>Reference</p> <p>[1] https://arxiv.org/abs/2109.13488</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>class Affine(DualTransform):\n    \"\"\"Augmentation to apply affine transformations to images.\n\n    Affine transformations involve:\n\n        - Translation (\"move\" image on the x-/y-axis)\n        - Rotation\n        - Scaling (\"zoom\" in/out)\n        - Shear (move one side of the image, turning a square into a trapezoid)\n\n    All such transformations can create \"new\" pixels in the image without a defined content, e.g.\n    if the image is translated to the left, pixels are created on the right.\n    A method has to be defined to deal with these pixel values.\n    The parameters `cval` and `mode` of this class deal with this.\n\n    Some transformations involve interpolations between several pixels\n    of the input image to generate output pixel values. The parameters `interpolation` and\n    `mask_interpolation` deals with the method of interpolation used for this.\n\n    Args:\n        scale (number, tuple of number or dict): Scaling factor to use, where ``1.0`` denotes \"no change\" and\n            ``0.5`` is zoomed out to ``50`` percent of the original size.\n                * If a single number, then that value will be used for all images.\n                * If a tuple ``(a, b)``, then a value will be uniformly sampled per image from the interval ``[a, b]``.\n                  That the same range will be used for both x- and y-axis. To keep the aspect ratio, set\n                  ``keep_ratio=True``, then the same value will be used for both x- and y-axis.\n                * If a dictionary, then it is expected to have the keys ``x`` and/or ``y``.\n                  Each of these keys can have the same values as described above.\n                  Using a dictionary allows to set different values for the two axis and sampling will then happen\n                  *independently* per axis, resulting in samples that differ between the axes. Note that when\n                  the ``keep_ratio=True``, the x- and y-axis ranges should be the same.\n        translate_percent (None, number, tuple of number or dict): Translation as a fraction of the image height/width\n            (x-translation, y-translation), where ``0`` denotes \"no change\"\n            and ``0.5`` denotes \"half of the axis size\".\n                * If ``None`` then equivalent to ``0.0`` unless `translate_px` has a value other than ``None``.\n                * If a single number, then that value will be used for all images.\n                * If a tuple ``(a, b)``, then a value will be uniformly sampled per image from the interval ``[a, b]``.\n                  That sampled fraction value will be used identically for both x- and y-axis.\n                * If a dictionary, then it is expected to have the keys ``x`` and/or ``y``.\n                  Each of these keys can have the same values as described above.\n                  Using a dictionary allows to set different values for the two axis and sampling will then happen\n                  *independently* per axis, resulting in samples that differ between the axes.\n        translate_px (None, int, tuple of int or dict): Translation in pixels.\n                * If ``None`` then equivalent to ``0`` unless `translate_percent` has a value other than ``None``.\n                * If a single int, then that value will be used for all images.\n                * If a tuple ``(a, b)``, then a value will be uniformly sampled per image from\n                  the discrete interval ``[a..b]``. That number will be used identically for both x- and y-axis.\n                * If a dictionary, then it is expected to have the keys ``x`` and/or ``y``.\n                  Each of these keys can have the same values as described above.\n                  Using a dictionary allows to set different values for the two axis and sampling will then happen\n                  *independently* per axis, resulting in samples that differ between the axes.\n        rotate (number or tuple of number): Rotation in degrees (**NOT** radians), i.e. expected value range is\n            around ``[-360, 360]``. Rotation happens around the *center* of the image,\n            not the top left corner as in some other frameworks.\n                * If a number, then that value will be used for all images.\n                * If a tuple ``(a, b)``, then a value will be uniformly sampled per image from the interval ``[a, b]``\n                  and used as the rotation value.\n        shear (number, tuple of number or dict): Shear in degrees (**NOT** radians), i.e. expected value range is\n            around ``[-360, 360]``, with reasonable values being in the range of ``[-45, 45]``.\n                * If a number, then that value will be used for all images as\n                  the shear on the x-axis (no shear on the y-axis will be done).\n                * If a tuple ``(a, b)``, then two value will be uniformly sampled per image\n                  from the interval ``[a, b]`` and be used as the x- and y-shear value.\n                * If a dictionary, then it is expected to have the keys ``x`` and/or ``y``.\n                  Each of these keys can have the same values as described above.\n                  Using a dictionary allows to set different values for the two axis and sampling will then happen\n                  *independently* per axis, resulting in samples that differ between the axes.\n        interpolation (int): OpenCV interpolation flag.\n        mask_interpolation (int): OpenCV interpolation flag.\n        cval (number or sequence of number): The constant value to use when filling in newly created pixels.\n            (E.g. translating by 1px to the right will create a new 1px-wide column of pixels\n            on the left of the image).\n            The value is only used when `mode=constant`. The expected value range is ``[0, 255]`` for ``uint8`` images.\n        cval_mask (number or tuple of number): Same as cval but only for masks.\n        mode (int): OpenCV border flag.\n        fit_output (bool): If True, the image plane size and position will be adjusted to tightly capture\n            the whole image after affine transformation (`translate_percent` and `translate_px` are ignored).\n            Otherwise (``False``),  parts of the transformed image may end up outside the image plane.\n            Fitting the output shape can be useful to avoid corners of the image being outside the image plane\n            after applying rotations. Default: False\n        keep_ratio (bool): When True, the original aspect ratio will be kept when the random scale is applied.\n            Default: False.\n        rotate_method (Literal[\"largest_box\", \"ellipse\"]): rotation method used for the bounding boxes.\n            Should be one of \"largest_box\" or \"ellipse\"[1]. Default: \"largest_box\"\n        balanced_scale (bool): When True, scaling factors are chosen to be either entirely below or above 1,\n            ensuring balanced scaling. Default: False.\n\n            This is important because without it, scaling tends to lean towards upscaling. For example, if we want\n            the image to zoom in and out by 2x, we may pick an interval [0.5, 2]. Since the interval [0.5, 1] is\n            three times smaller than [1, 2], values above 1 are picked three times more often if sampled directly\n            from [0.5, 2]. With `balanced_scale`, the  function ensures that half the time, the scaling\n            factor is picked from below 1 (zooming out), and the other half from above 1 (zooming in).\n            This makes the zooming in and out process more balanced.\n        p (float): probability of applying the transform. Default: 0.5.\n\n    Targets:\n        image, mask, keypoints, bboxes\n\n    Image types:\n        uint8, float32\n\n    Reference:\n        [1] https://arxiv.org/abs/2109.13488\n\n    \"\"\"\n\n    _targets = (Targets.IMAGE, Targets.MASK, Targets.BBOXES, Targets.KEYPOINTS)\n\n    class InitSchema(BaseTransformInitSchema):\n        scale: ScaleFloatType | dict[str, Any] | None = Field(\n            default=None,\n            description=\"Scaling factor or dictionary for independent axis scaling.\",\n        )\n        translate_percent: ScaleFloatType | dict[str, Any] | None = Field(\n            default=None,\n            description=\"Translation as a fraction of the image dimension.\",\n        )\n        translate_px: ScaleIntType | dict[str, Any] | None = Field(\n            default=None,\n            description=\"Translation in pixels.\",\n        )\n        rotate: ScaleFloatType | None = Field(default=None, description=\"Rotation angle in degrees.\")\n        shear: ScaleFloatType | dict[str, Any] | None = Field(\n            default=None,\n            description=\"Shear angle in degrees.\",\n        )\n        interpolation: InterpolationType = cv2.INTER_LINEAR\n        mask_interpolation: InterpolationType = cv2.INTER_NEAREST\n\n        cval: ColorType = Field(default=0, description=\"Value used for constant padding.\")\n        cval_mask: ColorType = Field(default=0, description=\"Value used for mask constant padding.\")\n        mode: BorderModeType = cv2.BORDER_CONSTANT\n        fit_output: Annotated[bool, Field(default=False, description=\"Adjust output to capture whole image.\")]\n        keep_ratio: Annotated[bool, Field(default=False, description=\"Maintain aspect ratio when scaling.\")]\n        rotate_method: Literal[\"largest_box\", \"ellipse\"] = \"largest_box\"\n        balanced_scale: Annotated[bool, Field(default=False, description=\"Use balanced scaling.\")]\n\n    def __init__(\n        self,\n        scale: ScaleFloatType | dict[str, Any] | None = None,\n        translate_percent: ScaleFloatType | dict[str, Any] | None = None,\n        translate_px: ScaleIntType | dict[str, Any] | None = None,\n        rotate: ScaleFloatType | None = None,\n        shear: ScaleFloatType | dict[str, Any] | None = None,\n        interpolation: int = cv2.INTER_LINEAR,\n        mask_interpolation: int = cv2.INTER_NEAREST,\n        cval: ColorType = 0,\n        cval_mask: ColorType = 0,\n        mode: int = cv2.BORDER_CONSTANT,\n        fit_output: bool = False,\n        keep_ratio: bool = False,\n        rotate_method: Literal[\"largest_box\", \"ellipse\"] = \"largest_box\",\n        balanced_scale: bool = False,\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p=p, always_apply=always_apply)\n\n        params = [scale, translate_percent, translate_px, rotate, shear]\n        if all(p is None for p in params):\n            scale = {\"x\": (0.9, 1.1), \"y\": (0.9, 1.1)}\n            translate_percent = {\"x\": (-0.1, 0.1), \"y\": (-0.1, 0.1)}\n            rotate = (-15, 15)\n            shear = {\"x\": (-10, 10), \"y\": (-10, 10)}\n        else:\n            scale = scale if scale is not None else 1.0\n            rotate = rotate if rotate is not None else 0.0\n            shear = shear if shear is not None else 0.0\n\n        self.interpolation = interpolation\n        self.mask_interpolation = mask_interpolation\n        self.cval = cval\n        self.cval_mask = cval_mask\n        self.mode = mode\n        self.scale = self._handle_dict_arg(scale, \"scale\")\n        self.translate_percent, self.translate_px = self._handle_translate_arg(translate_px, translate_percent)\n        self.rotate = to_tuple(rotate, rotate)\n        self.fit_output = fit_output\n        self.shear = self._handle_dict_arg(shear, \"shear\")\n        self.keep_ratio = keep_ratio\n        self.rotate_method = rotate_method\n        self.balanced_scale = balanced_scale\n\n        if self.keep_ratio and self.scale[\"x\"] != self.scale[\"y\"]:\n            raise ValueError(f\"When keep_ratio is True, the x and y scale range should be identical. got {self.scale}\")\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return (\n            \"interpolation\",\n            \"mask_interpolation\",\n            \"cval\",\n            \"mode\",\n            \"scale\",\n            \"translate_percent\",\n            \"translate_px\",\n            \"rotate\",\n            \"fit_output\",\n            \"shear\",\n            \"cval_mask\",\n            \"keep_ratio\",\n            \"rotate_method\",\n            \"balanced_scale\",\n        )\n\n    @staticmethod\n    def _handle_dict_arg(\n        val: float | tuple[float, float] | dict[str, Any],\n        name: str,\n        default: float = 1.0,\n    ) -&gt; dict[str, Any]:\n        if isinstance(val, dict):\n            if \"x\" not in val and \"y\" not in val:\n                raise ValueError(\n                    f'Expected {name} dictionary to contain at least key \"x\" or key \"y\". Found neither of them.',\n                )\n            x = val.get(\"x\", default)\n            y = val.get(\"y\", default)\n            return {\"x\": to_tuple(x, x), \"y\": to_tuple(y, y)}\n        return {\"x\": to_tuple(val, val), \"y\": to_tuple(val, val)}\n\n    @classmethod\n    def _handle_translate_arg(\n        cls,\n        translate_px: ScaleFloatType | dict[str, Any] | None,\n        translate_percent: ScaleFloatType | dict[str, Any] | None,\n    ) -&gt; Any:\n        if translate_percent is None and translate_px is None:\n            translate_px = 0\n\n        if translate_percent is not None and translate_px is not None:\n            msg = \"Expected either translate_percent or translate_px to be provided, but both were provided.\"\n            raise ValueError(msg)\n\n        if translate_percent is not None:\n            # translate by percent\n            return cls._handle_dict_arg(translate_percent, \"translate_percent\", default=0.0), translate_px\n\n        if translate_px is None:\n            msg = \"translate_px is None.\"\n            raise ValueError(msg)\n        # translate by pixels\n        return translate_percent, cls._handle_dict_arg(translate_px, \"translate_px\")\n\n    def apply(\n        self,\n        img: np.ndarray,\n        matrix: skimage.transform.ProjectiveTransform,\n        output_shape: SizeType,\n        **params: Any,\n    ) -&gt; np.ndarray:\n        return fgeometric.warp_affine(\n            img,\n            matrix,\n            interpolation=self.interpolation,\n            cval=self.cval,\n            mode=self.mode,\n            output_shape=output_shape,\n        )\n\n    def apply_to_mask(\n        self,\n        mask: np.ndarray,\n        matrix: skimage.transform.ProjectiveTransform,\n        output_shape: SizeType,\n        **params: Any,\n    ) -&gt; np.ndarray:\n        return fgeometric.warp_affine(\n            mask,\n            matrix,\n            interpolation=self.mask_interpolation,\n            cval=self.cval_mask,\n            mode=self.mode,\n            output_shape=output_shape,\n        )\n\n    def apply_to_bboxes(\n        self,\n        bboxes: Sequence[BoxType],\n        bbox_matrix: skimage.transform.AffineTransform,\n        output_shape: SizeType,\n        **params: Any,\n    ) -&gt; list[BoxType]:\n        bboxes_np = np.array(bboxes)\n        result = fgeometric.bboxes_affine(\n            bboxes_np,\n            bbox_matrix,\n            self.rotate_method,\n            params[\"shape\"][:2],\n            self.mode,\n            output_shape,\n        )\n        return result.tolist()\n\n    def apply_to_keypoints(\n        self,\n        keypoints: Sequence[KeypointType],\n        matrix: skimage.transform.AffineTransform,\n        scale: dict[str, Any],\n        **params: Any,\n    ) -&gt; list[KeypointType]:\n        # Convert keypoints to numpy array, including all attributes\n        keypoints_array = np.array([list(kp) for kp in keypoints], dtype=np.float32)\n\n        padded_keypoints = fgeometric.keypoints_affine(keypoints_array, matrix, params[\"shape\"][:2], scale, self.mode)\n\n        # Convert back to list of tuples\n        return [tuple(kp) for kp in padded_keypoints]\n\n    @staticmethod\n    def get_scale(\n        scale: dict[str, tuple[float, float]],\n        keep_ratio: bool,\n        balanced_scale: bool,\n    ) -&gt; fgeometric.ScaleDict:\n        result_scale = {}\n        if balanced_scale:\n            for key, value in scale.items():\n                lower_interval = (value[0], 1.0) if value[0] &lt; 1 else None\n                upper_interval = (1.0, value[1]) if value[1] &gt; 1 else None\n\n                if lower_interval is not None and upper_interval is not None:\n                    selected_interval = random.choice([lower_interval, upper_interval])\n                elif lower_interval is not None:\n                    selected_interval = lower_interval\n                elif upper_interval is not None:\n                    selected_interval = upper_interval\n                else:\n                    raise ValueError(f\"Both lower_interval and upper_interval are None for key: {key}\")\n\n                result_scale[key] = random.uniform(*selected_interval)\n        else:\n            result_scale = {key: random.uniform(*value) for key, value in scale.items()}\n\n        if keep_ratio:\n            result_scale[\"y\"] = result_scale[\"x\"]\n\n        return cast(fgeometric.ScaleDict, result_scale)\n\n    def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n        image_shape = params[\"shape\"][:2]\n\n        translate = self._get_translate_params(image_shape)\n        shear = self._get_shear_params()\n        scale = self.get_scale(self.scale, self.keep_ratio, self.balanced_scale)\n        rotate = -random.uniform(*self.rotate)\n\n        image_shift = center(image_shape)\n        bbox_shift = center_bbox(image_shape)\n\n        matrix = fgeometric.create_affine_transformation_matrix(translate, shear, scale, rotate, image_shift)\n        bbox_matrix = fgeometric.create_affine_transformation_matrix(translate, shear, scale, rotate, bbox_shift)\n\n        if self.fit_output:\n            matrix, output_shape = fgeometric.compute_affine_warp_output_shape(matrix, image_shape)\n            bbox_matrix, _ = fgeometric.compute_affine_warp_output_shape(bbox_matrix, image_shape)\n        else:\n            output_shape = image_shape\n\n        return {\n            \"rotate\": rotate,\n            \"scale\": scale,\n            \"matrix\": matrix,\n            \"bbox_matrix\": bbox_matrix,\n            \"output_shape\": output_shape,\n        }\n\n    def _get_translate_params(self, image_shape: tuple[int, int]) -&gt; fgeometric.TranslateDict:\n        height, width = image_shape[:2]\n        if self.translate_px is not None:\n            return cast(\n                fgeometric.TranslateDict,\n                {key: random.randint(*value) for key, value in self.translate_px.items()},\n            )\n        if self.translate_percent is not None:\n            translate = {key: random.uniform(*value) for key, value in self.translate_percent.items()}\n            return cast(fgeometric.TranslateDict, {\"x\": translate[\"x\"] * width, \"y\": translate[\"y\"] * height})\n        return cast(fgeometric.TranslateDict, {\"x\": 0, \"y\": 0})\n\n    def _get_shear_params(self) -&gt; fgeometric.ShearDict:\n        return cast(fgeometric.ShearDict, {key: -random.uniform(*value) for key, value in self.shear.items()})\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.transforms.Affine.apply","title":"<code>apply (self, img, matrix, output_shape, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def apply(\n    self,\n    img: np.ndarray,\n    matrix: skimage.transform.ProjectiveTransform,\n    output_shape: SizeType,\n    **params: Any,\n) -&gt; np.ndarray:\n    return fgeometric.warp_affine(\n        img,\n        matrix,\n        interpolation=self.interpolation,\n        cval=self.cval,\n        mode=self.mode,\n        output_shape=output_shape,\n    )\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.transforms.Affine.get_params_dependent_on_data","title":"<code>get_params_dependent_on_data (self, params, data)</code>","text":"<p>Returns parameters dependent on input.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n    image_shape = params[\"shape\"][:2]\n\n    translate = self._get_translate_params(image_shape)\n    shear = self._get_shear_params()\n    scale = self.get_scale(self.scale, self.keep_ratio, self.balanced_scale)\n    rotate = -random.uniform(*self.rotate)\n\n    image_shift = center(image_shape)\n    bbox_shift = center_bbox(image_shape)\n\n    matrix = fgeometric.create_affine_transformation_matrix(translate, shear, scale, rotate, image_shift)\n    bbox_matrix = fgeometric.create_affine_transformation_matrix(translate, shear, scale, rotate, bbox_shift)\n\n    if self.fit_output:\n        matrix, output_shape = fgeometric.compute_affine_warp_output_shape(matrix, image_shape)\n        bbox_matrix, _ = fgeometric.compute_affine_warp_output_shape(bbox_matrix, image_shape)\n    else:\n        output_shape = image_shape\n\n    return {\n        \"rotate\": rotate,\n        \"scale\": scale,\n        \"matrix\": matrix,\n        \"bbox_matrix\": bbox_matrix,\n        \"output_shape\": output_shape,\n    }\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.transforms.Affine.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return (\n        \"interpolation\",\n        \"mask_interpolation\",\n        \"cval\",\n        \"mode\",\n        \"scale\",\n        \"translate_percent\",\n        \"translate_px\",\n        \"rotate\",\n        \"fit_output\",\n        \"shear\",\n        \"cval_mask\",\n        \"keep_ratio\",\n        \"rotate_method\",\n        \"balanced_scale\",\n    )\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.transforms.D4","title":"<code>class  D4</code> <code>     (always_apply=None, p=1)                 </code>  [view source on GitHub]","text":"<p>Applies one of the eight possible D4 dihedral group transformations to a square-shaped input,     maintaining the square shape. These transformations correspond to the symmetries of a square,     including rotations and reflections.</p> <p>The D4 group transformations include: - 'e' (identity): No transformation is applied. - 'r90' (rotation by 90 degrees counterclockwise) - 'r180' (rotation by 180 degrees) - 'r270' (rotation by 270 degrees counterclockwise) - 'v' (reflection across the vertical midline) - 'hvt' (reflection across the anti-diagonal) - 'h' (reflection across the horizontal midline) - 't' (reflection across the main diagonal)</p> <p>Even if the probability (<code>p</code>) of applying the transform is set to 1, the identity transformation 'e' may still occur, which means the input will remain unchanged in one out of eight cases.</p> <p>Parameters:</p> Name Type Description <code>p</code> <code>float</code> <p>Probability of applying the transform. Default is 1, meaning the        transform is applied every time it is called.</p> <p>Targets</p> <p>image, mask, bboxes, keypoints</p> <p>Image types:     uint8, float32</p> <p>Note</p> <p>This transform is particularly useful when augmenting data that does not have a clear orientation: - Top view satellite or drone imagery - Medical images</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>class D4(DualTransform):\n    \"\"\"Applies one of the eight possible D4 dihedral group transformations to a square-shaped input,\n        maintaining the square shape. These transformations correspond to the symmetries of a square,\n        including rotations and reflections.\n\n    The D4 group transformations include:\n    - 'e' (identity): No transformation is applied.\n    - 'r90' (rotation by 90 degrees counterclockwise)\n    - 'r180' (rotation by 180 degrees)\n    - 'r270' (rotation by 270 degrees counterclockwise)\n    - 'v' (reflection across the vertical midline)\n    - 'hvt' (reflection across the anti-diagonal)\n    - 'h' (reflection across the horizontal midline)\n    - 't' (reflection across the main diagonal)\n\n    Even if the probability (`p`) of applying the transform is set to 1, the identity transformation\n    'e' may still occur, which means the input will remain unchanged in one out of eight cases.\n\n    Args:\n        p (float): Probability of applying the transform. Default is 1, meaning the\n                   transform is applied every time it is called.\n\n    Targets:\n        image, mask, bboxes, keypoints\n\n    Image types:\n        uint8, float32\n\n    Note:\n        This transform is particularly useful when augmenting data that does not have a clear orientation:\n        - Top view satellite or drone imagery\n        - Medical images\n\n    \"\"\"\n\n    _targets = (Targets.IMAGE, Targets.MASK, Targets.BBOXES, Targets.KEYPOINTS)\n\n    class InitSchema(BaseTransformInitSchema):\n        p: ProbabilityType = 1\n\n    def __init__(\n        self,\n        always_apply: bool | None = None,\n        p: float = 1,\n    ):\n        super().__init__(p=p, always_apply=always_apply)\n\n    def apply(self, img: np.ndarray, group_element: D4Type, **params: Any) -&gt; np.ndarray:\n        return fgeometric.d4(img, group_element)\n\n    def apply_to_bbox(self, bbox: BoxInternalType, group_element: D4Type, **params: Any) -&gt; BoxInternalType:\n        return fgeometric.bbox_d4(bbox, group_element)\n\n    def apply_to_keypoint(\n        self,\n        keypoint: KeypointInternalType,\n        group_element: D4Type,\n        **params: Any,\n    ) -&gt; KeypointInternalType:\n        return fgeometric.keypoint_d4(keypoint, group_element, params[\"shape\"])\n\n    def get_params(self) -&gt; dict[str, D4Type]:\n        return {\n            \"group_element\": random_utils.choice(d4_group_elements),\n        }\n\n    def get_transform_init_args_names(self) -&gt; tuple[()]:\n        return ()\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.transforms.D4.apply","title":"<code>apply (self, img, group_element, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def apply(self, img: np.ndarray, group_element: D4Type, **params: Any) -&gt; np.ndarray:\n    return fgeometric.d4(img, group_element)\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.transforms.D4.get_params","title":"<code>get_params (self)</code>","text":"<p>Returns parameters independent of input.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def get_params(self) -&gt; dict[str, D4Type]:\n    return {\n        \"group_element\": random_utils.choice(d4_group_elements),\n    }\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.transforms.D4.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[()]:\n    return ()\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.transforms.ElasticTransform","title":"<code>class  ElasticTransform</code> <code>     (alpha=3, sigma=50, alpha_affine=None, interpolation=1, border_mode=4, value=None, mask_value=None, always_apply=None, approximate=False, same_dxdy=False, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Apply elastic deformation to images, masks, and bounding boxes as described in [Simard2003]_.</p> <p>This transformation introduces random elastic distortions to images, which can be useful for data augmentation in training convolutional neural networks. The transformation can be applied in an approximate or precise manner, with an option to use the same displacement field for both x and y directions to speed up the process.</p> <p>Parameters:</p> Name Type Description <code>alpha</code> <code>float</code> <p>Scaling factor for the random displacement fields.</p> <code>sigma</code> <code>float</code> <p>Standard deviation for Gaussian filter applied to the displacement fields.</p> <code>interpolation</code> <code>int</code> <p>Interpolation method to be used. Should be one of: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4. Default is cv2.INTER_LINEAR.</p> <code>border_mode</code> <code>int</code> <p>Pixel extrapolation method. Should be one of: cv2.BORDER_CONSTANT, cv2.BORDER_REPLICATE, cv2.BORDER_REFLECT, cv2.BORDER_WRAP, cv2.BORDER_REFLECT_101. Default is cv2.BORDER_REFLECT_101.</p> <code>value</code> <code>int, float, list of int, list of float</code> <p>Padding value if border_mode is cv2.BORDER_CONSTANT.</p> <code>mask_value</code> <code>int, float, list of int, list of float</code> <p>Padding value if border_mode is cv2.BORDER_CONSTANT, applied to masks.</p> <code>approximate</code> <code>bool</code> <p>Whether to smooth displacement map with a fixed kernel size. Enabling this option gives ~2X speedup on large images. Default is False.</p> <code>same_dxdy</code> <code>bool</code> <p>Whether to use the same random displacement for x and y directions. Enabling this option gives ~2X speedup. Default is False.</p> <p>Targets</p> <p>image, mask, bboxes</p> <p>Image types:     uint8, float32</p> <p>Reference</p> <p>Simard, Steinkraus and Platt, \"Best Practices for Convolutional Neural Networks applied to Visual Document Analysis\", in Proc. of the International Conference on Document Analysis and Recognition, 2003. https://gist.github.com/ernestum/601cdf56d2b424757de5</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>class ElasticTransform(DualTransform):\n    \"\"\"Apply elastic deformation to images, masks, and bounding boxes as described in [Simard2003]_.\n\n    This transformation introduces random elastic distortions to images, which can be useful for data augmentation\n    in training convolutional neural networks. The transformation can be applied in an approximate or precise manner,\n    with an option to use the same displacement field for both x and y directions to speed up the process.\n\n    Args:\n        alpha (float): Scaling factor for the random displacement fields.\n        sigma (float): Standard deviation for Gaussian filter applied to the displacement fields.\n        interpolation (int): Interpolation method to be used. Should be one of:\n            cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.\n            Default is cv2.INTER_LINEAR.\n        border_mode (int): Pixel extrapolation method. Should be one of:\n            cv2.BORDER_CONSTANT, cv2.BORDER_REPLICATE, cv2.BORDER_REFLECT, cv2.BORDER_WRAP, cv2.BORDER_REFLECT_101.\n            Default is cv2.BORDER_REFLECT_101.\n        value (int, float, list of int, list of float, optional): Padding value if border_mode is cv2.BORDER_CONSTANT.\n        mask_value (int, float, list of int, list of float, optional): Padding value if border_mode is\n            cv2.BORDER_CONSTANT, applied to masks.\n        approximate (bool, optional): Whether to smooth displacement map with a fixed kernel size.\n            Enabling this option gives ~2X speedup on large images. Default is False.\n        same_dxdy (bool, optional): Whether to use the same random displacement for x and y directions.\n            Enabling this option gives ~2X speedup. Default is False.\n\n    Targets:\n        image, mask, bboxes\n\n    Image types:\n        uint8, float32\n\n    Reference:\n        Simard, Steinkraus and Platt, \"Best Practices for Convolutional Neural Networks applied to\n        Visual Document Analysis\", in Proc. of the International Conference on Document Analysis and Recognition, 2003.\n        https://gist.github.com/ernestum/601cdf56d2b424757de5\n    \"\"\"\n\n    _targets = (Targets.IMAGE, Targets.MASK, Targets.BBOXES)\n\n    class InitSchema(BaseTransformInitSchema):\n        alpha: Annotated[float, Field(description=\"Alpha parameter.\", ge=0)]\n        sigma: Annotated[float, Field(default=50, description=\"Sigma parameter for Gaussian filter.\", ge=1)]\n        alpha_affine: None = Field(\n            description=\"Alpha affine parameter.\",\n            deprecated=\"Use Affine transform to get affine effects\",\n        )\n        interpolation: InterpolationType = cv2.INTER_LINEAR\n        border_mode: BorderModeType = cv2.BORDER_REFLECT_101\n        value: int | float | list[int] | list[float] | None = Field(\n            default=None,\n            description=\"Padding value if border_mode is cv2.BORDER_CONSTANT.\",\n        )\n        mask_value: float | list[int] | list[float] | None = Field(\n            default=None,\n            description=\"Padding value if border_mode is cv2.BORDER_CONSTANT applied for masks.\",\n        )\n        approximate: Annotated[bool, Field(default=False, description=\"Approximate displacement map smoothing.\")]\n        same_dxdy: Annotated[bool, Field(default=False, description=\"Use same shift for x and y.\")]\n\n    def __init__(\n        self,\n        alpha: float = 3,\n        sigma: float = 50,\n        alpha_affine: None = None,\n        interpolation: int = cv2.INTER_LINEAR,\n        border_mode: int = cv2.BORDER_REFLECT_101,\n        value: ScalarType | list[ScalarType] | None = None,\n        mask_value: ScalarType | list[ScalarType] | None = None,\n        always_apply: bool | None = None,\n        approximate: bool = False,\n        same_dxdy: bool = False,\n        p: float = 0.5,\n    ):\n        super().__init__(p=p, always_apply=always_apply)\n        self.alpha = alpha\n        self.sigma = sigma\n        self.interpolation = interpolation\n        self.border_mode = border_mode\n        self.value = value\n        self.mask_value = mask_value\n        self.approximate = approximate\n        self.same_dxdy = same_dxdy\n\n    def apply(\n        self,\n        img: np.ndarray,\n        random_seed: int,\n        interpolation: int,\n        **params: Any,\n    ) -&gt; np.ndarray:\n        return fgeometric.elastic_transform(\n            img,\n            self.alpha,\n            self.sigma,\n            interpolation,\n            self.border_mode,\n            self.value,\n            np.random.RandomState(random_seed),\n            self.approximate,\n            self.same_dxdy,\n        )\n\n    def apply_to_mask(self, mask: np.ndarray, random_seed: int, **params: Any) -&gt; np.ndarray:\n        return fgeometric.elastic_transform(\n            mask,\n            self.alpha,\n            self.sigma,\n            cv2.INTER_NEAREST,\n            self.border_mode,\n            self.mask_value,\n            np.random.RandomState(random_seed),\n            self.approximate,\n            self.same_dxdy,\n        )\n\n    def apply_to_bbox(\n        self,\n        bbox: BoxInternalType,\n        random_seed: int,\n        **params: Any,\n    ) -&gt; BoxInternalType:\n        image_shape = params[\"shape\"][:2]\n\n        mask = np.zeros(image_shape, dtype=np.uint8)\n        bbox_denorm = fgeometric.denormalize_bbox(bbox, image_shape)\n        x_min, y_min, x_max, y_max = bbox_denorm[:4]\n        x_min, y_min, x_max, y_max = int(x_min), int(y_min), int(x_max), int(y_max)\n        mask[y_min:y_max, x_min:x_max] = 1\n        mask = fgeometric.elastic_transform(\n            mask,\n            self.alpha,\n            self.sigma,\n            cv2.INTER_NEAREST,\n            self.border_mode,\n            self.mask_value,\n            np.random.RandomState(random_seed),\n            self.approximate,\n        )\n        bbox_returned = bbox_from_mask(mask)\n        return cast(BoxInternalType, fgeometric.normalize_bbox(bbox_returned, image_shape))\n\n    def get_params(self) -&gt; dict[str, int]:\n        return {\"random_seed\": random_utils.get_random_seed()}\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return (\n            \"alpha\",\n            \"sigma\",\n            \"interpolation\",\n            \"border_mode\",\n            \"value\",\n            \"mask_value\",\n            \"approximate\",\n            \"same_dxdy\",\n        )\n\n    @property\n    def targets(self) -&gt; dict[str, Callable[..., Any]]:\n        return {\n            \"image\": self.apply,\n            \"mask\": self.apply_to_mask,\n            \"masks\": self.apply_to_masks,\n            \"bboxes\": self.apply_to_bboxes,\n        }\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.transforms.ElasticTransform.apply","title":"<code>apply (self, img, random_seed, interpolation, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def apply(\n    self,\n    img: np.ndarray,\n    random_seed: int,\n    interpolation: int,\n    **params: Any,\n) -&gt; np.ndarray:\n    return fgeometric.elastic_transform(\n        img,\n        self.alpha,\n        self.sigma,\n        interpolation,\n        self.border_mode,\n        self.value,\n        np.random.RandomState(random_seed),\n        self.approximate,\n        self.same_dxdy,\n    )\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.transforms.ElasticTransform.get_params","title":"<code>get_params (self)</code>","text":"<p>Returns parameters independent of input.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def get_params(self) -&gt; dict[str, int]:\n    return {\"random_seed\": random_utils.get_random_seed()}\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.transforms.ElasticTransform.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return (\n        \"alpha\",\n        \"sigma\",\n        \"interpolation\",\n        \"border_mode\",\n        \"value\",\n        \"mask_value\",\n        \"approximate\",\n        \"same_dxdy\",\n    )\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.transforms.Flip","title":"<code>class  Flip</code> <code>     (always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Deprecated. Consider using HorizontalFlip, VerticalFlip, RandomRotate90 or D4.</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>class Flip(DualTransform):\n    \"\"\"Deprecated. Consider using HorizontalFlip, VerticalFlip, RandomRotate90 or D4.\"\"\"\n\n    _targets = (Targets.IMAGE, Targets.MASK, Targets.BBOXES, Targets.KEYPOINTS)\n\n    def __init__(self, always_apply: bool | None = None, p: float = 0.5):\n        super().__init__(p=p, always_apply=always_apply)\n        warn(\n            \"Flip is deprecated. Consider using HorizontalFlip, VerticalFlip, RandomRotate90 or D4.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n\n    def apply(self, img: np.ndarray, d: int, **params: Any) -&gt; np.ndarray:\n        \"\"\"Args:\n        d (int): code that specifies how to flip the input. 0 for vertical flipping, 1 for horizontal flipping,\n                -1 for both vertical and horizontal flipping (which is also could be seen as rotating the input by\n                180 degrees).\n        \"\"\"\n        return fgeometric.random_flip(img, d)\n\n    def get_params(self) -&gt; dict[str, int]:\n        # Random int in the range [-1, 1]\n        return {\"d\": random.randint(-1, 1)}\n\n    def apply_to_bbox(self, bbox: BoxInternalType, **params: Any) -&gt; BoxInternalType:\n        return fgeometric.bbox_flip(bbox, params[\"d\"])\n\n    def apply_to_keypoint(self, keypoint: KeypointInternalType, **params: Any) -&gt; KeypointInternalType:\n        return fgeometric.keypoint_flip(keypoint, params[\"d\"], params[\"shape\"])\n\n    def get_transform_init_args_names(self) -&gt; tuple[()]:\n        return ()\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.transforms.Flip.__init__","title":"<code>__init__ (self, always_apply=None, p=0.5)</code>  <code>special</code>","text":"<p>Initialize self.  See help(type(self)) for accurate signature.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def __init__(self, always_apply: bool | None = None, p: float = 0.5):\n    super().__init__(p=p, always_apply=always_apply)\n    warn(\n        \"Flip is deprecated. Consider using HorizontalFlip, VerticalFlip, RandomRotate90 or D4.\",\n        DeprecationWarning,\n        stacklevel=2,\n    )\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.transforms.Flip.apply","title":"<code>apply (self, img, d, **params)</code>","text":"<p>d (int): code that specifies how to flip the input. 0 for vertical flipping, 1 for horizontal flipping,         -1 for both vertical and horizontal flipping (which is also could be seen as rotating the input by         180 degrees).</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def apply(self, img: np.ndarray, d: int, **params: Any) -&gt; np.ndarray:\n    \"\"\"Args:\n    d (int): code that specifies how to flip the input. 0 for vertical flipping, 1 for horizontal flipping,\n            -1 for both vertical and horizontal flipping (which is also could be seen as rotating the input by\n            180 degrees).\n    \"\"\"\n    return fgeometric.random_flip(img, d)\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.transforms.Flip.get_params","title":"<code>get_params (self)</code>","text":"<p>Returns parameters independent of input.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def get_params(self) -&gt; dict[str, int]:\n    # Random int in the range [-1, 1]\n    return {\"d\": random.randint(-1, 1)}\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.transforms.Flip.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[()]:\n    return ()\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.transforms.GridDistortion","title":"<code>class  GridDistortion</code> <code>     (num_steps=5, distort_limit=(-0.3, 0.3), interpolation=1, border_mode=4, value=None, mask_value=None, normalized=False, always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Applies grid distortion augmentation to images, masks, and bounding boxes. This technique involves dividing the image into a grid of cells and randomly displacing the intersection points of the grid, resulting in localized distortions.</p> <p>Parameters:</p> Name Type Description <code>num_steps</code> <code>int</code> <p>Number of grid cells on each side (minimum 1).</p> <code>distort_limit</code> <code>float, (float, float</code> <p>Range of distortion limits. If a single float is provided, the range will be from (-distort_limit, distort_limit). Default: (-0.3, 0.3).</p> <code>interpolation</code> <code>OpenCV flag</code> <p>Interpolation algorithm used for image transformation. Options are: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4. Default: cv2.INTER_LINEAR.</p> <code>border_mode</code> <code>OpenCV flag</code> <p>Pixel extrapolation method used when pixels outside the image are required. Options are: cv2.BORDER_CONSTANT, cv2.BORDER_REPLICATE, cv2.BORDER_REFLECT, cv2.BORDER_WRAP, cv2.BORDER_REFLECT_101. Default: cv2.BORDER_REFLECT_101.</p> <code>value</code> <code>int, float, list of ints, list of floats</code> <p>Value used for padding when border_mode is cv2.BORDER_CONSTANT.</p> <code>mask_value</code> <code>int, float, list of ints, list of floats</code> <p>Padding value for masks when border_mode is cv2.BORDER_CONSTANT.</p> <code>normalized</code> <code>bool</code> <p>If True, ensures that distortion does not exceed image boundaries. Default: False. Reference: https://github.com/albumentations-team/albumentations/pull/722</p> <p>Targets</p> <p>image, mask, bboxes</p> <p>Image types:     uint8, float32</p> <p>Note</p> <p>This transform is helpful in medical imagery, Optical Character Recognition, and other tasks where local distance may not be preserved.</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>class GridDistortion(DualTransform):\n    \"\"\"Applies grid distortion augmentation to images, masks, and bounding boxes. This technique involves dividing\n    the image into a grid of cells and randomly displacing the intersection points of the grid,\n    resulting in localized distortions.\n\n    Args:\n        num_steps (int): Number of grid cells on each side (minimum 1).\n        distort_limit (float, (float, float)): Range of distortion limits. If a single float is provided,\n            the range will be from (-distort_limit, distort_limit). Default: (-0.3, 0.3).\n        interpolation (OpenCV flag): Interpolation algorithm used for image transformation. Options are:\n            cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.\n            Default: cv2.INTER_LINEAR.\n        border_mode (OpenCV flag): Pixel extrapolation method used when pixels outside the image are required.\n            Options are: cv2.BORDER_CONSTANT, cv2.BORDER_REPLICATE, cv2.BORDER_REFLECT, cv2.BORDER_WRAP,\n            cv2.BORDER_REFLECT_101.\n            Default: cv2.BORDER_REFLECT_101.\n        value (int, float, list of ints, list of floats, optional): Value used for padding when\n            border_mode is cv2.BORDER_CONSTANT.\n        mask_value (int, float, list of ints, list of floats, optional): Padding value for masks when\n            border_mode is cv2.BORDER_CONSTANT.\n        normalized (bool): If True, ensures that distortion does not exceed image boundaries. Default: False.\n            Reference: https://github.com/albumentations-team/albumentations/pull/722\n\n    Targets:\n        image, mask, bboxes\n\n    Image types:\n        uint8, float32\n\n    Note:\n        This transform is helpful in medical imagery, Optical Character Recognition, and other tasks where local\n        distance may not be preserved.\n    \"\"\"\n\n    _targets = (Targets.IMAGE, Targets.MASK, Targets.BBOXES)\n\n    class InitSchema(BaseTransformInitSchema):\n        num_steps: Annotated[int, Field(ge=1, description=\"Count of grid cells on each side.\")]\n        distort_limit: SymmetricRangeType = (-0.3, 0.3)\n        interpolation: InterpolationType = cv2.INTER_LINEAR\n        border_mode: BorderModeType = cv2.BORDER_REFLECT_101\n        value: ColorType | None = Field(\n            default=None,\n            description=\"Padding value if border_mode is cv2.BORDER_CONSTANT.\",\n        )\n        mask_value: ColorType | None = Field(\n            default=None,\n            description=\"Padding value for mask if border_mode is cv2.BORDER_CONSTANT.\",\n        )\n        normalized: bool = Field(\n            default=False,\n            description=\"If true, distortion will be normalized to not go outside the image.\",\n        )\n\n        @field_validator(\"distort_limit\")\n        @classmethod\n        def check_limits(cls, v: tuple[float, float], info: ValidationInfo) -&gt; tuple[float, float]:\n            bounds = -1, 1\n            result = to_tuple(v)\n            check_range(result, *bounds, info.field_name)\n            return result\n\n    def __init__(\n        self,\n        num_steps: int = 5,\n        distort_limit: ScaleFloatType = (-0.3, 0.3),\n        interpolation: int = cv2.INTER_LINEAR,\n        border_mode: int = cv2.BORDER_REFLECT_101,\n        value: ColorType | None = None,\n        mask_value: ColorType | None = None,\n        normalized: bool = False,\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p, always_apply)\n\n        self.num_steps = num_steps\n        self.distort_limit = cast(Tuple[float, float], distort_limit)\n        self.interpolation = interpolation\n        self.border_mode = border_mode\n        self.value = value\n        self.mask_value = mask_value\n        self.normalized = normalized\n\n    def apply(\n        self,\n        img: np.ndarray,\n        stepsx: tuple[()],\n        stepsy: tuple[()],\n        interpolation: int,\n        **params: Any,\n    ) -&gt; np.ndarray:\n        return fgeometric.grid_distortion(\n            img,\n            self.num_steps,\n            stepsx,\n            stepsy,\n            interpolation,\n            self.border_mode,\n            self.value,\n        )\n\n    def apply_to_mask(\n        self,\n        mask: np.ndarray,\n        stepsx: tuple[()],\n        stepsy: tuple[()],\n        **params: Any,\n    ) -&gt; np.ndarray:\n        return fgeometric.grid_distortion(\n            mask,\n            self.num_steps,\n            stepsx,\n            stepsy,\n            cv2.INTER_NEAREST,\n            self.border_mode,\n            self.mask_value,\n        )\n\n    def apply_to_bbox(\n        self,\n        bbox: BoxInternalType,\n        stepsx: tuple[()],\n        stepsy: tuple[()],\n        **params: Any,\n    ) -&gt; BoxInternalType:\n        image_shape = params[\"shape\"][:2]\n        mask = np.zeros(image_shape, dtype=np.uint8)\n        bbox_denorm = fgeometric.denormalize_bbox(bbox, image_shape)\n        x_min, y_min, x_max, y_max = bbox_denorm[:4]\n        x_min, y_min, x_max, y_max = int(x_min), int(y_min), int(x_max), int(y_max)\n        mask[y_min:y_max, x_min:x_max] = 1\n        mask = fgeometric.grid_distortion(\n            mask,\n            self.num_steps,\n            stepsx,\n            stepsy,\n            cv2.INTER_NEAREST,\n            self.border_mode,\n            self.mask_value,\n        )\n        bbox_returned = bbox_from_mask(mask)\n        return cast(BoxInternalType, fgeometric.normalize_bbox(bbox_returned, image_shape))\n\n    def _normalize(self, h: int, w: int, xsteps: list[float], ysteps: list[float]) -&gt; dict[str, Any]:\n        # compensate for smaller last steps in source image.\n        x_step = w // self.num_steps\n        last_x_step = min(w, ((self.num_steps + 1) * x_step)) - (self.num_steps * x_step)\n        xsteps[-1] *= last_x_step / x_step\n\n        y_step = h // self.num_steps\n        last_y_step = min(h, ((self.num_steps + 1) * y_step)) - (self.num_steps * y_step)\n        ysteps[-1] *= last_y_step / y_step\n\n        # now normalize such that distortion never leaves image bounds.\n        tx = w / math.floor(w / self.num_steps)\n        ty = h / math.floor(h / self.num_steps)\n        xsteps = np.array(xsteps) * (tx / np.sum(xsteps))\n        ysteps = np.array(ysteps) * (ty / np.sum(ysteps))\n\n        return {\"stepsx\": xsteps, \"stepsy\": ysteps}\n\n    def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n        height, width = params[\"shape\"][:2]\n\n        stepsx = [1 + random.uniform(self.distort_limit[0], self.distort_limit[1]) for _ in range(self.num_steps + 1)]\n        stepsy = [1 + random.uniform(self.distort_limit[0], self.distort_limit[1]) for _ in range(self.num_steps + 1)]\n\n        if self.normalized:\n            return self._normalize(height, width, stepsx, stepsy)\n\n        return {\"stepsx\": stepsx, \"stepsy\": stepsy}\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return \"num_steps\", \"distort_limit\", \"interpolation\", \"border_mode\", \"value\", \"mask_value\", \"normalized\"\n\n    @property\n    def targets(self) -&gt; dict[str, Callable[..., Any]]:\n        return {\n            \"image\": self.apply,\n            \"mask\": self.apply_to_mask,\n            \"masks\": self.apply_to_masks,\n            \"bboxes\": self.apply_to_bboxes,\n        }\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.transforms.GridDistortion.apply","title":"<code>apply (self, img, stepsx, stepsy, interpolation, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def apply(\n    self,\n    img: np.ndarray,\n    stepsx: tuple[()],\n    stepsy: tuple[()],\n    interpolation: int,\n    **params: Any,\n) -&gt; np.ndarray:\n    return fgeometric.grid_distortion(\n        img,\n        self.num_steps,\n        stepsx,\n        stepsy,\n        interpolation,\n        self.border_mode,\n        self.value,\n    )\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.transforms.GridDistortion.get_params_dependent_on_data","title":"<code>get_params_dependent_on_data (self, params, data)</code>","text":"<p>Returns parameters dependent on input.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n    height, width = params[\"shape\"][:2]\n\n    stepsx = [1 + random.uniform(self.distort_limit[0], self.distort_limit[1]) for _ in range(self.num_steps + 1)]\n    stepsy = [1 + random.uniform(self.distort_limit[0], self.distort_limit[1]) for _ in range(self.num_steps + 1)]\n\n    if self.normalized:\n        return self._normalize(height, width, stepsx, stepsy)\n\n    return {\"stepsx\": stepsx, \"stepsy\": stepsy}\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.transforms.GridDistortion.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return \"num_steps\", \"distort_limit\", \"interpolation\", \"border_mode\", \"value\", \"mask_value\", \"normalized\"\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.transforms.GridElasticDeform","title":"<code>class  GridElasticDeform</code> <code>     (num_grid_xy, magnitude, interpolation=1, mask_interpolation=0, p=1.0, always_apply=None)                 </code>  [view source on GitHub]","text":"<p>Grid-based Elastic deformation Albumentation implementation</p> <p>This class applies elastic transformations using a grid-based approach. The granularity and intensity of the distortions can be controlled using the dimensions of the overlaying distortion grid and the magnitude parameter. Larger grid sizes result in finer, less severe distortions.</p> <p>Parameters:</p> Name Type Description <code>num_grid_xy</code> <code>tuple[int, int]</code> <p>Number of grid cells along the width and height. Specified as (grid_width, grid_height). Each value must be greater than 1.</p> <code>magnitude</code> <code>int</code> <p>Maximum pixel-wise displacement for distortion. Must be greater than 0.</p> <code>interpolation</code> <code>int</code> <p>Interpolation method to be used for the image transformation. Default: cv2.INTER_LINEAR</p> <code>mask_interpolation</code> <code>int</code> <p>Interpolation method to be used for mask transformation. Default: cv2.INTER_NEAREST</p> <code>p</code> <code>float</code> <p>Probability of applying the transform. Default: 1.0.</p> <p>Targets</p> <p>image, mask</p> <p>Image types:     uint8, float32</p> <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; transform = GridElasticDeform(num_grid_xy=(4, 4), magnitude=10, p=1.0)\n&gt;&gt;&gt; result = transform(image=image, mask=mask)\n&gt;&gt;&gt; transformed_image, transformed_mask = result['image'], result['mask']\n</code></pre> <p>Note</p> <p>This transformation is particularly useful for data augmentation in medical imaging and other domains where elastic deformations can simulate realistic variations.</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>class GridElasticDeform(DualTransform):\n    \"\"\"Grid-based Elastic deformation Albumentation implementation\n\n    This class applies elastic transformations using a grid-based approach.\n    The granularity and intensity of the distortions can be controlled using\n    the dimensions of the overlaying distortion grid and the magnitude parameter.\n    Larger grid sizes result in finer, less severe distortions.\n\n    Args:\n        num_grid_xy (tuple[int, int]): Number of grid cells along the width and height.\n            Specified as (grid_width, grid_height). Each value must be greater than 1.\n        magnitude (int): Maximum pixel-wise displacement for distortion. Must be greater than 0.\n        interpolation (int): Interpolation method to be used for the image transformation.\n            Default: cv2.INTER_LINEAR\n        mask_interpolation (int): Interpolation method to be used for mask transformation.\n            Default: cv2.INTER_NEAREST\n        p (float): Probability of applying the transform. Default: 1.0.\n\n    Targets:\n        image, mask\n\n    Image types:\n        uint8, float32\n\n    Example:\n        &gt;&gt;&gt; transform = GridElasticDeform(num_grid_xy=(4, 4), magnitude=10, p=1.0)\n        &gt;&gt;&gt; result = transform(image=image, mask=mask)\n        &gt;&gt;&gt; transformed_image, transformed_mask = result['image'], result['mask']\n\n    Note:\n        This transformation is particularly useful for data augmentation in medical imaging\n        and other domains where elastic deformations can simulate realistic variations.\n    \"\"\"\n\n    _targets = (Targets.IMAGE, Targets.MASK)\n\n    class InitSchema(BaseTransformInitSchema):\n        num_grid_xy: Annotated[tuple[int, int], AfterValidator(check_1plus)]\n        p: ProbabilityType = 1.0\n        magnitude: int = Field(gt=0)\n        interpolation: InterpolationType = cv2.INTER_LINEAR\n        mask_interpolation: InterpolationType = cv2.INTER_NEAREST\n\n    def __init__(\n        self,\n        num_grid_xy: tuple[int, int],\n        magnitude: int,\n        interpolation: int = cv2.INTER_LINEAR,\n        mask_interpolation: int = cv2.INTER_NEAREST,\n        p: float = 1.0,\n        always_apply: bool | None = None,\n    ):\n        super().__init__(p=p, always_apply=always_apply)\n        self.num_grid_xy = num_grid_xy\n        self.magnitude = magnitude\n        self.interpolation = interpolation\n        self.mask_interpolation = mask_interpolation\n\n    @staticmethod\n    def generate_mesh(polygons: np.ndarray, dimensions: np.ndarray) -&gt; np.ndarray:\n        return np.hstack((dimensions.reshape(-1, 4), polygons))\n\n    def get_params_dependent_on_data(\n        self,\n        params: dict[str, Any],\n        data: dict[str, Any],\n    ) -&gt; dict[str, Any]:\n        image_shape = np.array(params[\"shape\"][:2])\n\n        dimensions = fgeometric.calculate_grid_dimensions(image_shape, self.num_grid_xy)\n\n        polygons = fgeometric.generate_distorted_grid_polygons(dimensions, self.magnitude)\n\n        generated_mesh = self.generate_mesh(polygons, dimensions)\n\n        return {\"generated_mesh\": generated_mesh}\n\n    def apply(self, img: np.ndarray, generated_mesh: np.ndarray, **params: Any) -&gt; np.ndarray:\n        return fgeometric.distort_image(img, generated_mesh, self.interpolation)\n\n    def apply_to_mask(self, mask: np.ndarray, generated_mesh: np.ndarray, **params: Any) -&gt; np.ndarray:\n        return fgeometric.distort_image(mask, generated_mesh, self.mask_interpolation)\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return \"num_grid_xy\", \"magnitude\", \"interpolation\", \"mask_interpolation\"\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.transforms.GridElasticDeform.apply","title":"<code>apply (self, img, generated_mesh, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def apply(self, img: np.ndarray, generated_mesh: np.ndarray, **params: Any) -&gt; np.ndarray:\n    return fgeometric.distort_image(img, generated_mesh, self.interpolation)\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.transforms.GridElasticDeform.get_params_dependent_on_data","title":"<code>get_params_dependent_on_data (self, params, data)</code>","text":"<p>Returns parameters dependent on input.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def get_params_dependent_on_data(\n    self,\n    params: dict[str, Any],\n    data: dict[str, Any],\n) -&gt; dict[str, Any]:\n    image_shape = np.array(params[\"shape\"][:2])\n\n    dimensions = fgeometric.calculate_grid_dimensions(image_shape, self.num_grid_xy)\n\n    polygons = fgeometric.generate_distorted_grid_polygons(dimensions, self.magnitude)\n\n    generated_mesh = self.generate_mesh(polygons, dimensions)\n\n    return {\"generated_mesh\": generated_mesh}\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.transforms.GridElasticDeform.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return \"num_grid_xy\", \"magnitude\", \"interpolation\", \"mask_interpolation\"\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.transforms.HorizontalFlip","title":"<code>class  HorizontalFlip</code> <code> </code>  [view source on GitHub]","text":"<p>Flip the input horizontally around the y-axis.</p> <p>Parameters:</p> Name Type Description <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 0.5.</p> <p>Targets</p> <p>image, mask, bboxes, keypoints</p> <p>Image types:     uint8, float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>class HorizontalFlip(DualTransform):\n    \"\"\"Flip the input horizontally around the y-axis.\n\n    Args:\n        p (float): probability of applying the transform. Default: 0.5.\n\n    Targets:\n        image, mask, bboxes, keypoints\n\n    Image types:\n        uint8, float32\n\n    \"\"\"\n\n    _targets = (Targets.IMAGE, Targets.MASK, Targets.BBOXES, Targets.KEYPOINTS)\n\n    def apply(self, img: np.ndarray, **params: Any) -&gt; np.ndarray:\n        if get_num_channels(img) &gt; 1 and img.dtype == np.uint8:\n            # Opencv is faster than numpy only in case of\n            # non-gray scale 8bits images\n            return fgeometric.hflip_cv2(img)\n\n        return fgeometric.hflip(img)\n\n    def apply_to_bbox(self, bbox: BoxInternalType, **params: Any) -&gt; BoxInternalType:\n        return fgeometric.bbox_hflip(bbox)\n\n    def apply_to_keypoint(self, keypoint: KeypointInternalType, **params: Any) -&gt; KeypointInternalType:\n        return fgeometric.keypoint_hflip(keypoint, params[\"cols\"])\n\n    def get_transform_init_args_names(self) -&gt; tuple[()]:\n        return ()\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.transforms.HorizontalFlip.apply","title":"<code>apply (self, img, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def apply(self, img: np.ndarray, **params: Any) -&gt; np.ndarray:\n    if get_num_channels(img) &gt; 1 and img.dtype == np.uint8:\n        # Opencv is faster than numpy only in case of\n        # non-gray scale 8bits images\n        return fgeometric.hflip_cv2(img)\n\n    return fgeometric.hflip(img)\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.transforms.HorizontalFlip.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[()]:\n    return ()\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.transforms.OpticalDistortion","title":"<code>class  OpticalDistortion</code> <code>     (distort_limit=(-0.05, 0.05), shift_limit=(-0.05, 0.05), interpolation=1, border_mode=4, value=None, mask_value=None, always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Parameters:</p> Name Type Description <code>distort_limit</code> <code>float, (float, float</code> <p>If distort_limit is a single float, the range will be (-distort_limit, distort_limit). Default: (-0.05, 0.05).</p> <code>shift_limit</code> <code>float, (float, float</code> <p>If shift_limit is a single float, the range will be (-shift_limit, shift_limit). Default: (-0.05, 0.05).</p> <code>interpolation</code> <code>OpenCV flag</code> <p>flag that is used to specify the interpolation algorithm. Should be one of: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4. Default: cv2.INTER_LINEAR.</p> <code>border_mode</code> <code>OpenCV flag</code> <p>flag that is used to specify the pixel extrapolation method. Should be one of: cv2.BORDER_CONSTANT, cv2.BORDER_REPLICATE, cv2.BORDER_REFLECT, cv2.BORDER_WRAP, cv2.BORDER_REFLECT_101. Default: cv2.BORDER_REFLECT_101</p> <code>value</code> <code>int, float, list of ints, list of float</code> <p>padding value if border_mode is cv2.BORDER_CONSTANT.</p> <code>mask_value</code> <code>int, float,         list of ints,         list of float</code> <p>padding value if border_mode is cv2.BORDER_CONSTANT applied for masks.</p> <p>Targets</p> <p>image, mask, bboxes</p> <p>Image types:     uint8, float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>class OpticalDistortion(DualTransform):\n    \"\"\"Args:\n        distort_limit (float, (float, float)): If distort_limit is a single float, the range\n            will be (-distort_limit, distort_limit). Default: (-0.05, 0.05).\n        shift_limit (float, (float, float))): If shift_limit is a single float, the range\n            will be (-shift_limit, shift_limit). Default: (-0.05, 0.05).\n        interpolation (OpenCV flag): flag that is used to specify the interpolation algorithm. Should be one of:\n            cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.\n            Default: cv2.INTER_LINEAR.\n        border_mode (OpenCV flag): flag that is used to specify the pixel extrapolation method. Should be one of:\n            cv2.BORDER_CONSTANT, cv2.BORDER_REPLICATE, cv2.BORDER_REFLECT, cv2.BORDER_WRAP, cv2.BORDER_REFLECT_101.\n            Default: cv2.BORDER_REFLECT_101\n        value (int, float, list of ints, list of float): padding value if border_mode is cv2.BORDER_CONSTANT.\n        mask_value (int, float,\n                    list of ints,\n                    list of float): padding value if border_mode is cv2.BORDER_CONSTANT applied for masks.\n\n    Targets:\n        image, mask, bboxes\n\n    Image types:\n        uint8, float32\n\n    \"\"\"\n\n    _targets = (Targets.IMAGE, Targets.MASK, Targets.BBOXES)\n\n    class InitSchema(BaseTransformInitSchema):\n        distort_limit: SymmetricRangeType = (-0.05, 0.05)\n        shift_limit: SymmetricRangeType = (-0.05, 0.05)\n        interpolation: InterpolationType = cv2.INTER_LINEAR\n        border_mode: BorderModeType = cv2.BORDER_REFLECT_101\n        value: ColorType | None = Field(\n            default=None,\n            description=\"Padding value if border_mode is cv2.BORDER_CONSTANT.\",\n        )\n        mask_value: ColorType | None = Field(\n            default=None,\n            description=\"Padding value for mask if border_mode is cv2.BORDER_CONSTANT.\",\n        )\n\n    def __init__(\n        self,\n        distort_limit: ScaleFloatType = (-0.05, 0.05),\n        shift_limit: ScaleFloatType = (-0.05, 0.05),\n        interpolation: int = cv2.INTER_LINEAR,\n        border_mode: int = cv2.BORDER_REFLECT_101,\n        value: ColorType | None = None,\n        mask_value: ColorType | None = None,\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p, always_apply)\n        self.shift_limit = cast(Tuple[float, float], shift_limit)\n        self.distort_limit = cast(Tuple[float, float], distort_limit)\n        self.interpolation = interpolation\n        self.border_mode = border_mode\n        self.value = value\n        self.mask_value = mask_value\n\n    def apply(\n        self,\n        img: np.ndarray,\n        k: int,\n        dx: int,\n        dy: int,\n        interpolation: int,\n        **params: Any,\n    ) -&gt; np.ndarray:\n        return fgeometric.optical_distortion(img, k, dx, dy, interpolation, self.border_mode, self.value)\n\n    def apply_to_mask(self, mask: np.ndarray, k: int, dx: int, dy: int, **params: Any) -&gt; np.ndarray:\n        return fgeometric.optical_distortion(mask, k, dx, dy, cv2.INTER_NEAREST, self.border_mode, self.mask_value)\n\n    def apply_to_bbox(\n        self,\n        bbox: BoxInternalType,\n        k: int,\n        dx: int,\n        dy: int,\n        **params: Any,\n    ) -&gt; BoxInternalType:\n        image_shape = params[\"shape\"]\n        mask = np.zeros(image_shape[:2], dtype=np.uint8)\n        bbox_denorm = fgeometric.denormalize_bbox(bbox, image_shape)\n        x_min, y_min, x_max, y_max = bbox_denorm[:4]\n        x_min, y_min, x_max, y_max = int(x_min), int(y_min), int(x_max), int(y_max)\n        mask[y_min:y_max, x_min:x_max] = 1\n        mask = fgeometric.optical_distortion(mask, k, dx, dy, cv2.INTER_NEAREST, self.border_mode, self.mask_value)\n        bbox_returned = bbox_from_mask(mask)\n        return cast(BoxInternalType, fgeometric.normalize_bbox(bbox_returned, image_shape))\n\n    def get_params(self) -&gt; dict[str, Any]:\n        return {\n            \"k\": random.uniform(*self.distort_limit),\n            \"dx\": round(random.uniform(*self.shift_limit)),\n            \"dy\": round(random.uniform(*self.shift_limit)),\n        }\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return (\n            \"distort_limit\",\n            \"shift_limit\",\n            \"interpolation\",\n            \"border_mode\",\n            \"value\",\n            \"mask_value\",\n        )\n\n    @property\n    def targets(self) -&gt; dict[str, Callable[..., Any]]:\n        return {\n            \"image\": self.apply,\n            \"mask\": self.apply_to_mask,\n            \"masks\": self.apply_to_masks,\n            \"bboxes\": self.apply_to_bboxes,\n        }\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.transforms.OpticalDistortion.apply","title":"<code>apply (self, img, k, dx, dy, interpolation, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def apply(\n    self,\n    img: np.ndarray,\n    k: int,\n    dx: int,\n    dy: int,\n    interpolation: int,\n    **params: Any,\n) -&gt; np.ndarray:\n    return fgeometric.optical_distortion(img, k, dx, dy, interpolation, self.border_mode, self.value)\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.transforms.OpticalDistortion.get_params","title":"<code>get_params (self)</code>","text":"<p>Returns parameters independent of input.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def get_params(self) -&gt; dict[str, Any]:\n    return {\n        \"k\": random.uniform(*self.distort_limit),\n        \"dx\": round(random.uniform(*self.shift_limit)),\n        \"dy\": round(random.uniform(*self.shift_limit)),\n    }\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.transforms.OpticalDistortion.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return (\n        \"distort_limit\",\n        \"shift_limit\",\n        \"interpolation\",\n        \"border_mode\",\n        \"value\",\n        \"mask_value\",\n    )\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.transforms.PadIfNeeded","title":"<code>class  PadIfNeeded</code> <code>     (min_height=1024, min_width=1024, pad_height_divisor=None, pad_width_divisor=None, position=&lt;PositionType.CENTER: 'center'&gt;, border_mode=4, value=None, mask_value=None, always_apply=None, p=1.0)                 </code>  [view source on GitHub]","text":"<p>Pads the sides of an image if the image dimensions are less than the specified minimum dimensions. If the <code>pad_height_divisor</code> or <code>pad_width_divisor</code> is specified, the function additionally ensures that the image dimensions are divisible by these values.</p> <p>Parameters:</p> Name Type Description <code>min_height</code> <code>int</code> <p>Minimum desired height of the image. Ensures image height is at least this value.</p> <code>min_width</code> <code>int</code> <p>Minimum desired width of the image. Ensures image width is at least this value.</p> <code>pad_height_divisor</code> <code>int</code> <p>If set, pads the image height to make it divisible by this value.</p> <code>pad_width_divisor</code> <code>int</code> <p>If set, pads the image width to make it divisible by this value.</p> <code>position</code> <code>Union[str, PositionType]</code> <p>Position where the image is to be placed after padding. Can be one of 'center', 'top_left', 'top_right', 'bottom_left', 'bottom_right', or 'random'. Default is 'center'.</p> <code>border_mode</code> <code>int</code> <p>Specifies the border mode to use if padding is required. The default is <code>cv2.BORDER_REFLECT_101</code>.</p> <code>value</code> <code>Union[int, float, list[int], list[float]]</code> <p>Value to fill the border pixels if the border mode is <code>cv2.BORDER_CONSTANT</code>. Default is None.</p> <code>mask_value</code> <code>Union[int, float, list[int], list[float]]</code> <p>Similar to <code>value</code> but used for padding masks. Default is None.</p> <code>p</code> <code>float</code> <p>Probability of applying the transform. Default is 1.0.</p> <p>Targets</p> <p>image, mask, bboxes, keypoints</p> <p>Image types:     uint8, float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>class PadIfNeeded(DualTransform):\n    \"\"\"Pads the sides of an image if the image dimensions are less than the specified minimum dimensions.\n    If the `pad_height_divisor` or `pad_width_divisor` is specified, the function additionally ensures\n    that the image dimensions are divisible by these values.\n\n    Args:\n        min_height (int): Minimum desired height of the image. Ensures image height is at least this value.\n        min_width (int): Minimum desired width of the image. Ensures image width is at least this value.\n        pad_height_divisor (int, optional): If set, pads the image height to make it divisible by this value.\n        pad_width_divisor (int, optional): If set, pads the image width to make it divisible by this value.\n        position (Union[str, PositionType]): Position where the image is to be placed after padding.\n            Can be one of 'center', 'top_left', 'top_right', 'bottom_left', 'bottom_right', or 'random'.\n            Default is 'center'.\n        border_mode (int): Specifies the border mode to use if padding is required.\n            The default is `cv2.BORDER_REFLECT_101`.\n        value (Union[int, float, list[int], list[float]], optional): Value to fill the border pixels if\n            the border mode is `cv2.BORDER_CONSTANT`. Default is None.\n        mask_value (Union[int, float, list[int], list[float]], optional): Similar to `value` but used for padding masks.\n            Default is None.\n        p (float): Probability of applying the transform. Default is 1.0.\n\n    Targets:\n        image, mask, bboxes, keypoints\n\n    Image types:\n        uint8, float32\n\n    \"\"\"\n\n    class PositionType(Enum):\n        \"\"\"Enumerates the types of positions for placing an object within a container.\n\n        This Enum class is utilized to define specific anchor positions that an object can\n        assume relative to a container. It's particularly useful in image processing, UI layout,\n        and graphic design to specify the alignment and positioning of elements.\n\n        Attributes:\n            CENTER (str): Specifies that the object should be placed at the center.\n            TOP_LEFT (str): Specifies that the object should be placed at the top-left corner.\n            TOP_RIGHT (str): Specifies that the object should be placed at the top-right corner.\n            BOTTOM_LEFT (str): Specifies that the object should be placed at the bottom-left corner.\n            BOTTOM_RIGHT (str): Specifies that the object should be placed at the bottom-right corner.\n            RANDOM (str): Indicates that the object's position should be determined randomly.\n\n        \"\"\"\n\n        CENTER = \"center\"\n        TOP_LEFT = \"top_left\"\n        TOP_RIGHT = \"top_right\"\n        BOTTOM_LEFT = \"bottom_left\"\n        BOTTOM_RIGHT = \"bottom_right\"\n        RANDOM = \"random\"\n\n    _targets = (Targets.IMAGE, Targets.MASK, Targets.BBOXES, Targets.KEYPOINTS)\n\n    class InitSchema(BaseTransformInitSchema):\n        min_height: int | None = Field(default=None, ge=1, description=\"Minimal result image height.\")\n        min_width: int | None = Field(default=None, ge=1, description=\"Minimal result image width.\")\n        pad_height_divisor: int | None = Field(\n            default=None,\n            ge=1,\n            description=\"Ensures image height is divisible by this value.\",\n        )\n        pad_width_divisor: int | None = Field(\n            default=None,\n            ge=1,\n            description=\"Ensures image width is divisible by this value.\",\n        )\n        position: str = Field(default=\"center\", description=\"Position of the padded image.\")\n        border_mode: BorderModeType = cv2.BORDER_REFLECT_101\n        value: ColorType | None = Field(default=None, description=\"Value for border if BORDER_CONSTANT is used.\")\n        mask_value: ColorType | None = Field(\n            default=None,\n            description=\"Value for mask border if BORDER_CONSTANT is used.\",\n        )\n        p: ProbabilityType = 1.0\n\n        @model_validator(mode=\"after\")\n        def validate_divisibility(self) -&gt; Self:\n            if (self.min_height is None) == (self.pad_height_divisor is None):\n                msg = \"Only one of 'min_height' and 'pad_height_divisor' parameters must be set\"\n                raise ValueError(msg)\n            if (self.min_width is None) == (self.pad_width_divisor is None):\n                msg = \"Only one of 'min_width' and 'pad_width_divisor' parameters must be set\"\n                raise ValueError(msg)\n\n            if self.border_mode == cv2.BORDER_CONSTANT and self.value is None:\n                msg = \"If 'border_mode' is set to 'BORDER_CONSTANT', 'value' must be provided.\"\n                raise ValueError(msg)\n\n            return self\n\n    def __init__(\n        self,\n        min_height: int | None = 1024,\n        min_width: int | None = 1024,\n        pad_height_divisor: int | None = None,\n        pad_width_divisor: int | None = None,\n        position: PositionType | str = PositionType.CENTER,\n        border_mode: int = cv2.BORDER_REFLECT_101,\n        value: ColorType | None = None,\n        mask_value: ColorType | None = None,\n        always_apply: bool | None = None,\n        p: float = 1.0,\n    ):\n        super().__init__(p, always_apply)\n        self.min_height = min_height\n        self.min_width = min_width\n        self.pad_width_divisor = pad_width_divisor\n        self.pad_height_divisor = pad_height_divisor\n        self.position = PadIfNeeded.PositionType(position)\n        self.border_mode = border_mode\n        self.value = value\n        self.mask_value = mask_value\n\n    def update_params(self, params: dict[str, Any], **kwargs: Any) -&gt; dict[str, Any]:\n        params = super().update_params(params, **kwargs)\n        rows, cols = params[\"shape\"][:2]\n\n        if self.min_height is not None:\n            if rows &lt; self.min_height:\n                h_pad_top = int((self.min_height - rows) / 2.0)\n                h_pad_bottom = self.min_height - rows - h_pad_top\n            else:\n                h_pad_top = 0\n                h_pad_bottom = 0\n        else:\n            pad_remained = rows % self.pad_height_divisor\n            pad_rows = self.pad_height_divisor - pad_remained if pad_remained &gt; 0 else 0\n\n            h_pad_top = pad_rows // 2\n            h_pad_bottom = pad_rows - h_pad_top\n\n        if self.min_width is not None:\n            if cols &lt; self.min_width:\n                w_pad_left = int((self.min_width - cols) / 2.0)\n                w_pad_right = self.min_width - cols - w_pad_left\n            else:\n                w_pad_left = 0\n                w_pad_right = 0\n        else:\n            pad_remainder = cols % self.pad_width_divisor\n            pad_cols = self.pad_width_divisor - pad_remainder if pad_remainder &gt; 0 else 0\n\n            w_pad_left = pad_cols // 2\n            w_pad_right = pad_cols - w_pad_left\n\n        h_pad_top, h_pad_bottom, w_pad_left, w_pad_right = self.__update_position_params(\n            h_top=h_pad_top,\n            h_bottom=h_pad_bottom,\n            w_left=w_pad_left,\n            w_right=w_pad_right,\n        )\n\n        params.update(\n            {\n                \"pad_top\": h_pad_top,\n                \"pad_bottom\": h_pad_bottom,\n                \"pad_left\": w_pad_left,\n                \"pad_right\": w_pad_right,\n            },\n        )\n        return params\n\n    def apply(\n        self,\n        img: np.ndarray,\n        pad_top: int,\n        pad_bottom: int,\n        pad_left: int,\n        pad_right: int,\n        **params: Any,\n    ) -&gt; np.ndarray:\n        return fgeometric.pad_with_params(\n            img,\n            pad_top,\n            pad_bottom,\n            pad_left,\n            pad_right,\n            border_mode=self.border_mode,\n            value=self.value,\n        )\n\n    def apply_to_mask(\n        self,\n        mask: np.ndarray,\n        pad_top: int,\n        pad_bottom: int,\n        pad_left: int,\n        pad_right: int,\n        **params: Any,\n    ) -&gt; np.ndarray:\n        return fgeometric.pad_with_params(\n            mask,\n            pad_top,\n            pad_bottom,\n            pad_left,\n            pad_right,\n            border_mode=self.border_mode,\n            value=self.mask_value,\n        )\n\n    def apply_to_bboxes(\n        self,\n        bboxes: Sequence[BoxType],\n        pad_top: int,\n        pad_bottom: int,\n        pad_left: int,\n        pad_right: int,\n        **params: Any,\n    ) -&gt; list[BoxType]:\n        image_shape = params[\"shape\"][:2]\n\n        bboxes_np = np.array(bboxes)\n        bboxes_np = denormalize_bboxes(bboxes_np, params[\"shape\"])\n\n        result = fgeometric.pad_bboxes(\n            bboxes_np,\n            pad_top,\n            pad_bottom,\n            pad_left,\n            pad_right,\n            self.border_mode,\n            image_shape=image_shape,\n        )\n\n        rows, cols = params[\"shape\"][:2]\n\n        return list(normalize_bboxes(result, (rows + pad_top + pad_bottom, cols + pad_left + pad_right)))\n\n    def apply_to_keypoints(\n        self,\n        keypoints: Sequence[KeypointType],\n        pad_top: int,\n        pad_bottom: int,\n        pad_left: int,\n        pad_right: int,\n        **params: Any,\n    ) -&gt; Sequence[KeypointType]:\n        # Convert keypoints to numpy array, including all attributes\n        keypoints_array = np.array([list(kp) for kp in keypoints])\n\n        padded_keypoints = fgeometric.pad_keypoints(\n            keypoints_array,\n            pad_top,\n            pad_bottom,\n            pad_left,\n            pad_right,\n            self.border_mode,\n            image_shape=params[\"shape\"][:2],\n        )\n\n        # Convert back to list of tuples\n        return [tuple(kp) for kp in padded_keypoints]\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return (\n            \"min_height\",\n            \"min_width\",\n            \"pad_height_divisor\",\n            \"pad_width_divisor\",\n            \"position\",\n            \"border_mode\",\n            \"value\",\n            \"mask_value\",\n        )\n\n    def __update_position_params(\n        self,\n        h_top: int,\n        h_bottom: int,\n        w_left: int,\n        w_right: int,\n    ) -&gt; tuple[int, int, int, int]:\n        if self.position == PadIfNeeded.PositionType.TOP_LEFT:\n            h_bottom += h_top\n            w_right += w_left\n            h_top = 0\n            w_left = 0\n\n        elif self.position == PadIfNeeded.PositionType.TOP_RIGHT:\n            h_bottom += h_top\n            w_left += w_right\n            h_top = 0\n            w_right = 0\n\n        elif self.position == PadIfNeeded.PositionType.BOTTOM_LEFT:\n            h_top += h_bottom\n            w_right += w_left\n            h_bottom = 0\n            w_left = 0\n\n        elif self.position == PadIfNeeded.PositionType.BOTTOM_RIGHT:\n            h_top += h_bottom\n            w_left += w_right\n            h_bottom = 0\n            w_right = 0\n\n        elif self.position == PadIfNeeded.PositionType.RANDOM:\n            h_pad = h_top + h_bottom\n            w_pad = w_left + w_right\n            h_top = random.randint(0, h_pad)\n            h_bottom = h_pad - h_top\n            w_left = random.randint(0, w_pad)\n            w_right = w_pad - w_left\n\n        return h_top, h_bottom, w_left, w_right\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.transforms.PadIfNeeded.PositionType","title":"<code>class  PositionType</code> <code> </code>","text":"<p>Enumerates the types of positions for placing an object within a container.</p> <p>This Enum class is utilized to define specific anchor positions that an object can assume relative to a container. It's particularly useful in image processing, UI layout, and graphic design to specify the alignment and positioning of elements.</p> <p>Attributes:</p> Name Type Description <code>CENTER</code> <code>str</code> <p>Specifies that the object should be placed at the center.</p> <code>TOP_LEFT</code> <code>str</code> <p>Specifies that the object should be placed at the top-left corner.</p> <code>TOP_RIGHT</code> <code>str</code> <p>Specifies that the object should be placed at the top-right corner.</p> <code>BOTTOM_LEFT</code> <code>str</code> <p>Specifies that the object should be placed at the bottom-left corner.</p> <code>BOTTOM_RIGHT</code> <code>str</code> <p>Specifies that the object should be placed at the bottom-right corner.</p> <code>RANDOM</code> <code>str</code> <p>Indicates that the object's position should be determined randomly.</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>class PositionType(Enum):\n    \"\"\"Enumerates the types of positions for placing an object within a container.\n\n    This Enum class is utilized to define specific anchor positions that an object can\n    assume relative to a container. It's particularly useful in image processing, UI layout,\n    and graphic design to specify the alignment and positioning of elements.\n\n    Attributes:\n        CENTER (str): Specifies that the object should be placed at the center.\n        TOP_LEFT (str): Specifies that the object should be placed at the top-left corner.\n        TOP_RIGHT (str): Specifies that the object should be placed at the top-right corner.\n        BOTTOM_LEFT (str): Specifies that the object should be placed at the bottom-left corner.\n        BOTTOM_RIGHT (str): Specifies that the object should be placed at the bottom-right corner.\n        RANDOM (str): Indicates that the object's position should be determined randomly.\n\n    \"\"\"\n\n    CENTER = \"center\"\n    TOP_LEFT = \"top_left\"\n    TOP_RIGHT = \"top_right\"\n    BOTTOM_LEFT = \"bottom_left\"\n    BOTTOM_RIGHT = \"bottom_right\"\n    RANDOM = \"random\"\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.transforms.PadIfNeeded.apply","title":"<code>apply (self, img, pad_top, pad_bottom, pad_left, pad_right, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def apply(\n    self,\n    img: np.ndarray,\n    pad_top: int,\n    pad_bottom: int,\n    pad_left: int,\n    pad_right: int,\n    **params: Any,\n) -&gt; np.ndarray:\n    return fgeometric.pad_with_params(\n        img,\n        pad_top,\n        pad_bottom,\n        pad_left,\n        pad_right,\n        border_mode=self.border_mode,\n        value=self.value,\n    )\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.transforms.PadIfNeeded.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return (\n        \"min_height\",\n        \"min_width\",\n        \"pad_height_divisor\",\n        \"pad_width_divisor\",\n        \"position\",\n        \"border_mode\",\n        \"value\",\n        \"mask_value\",\n    )\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.transforms.PadIfNeeded.update_params","title":"<code>update_params (self, params, **kwargs)</code>","text":"<p>Update parameters with transform specific params. This method is deprecated, use: - <code>get_params</code> for transform specific params like interpolation and - <code>update_params_shape</code> for data like shape.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def update_params(self, params: dict[str, Any], **kwargs: Any) -&gt; dict[str, Any]:\n    params = super().update_params(params, **kwargs)\n    rows, cols = params[\"shape\"][:2]\n\n    if self.min_height is not None:\n        if rows &lt; self.min_height:\n            h_pad_top = int((self.min_height - rows) / 2.0)\n            h_pad_bottom = self.min_height - rows - h_pad_top\n        else:\n            h_pad_top = 0\n            h_pad_bottom = 0\n    else:\n        pad_remained = rows % self.pad_height_divisor\n        pad_rows = self.pad_height_divisor - pad_remained if pad_remained &gt; 0 else 0\n\n        h_pad_top = pad_rows // 2\n        h_pad_bottom = pad_rows - h_pad_top\n\n    if self.min_width is not None:\n        if cols &lt; self.min_width:\n            w_pad_left = int((self.min_width - cols) / 2.0)\n            w_pad_right = self.min_width - cols - w_pad_left\n        else:\n            w_pad_left = 0\n            w_pad_right = 0\n    else:\n        pad_remainder = cols % self.pad_width_divisor\n        pad_cols = self.pad_width_divisor - pad_remainder if pad_remainder &gt; 0 else 0\n\n        w_pad_left = pad_cols // 2\n        w_pad_right = pad_cols - w_pad_left\n\n    h_pad_top, h_pad_bottom, w_pad_left, w_pad_right = self.__update_position_params(\n        h_top=h_pad_top,\n        h_bottom=h_pad_bottom,\n        w_left=w_pad_left,\n        w_right=w_pad_right,\n    )\n\n    params.update(\n        {\n            \"pad_top\": h_pad_top,\n            \"pad_bottom\": h_pad_bottom,\n            \"pad_left\": w_pad_left,\n            \"pad_right\": w_pad_right,\n        },\n    )\n    return params\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.transforms.Perspective","title":"<code>class  Perspective</code> <code>     (scale=(0.05, 0.1), keep_size=True, pad_mode=0, pad_val=0, mask_pad_val=0, fit_output=False, interpolation=1, always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Perform a random four point perspective transform of the input.</p> <p>Parameters:</p> Name Type Description <code>scale</code> <code>ScaleFloatType</code> <p>standard deviation of the normal distributions. These are used to sample the random distances of the subimage's corners from the full image's corners. If scale is a single float value, the range will be (0, scale). Default: (0.05, 0.1).</p> <code>keep_size</code> <code>bool</code> <p>Whether to resize image back to their original size after applying the perspective transform. If set to False, the resulting images may end up having different shapes and will always be a list, never an array. Default: True</p> <code>pad_mode</code> <code>OpenCV flag</code> <p>OpenCV border mode.</p> <code>pad_val</code> <code>int, float, list of int, list of float</code> <p>padding value if border_mode is cv2.BORDER_CONSTANT. Default: 0</p> <code>mask_pad_val</code> <code>int, float, list of int, list of float</code> <p>padding value for mask if border_mode is cv2.BORDER_CONSTANT. Default: 0</p> <code>fit_output</code> <code>bool</code> <p>If True, the image plane size and position will be adjusted to still capture the whole image after perspective transformation. (Followed by image resizing if keep_size is set to True.) Otherwise, parts of the transformed image may be outside of the image plane. This setting should not be set to True when using large scale values as it could lead to very large images. Default: False</p> <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 0.5.</p> <p>Targets</p> <p>image, mask, keypoints, bboxes</p> <p>Image types:     uint8, float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>class Perspective(DualTransform):\n    \"\"\"Perform a random four point perspective transform of the input.\n\n    Args:\n        scale: standard deviation of the normal distributions. These are used to sample\n            the random distances of the subimage's corners from the full image's corners.\n            If scale is a single float value, the range will be (0, scale). Default: (0.05, 0.1).\n        keep_size: Whether to resize image back to their original size after applying the perspective\n            transform. If set to False, the resulting images may end up having different shapes\n            and will always be a list, never an array. Default: True\n        pad_mode (OpenCV flag): OpenCV border mode.\n        pad_val (int, float, list of int, list of float): padding value if border_mode is cv2.BORDER_CONSTANT.\n            Default: 0\n        mask_pad_val (int, float, list of int, list of float): padding value for mask\n            if border_mode is cv2.BORDER_CONSTANT. Default: 0\n        fit_output (bool): If True, the image plane size and position will be adjusted to still capture\n            the whole image after perspective transformation. (Followed by image resizing if keep_size is set to True.)\n            Otherwise, parts of the transformed image may be outside of the image plane.\n            This setting should not be set to True when using large scale values as it could lead to very large images.\n            Default: False\n        p (float): probability of applying the transform. Default: 0.5.\n\n    Targets:\n        image, mask, keypoints, bboxes\n\n    Image types:\n        uint8, float32\n\n    \"\"\"\n\n    _targets = (Targets.IMAGE, Targets.MASK, Targets.KEYPOINTS, Targets.BBOXES)\n\n    class InitSchema(BaseTransformInitSchema):\n        scale: NonNegativeFloatRangeType = (0.05, 0.1)\n        keep_size: Annotated[bool, Field(default=True, description=\"Keep size after transform.\")]\n        pad_mode: BorderModeType = cv2.BORDER_CONSTANT\n        pad_val: ColorType | None = Field(\n            default=0,\n            description=\"Padding value if border_mode is cv2.BORDER_CONSTANT.\",\n        )\n        mask_pad_val: ColorType | None = Field(\n            default=0,\n            description=\"Mask padding value if border_mode is cv2.BORDER_CONSTANT.\",\n        )\n        fit_output: Annotated[bool, Field(default=False, description=\"Adjust image plane to capture whole image.\")]\n        interpolation: InterpolationType = cv2.INTER_LINEAR\n\n    def __init__(\n        self,\n        scale: ScaleFloatType = (0.05, 0.1),\n        keep_size: bool = True,\n        pad_mode: int = cv2.BORDER_CONSTANT,\n        pad_val: ColorType = 0,\n        mask_pad_val: ColorType = 0,\n        fit_output: bool = False,\n        interpolation: int = cv2.INTER_LINEAR,\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p, always_apply)\n        self.scale = cast(Tuple[float, float], scale)\n        self.keep_size = keep_size\n        self.pad_mode = pad_mode\n        self.pad_val = pad_val\n        self.mask_pad_val = mask_pad_val\n        self.fit_output = fit_output\n        self.interpolation = interpolation\n\n    def apply(\n        self,\n        img: np.ndarray,\n        matrix: np.ndarray,\n        max_height: int,\n        max_width: int,\n        **params: Any,\n    ) -&gt; np.ndarray:\n        return fgeometric.perspective(\n            img,\n            matrix,\n            max_width,\n            max_height,\n            self.pad_val,\n            self.pad_mode,\n            self.keep_size,\n            params[\"interpolation\"],\n        )\n\n    def apply_to_bbox(\n        self,\n        bbox: BoxInternalType,\n        matrix: np.ndarray,\n        max_height: int,\n        max_width: int,\n        **params: Any,\n    ) -&gt; BoxInternalType:\n        return fgeometric.perspective_bbox(\n            bbox,\n            params[\"shape\"],\n            matrix,\n            max_width,\n            max_height,\n            self.keep_size,\n        )\n\n    def apply_to_keypoint(\n        self,\n        keypoint: KeypointInternalType,\n        matrix: np.ndarray,\n        max_height: int,\n        max_width: int,\n        **params: Any,\n    ) -&gt; np.ndarray:\n        return fgeometric.perspective_keypoint(\n            keypoint,\n            params[\"shape\"],\n            matrix,\n            max_width,\n            max_height,\n            self.keep_size,\n        )\n\n    def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n        height, width = params[\"shape\"][:2]\n\n        scale = random.uniform(*self.scale)\n        points = random_utils.normal(0, scale, [4, 2])\n        points = np.mod(np.abs(points), 0.32)\n\n        # top left -- no changes needed, just use jitter\n        # top right\n        points[1, 0] = 1.0 - points[1, 0]  # w = 1.0 - jitter\n        # bottom right\n        points[2] = 1.0 - points[2]  # w = 1.0 - jitt\n        # bottom left\n        points[3, 1] = 1.0 - points[3, 1]  # h = 1.0 - jitter\n\n        points[:, 0] *= width\n        points[:, 1] *= height\n\n        # Obtain a consistent order of the points and unpack them individually.\n        # Warning: don't just do (tl, tr, br, bl) = _order_points(...)\n        # here, because the reordered points is used further below.\n        points = self._order_points(points)\n        tl, tr, br, bl = points\n\n        # compute the width of the new image, which will be the\n        # maximum distance between bottom-right and bottom-left\n        # x-coordiates or the top-right and top-left x-coordinates\n        min_width = None\n        max_width = None\n        while min_width is None or min_width &lt; TWO:\n            width_top = np.sqrt(((tr[0] - tl[0]) ** 2) + ((tr[1] - tl[1]) ** 2))\n            width_bottom = np.sqrt(((br[0] - bl[0]) ** 2) + ((br[1] - bl[1]) ** 2))\n            max_width = int(max(width_top, width_bottom))\n            min_width = int(min(width_top, width_bottom))\n            if min_width &lt; TWO:\n                step_size = (2 - min_width) / 2\n                tl[0] -= step_size\n                tr[0] += step_size\n                bl[0] -= step_size\n                br[0] += step_size\n\n        # compute the height of the new image, which will be the maximum distance between the top-right\n        # and bottom-right y-coordinates or the top-left and bottom-left y-coordinates\n        min_height = None\n        max_height = None\n        while min_height is None or min_height &lt; TWO:\n            height_right = np.sqrt(((tr[0] - br[0]) ** 2) + ((tr[1] - br[1]) ** 2))\n            height_left = np.sqrt(((tl[0] - bl[0]) ** 2) + ((tl[1] - bl[1]) ** 2))\n            max_height = int(max(height_right, height_left))\n            min_height = int(min(height_right, height_left))\n            if min_height &lt; TWO:\n                step_size = (2 - min_height) / 2\n                tl[1] -= step_size\n                tr[1] -= step_size\n                bl[1] += step_size\n                br[1] += step_size\n\n        # now that we have the dimensions of the new image, construct\n        # the set of destination points to obtain a \"birds eye view\",\n        # (i.e. top-down view) of the image, again specifying points\n        # in the top-left, top-right, bottom-right, and bottom-left order\n        # do not use width-1 or height-1 here, as for e.g. width=3, height=2\n        # the bottom right coordinate is at (3.0, 2.0) and not (2.0, 1.0)\n        dst = np.array([[0, 0], [max_width, 0], [max_width, max_height], [0, max_height]], dtype=np.float32)\n\n        # compute the perspective transform matrix and then apply it\n        m = cv2.getPerspectiveTransform(points, dst)\n\n        if self.fit_output:\n            m, max_width, max_height = self._expand_transform(m, (height, width))\n\n        return {\"matrix\": m, \"max_height\": max_height, \"max_width\": max_width, \"interpolation\": self.interpolation}\n\n    @classmethod\n    def _expand_transform(cls, matrix: np.ndarray, shape: SizeType) -&gt; tuple[np.ndarray, int, int]:\n        height, width = shape[:2]\n        # do not use width-1 or height-1 here, as for e.g. width=3, height=2, max_height\n        # the bottom right coordinate is at (3.0, 2.0) and not (2.0, 1.0)\n        rect = np.array([[0, 0], [width, 0], [width, height], [0, height]], dtype=np.float32)\n        dst = cv2.perspectiveTransform(np.array([rect]), matrix)[0]\n\n        # get min x, y over transformed 4 points\n        # then modify target points by subtracting these minima  =&gt; shift to (0, 0)\n        dst -= dst.min(axis=0, keepdims=True)\n        dst = np.around(dst, decimals=0)\n\n        matrix_expanded = cv2.getPerspectiveTransform(rect, dst)\n        max_width, max_height = dst.max(axis=0)\n        return matrix_expanded, int(max_width), int(max_height)\n\n    @staticmethod\n    def _order_points(pts: np.ndarray) -&gt; np.ndarray:\n        pts = np.array(sorted(pts, key=lambda x: x[0]))\n        left = pts[:2]  # points with smallest x coordinate - left points\n        right = pts[2:]  # points with greatest x coordinate - right points\n\n        if left[0][1] &lt; left[1][1]:\n            tl, bl = left\n        else:\n            bl, tl = left\n\n        if right[0][1] &lt; right[1][1]:\n            tr, br = right\n        else:\n            br, tr = right\n\n        return np.array([tl, tr, br, bl], dtype=np.float32)\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return \"scale\", \"keep_size\", \"pad_mode\", \"pad_val\", \"mask_pad_val\", \"fit_output\", \"interpolation\"\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.transforms.Perspective.apply","title":"<code>apply (self, img, matrix, max_height, max_width, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def apply(\n    self,\n    img: np.ndarray,\n    matrix: np.ndarray,\n    max_height: int,\n    max_width: int,\n    **params: Any,\n) -&gt; np.ndarray:\n    return fgeometric.perspective(\n        img,\n        matrix,\n        max_width,\n        max_height,\n        self.pad_val,\n        self.pad_mode,\n        self.keep_size,\n        params[\"interpolation\"],\n    )\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.transforms.Perspective.get_params_dependent_on_data","title":"<code>get_params_dependent_on_data (self, params, data)</code>","text":"<p>Returns parameters dependent on input.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n    height, width = params[\"shape\"][:2]\n\n    scale = random.uniform(*self.scale)\n    points = random_utils.normal(0, scale, [4, 2])\n    points = np.mod(np.abs(points), 0.32)\n\n    # top left -- no changes needed, just use jitter\n    # top right\n    points[1, 0] = 1.0 - points[1, 0]  # w = 1.0 - jitter\n    # bottom right\n    points[2] = 1.0 - points[2]  # w = 1.0 - jitt\n    # bottom left\n    points[3, 1] = 1.0 - points[3, 1]  # h = 1.0 - jitter\n\n    points[:, 0] *= width\n    points[:, 1] *= height\n\n    # Obtain a consistent order of the points and unpack them individually.\n    # Warning: don't just do (tl, tr, br, bl) = _order_points(...)\n    # here, because the reordered points is used further below.\n    points = self._order_points(points)\n    tl, tr, br, bl = points\n\n    # compute the width of the new image, which will be the\n    # maximum distance between bottom-right and bottom-left\n    # x-coordiates or the top-right and top-left x-coordinates\n    min_width = None\n    max_width = None\n    while min_width is None or min_width &lt; TWO:\n        width_top = np.sqrt(((tr[0] - tl[0]) ** 2) + ((tr[1] - tl[1]) ** 2))\n        width_bottom = np.sqrt(((br[0] - bl[0]) ** 2) + ((br[1] - bl[1]) ** 2))\n        max_width = int(max(width_top, width_bottom))\n        min_width = int(min(width_top, width_bottom))\n        if min_width &lt; TWO:\n            step_size = (2 - min_width) / 2\n            tl[0] -= step_size\n            tr[0] += step_size\n            bl[0] -= step_size\n            br[0] += step_size\n\n    # compute the height of the new image, which will be the maximum distance between the top-right\n    # and bottom-right y-coordinates or the top-left and bottom-left y-coordinates\n    min_height = None\n    max_height = None\n    while min_height is None or min_height &lt; TWO:\n        height_right = np.sqrt(((tr[0] - br[0]) ** 2) + ((tr[1] - br[1]) ** 2))\n        height_left = np.sqrt(((tl[0] - bl[0]) ** 2) + ((tl[1] - bl[1]) ** 2))\n        max_height = int(max(height_right, height_left))\n        min_height = int(min(height_right, height_left))\n        if min_height &lt; TWO:\n            step_size = (2 - min_height) / 2\n            tl[1] -= step_size\n            tr[1] -= step_size\n            bl[1] += step_size\n            br[1] += step_size\n\n    # now that we have the dimensions of the new image, construct\n    # the set of destination points to obtain a \"birds eye view\",\n    # (i.e. top-down view) of the image, again specifying points\n    # in the top-left, top-right, bottom-right, and bottom-left order\n    # do not use width-1 or height-1 here, as for e.g. width=3, height=2\n    # the bottom right coordinate is at (3.0, 2.0) and not (2.0, 1.0)\n    dst = np.array([[0, 0], [max_width, 0], [max_width, max_height], [0, max_height]], dtype=np.float32)\n\n    # compute the perspective transform matrix and then apply it\n    m = cv2.getPerspectiveTransform(points, dst)\n\n    if self.fit_output:\n        m, max_width, max_height = self._expand_transform(m, (height, width))\n\n    return {\"matrix\": m, \"max_height\": max_height, \"max_width\": max_width, \"interpolation\": self.interpolation}\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.transforms.Perspective.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return \"scale\", \"keep_size\", \"pad_mode\", \"pad_val\", \"mask_pad_val\", \"fit_output\", \"interpolation\"\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.transforms.PiecewiseAffine","title":"<code>class  PiecewiseAffine</code> <code>     (scale=(0.03, 0.05), nb_rows=4, nb_cols=4, interpolation=1, mask_interpolation=0, cval=0, cval_mask=0, mode='constant', absolute_scale=False, always_apply=None, keypoints_threshold=0.01, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Apply affine transformations that differ between local neighborhoods. This augmentation places a regular grid of points on an image and randomly moves the neighborhood of these point around via affine transformations. This leads to local distortions.</p> <p>This is mostly a wrapper around scikit-image's <code>PiecewiseAffine</code>. See also <code>Affine</code> for a similar technique.</p> <p>Note</p> <p>This augmenter is very slow. Try to use <code>ElasticTransformation</code> instead, which is at least 10x faster.</p> <p>Note</p> <p>For coordinate-based inputs (keypoints, bounding boxes, polygons, ...), this augmenter still has to perform an image-based augmentation, which will make it significantly slower and not fully correct for such inputs than other transforms.</p> <p>Parameters:</p> Name Type Description <code>scale</code> <code>float, tuple of float</code> <p>Each point on the regular grid is moved around via a normal distribution. This scale factor is equivalent to the normal distribution's sigma. Note that the jitter (how far each point is moved in which direction) is multiplied by the height/width of the image if <code>absolute_scale=False</code> (default), so this scale can be the same for different sized images. Recommended values are in the range <code>0.01</code> to <code>0.05</code> (weak to strong augmentations).     * If a single <code>float</code>, then that value will always be used as the scale.     * If a tuple <code>(a, b)</code> of <code>float</code> s, then a random value will       be uniformly sampled per image from the interval <code>[a, b]</code>.</p> <code>nb_rows</code> <code>int, tuple of int</code> <p>Number of rows of points that the regular grid should have. Must be at least <code>2</code>. For large images, you might want to pick a higher value than <code>4</code>. You might have to then adjust scale to lower values.     * If a single <code>int</code>, then that value will always be used as the number of rows.     * If a tuple <code>(a, b)</code>, then a value from the discrete interval       <code>[a..b]</code> will be uniformly sampled per image.</p> <code>nb_cols</code> <code>int, tuple of int</code> <p>Number of columns. Analogous to <code>nb_rows</code>.</p> <code>interpolation</code> <code>int</code> <p>The order of interpolation. The order has to be in the range 0-5:  - 0: Nearest-neighbor  - 1: Bi-linear (default)  - 2: Bi-quadratic  - 3: Bi-cubic  - 4: Bi-quartic  - 5: Bi-quintic</p> <code>mask_interpolation</code> <code>int</code> <p>same as interpolation but for mask.</p> <code>cval</code> <code>number</code> <p>The constant value to use when filling in newly created pixels.</p> <code>cval_mask</code> <code>number</code> <p>Same as cval but only for masks.</p> <code>mode</code> <code>str</code> <p>{'constant', 'edge', 'symmetric', 'reflect', 'wrap'}, optional Points outside the boundaries of the input are filled according to the given mode.  Modes match the behaviour of <code>numpy.pad</code>.</p> <code>absolute_scale</code> <code>bool</code> <p>Take <code>scale</code> as an absolute value rather than a relative value.</p> <code>keypoints_threshold</code> <code>float</code> <p>Used as threshold in conversion from distance maps to keypoints. The search for keypoints works by searching for the argmin (non-inverted) or argmax (inverted) in each channel. This parameters contains the maximum (non-inverted) or minimum (inverted) value to accept in order to view a hit as a keypoint. Use <code>None</code> to use no min/max. Default: 0.01</p> <p>Targets</p> <p>image, mask, keypoints, bboxes</p> <p>Image types:     uint8, float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>class PiecewiseAffine(DualTransform):\n    \"\"\"Apply affine transformations that differ between local neighborhoods.\n    This augmentation places a regular grid of points on an image and randomly moves the neighborhood of these point\n    around via affine transformations. This leads to local distortions.\n\n    This is mostly a wrapper around scikit-image's ``PiecewiseAffine``.\n    See also ``Affine`` for a similar technique.\n\n    Note:\n        This augmenter is very slow. Try to use ``ElasticTransformation`` instead, which is at least 10x faster.\n\n    Note:\n        For coordinate-based inputs (keypoints, bounding boxes, polygons, ...),\n        this augmenter still has to perform an image-based augmentation,\n        which will make it significantly slower and not fully correct for such inputs than other transforms.\n\n    Args:\n        scale (float, tuple of float): Each point on the regular grid is moved around via a normal distribution.\n            This scale factor is equivalent to the normal distribution's sigma.\n            Note that the jitter (how far each point is moved in which direction) is multiplied by the height/width of\n            the image if ``absolute_scale=False`` (default), so this scale can be the same for different sized images.\n            Recommended values are in the range ``0.01`` to ``0.05`` (weak to strong augmentations).\n                * If a single ``float``, then that value will always be used as the scale.\n                * If a tuple ``(a, b)`` of ``float`` s, then a random value will\n                  be uniformly sampled per image from the interval ``[a, b]``.\n        nb_rows (int, tuple of int): Number of rows of points that the regular grid should have.\n            Must be at least ``2``. For large images, you might want to pick a higher value than ``4``.\n            You might have to then adjust scale to lower values.\n                * If a single ``int``, then that value will always be used as the number of rows.\n                * If a tuple ``(a, b)``, then a value from the discrete interval\n                  ``[a..b]`` will be uniformly sampled per image.\n        nb_cols (int, tuple of int): Number of columns. Analogous to `nb_rows`.\n        interpolation (int): The order of interpolation. The order has to be in the range 0-5:\n             - 0: Nearest-neighbor\n             - 1: Bi-linear (default)\n             - 2: Bi-quadratic\n             - 3: Bi-cubic\n             - 4: Bi-quartic\n             - 5: Bi-quintic\n        mask_interpolation (int): same as interpolation but for mask.\n        cval (number): The constant value to use when filling in newly created pixels.\n        cval_mask (number): Same as cval but only for masks.\n        mode (str): {'constant', 'edge', 'symmetric', 'reflect', 'wrap'}, optional\n            Points outside the boundaries of the input are filled according\n            to the given mode.  Modes match the behaviour of `numpy.pad`.\n        absolute_scale (bool): Take `scale` as an absolute value rather than a relative value.\n        keypoints_threshold (float): Used as threshold in conversion from distance maps to keypoints.\n            The search for keypoints works by searching for the\n            argmin (non-inverted) or argmax (inverted) in each channel. This\n            parameters contains the maximum (non-inverted) or minimum (inverted) value to accept in order to view a hit\n            as a keypoint. Use ``None`` to use no min/max. Default: 0.01\n\n    Targets:\n        image, mask, keypoints, bboxes\n\n    Image types:\n        uint8, float32\n\n    \"\"\"\n\n    _targets = (Targets.IMAGE, Targets.MASK, Targets.BBOXES, Targets.KEYPOINTS)\n\n    class InitSchema(BaseTransformInitSchema):\n        scale: NonNegativeFloatRangeType = (0.03, 0.05)\n        nb_rows: ScaleIntType = Field(default=4, description=\"Number of rows in the regular grid.\")\n        nb_cols: ScaleIntType = Field(default=4, description=\"Number of columns in the regular grid.\")\n        interpolation: InterpolationType = cv2.INTER_LINEAR\n        mask_interpolation: InterpolationType = cv2.INTER_NEAREST\n        cval: int = Field(default=0, description=\"Constant value used for newly created pixels.\")\n        cval_mask: int = Field(default=0, description=\"Constant value used for newly created mask pixels.\")\n        mode: Literal[\"constant\", \"edge\", \"symmetric\", \"reflect\", \"wrap\"] = \"constant\"\n        absolute_scale: bool = Field(\n            default=False,\n            description=\"Whether scale is an absolute value rather than relative.\",\n        )\n        keypoints_threshold: float = Field(\n            default=0.01,\n            description=\"Threshold for conversion from distance maps to keypoints.\",\n        )\n\n        @field_validator(\"nb_rows\", \"nb_cols\")\n        @classmethod\n        def process_range(cls, value: ScaleFloatType, info: ValidationInfo) -&gt; tuple[float, float]:\n            bounds = 2, BIG_INTEGER\n            result = to_tuple(value, value)\n            check_range(result, *bounds, info.field_name)\n            return result\n\n    def __init__(\n        self,\n        scale: ScaleFloatType = (0.03, 0.05),\n        nb_rows: ScaleIntType = 4,\n        nb_cols: ScaleIntType = 4,\n        interpolation: int = cv2.INTER_LINEAR,\n        mask_interpolation: int = cv2.INTER_NEAREST,\n        cval: int = 0,\n        cval_mask: int = 0,\n        mode: Literal[\"constant\", \"edge\", \"symmetric\", \"reflect\", \"wrap\"] = \"constant\",\n        absolute_scale: bool = False,\n        always_apply: bool | None = None,\n        keypoints_threshold: float = 0.01,\n        p: float = 0.5,\n    ):\n        super().__init__(p, always_apply)\n\n        warn(\n            \"This augmenter is very slow. Try to use ``ElasticTransformation`` instead, which is at least 10x faster.\",\n            stacklevel=2,\n        )\n\n        self.scale = cast(Tuple[float, float], scale)\n        self.nb_rows = cast(Tuple[int, int], nb_rows)\n        self.nb_cols = cast(Tuple[int, int], nb_cols)\n        self.interpolation = interpolation\n        self.mask_interpolation = mask_interpolation\n        self.cval = cval\n        self.cval_mask = cval_mask\n        self.mode = mode\n        self.absolute_scale = absolute_scale\n        self.keypoints_threshold = keypoints_threshold\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return (\n            \"scale\",\n            \"nb_rows\",\n            \"nb_cols\",\n            \"interpolation\",\n            \"mask_interpolation\",\n            \"cval\",\n            \"cval_mask\",\n            \"mode\",\n            \"absolute_scale\",\n            \"keypoints_threshold\",\n        )\n\n    def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n        height, width = params[\"shape\"][:2]\n\n        nb_rows = np.clip(random.randint(*self.nb_rows), 2, None)\n        nb_cols = np.clip(random.randint(*self.nb_cols), 2, None)\n        nb_cells = nb_cols * nb_rows\n        scale = random.uniform(*self.scale)\n\n        jitter: np.ndarray = random_utils.normal(0, scale, (nb_cells, 2))\n        if not np.any(jitter &gt; 0):\n            for _ in range(10):  # See: https://github.com/albumentations-team/albumentations/issues/1442\n                jitter = random_utils.normal(0, scale, (nb_cells, 2))\n                if np.any(jitter &gt; 0):\n                    break\n            if not np.any(jitter &gt; 0):\n                return {\"matrix\": None}\n\n        y = np.linspace(0, height, nb_rows)\n        x = np.linspace(0, width, nb_cols)\n\n        # (H, W) and (H, W) for H=rows, W=cols\n        xx_src, yy_src = np.meshgrid(x, y)\n\n        # (1, HW, 2) =&gt; (HW, 2) for H=rows, W=cols\n        points_src = np.dstack([yy_src.flat, xx_src.flat])[0]\n\n        if self.absolute_scale:\n            jitter[:, 0] = jitter[:, 0] / height if height &gt; 0 else 0.0\n            jitter[:, 1] = jitter[:, 1] / width if width &gt; 0 else 0.0\n\n        jitter[:, 0] = jitter[:, 0] * height\n        jitter[:, 1] = jitter[:, 1] * width\n\n        points_dest = np.copy(points_src)\n        points_dest[:, 0] = points_dest[:, 0] + jitter[:, 0]\n        points_dest[:, 1] = points_dest[:, 1] + jitter[:, 1]\n\n        # Restrict all destination points to be inside the image plane.\n        # This is necessary, as otherwise keypoints could be augmented\n        # outside of the image plane and these would be replaced by\n        # (-1, -1), which would not conform with the behaviour of the other augmenters.\n        points_dest[:, 0] = np.clip(points_dest[:, 0], 0, height - 1)\n        points_dest[:, 1] = np.clip(points_dest[:, 1], 0, width - 1)\n\n        matrix = skimage.transform.PiecewiseAffineTransform()\n        matrix.estimate(points_src[:, ::-1], points_dest[:, ::-1])\n\n        return {\n            \"matrix\": matrix,\n        }\n\n    def apply(\n        self,\n        img: np.ndarray,\n        matrix: skimage.transform.PiecewiseAffineTransform,\n        **params: Any,\n    ) -&gt; np.ndarray:\n        return fgeometric.piecewise_affine(img, matrix, self.interpolation, self.mode, self.cval)\n\n    def apply_to_mask(\n        self,\n        mask: np.ndarray,\n        matrix: skimage.transform.PiecewiseAffineTransform,\n        **params: Any,\n    ) -&gt; np.ndarray:\n        return fgeometric.piecewise_affine(mask, matrix, self.mask_interpolation, self.mode, self.cval_mask)\n\n    def apply_to_bbox(\n        self,\n        bbox: BoxInternalType,\n        rows: int,\n        cols: int,\n        matrix: skimage.transform.PiecewiseAffineTransform,\n        **params: Any,\n    ) -&gt; BoxInternalType:\n        return fgeometric.bbox_piecewise_affine(bbox, matrix, params[\"shape\"], self.keypoints_threshold)\n\n    def apply_to_keypoint(\n        self,\n        keypoint: KeypointInternalType,\n        rows: int,\n        cols: int,\n        matrix: skimage.transform.PiecewiseAffineTransform,\n        **params: Any,\n    ) -&gt; KeypointInternalType:\n        return fgeometric.keypoint_piecewise_affine(keypoint, matrix, params[\"shape\"], self.keypoints_threshold)\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.transforms.PiecewiseAffine.apply","title":"<code>apply (self, img, matrix, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def apply(\n    self,\n    img: np.ndarray,\n    matrix: skimage.transform.PiecewiseAffineTransform,\n    **params: Any,\n) -&gt; np.ndarray:\n    return fgeometric.piecewise_affine(img, matrix, self.interpolation, self.mode, self.cval)\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.transforms.PiecewiseAffine.get_params_dependent_on_data","title":"<code>get_params_dependent_on_data (self, params, data)</code>","text":"<p>Returns parameters dependent on input.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n    height, width = params[\"shape\"][:2]\n\n    nb_rows = np.clip(random.randint(*self.nb_rows), 2, None)\n    nb_cols = np.clip(random.randint(*self.nb_cols), 2, None)\n    nb_cells = nb_cols * nb_rows\n    scale = random.uniform(*self.scale)\n\n    jitter: np.ndarray = random_utils.normal(0, scale, (nb_cells, 2))\n    if not np.any(jitter &gt; 0):\n        for _ in range(10):  # See: https://github.com/albumentations-team/albumentations/issues/1442\n            jitter = random_utils.normal(0, scale, (nb_cells, 2))\n            if np.any(jitter &gt; 0):\n                break\n        if not np.any(jitter &gt; 0):\n            return {\"matrix\": None}\n\n    y = np.linspace(0, height, nb_rows)\n    x = np.linspace(0, width, nb_cols)\n\n    # (H, W) and (H, W) for H=rows, W=cols\n    xx_src, yy_src = np.meshgrid(x, y)\n\n    # (1, HW, 2) =&gt; (HW, 2) for H=rows, W=cols\n    points_src = np.dstack([yy_src.flat, xx_src.flat])[0]\n\n    if self.absolute_scale:\n        jitter[:, 0] = jitter[:, 0] / height if height &gt; 0 else 0.0\n        jitter[:, 1] = jitter[:, 1] / width if width &gt; 0 else 0.0\n\n    jitter[:, 0] = jitter[:, 0] * height\n    jitter[:, 1] = jitter[:, 1] * width\n\n    points_dest = np.copy(points_src)\n    points_dest[:, 0] = points_dest[:, 0] + jitter[:, 0]\n    points_dest[:, 1] = points_dest[:, 1] + jitter[:, 1]\n\n    # Restrict all destination points to be inside the image plane.\n    # This is necessary, as otherwise keypoints could be augmented\n    # outside of the image plane and these would be replaced by\n    # (-1, -1), which would not conform with the behaviour of the other augmenters.\n    points_dest[:, 0] = np.clip(points_dest[:, 0], 0, height - 1)\n    points_dest[:, 1] = np.clip(points_dest[:, 1], 0, width - 1)\n\n    matrix = skimage.transform.PiecewiseAffineTransform()\n    matrix.estimate(points_src[:, ::-1], points_dest[:, ::-1])\n\n    return {\n        \"matrix\": matrix,\n    }\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.transforms.PiecewiseAffine.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return (\n        \"scale\",\n        \"nb_rows\",\n        \"nb_cols\",\n        \"interpolation\",\n        \"mask_interpolation\",\n        \"cval\",\n        \"cval_mask\",\n        \"mode\",\n        \"absolute_scale\",\n        \"keypoints_threshold\",\n    )\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.transforms.ShiftScaleRotate","title":"<code>class  ShiftScaleRotate</code> <code>     (shift_limit=(-0.0625, 0.0625), scale_limit=(-0.1, 0.1), rotate_limit=(-45, 45), interpolation=1, border_mode=4, value=0, mask_value=0, shift_limit_x=None, shift_limit_y=None, rotate_method='largest_box', always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Randomly apply affine transforms: translate, scale and rotate the input.</p> <p>Parameters:</p> Name Type Description <code>shift_limit</code> <code>float, float) or float</code> <p>shift factor range for both height and width. If shift_limit is a single float value, the range will be (-shift_limit, shift_limit). Absolute values for lower and upper bounds should lie in range [-1, 1]. Default: (-0.0625, 0.0625).</p> <code>scale_limit</code> <code>float, float) or float</code> <p>scaling factor range. If scale_limit is a single float value, the range will be (-scale_limit, scale_limit). Note that the scale_limit will be biased by 1. If scale_limit is a tuple, like (low, high), sampling will be done from the range (1 + low, 1 + high). Default: (-0.1, 0.1).</p> <code>rotate_limit</code> <code>int, int) or int</code> <p>rotation range. If rotate_limit is a single int value, the range will be (-rotate_limit, rotate_limit). Default: (-45, 45).</p> <code>interpolation</code> <code>OpenCV flag</code> <p>flag that is used to specify the interpolation algorithm. Should be one of: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4. Default: cv2.INTER_LINEAR.</p> <code>border_mode</code> <code>OpenCV flag</code> <p>flag that is used to specify the pixel extrapolation method. Should be one of: cv2.BORDER_CONSTANT, cv2.BORDER_REPLICATE, cv2.BORDER_REFLECT, cv2.BORDER_WRAP, cv2.BORDER_REFLECT_101. Default: cv2.BORDER_REFLECT_101</p> <code>value</code> <code>int, float, list of int, list of float</code> <p>padding value if border_mode is cv2.BORDER_CONSTANT.</p> <code>mask_value</code> <code>int, float,         list of int,         list of float</code> <p>padding value if border_mode is cv2.BORDER_CONSTANT applied for masks.</p> <code>shift_limit_x</code> <code>float, float) or float</code> <p>shift factor range for width. If it is set then this value instead of shift_limit will be used for shifting width.  If shift_limit_x is a single float value, the range will be (-shift_limit_x, shift_limit_x). Absolute values for lower and upper bounds should lie in the range [-1, 1]. Default: None.</p> <code>shift_limit_y</code> <code>float, float) or float</code> <p>shift factor range for height. If it is set then this value instead of shift_limit will be used for shifting height.  If shift_limit_y is a single float value, the range will be (-shift_limit_y, shift_limit_y). Absolute values for lower and upper bounds should lie in the range [-, 1]. Default: None.</p> <code>rotate_method</code> <code>str</code> <p>rotation method used for the bounding boxes. Should be one of \"largest_box\" or \"ellipse\". Default: \"largest_box\"</p> <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 0.5.</p> <p>Targets</p> <p>image, mask, keypoints, bboxes</p> <p>Image types:     uint8, float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>class ShiftScaleRotate(Affine):\n    \"\"\"Randomly apply affine transforms: translate, scale and rotate the input.\n\n    Args:\n        shift_limit ((float, float) or float): shift factor range for both height and width. If shift_limit\n            is a single float value, the range will be (-shift_limit, shift_limit). Absolute values for lower and\n            upper bounds should lie in range [-1, 1]. Default: (-0.0625, 0.0625).\n        scale_limit ((float, float) or float): scaling factor range. If scale_limit is a single float value, the\n            range will be (-scale_limit, scale_limit). Note that the scale_limit will be biased by 1.\n            If scale_limit is a tuple, like (low, high), sampling will be done from the range (1 + low, 1 + high).\n            Default: (-0.1, 0.1).\n        rotate_limit ((int, int) or int): rotation range. If rotate_limit is a single int value, the\n            range will be (-rotate_limit, rotate_limit). Default: (-45, 45).\n        interpolation (OpenCV flag): flag that is used to specify the interpolation algorithm. Should be one of:\n            cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.\n            Default: cv2.INTER_LINEAR.\n        border_mode (OpenCV flag): flag that is used to specify the pixel extrapolation method. Should be one of:\n            cv2.BORDER_CONSTANT, cv2.BORDER_REPLICATE, cv2.BORDER_REFLECT, cv2.BORDER_WRAP, cv2.BORDER_REFLECT_101.\n            Default: cv2.BORDER_REFLECT_101\n        value (int, float, list of int, list of float): padding value if border_mode is cv2.BORDER_CONSTANT.\n        mask_value (int, float,\n                    list of int,\n                    list of float): padding value if border_mode is cv2.BORDER_CONSTANT applied for masks.\n        shift_limit_x ((float, float) or float): shift factor range for width. If it is set then this value\n            instead of shift_limit will be used for shifting width.  If shift_limit_x is a single float value,\n            the range will be (-shift_limit_x, shift_limit_x). Absolute values for lower and upper bounds should lie in\n            the range [-1, 1]. Default: None.\n        shift_limit_y ((float, float) or float): shift factor range for height. If it is set then this value\n            instead of shift_limit will be used for shifting height.  If shift_limit_y is a single float value,\n            the range will be (-shift_limit_y, shift_limit_y). Absolute values for lower and upper bounds should lie\n            in the range [-, 1]. Default: None.\n        rotate_method (str): rotation method used for the bounding boxes. Should be one of \"largest_box\" or \"ellipse\".\n            Default: \"largest_box\"\n        p (float): probability of applying the transform. Default: 0.5.\n\n    Targets:\n        image, mask, keypoints, bboxes\n\n    Image types:\n        uint8, float32\n\n    \"\"\"\n\n    _targets = (Targets.IMAGE, Targets.MASK, Targets.KEYPOINTS, Targets.BBOXES)\n\n    class InitSchema(BaseTransformInitSchema):\n        shift_limit: SymmetricRangeType = (-0.0625, 0.0625)\n        scale_limit: SymmetricRangeType = (-0.1, 0.1)\n        rotate_limit: SymmetricRangeType = (-45, 45)\n        interpolation: InterpolationType = cv2.INTER_LINEAR\n        border_mode: BorderModeType = cv2.BORDER_REFLECT_101\n        value: ColorType = 0\n        mask_value: ColorType = 0\n        shift_limit_x: ScaleFloatType | None = Field(default=None)\n        shift_limit_y: ScaleFloatType | None = Field(default=None)\n        rotate_method: Literal[\"largest_box\", \"ellipse\"] = \"largest_box\"\n\n        @model_validator(mode=\"after\")\n        def check_shift_limit(self) -&gt; Self:\n            bounds = -1, 1\n            self.shift_limit_x = to_tuple(self.shift_limit_x if self.shift_limit_x is not None else self.shift_limit)\n            check_range(self.shift_limit_x, *bounds, \"shift_limit_x\")\n            self.shift_limit_y = to_tuple(self.shift_limit_y if self.shift_limit_y is not None else self.shift_limit)\n            check_range(self.shift_limit_y, *bounds, \"shift_limit_y\")\n            return self\n\n        @field_validator(\"scale_limit\")\n        @classmethod\n        def check_scale_limit(cls, value: ScaleFloatType, info: ValidationInfo) -&gt; ScaleFloatType:\n            bounds = 0, float(\"inf\")\n            result = to_tuple(value, bias=1.0)\n            check_range(result, *bounds, str(info.field_name))\n            return result\n\n    def __init__(\n        self,\n        shift_limit: ScaleFloatType = (-0.0625, 0.0625),\n        scale_limit: ScaleFloatType = (-0.1, 0.1),\n        rotate_limit: ScaleFloatType = (-45, 45),\n        interpolation: int = cv2.INTER_LINEAR,\n        border_mode: int = cv2.BORDER_REFLECT_101,\n        value: ColorType = 0,\n        mask_value: ColorType = 0,\n        shift_limit_x: ScaleFloatType | None = None,\n        shift_limit_y: ScaleFloatType | None = None,\n        rotate_method: Literal[\"largest_box\", \"ellipse\"] = \"largest_box\",\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(\n            scale=scale_limit,\n            translate_percent={\"x\": shift_limit_x, \"y\": shift_limit_y},\n            rotate=rotate_limit,\n            shear=(0, 0),\n            interpolation=interpolation,\n            mask_interpolation=cv2.INTER_NEAREST,\n            cval=value,\n            cval_mask=mask_value,\n            mode=border_mode,\n            fit_output=False,\n            keep_ratio=False,\n            rotate_method=rotate_method,\n            always_apply=always_apply,\n            p=p,\n        )\n        warn(\n            \"ShiftScaleRotate is deprecated. Please use Affine transform instead .\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        self.shift_limit_x = cast(Tuple[float, float], shift_limit_x)\n        self.shift_limit_y = cast(Tuple[float, float], shift_limit_y)\n        self.scale_limit = cast(Tuple[float, float], scale_limit)\n        self.rotate_limit = cast(Tuple[int, int], rotate_limit)\n        self.border_mode = border_mode\n        self.value = value\n        self.mask_value = mask_value\n\n    def get_transform_init_args(self) -&gt; dict[str, Any]:\n        return {\n            \"shift_limit_x\": self.shift_limit_x,\n            \"shift_limit_y\": self.shift_limit_y,\n            \"scale_limit\": to_tuple(self.scale_limit, bias=-1.0),\n            \"rotate_limit\": self.rotate_limit,\n            \"interpolation\": self.interpolation,\n            \"border_mode\": self.border_mode,\n            \"value\": self.value,\n            \"mask_value\": self.mask_value,\n            \"rotate_method\": self.rotate_method,\n        }\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.transforms.Transpose","title":"<code>class  Transpose</code> <code> </code>  [view source on GitHub]","text":"<p>Transpose the input by swapping rows and columns.</p> <p>Parameters:</p> Name Type Description <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 0.5.</p> <p>Targets</p> <p>image, mask, bboxes, keypoints</p> <p>Image types:     uint8, float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>class Transpose(DualTransform):\n    \"\"\"Transpose the input by swapping rows and columns.\n\n    Args:\n        p (float): probability of applying the transform. Default: 0.5.\n\n    Targets:\n        image, mask, bboxes, keypoints\n\n    Image types:\n        uint8, float32\n\n    \"\"\"\n\n    _targets = (Targets.IMAGE, Targets.MASK, Targets.BBOXES, Targets.KEYPOINTS)\n\n    def apply(self, img: np.ndarray, **params: Any) -&gt; np.ndarray:\n        return fgeometric.transpose(img)\n\n    def apply_to_bbox(self, bbox: BoxInternalType, **params: Any) -&gt; BoxInternalType:\n        return fgeometric.bbox_transpose(bbox)\n\n    def apply_to_keypoint(self, keypoint: KeypointInternalType, **params: Any) -&gt; KeypointInternalType:\n        return fgeometric.keypoint_transpose(keypoint)\n\n    def get_transform_init_args_names(self) -&gt; tuple[()]:\n        return ()\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.transforms.Transpose.apply","title":"<code>apply (self, img, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def apply(self, img: np.ndarray, **params: Any) -&gt; np.ndarray:\n    return fgeometric.transpose(img)\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.transforms.Transpose.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[()]:\n    return ()\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.transforms.VerticalFlip","title":"<code>class  VerticalFlip</code> <code> </code>  [view source on GitHub]","text":"<p>Flip the input vertically around the x-axis.</p> <p>Parameters:</p> Name Type Description <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 0.5.</p> <p>Targets</p> <p>image, mask, bboxes, keypoints</p> <p>Image types:     uint8, float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>class VerticalFlip(DualTransform):\n    \"\"\"Flip the input vertically around the x-axis.\n\n    Args:\n        p (float): probability of applying the transform. Default: 0.5.\n\n    Targets:\n        image, mask, bboxes, keypoints\n\n    Image types:\n        uint8, float32\n\n    \"\"\"\n\n    _targets = (Targets.IMAGE, Targets.MASK, Targets.BBOXES, Targets.KEYPOINTS)\n\n    def apply(self, img: np.ndarray, **params: Any) -&gt; np.ndarray:\n        return fgeometric.vflip(img)\n\n    def apply_to_bbox(self, bbox: BoxInternalType, **params: Any) -&gt; BoxInternalType:\n        return fgeometric.bbox_vflip(bbox)\n\n    def apply_to_keypoint(self, keypoint: KeypointInternalType, **params: Any) -&gt; KeypointInternalType:\n        return fgeometric.keypoint_vflip(keypoint, params[\"rows\"])\n\n    def get_transform_init_args_names(self) -&gt; tuple[()]:\n        return ()\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.transforms.VerticalFlip.apply","title":"<code>apply (self, img, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def apply(self, img: np.ndarray, **params: Any) -&gt; np.ndarray:\n    return fgeometric.vflip(img)\n</code></pre>"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.transforms.VerticalFlip.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[()]:\n    return ()\n</code></pre>"},{"location":"api_reference/augmentations/transforms/","title":"Transforms (augmentations.transforms)","text":""},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.CLAHE","title":"<code>class  CLAHE</code> <code>     (clip_limit=4.0, tile_grid_size=(8, 8), always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Apply Contrast Limited Adaptive Histogram Equalization to the input image.</p> <p>Parameters:</p> Name Type Description <code>clip_limit</code> <code>ScaleFloatType</code> <p>upper threshold value for contrast limiting. If clip_limit is a single float value, the range will be (1, clip_limit). Default: (1, 4).</p> <code>tile_grid_size</code> <code>tuple[int, int]</code> <p>size of grid for histogram equalization. Default: (8, 8).</p> <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 0.5.</p> <p>Targets</p> <p>image</p> <p>Image types:     uint8</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>class CLAHE(ImageOnlyTransform):\n    \"\"\"Apply Contrast Limited Adaptive Histogram Equalization to the input image.\n\n    Args:\n        clip_limit: upper threshold value for contrast limiting.\n            If clip_limit is a single float value, the range will be (1, clip_limit). Default: (1, 4).\n        tile_grid_size: size of grid for histogram equalization. Default: (8, 8).\n        p: probability of applying the transform. Default: 0.5.\n\n    Targets:\n        image\n\n    Image types:\n        uint8\n\n    \"\"\"\n\n    class InitSchema(BaseTransformInitSchema):\n        clip_limit: OnePlusFloatRangeType = (1.0, 4.0)\n        tile_grid_size: OnePlusIntRangeType = (8, 8)\n\n    def __init__(\n        self,\n        clip_limit: ScaleFloatType = 4.0,\n        tile_grid_size: tuple[int, int] = (8, 8),\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p=p, always_apply=always_apply)\n        self.clip_limit = cast(Tuple[float, float], clip_limit)\n        self.tile_grid_size = tile_grid_size\n\n    def apply(self, img: np.ndarray, clip_limit: float, **params: Any) -&gt; np.ndarray:\n        if not is_rgb_image(img) and not is_grayscale_image(img):\n            msg = \"CLAHE transformation expects 1-channel or 3-channel images.\"\n            raise TypeError(msg)\n\n        return fmain.clahe(img, clip_limit, self.tile_grid_size)\n\n    def get_params(self) -&gt; dict[str, float]:\n        return {\"clip_limit\": random.uniform(self.clip_limit[0], self.clip_limit[1])}\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, str]:\n        return (\"clip_limit\", \"tile_grid_size\")\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.CLAHE.apply","title":"<code>apply (self, img, clip_limit, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def apply(self, img: np.ndarray, clip_limit: float, **params: Any) -&gt; np.ndarray:\n    if not is_rgb_image(img) and not is_grayscale_image(img):\n        msg = \"CLAHE transformation expects 1-channel or 3-channel images.\"\n        raise TypeError(msg)\n\n    return fmain.clahe(img, clip_limit, self.tile_grid_size)\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.CLAHE.get_params","title":"<code>get_params (self)</code>","text":"<p>Returns parameters independent of input.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_params(self) -&gt; dict[str, float]:\n    return {\"clip_limit\": random.uniform(self.clip_limit[0], self.clip_limit[1])}\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.CLAHE.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, str]:\n    return (\"clip_limit\", \"tile_grid_size\")\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.ChannelShuffle","title":"<code>class  ChannelShuffle</code> <code> </code>  [view source on GitHub]","text":"<p>Randomly rearrange channels of the image.</p> <p>Parameters:</p> Name Type Description <code>p</code> <p>probability of applying the transform. Default: 0.5.</p> <p>Targets</p> <p>image</p> <p>Image types:     uint8, float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>class ChannelShuffle(ImageOnlyTransform):\n    \"\"\"Randomly rearrange channels of the image.\n\n    Args:\n        p: probability of applying the transform. Default: 0.5.\n\n    Targets:\n        image\n\n    Image types:\n        uint8, float32\n\n    \"\"\"\n\n    def apply(self, img: np.ndarray, channels_shuffled: tuple[int, ...], **params: Any) -&gt; np.ndarray:\n        return fmain.channel_shuffle(img, channels_shuffled)\n\n    def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n        ch_arr = list(range(params[\"shape\"][2]))\n        ch_arr = random_utils.shuffle(ch_arr)\n        return {\"channels_shuffled\": ch_arr}\n\n    def get_transform_init_args_names(self) -&gt; tuple[()]:\n        return ()\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.ChannelShuffle.apply","title":"<code>apply (self, img, channels_shuffled, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def apply(self, img: np.ndarray, channels_shuffled: tuple[int, ...], **params: Any) -&gt; np.ndarray:\n    return fmain.channel_shuffle(img, channels_shuffled)\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.ChannelShuffle.get_params_dependent_on_data","title":"<code>get_params_dependent_on_data (self, params, data)</code>","text":"<p>Returns parameters dependent on input.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n    ch_arr = list(range(params[\"shape\"][2]))\n    ch_arr = random_utils.shuffle(ch_arr)\n    return {\"channels_shuffled\": ch_arr}\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.ChannelShuffle.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[()]:\n    return ()\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.ChromaticAberration","title":"<code>class  ChromaticAberration</code> <code>     (primary_distortion_limit=(-0.02, 0.02), secondary_distortion_limit=(-0.05, 0.05), mode='green_purple', interpolation=1, always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Add lateral chromatic aberration by distorting the red and blue channels of the input image.</p> <p>Parameters:</p> Name Type Description <code>primary_distortion_limit</code> <code>ScaleFloatType</code> <p>range of the primary radial distortion coefficient. If primary_distortion_limit is a single float value, the range will be (-primary_distortion_limit, primary_distortion_limit). Controls the distortion in the center of the image (positive values result in pincushion distortion, negative values result in barrel distortion). Default: 0.02.</p> <code>secondary_distortion_limit</code> <code>ScaleFloatType</code> <p>range of the secondary radial distortion coefficient. If secondary_distortion_limit is a single float value, the range will be (-secondary_distortion_limit, secondary_distortion_limit). Controls the distortion in the corners of the image (positive values result in pincushion distortion, negative values result in barrel distortion). Default: 0.05.</p> <code>mode</code> <code>ChromaticAberrationMode</code> <p>type of color fringing. Supported modes are 'green_purple', 'red_blue' and 'random'. 'random' will choose one of the modes 'green_purple' or 'red_blue' randomly. Default: 'green_purple'.</p> <code>interpolation</code> <code>int</code> <p>flag that is used to specify the interpolation algorithm. Should be one of: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4. Default: cv2.INTER_LINEAR.</p> <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 0.5.</p> <p>Targets</p> <p>image</p> <p>Image types:     uint8, float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>class ChromaticAberration(ImageOnlyTransform):\n    \"\"\"Add lateral chromatic aberration by distorting the red and blue channels of the input image.\n\n    Args:\n        primary_distortion_limit: range of the primary radial distortion coefficient.\n            If primary_distortion_limit is a single float value, the range will be\n            (-primary_distortion_limit, primary_distortion_limit).\n            Controls the distortion in the center of the image (positive values result in pincushion distortion,\n            negative values result in barrel distortion).\n            Default: 0.02.\n        secondary_distortion_limit: range of the secondary radial distortion coefficient.\n            If secondary_distortion_limit is a single float value, the range will be\n            (-secondary_distortion_limit, secondary_distortion_limit).\n            Controls the distortion in the corners of the image (positive values result in pincushion distortion,\n            negative values result in barrel distortion).\n            Default: 0.05.\n        mode: type of color fringing.\n            Supported modes are 'green_purple', 'red_blue' and 'random'.\n            'random' will choose one of the modes 'green_purple' or 'red_blue' randomly.\n            Default: 'green_purple'.\n        interpolation: flag that is used to specify the interpolation algorithm. Should be one of:\n            cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.\n            Default: cv2.INTER_LINEAR.\n        p: probability of applying the transform.\n            Default: 0.5.\n\n    Targets:\n        image\n\n    Image types:\n        uint8, float32\n\n    \"\"\"\n\n    class InitSchema(BaseTransformInitSchema):\n        primary_distortion_limit: SymmetricRangeType = (-0.02, 0.02)\n        secondary_distortion_limit: SymmetricRangeType = (-0.05, 0.05)\n        mode: ChromaticAberrationMode = Field(default=\"green_purple\", description=\"Type of color fringing.\")\n        interpolation: InterpolationType = cv2.INTER_LINEAR\n\n    def __init__(\n        self,\n        primary_distortion_limit: ScaleFloatType = (-0.02, 0.02),\n        secondary_distortion_limit: ScaleFloatType = (-0.05, 0.05),\n        mode: ChromaticAberrationMode = \"green_purple\",\n        interpolation: int = cv2.INTER_LINEAR,\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p=p, always_apply=always_apply)\n        self.primary_distortion_limit = cast(Tuple[float, float], primary_distortion_limit)\n        self.secondary_distortion_limit = cast(Tuple[float, float], secondary_distortion_limit)\n        self.mode = mode\n        self.interpolation = interpolation\n\n    def apply(\n        self,\n        img: np.ndarray,\n        primary_distortion_red: float,\n        secondary_distortion_red: float,\n        primary_distortion_blue: float,\n        secondary_distortion_blue: float,\n        **params: Any,\n    ) -&gt; np.ndarray:\n        return fmain.chromatic_aberration(\n            img,\n            primary_distortion_red,\n            secondary_distortion_red,\n            primary_distortion_blue,\n            secondary_distortion_blue,\n            self.interpolation,\n        )\n\n    def get_params(self) -&gt; dict[str, float]:\n        primary_distortion_red = random.uniform(*self.primary_distortion_limit)\n        secondary_distortion_red = random.uniform(*self.secondary_distortion_limit)\n        primary_distortion_blue = random.uniform(*self.primary_distortion_limit)\n        secondary_distortion_blue = random.uniform(*self.secondary_distortion_limit)\n\n        secondary_distortion_red = self._match_sign(primary_distortion_red, secondary_distortion_red)\n        secondary_distortion_blue = self._match_sign(primary_distortion_blue, secondary_distortion_blue)\n\n        if self.mode == \"green_purple\":\n            # distortion coefficients of the red and blue channels have the same sign\n            primary_distortion_blue = self._match_sign(primary_distortion_red, primary_distortion_blue)\n            secondary_distortion_blue = self._match_sign(secondary_distortion_red, secondary_distortion_blue)\n        if self.mode == \"red_blue\":\n            # distortion coefficients of the red and blue channels have the opposite sign\n            primary_distortion_blue = self._unmatch_sign(primary_distortion_red, primary_distortion_blue)\n            secondary_distortion_blue = self._unmatch_sign(secondary_distortion_red, secondary_distortion_blue)\n\n        return {\n            \"primary_distortion_red\": primary_distortion_red,\n            \"secondary_distortion_red\": secondary_distortion_red,\n            \"primary_distortion_blue\": primary_distortion_blue,\n            \"secondary_distortion_blue\": secondary_distortion_blue,\n        }\n\n    @staticmethod\n    def _match_sign(a: float, b: float) -&gt; float:\n        # Match the sign of b to a\n        if (a &lt; 0 &lt; b) or (a &gt; 0 &gt; b):\n            return -b\n        return b\n\n    @staticmethod\n    def _unmatch_sign(a: float, b: float) -&gt; float:\n        # Unmatch the sign of b to a\n        if (a &lt; 0 and b &lt; 0) or (a &gt; 0 and b &gt; 0):\n            return -b\n        return b\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, str, str, str]:\n        return \"primary_distortion_limit\", \"secondary_distortion_limit\", \"mode\", \"interpolation\"\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.ChromaticAberration.apply","title":"<code>apply (self, img, primary_distortion_red, secondary_distortion_red, primary_distortion_blue, secondary_distortion_blue, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def apply(\n    self,\n    img: np.ndarray,\n    primary_distortion_red: float,\n    secondary_distortion_red: float,\n    primary_distortion_blue: float,\n    secondary_distortion_blue: float,\n    **params: Any,\n) -&gt; np.ndarray:\n    return fmain.chromatic_aberration(\n        img,\n        primary_distortion_red,\n        secondary_distortion_red,\n        primary_distortion_blue,\n        secondary_distortion_blue,\n        self.interpolation,\n    )\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.ChromaticAberration.get_params","title":"<code>get_params (self)</code>","text":"<p>Returns parameters independent of input.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_params(self) -&gt; dict[str, float]:\n    primary_distortion_red = random.uniform(*self.primary_distortion_limit)\n    secondary_distortion_red = random.uniform(*self.secondary_distortion_limit)\n    primary_distortion_blue = random.uniform(*self.primary_distortion_limit)\n    secondary_distortion_blue = random.uniform(*self.secondary_distortion_limit)\n\n    secondary_distortion_red = self._match_sign(primary_distortion_red, secondary_distortion_red)\n    secondary_distortion_blue = self._match_sign(primary_distortion_blue, secondary_distortion_blue)\n\n    if self.mode == \"green_purple\":\n        # distortion coefficients of the red and blue channels have the same sign\n        primary_distortion_blue = self._match_sign(primary_distortion_red, primary_distortion_blue)\n        secondary_distortion_blue = self._match_sign(secondary_distortion_red, secondary_distortion_blue)\n    if self.mode == \"red_blue\":\n        # distortion coefficients of the red and blue channels have the opposite sign\n        primary_distortion_blue = self._unmatch_sign(primary_distortion_red, primary_distortion_blue)\n        secondary_distortion_blue = self._unmatch_sign(secondary_distortion_red, secondary_distortion_blue)\n\n    return {\n        \"primary_distortion_red\": primary_distortion_red,\n        \"secondary_distortion_red\": secondary_distortion_red,\n        \"primary_distortion_blue\": primary_distortion_blue,\n        \"secondary_distortion_blue\": secondary_distortion_blue,\n    }\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.ChromaticAberration.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, str, str, str]:\n    return \"primary_distortion_limit\", \"secondary_distortion_limit\", \"mode\", \"interpolation\"\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.ColorJitter","title":"<code>class  ColorJitter</code> <code>     (brightness=(0.8, 1), contrast=(0.8, 1), saturation=(0.8, 1), hue=(-0.5, 0.5), always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Randomly changes the brightness, contrast, and saturation of an image. Compared to ColorJitter from torchvision, this transform gives a little bit different results because Pillow (used in torchvision) and OpenCV (used in Albumentations) transform an image to HSV format by different formulas. Another difference - Pillow uses uint8 overflow, but we use value saturation.</p> <p>Parameters:</p> Name Type Description <code>brightness</code> <code>float or tuple of float (min, max</code> <p>How much to jitter brightness. If float:     brightness_factor is chosen uniformly from [max(0, 1 - brightness), 1 + brightness] If tuple[float, float]] will be sampled from that range. Both values should be non negative numbers.</p> <code>contrast</code> <code>float or tuple of float (min, max</code> <p>How much to jitter contrast. If float:     contrast_factor is chosen uniformly from [max(0, 1 - brightness), 1 + brightness] If tuple[float, float]] will be sampled from that range. Both values should be non negative numbers.</p> <code>saturation</code> <code>float or tuple of float (min, max</code> <p>How much to jitter saturation. If float:    saturation_factor is chosen uniformly from [max(0, 1 - brightness), 1 + brightness] If tuple[float, float]] will be sampled from that range. Both values should be non negative numbers.</p> <code>hue</code> <code>float or tuple of float (min, max</code> <p>How much to jitter hue. If float:    saturation_factor is chosen uniformly from [-hue, hue]. Should have 0 &lt;= hue &lt;= 0.5. If tuple[float, float]] will be sampled from that range. Both values should be in range [-0.5, 0.5].</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>class ColorJitter(ImageOnlyTransform):\n    \"\"\"Randomly changes the brightness, contrast, and saturation of an image. Compared to ColorJitter from torchvision,\n    this transform gives a little bit different results because Pillow (used in torchvision) and OpenCV (used in\n    Albumentations) transform an image to HSV format by different formulas. Another difference - Pillow uses uint8\n    overflow, but we use value saturation.\n\n    Args:\n        brightness (float or tuple of float (min, max)): How much to jitter brightness.\n            If float:\n                brightness_factor is chosen uniformly from [max(0, 1 - brightness), 1 + brightness]\n            If tuple[float, float]] will be sampled from that range. Both values should be non negative numbers.\n        contrast (float or tuple of float (min, max)): How much to jitter contrast.\n            If float:\n                contrast_factor is chosen uniformly from [max(0, 1 - brightness), 1 + brightness]\n            If tuple[float, float]] will be sampled from that range. Both values should be non negative numbers.\n        saturation (float or tuple of float (min, max)): How much to jitter saturation.\n            If float:\n               saturation_factor is chosen uniformly from [max(0, 1 - brightness), 1 + brightness]\n            If tuple[float, float]] will be sampled from that range. Both values should be non negative numbers.\n        hue (float or tuple of float (min, max)): How much to jitter hue.\n            If float:\n               saturation_factor is chosen uniformly from [-hue, hue]. Should have 0 &lt;= hue &lt;= 0.5.\n            If tuple[float, float]] will be sampled from that range. Both values should be in range [-0.5, 0.5].\n\n    \"\"\"\n\n    class InitSchema(BaseTransformInitSchema):\n        brightness: Annotated[ScaleFloatType, Field(default=0.2, description=\"Range for jittering brightness.\")]\n        contrast: Annotated[ScaleFloatType, Field(default=0.2, description=\"Range for jittering contrast.\")]\n        saturation: Annotated[ScaleFloatType, Field(default=0.2, description=\"Range for jittering saturation.\")]\n        hue: Annotated[ScaleFloatType, Field(default=0.2, description=\"Range for jittering hue.\")]\n\n        @field_validator(\"brightness\", \"contrast\", \"saturation\", \"hue\")\n        @classmethod\n        def check_ranges(cls, value: ScaleFloatType, info: ValidationInfo) -&gt; tuple[float, float]:\n            if info.field_name == \"hue\":\n                bounds = -0.5, 0.5\n                bias = 0\n                clip = False\n            elif info.field_name in [\"brightness\", \"contrast\", \"saturation\"]:\n                bounds = 0, float(\"inf\")\n                bias = 1\n                clip = True\n\n            if isinstance(value, numbers.Number):\n                if value &lt; 0:\n                    raise ValueError(f\"If {info.field_name} is a single number, it must be non negative.\")\n                left = bias - value\n                if clip:\n                    left = max(left, 0)\n                value = (left, bias + value)\n            elif isinstance(value, tuple) and len(value) == PAIR:\n                check_range(value, *bounds, info.field_name)\n\n            return cast(Tuple[float, float], value)\n\n    def __init__(\n        self,\n        brightness: ScaleFloatType = (0.8, 1),\n        contrast: ScaleFloatType = (0.8, 1),\n        saturation: ScaleFloatType = (0.8, 1),\n        hue: ScaleFloatType = (-0.5, 0.5),\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p=p, always_apply=always_apply)\n\n        self.brightness = cast(Tuple[float, float], brightness)\n        self.contrast = cast(Tuple[float, float], contrast)\n        self.saturation = cast(Tuple[float, float], saturation)\n        self.hue = cast(Tuple[float, float], hue)\n\n        self.transforms = [\n            fmain.adjust_brightness_torchvision,\n            fmain.adjust_contrast_torchvision,\n            fmain.adjust_saturation_torchvision,\n            fmain.adjust_hue_torchvision,\n        ]\n\n    def get_params(self) -&gt; dict[str, Any]:\n        brightness = random.uniform(self.brightness[0], self.brightness[1])\n        contrast = random.uniform(self.contrast[0], self.contrast[1])\n        saturation = random.uniform(self.saturation[0], self.saturation[1])\n        hue = random.uniform(self.hue[0], self.hue[1])\n\n        order = [0, 1, 2, 3]\n        order = random_utils.shuffle(order)\n\n        return {\n            \"brightness\": brightness,\n            \"contrast\": contrast,\n            \"saturation\": saturation,\n            \"hue\": hue,\n            \"order\": order,\n        }\n\n    def apply(\n        self,\n        img: np.ndarray,\n        brightness: float,\n        contrast: float,\n        saturation: float,\n        hue: float,\n        order: list[int],\n        **params: Any,\n    ) -&gt; np.ndarray:\n        if order is None:\n            order = [0, 1, 2, 3]\n        if not is_rgb_image(img) and not is_grayscale_image(img):\n            msg = \"ColorJitter transformation expects 1-channel or 3-channel images.\"\n            raise TypeError(msg)\n        color_transforms = [brightness, contrast, saturation, hue]\n        for i in order:\n            img = self.transforms[i](img, color_transforms[i])\n        return img\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, str, str, str]:\n        return (\"brightness\", \"contrast\", \"saturation\", \"hue\")\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.ColorJitter.apply","title":"<code>apply (self, img, brightness, contrast, saturation, hue, order, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def apply(\n    self,\n    img: np.ndarray,\n    brightness: float,\n    contrast: float,\n    saturation: float,\n    hue: float,\n    order: list[int],\n    **params: Any,\n) -&gt; np.ndarray:\n    if order is None:\n        order = [0, 1, 2, 3]\n    if not is_rgb_image(img) and not is_grayscale_image(img):\n        msg = \"ColorJitter transformation expects 1-channel or 3-channel images.\"\n        raise TypeError(msg)\n    color_transforms = [brightness, contrast, saturation, hue]\n    for i in order:\n        img = self.transforms[i](img, color_transforms[i])\n    return img\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.ColorJitter.get_params","title":"<code>get_params (self)</code>","text":"<p>Returns parameters independent of input.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_params(self) -&gt; dict[str, Any]:\n    brightness = random.uniform(self.brightness[0], self.brightness[1])\n    contrast = random.uniform(self.contrast[0], self.contrast[1])\n    saturation = random.uniform(self.saturation[0], self.saturation[1])\n    hue = random.uniform(self.hue[0], self.hue[1])\n\n    order = [0, 1, 2, 3]\n    order = random_utils.shuffle(order)\n\n    return {\n        \"brightness\": brightness,\n        \"contrast\": contrast,\n        \"saturation\": saturation,\n        \"hue\": hue,\n        \"order\": order,\n    }\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.ColorJitter.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, str, str, str]:\n    return (\"brightness\", \"contrast\", \"saturation\", \"hue\")\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.Downscale","title":"<code>class  Downscale</code> <code>     (scale_min=None, scale_max=None, interpolation=None, scale_range=(0.25, 0.25), interpolation_pair={'upscale': 0, 'downscale': 0}, always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Decreases image quality by downscaling and then upscaling it back to its original size.</p> <p>Parameters:</p> Name Type Description <code>scale_range</code> <code>tuple[float, float]</code> <p>A tuple defining the minimum and maximum scale to which the image will be downscaled. The range should be between 0 and 1, inclusive at minimum and exclusive at maximum. The first value should be less than or equal to the second value.</p> <code>interpolation_pair</code> <code>InterpolationDict</code> <p>A dictionary specifying the interpolation methods to use for downscaling and upscaling. Should include keys 'downscale' and 'upscale' with cv2 interpolation     flags as values. Example: {\"downscale\": cv2.INTER_NEAREST, \"upscale\": cv2.INTER_LINEAR}.</p> <p>Targets</p> <p>image</p> <p>Image types:     uint8, float32</p> <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; transform = Downscale(scale_range=(0.5, 0.9), interpolation_pair={\"downscale\": cv2.INTER_AREA,\n                                                  \"upscale\": cv2.INTER_CUBIC})\n&gt;&gt;&gt; transformed = transform(image=img)\n</code></pre> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>class Downscale(ImageOnlyTransform):\n    \"\"\"Decreases image quality by downscaling and then upscaling it back to its original size.\n\n    Args:\n        scale_range (tuple[float, float]): A tuple defining the minimum and maximum scale to which the image\n            will be downscaled. The range should be between 0 and 1, inclusive at minimum and exclusive at maximum.\n            The first value should be less than or equal to the second value.\n        interpolation_pair (InterpolationDict): A dictionary specifying the interpolation methods to use for\n            downscaling and upscaling. Should include keys 'downscale' and 'upscale' with cv2 interpolation\n                flags as values.\n            Example: {\"downscale\": cv2.INTER_NEAREST, \"upscale\": cv2.INTER_LINEAR}.\n\n    Targets:\n        image\n\n    Image types:\n        uint8, float32\n\n    Example:\n        &gt;&gt;&gt; transform = Downscale(scale_range=(0.5, 0.9), interpolation_pair={\"downscale\": cv2.INTER_AREA,\n                                                          \"upscale\": cv2.INTER_CUBIC})\n        &gt;&gt;&gt; transformed = transform(image=img)\n    \"\"\"\n\n    class InitSchema(BaseTransformInitSchema):\n        scale_min: float | None = Field(\n            default=None,\n            ge=0,\n            le=1,\n            description=\"Lower bound on the image scale.\",\n        )\n        scale_max: float | None = Field(\n            default=None,\n            ge=0,\n            lt=1,\n            description=\"Upper bound on the image scale.\",\n        )\n\n        interpolation: int | Interpolation | InterpolationDict | None = Field(\n            default_factory=lambda: Interpolation(downscale=cv2.INTER_NEAREST, upscale=cv2.INTER_NEAREST),\n        )\n        interpolation_pair: InterpolationPydantic\n\n        scale_range: Annotated[tuple[float, float], AfterValidator(check_01), AfterValidator(nondecreasing)] = (\n            0.25,\n            0.25,\n        )\n\n        @model_validator(mode=\"after\")\n        def validate_params(self) -&gt; Self:\n            if self.scale_min is not None and self.scale_max is not None:\n                warn(\n                    \"scale_min and scale_max are deprecated. Use scale_range instead.\",\n                    DeprecationWarning,\n                    stacklevel=2,\n                )\n\n                self.scale_range = (self.scale_min, self.scale_max)\n                self.scale_min = None\n                self.scale_max = None\n\n            if self.interpolation is not None:\n                warn(\n                    \"Downscale.interpolation is deprecated. Use Downscale.interpolation_pair instead.\",\n                    DeprecationWarning,\n                    stacklevel=2,\n                )\n\n                if isinstance(self.interpolation, dict):\n                    self.interpolation_pair = InterpolationPydantic(**self.interpolation)\n                elif isinstance(self.interpolation, int):\n                    self.interpolation_pair = InterpolationPydantic(\n                        upscale=self.interpolation,\n                        downscale=self.interpolation,\n                    )\n                elif isinstance(self.interpolation, Interpolation):\n                    self.interpolation_pair = InterpolationPydantic(\n                        upscale=self.interpolation.upscale,\n                        downscale=self.interpolation.downscale,\n                    )\n                self.interpolation = None\n\n            return self\n\n    def __init__(\n        self,\n        scale_min: float | None = None,\n        scale_max: float | None = None,\n        interpolation: int | Interpolation | InterpolationDict | None = None,\n        scale_range: tuple[float, float] = (0.25, 0.25),\n        interpolation_pair: InterpolationDict = InterpolationDict(\n            {\"upscale\": cv2.INTER_NEAREST, \"downscale\": cv2.INTER_NEAREST},\n        ),\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p=p, always_apply=always_apply)\n        self.scale_range = scale_range\n        self.interpolation_pair = interpolation_pair\n\n    def apply(self, img: np.ndarray, scale: float, **params: Any) -&gt; np.ndarray:\n        return fmain.downscale(\n            img,\n            scale=scale,\n            down_interpolation=self.interpolation_pair[\"downscale\"],\n            up_interpolation=self.interpolation_pair[\"upscale\"],\n        )\n\n    def get_params(self) -&gt; dict[str, Any]:\n        return {\"scale\": random.uniform(self.scale_range[0], self.scale_range[1])}\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, str]:\n        return (\"scale_range\", \"interpolation_pair\")\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.Downscale.apply","title":"<code>apply (self, img, scale, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def apply(self, img: np.ndarray, scale: float, **params: Any) -&gt; np.ndarray:\n    return fmain.downscale(\n        img,\n        scale=scale,\n        down_interpolation=self.interpolation_pair[\"downscale\"],\n        up_interpolation=self.interpolation_pair[\"upscale\"],\n    )\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.Downscale.get_params","title":"<code>get_params (self)</code>","text":"<p>Returns parameters independent of input.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_params(self) -&gt; dict[str, Any]:\n    return {\"scale\": random.uniform(self.scale_range[0], self.scale_range[1])}\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.Downscale.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, str]:\n    return (\"scale_range\", \"interpolation_pair\")\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.Emboss","title":"<code>class  Emboss</code> <code>     (alpha=(0.2, 0.5), strength=(0.2, 0.7), always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Emboss the input image and overlays the result with the original image.</p> <p>Parameters:</p> Name Type Description <code>alpha</code> <code>tuple[float, float]</code> <p>range to choose the visibility of the embossed image. At 0, only the original image is visible,at 1.0 only its embossed version is visible. Default: (0.2, 0.5).</p> <code>strength</code> <code>tuple[float, float]</code> <p>strength range of the embossing. Default: (0.2, 0.7).</p> <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 0.5.</p> <p>Targets</p> <p>image</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>class Emboss(ImageOnlyTransform):\n    \"\"\"Emboss the input image and overlays the result with the original image.\n\n    Args:\n        alpha: range to choose the visibility of the embossed image. At 0, only the original image is\n            visible,at 1.0 only its embossed version is visible. Default: (0.2, 0.5).\n        strength: strength range of the embossing. Default: (0.2, 0.7).\n        p: probability of applying the transform. Default: 0.5.\n\n    Targets:\n        image\n\n    \"\"\"\n\n    class InitSchema(BaseTransformInitSchema):\n        alpha: ZeroOneRangeType = (0.2, 0.5)\n        strength: NonNegativeFloatRangeType = (0.2, 0.7)\n\n    def __init__(\n        self,\n        alpha: tuple[float, float] = (0.2, 0.5),\n        strength: tuple[float, float] = (0.2, 0.7),\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p=p, always_apply=always_apply)\n        self.alpha = alpha\n        self.strength = strength\n\n    @staticmethod\n    def __generate_emboss_matrix(alpha_sample: np.ndarray, strength_sample: np.ndarray) -&gt; np.ndarray:\n        matrix_nochange = np.array([[0, 0, 0], [0, 1, 0], [0, 0, 0]], dtype=np.float32)\n        matrix_effect = np.array(\n            [\n                [-1 - strength_sample, 0 - strength_sample, 0],\n                [0 - strength_sample, 1, 0 + strength_sample],\n                [0, 0 + strength_sample, 1 + strength_sample],\n            ],\n            dtype=np.float32,\n        )\n        return (1 - alpha_sample) * matrix_nochange + alpha_sample * matrix_effect\n\n    def get_params(self) -&gt; dict[str, np.ndarray]:\n        alpha = random.uniform(*self.alpha)\n        strength = random.uniform(*self.strength)\n        emboss_matrix = self.__generate_emboss_matrix(alpha_sample=alpha, strength_sample=strength)\n        return {\"emboss_matrix\": emboss_matrix}\n\n    def apply(self, img: np.ndarray, emboss_matrix: np.ndarray, **params: Any) -&gt; np.ndarray:\n        return fmain.convolve(img, emboss_matrix)\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, str]:\n        return (\"alpha\", \"strength\")\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.Emboss.apply","title":"<code>apply (self, img, emboss_matrix, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def apply(self, img: np.ndarray, emboss_matrix: np.ndarray, **params: Any) -&gt; np.ndarray:\n    return fmain.convolve(img, emboss_matrix)\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.Emboss.get_params","title":"<code>get_params (self)</code>","text":"<p>Returns parameters independent of input.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_params(self) -&gt; dict[str, np.ndarray]:\n    alpha = random.uniform(*self.alpha)\n    strength = random.uniform(*self.strength)\n    emboss_matrix = self.__generate_emboss_matrix(alpha_sample=alpha, strength_sample=strength)\n    return {\"emboss_matrix\": emboss_matrix}\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.Emboss.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, str]:\n    return (\"alpha\", \"strength\")\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.Equalize","title":"<code>class  Equalize</code> <code>     (mode='cv', by_channels=True, mask=None, mask_params=(), always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Equalize the image histogram.</p> <p>Parameters:</p> Name Type Description <code>mode</code> <code>str</code> <p>{'cv', 'pil'}. Use OpenCV or Pillow equalization method.</p> <code>by_channels</code> <code>bool</code> <p>If True, use equalization by channels separately, else convert image to YCbCr representation and use equalization by <code>Y</code> channel.</p> <code>mask</code> <code>np.ndarray, callable</code> <p>If given, only the pixels selected by the mask are included in the analysis. Maybe 1 channel or 3 channel array or callable. Function signature must include <code>image</code> argument.</p> <code>mask_params</code> <code>list of str</code> <p>Params for mask function.</p> <p>Targets</p> <p>image</p> <p>Image types:     uint8</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>class Equalize(ImageOnlyTransform):\n    \"\"\"Equalize the image histogram.\n\n    Args:\n        mode (str): {'cv', 'pil'}. Use OpenCV or Pillow equalization method.\n        by_channels (bool): If True, use equalization by channels separately,\n            else convert image to YCbCr representation and use equalization by `Y` channel.\n        mask (np.ndarray, callable): If given, only the pixels selected by\n            the mask are included in the analysis. Maybe 1 channel or 3 channel array or callable.\n            Function signature must include `image` argument.\n        mask_params (list of str): Params for mask function.\n\n    Targets:\n        image\n\n    Image types:\n        uint8\n\n    \"\"\"\n\n    class InitSchema(BaseTransformInitSchema):\n        mode: ImageMode = \"cv\"\n        by_channels: Annotated[bool, Field(default=True, description=\"Equalize channels separately if True\")]\n        mask: Annotated[\n            np.ndarray | Callable[..., Any] | None,\n            Field(default=None, description=\"Mask to apply for equalization\"),\n        ]\n        mask_params: Annotated[Sequence[str], Field(default=[], description=\"Parameters for mask function\")]\n\n    def __init__(\n        self,\n        mode: ImageMode = \"cv\",\n        by_channels: bool = True,\n        mask: np.ndarray | Callable[..., Any] | None = None,\n        mask_params: Sequence[str] = (),\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p=p, always_apply=always_apply)\n\n        self.mode = mode\n        self.by_channels = by_channels\n        self.mask = mask\n        self.mask_params = mask_params\n\n    def apply(self, img: np.ndarray, mask: np.ndarray, **params: Any) -&gt; np.ndarray:\n        return fmain.equalize(img, mode=self.mode, by_channels=self.by_channels, mask=mask)\n\n    def get_params_dependent_on_targets(self, params: dict[str, Any]) -&gt; dict[str, Any]:\n        if not callable(self.mask):\n            return {\"mask\": self.mask}\n\n        return {\"mask\": self.mask(**params)}\n\n    @property\n    def targets_as_params(self) -&gt; list[str]:\n        return [\"image\", *list(self.mask_params)]\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return (\"mode\", \"by_channels\", \"mask\", \"mask_params\")\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.Equalize.targets_as_params","title":"<code>targets_as_params: list[str]</code>  <code>property</code> <code>readonly</code>","text":"<p>Targets used to get params dependent on targets. This is used to check input has all required targets.</p>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.Equalize.apply","title":"<code>apply (self, img, mask, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def apply(self, img: np.ndarray, mask: np.ndarray, **params: Any) -&gt; np.ndarray:\n    return fmain.equalize(img, mode=self.mode, by_channels=self.by_channels, mask=mask)\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.Equalize.get_params_dependent_on_targets","title":"<code>get_params_dependent_on_targets (self, params)</code>","text":"<p>This method is deprecated. Use <code>get_params_dependent_on_data</code> instead. Returns parameters dependent on targets. Dependent target is defined in <code>self.targets_as_params</code></p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_params_dependent_on_targets(self, params: dict[str, Any]) -&gt; dict[str, Any]:\n    if not callable(self.mask):\n        return {\"mask\": self.mask}\n\n    return {\"mask\": self.mask(**params)}\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.Equalize.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return (\"mode\", \"by_channels\", \"mask\", \"mask_params\")\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.FancyPCA","title":"<code>class  FancyPCA</code> <code>     (alpha=0.1, p=0.5, always_apply=None)                 </code>  [view source on GitHub]","text":"<p>Augment RGB image using FancyPCA from Krizhevsky's paper \"ImageNet Classification with Deep Convolutional Neural Networks\"</p> <p>Parameters:</p> Name Type Description <code>alpha</code> <code>float</code> <p>how much to perturb/scale the eigen vectors and eigenvalues. scale is samples from gaussian distribution (mu=0, sigma=alpha)</p> <p>Targets</p> <p>image</p> <p>Image types:     3-channel uint8 images only</p> <p>Credit</p> <p>http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf https://deshanadesai.github.io/notes/Fancy-PCA-with-Scikit-Image https://pixelatedbrian.github.io/2018-04-29-fancy_pca/</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>class FancyPCA(ImageOnlyTransform):\n    \"\"\"Augment RGB image using FancyPCA from Krizhevsky's paper\n    \"ImageNet Classification with Deep Convolutional Neural Networks\"\n\n    Args:\n        alpha:  how much to perturb/scale the eigen vectors and eigenvalues.\n            scale is samples from gaussian distribution (mu=0, sigma=alpha)\n\n    Targets:\n        image\n\n    Image types:\n        3-channel uint8 images only\n\n    Credit:\n        http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf\n        https://deshanadesai.github.io/notes/Fancy-PCA-with-Scikit-Image\n        https://pixelatedbrian.github.io/2018-04-29-fancy_pca/\n\n    \"\"\"\n\n    class InitSchema(BaseTransformInitSchema):\n        alpha: float = Field(default=0.1, description=\"Scale for perturbing the eigen vectors and values\", ge=0)\n\n    def __init__(self, alpha: float = 0.1, p: float = 0.5, always_apply: bool | None = None):\n        super().__init__(p=p, always_apply=always_apply)\n        self.alpha = alpha\n\n    def apply(self, img: np.ndarray, alpha: float, **params: Any) -&gt; np.ndarray:\n        return fmain.fancy_pca(img, alpha)\n\n    def get_params(self) -&gt; dict[str, float]:\n        return {\"alpha\": random.gauss(0, self.alpha)}\n\n    def get_transform_init_args_names(self) -&gt; tuple[str]:\n        return (\"alpha\",)\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.FancyPCA.apply","title":"<code>apply (self, img, alpha, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def apply(self, img: np.ndarray, alpha: float, **params: Any) -&gt; np.ndarray:\n    return fmain.fancy_pca(img, alpha)\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.FancyPCA.get_params","title":"<code>get_params (self)</code>","text":"<p>Returns parameters independent of input.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_params(self) -&gt; dict[str, float]:\n    return {\"alpha\": random.gauss(0, self.alpha)}\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.FancyPCA.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str]:\n    return (\"alpha\",)\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.FromFloat","title":"<code>class  FromFloat</code> <code>     (dtype='uint16', max_value=None, always_apply=None, p=1.0)                 </code>  [view source on GitHub]","text":"<p>Take an input array where all values should lie in the range [0, 1.0], multiply them by <code>max_value</code> and then cast the resulted value to a type specified by <code>dtype</code>. If <code>max_value</code> is None the transform will try to infer the maximum value for the data type from the <code>dtype</code> argument.</p> <p>This is the inverse transform for :class:<code>~albumentations.augmentations.transforms.ToFloat</code>.</p> <p>Parameters:</p> Name Type Description <code>max_value</code> <code>float | None</code> <p>maximum possible input value. Default: None.</p> <code>dtype</code> <code>Literal['uint8', 'uint16', 'float32', 'float64']</code> <p>data type of the output. See the <code>'Data types' page from the NumPy docs</code>_. Default: 'uint16'.</p> <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 1.0.</p> <p>Targets</p> <p>image</p> <p>Image types:     float32</p> <p>.. _'Data types' page from the NumPy docs:    https://docs.scipy.org/doc/numpy/user/basics.types.html</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>class FromFloat(ImageOnlyTransform):\n    \"\"\"Take an input array where all values should lie in the range [0, 1.0], multiply them by `max_value` and then\n    cast the resulted value to a type specified by `dtype`. If `max_value` is None the transform will try to infer\n    the maximum value for the data type from the `dtype` argument.\n\n    This is the inverse transform for :class:`~albumentations.augmentations.transforms.ToFloat`.\n\n    Args:\n        max_value: maximum possible input value. Default: None.\n        dtype: data type of the output. See the `'Data types' page from the NumPy docs`_.\n            Default: 'uint16'.\n        p: probability of applying the transform. Default: 1.0.\n\n    Targets:\n        image\n\n    Image types:\n        float32\n\n    .. _'Data types' page from the NumPy docs:\n       https://docs.scipy.org/doc/numpy/user/basics.types.html\n\n    \"\"\"\n\n    class InitSchema(BaseTransformInitSchema):\n        dtype: Literal[\"uint8\", \"uint16\", \"float32\", \"float64\"]\n        max_value: float | None = Field(default=None, description=\"Maximum possible input value.\")\n        p: ProbabilityType = 1\n\n    def __init__(\n        self,\n        dtype: Literal[\"uint8\", \"uint16\", \"float32\", \"float64\"] = \"uint16\",\n        max_value: float | None = None,\n        always_apply: bool | None = None,\n        p: float = 1.0,\n    ):\n        super().__init__(p=p, always_apply=always_apply)\n        self.dtype = np.dtype(dtype)\n        self.max_value = max_value\n\n    def apply(self, img: np.ndarray, **params: Any) -&gt; np.ndarray:\n        return fmain.from_float(img, self.dtype, self.max_value)\n\n    def get_transform_init_args(self) -&gt; dict[str, Any]:\n        return {\"dtype\": self.dtype.name, \"max_value\": self.max_value}\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.FromFloat.apply","title":"<code>apply (self, img, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def apply(self, img: np.ndarray, **params: Any) -&gt; np.ndarray:\n    return fmain.from_float(img, self.dtype, self.max_value)\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.GaussNoise","title":"<code>class  GaussNoise</code> <code>     (var_limit=(10.0, 50.0), mean=0, per_channel=True, noise_scale_factor=1, always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Apply Gaussian noise to the input image.</p> <p>Parameters:</p> Name Type Description <code>var_limit</code> <code>Union[float, tuple[float, float]]</code> <p>Variance range for noise. If var_limit is a single float, the range will be (0, var_limit). Default: (10.0, 50.0).</p> <code>mean</code> <code>float</code> <p>Mean of the noise. Default: 0</p> <code>per_channel</code> <code>bool</code> <p>If set to True, noise will be sampled for each channel independently. Otherwise, the noise will be sampled once for all channels. Faster when <code>per_channel = False</code>. Default: True</p> <code>noise_scale_factor</code> <code>float</code> <p>Scaling factor for noise generation. Value should be in the range (0, 1]. When set to 1, noise is sampled for each pixel independently. If less, noise is sampled for a smaller size and resized to fit the shape of the image. Smaller values make the transform faster. Default: 1.0.</p> <code>p</code> <code>float</code> <p>Probability of applying the transform. Default: 0.5.</p> <p>Targets</p> <p>image</p> <p>Image types:     uint8, float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>class GaussNoise(ImageOnlyTransform):\n    \"\"\"Apply Gaussian noise to the input image.\n\n    Args:\n        var_limit (Union[float, tuple[float, float]]): Variance range for noise.\n            If var_limit is a single float, the range will be (0, var_limit). Default: (10.0, 50.0).\n        mean (float): Mean of the noise. Default: 0\n        per_channel (bool): If set to True, noise will be sampled for each channel independently.\n            Otherwise, the noise will be sampled once for all channels.\n            Faster when `per_channel = False`.\n            Default: True\n        noise_scale_factor (float): Scaling factor for noise generation. Value should be in the range (0, 1].\n            When set to 1, noise is sampled for each pixel independently. If less, noise is sampled for a smaller size\n            and resized to fit the shape of the image. Smaller values make the transform faster. Default: 1.0.\n        p (float): Probability of applying the transform. Default: 0.5.\n\n    Targets:\n        image\n\n    Image types:\n        uint8, float32\n\n    \"\"\"\n\n    class InitSchema(BaseTransformInitSchema):\n        var_limit: NonNegativeFloatRangeType = Field(default=(10.0, 50.0), description=\"Variance range for noise.\")\n        mean: float = Field(default=0, description=\"Mean of the noise.\")\n        per_channel: bool = Field(default=True, description=\"Apply noise per channel.\")\n        noise_scale_factor: float = Field(gt=0, le=1)\n\n    def __init__(\n        self,\n        var_limit: ScaleFloatType = (10.0, 50.0),\n        mean: float = 0,\n        per_channel: bool = True,\n        noise_scale_factor: float = 1,\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p=p, always_apply=always_apply)\n        self.var_limit = cast(Tuple[float, float], var_limit)\n        self.mean = mean\n        self.per_channel = per_channel\n        self.noise_scale_factor = noise_scale_factor\n\n    def apply(self, img: np.ndarray, gauss: np.ndarray, **params: Any) -&gt; np.ndarray:\n        return fmain.add_noise(img, gauss)\n\n    def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, float]:\n        image = data[\"image\"] if \"image\" in data else data[\"images\"][0]\n        var = random.uniform(self.var_limit[0], self.var_limit[1])\n        sigma = math.sqrt(var)\n\n        if self.per_channel:\n            target_shape = image.shape\n            if self.noise_scale_factor == 1:\n                gauss = random_utils.normal(self.mean, sigma, target_shape)\n            else:\n                gauss = fmain.generate_approx_gaussian_noise(target_shape, self.mean, sigma, self.noise_scale_factor)\n        else:\n            target_shape = image.shape[:2]\n            if self.noise_scale_factor == 1:\n                gauss = random_utils.normal(self.mean, sigma, target_shape)\n            else:\n                gauss = fmain.generate_approx_gaussian_noise(target_shape, self.mean, sigma, self.noise_scale_factor)\n\n            if image.ndim &gt; MONO_CHANNEL_DIMENSIONS:\n                gauss = np.expand_dims(gauss, -1)\n\n        return {\"gauss\": gauss}\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return \"var_limit\", \"per_channel\", \"mean\", \"noise_scale_factor\"\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.GaussNoise.apply","title":"<code>apply (self, img, gauss, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def apply(self, img: np.ndarray, gauss: np.ndarray, **params: Any) -&gt; np.ndarray:\n    return fmain.add_noise(img, gauss)\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.GaussNoise.get_params_dependent_on_data","title":"<code>get_params_dependent_on_data (self, params, data)</code>","text":"<p>Returns parameters dependent on input.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, float]:\n    image = data[\"image\"] if \"image\" in data else data[\"images\"][0]\n    var = random.uniform(self.var_limit[0], self.var_limit[1])\n    sigma = math.sqrt(var)\n\n    if self.per_channel:\n        target_shape = image.shape\n        if self.noise_scale_factor == 1:\n            gauss = random_utils.normal(self.mean, sigma, target_shape)\n        else:\n            gauss = fmain.generate_approx_gaussian_noise(target_shape, self.mean, sigma, self.noise_scale_factor)\n    else:\n        target_shape = image.shape[:2]\n        if self.noise_scale_factor == 1:\n            gauss = random_utils.normal(self.mean, sigma, target_shape)\n        else:\n            gauss = fmain.generate_approx_gaussian_noise(target_shape, self.mean, sigma, self.noise_scale_factor)\n\n        if image.ndim &gt; MONO_CHANNEL_DIMENSIONS:\n            gauss = np.expand_dims(gauss, -1)\n\n    return {\"gauss\": gauss}\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.GaussNoise.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return \"var_limit\", \"per_channel\", \"mean\", \"noise_scale_factor\"\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.HueSaturationValue","title":"<code>class  HueSaturationValue</code> <code>     (hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Randomly change hue, saturation and value of the input image.</p> <p>Parameters:</p> Name Type Description <code>hue_shift_limit</code> <code>ScaleIntType</code> <p>range for changing hue. If hue_shift_limit is a single int, the range will be (-hue_shift_limit, hue_shift_limit). Default: (-20, 20).</p> <code>sat_shift_limit</code> <code>ScaleIntType</code> <p>range for changing saturation. If sat_shift_limit is a single int, the range will be (-sat_shift_limit, sat_shift_limit). Default: (-30, 30).</p> <code>val_shift_limit</code> <code>ScaleIntType</code> <p>range for changing value. If val_shift_limit is a single int, the range will be (-val_shift_limit, val_shift_limit). Default: (-20, 20).</p> <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 0.5.</p> <p>Targets</p> <p>image</p> <p>Image types:     uint8, float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>class HueSaturationValue(ImageOnlyTransform):\n    \"\"\"Randomly change hue, saturation and value of the input image.\n\n    Args:\n        hue_shift_limit: range for changing hue. If hue_shift_limit is a single int, the range\n            will be (-hue_shift_limit, hue_shift_limit). Default: (-20, 20).\n        sat_shift_limit: range for changing saturation. If sat_shift_limit is a single int,\n            the range will be (-sat_shift_limit, sat_shift_limit). Default: (-30, 30).\n        val_shift_limit: range for changing value. If val_shift_limit is a single int, the range\n            will be (-val_shift_limit, val_shift_limit). Default: (-20, 20).\n        p (float): probability of applying the transform. Default: 0.5.\n\n    Targets:\n        image\n\n    Image types:\n        uint8, float32\n\n    \"\"\"\n\n    class InitSchema(BaseTransformInitSchema):\n        hue_shift_limit: SymmetricRangeType = (-20, 20)\n        sat_shift_limit: SymmetricRangeType = (-30, 30)\n        val_shift_limit: SymmetricRangeType = (-20, 20)\n\n    def __init__(\n        self,\n        hue_shift_limit: ScaleIntType = 20,\n        sat_shift_limit: ScaleIntType = 30,\n        val_shift_limit: ScaleIntType = 20,\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p=p, always_apply=always_apply)\n        self.hue_shift_limit = cast(Tuple[float, float], hue_shift_limit)\n        self.sat_shift_limit = cast(Tuple[float, float], sat_shift_limit)\n        self.val_shift_limit = cast(Tuple[float, float], val_shift_limit)\n\n    def apply(\n        self,\n        img: np.ndarray,\n        hue_shift: int,\n        sat_shift: int,\n        val_shift: int,\n        **params: Any,\n    ) -&gt; np.ndarray:\n        if not is_rgb_image(img) and not is_grayscale_image(img):\n            msg = \"HueSaturationValue transformation expects 1-channel or 3-channel images.\"\n            raise TypeError(msg)\n        return fmain.shift_hsv(img, hue_shift, sat_shift, val_shift)\n\n    def get_params(self) -&gt; dict[str, float]:\n        return {\n            \"hue_shift\": random.uniform(self.hue_shift_limit[0], self.hue_shift_limit[1]),\n            \"sat_shift\": random.uniform(self.sat_shift_limit[0], self.sat_shift_limit[1]),\n            \"val_shift\": random.uniform(self.val_shift_limit[0], self.val_shift_limit[1]),\n        }\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return (\"hue_shift_limit\", \"sat_shift_limit\", \"val_shift_limit\")\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.HueSaturationValue.apply","title":"<code>apply (self, img, hue_shift, sat_shift, val_shift, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def apply(\n    self,\n    img: np.ndarray,\n    hue_shift: int,\n    sat_shift: int,\n    val_shift: int,\n    **params: Any,\n) -&gt; np.ndarray:\n    if not is_rgb_image(img) and not is_grayscale_image(img):\n        msg = \"HueSaturationValue transformation expects 1-channel or 3-channel images.\"\n        raise TypeError(msg)\n    return fmain.shift_hsv(img, hue_shift, sat_shift, val_shift)\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.HueSaturationValue.get_params","title":"<code>get_params (self)</code>","text":"<p>Returns parameters independent of input.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_params(self) -&gt; dict[str, float]:\n    return {\n        \"hue_shift\": random.uniform(self.hue_shift_limit[0], self.hue_shift_limit[1]),\n        \"sat_shift\": random.uniform(self.sat_shift_limit[0], self.sat_shift_limit[1]),\n        \"val_shift\": random.uniform(self.val_shift_limit[0], self.val_shift_limit[1]),\n    }\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.HueSaturationValue.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return (\"hue_shift_limit\", \"sat_shift_limit\", \"val_shift_limit\")\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.ISONoise","title":"<code>class  ISONoise</code> <code>     (color_shift=(0.01, 0.05), intensity=(0.1, 0.5), always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Apply camera sensor noise.</p> <p>Parameters:</p> Name Type Description <code>color_shift</code> <code>float, float</code> <p>variance range for color hue change. Measured as a fraction of 360 degree Hue angle in HLS colorspace.</p> <code>intensity</code> <code>float, float</code> <p>Multiplicative factor that control strength of color and luminace noise.</p> <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 0.5.</p> <p>Targets</p> <p>image</p> <p>Image types:     uint8, float32</p> <p>Exceptions:</p> Type Description <code>TypeError</code> <p>If the input image is not RGB.</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>class ISONoise(ImageOnlyTransform):\n    \"\"\"Apply camera sensor noise.\n\n    Args:\n        color_shift (float, float): variance range for color hue change.\n            Measured as a fraction of 360 degree Hue angle in HLS colorspace.\n        intensity ((float, float): Multiplicative factor that control strength\n            of color and luminace noise.\n        p (float): probability of applying the transform. Default: 0.5.\n\n    Targets:\n        image\n\n    Image types:\n        uint8, float32\n\n    Raises:\n        TypeError: If the input image is not RGB.\n\n    \"\"\"\n\n    class InitSchema(BaseTransformInitSchema):\n        color_shift: Annotated[tuple[float, float], AfterValidator(check_01), AfterValidator(nondecreasing)] = Field(\n            default=(0.01, 0.05),\n            description=(\n                \"Variance range for color hue change. Measured as a fraction of 360 degree Hue angle in HLS colorspace.\"\n            ),\n        )\n        intensity: Annotated[tuple[float, float], AfterValidator(check_0plus), AfterValidator(nondecreasing)] = Field(\n            default=(0.1, 0.5),\n            description=\"Multiplicative factor that control strength of color and luminance noise.\",\n        )\n\n    def __init__(\n        self,\n        color_shift: tuple[float, float] = (0.01, 0.05),\n        intensity: tuple[float, float] = (0.1, 0.5),\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p=p, always_apply=always_apply)\n        self.intensity = intensity\n        self.color_shift = color_shift\n\n    def apply(\n        self,\n        img: np.ndarray,\n        color_shift: float,\n        intensity: float,\n        random_seed: int,\n        **params: Any,\n    ) -&gt; np.ndarray:\n        return fmain.iso_noise(img, color_shift, intensity, np.random.RandomState(random_seed))\n\n    def get_params(self) -&gt; dict[str, Any]:\n        return {\n            \"color_shift\": random.uniform(self.color_shift[0], self.color_shift[1]),\n            \"intensity\": random.uniform(self.intensity[0], self.intensity[1]),\n            \"random_seed\": random_utils.get_random_seed(),\n        }\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, str]:\n        return \"intensity\", \"color_shift\"\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.ISONoise.apply","title":"<code>apply (self, img, color_shift, intensity, random_seed, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def apply(\n    self,\n    img: np.ndarray,\n    color_shift: float,\n    intensity: float,\n    random_seed: int,\n    **params: Any,\n) -&gt; np.ndarray:\n    return fmain.iso_noise(img, color_shift, intensity, np.random.RandomState(random_seed))\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.ISONoise.get_params","title":"<code>get_params (self)</code>","text":"<p>Returns parameters independent of input.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_params(self) -&gt; dict[str, Any]:\n    return {\n        \"color_shift\": random.uniform(self.color_shift[0], self.color_shift[1]),\n        \"intensity\": random.uniform(self.intensity[0], self.intensity[1]),\n        \"random_seed\": random_utils.get_random_seed(),\n    }\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.ISONoise.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, str]:\n    return \"intensity\", \"color_shift\"\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.ImageCompression","title":"<code>class  ImageCompression</code> <code>     (quality_lower=None, quality_upper=None, compression_type=&lt;ImageCompressionType.JPEG: 0&gt;, quality_range=(99, 100), always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Decreases image quality by Jpeg, WebP compression of an image.</p> <p>Parameters:</p> Name Type Description <code>quality_range</code> <code>tuple[int, int]</code> <p>tuple of bounds on the image quality i.e. (quality_lower, quality_upper). Both values should be in [1, 100] range.</p> <code>compression_type</code> <code>ImageCompressionType</code> <p>should be ImageCompressionType.JPEG or ImageCompressionType.WEBP. Default: ImageCompressionType.JPEG</p> <p>Targets</p> <p>image</p> <p>Image types:     uint8, float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>class ImageCompression(ImageOnlyTransform):\n    \"\"\"Decreases image quality by Jpeg, WebP compression of an image.\n\n    Args:\n        quality_range: tuple of bounds on the image quality i.e. (quality_lower, quality_upper).\n            Both values should be in [1, 100] range.\n        compression_type (ImageCompressionType): should be ImageCompressionType.JPEG or ImageCompressionType.WEBP.\n            Default: ImageCompressionType.JPEG\n\n    Targets:\n        image\n\n    Image types:\n        uint8, float32\n\n    \"\"\"\n\n    class InitSchema(BaseTransformInitSchema):\n        quality_range: Annotated[tuple[int, int], AfterValidator(check_1plus), AfterValidator(nondecreasing)] = (\n            99,\n            100,\n        )\n\n        quality_lower: int | None = Field(\n            default=None,\n            description=\"Lower bound on the image quality\",\n            ge=1,\n            le=100,\n        )\n        quality_upper: int | None = Field(\n            default=None,\n            description=\"Upper bound on the image quality\",\n            ge=1,\n            le=100,\n        )\n        compression_type: ImageCompressionType = Field(\n            default=ImageCompressionType.JPEG,\n            description=\"Image compression format\",\n        )\n\n        @model_validator(mode=\"after\")\n        def validate_ranges(self) -&gt; Self:\n            # Update the quality_range based on the non-None values of quality_lower and quality_upper\n            if self.quality_lower is not None or self.quality_upper is not None:\n                if self.quality_lower is not None:\n                    warn(\n                        \"`quality_lower` is deprecated. Use `quality_range` as tuple\"\n                        \" (quality_lower, quality_upper) instead.\",\n                        DeprecationWarning,\n                        stacklevel=2,\n                    )\n                if self.quality_upper is not None:\n                    warn(\n                        \"`quality_upper` is deprecated. Use `quality_range` as tuple\"\n                        \" (quality_lower, quality_upper) instead.\",\n                        DeprecationWarning,\n                        stacklevel=2,\n                    )\n                lower = self.quality_lower if self.quality_lower is not None else self.quality_range[0]\n                upper = self.quality_upper if self.quality_upper is not None else self.quality_range[1]\n                self.quality_range = (lower, upper)\n                # Clear the deprecated individual quality settings\n                self.quality_lower = None\n                self.quality_upper = None\n\n            # Validate the quality_range\n            if not (1 &lt;= self.quality_range[0] &lt;= MAX_JPEG_QUALITY and 1 &lt;= self.quality_range[1] &lt;= MAX_JPEG_QUALITY):\n                raise ValueError(f\"Quality range values should be within [1, {MAX_JPEG_QUALITY}] range.\")\n\n            return self\n\n    def __init__(\n        self,\n        quality_lower: int | None = None,\n        quality_upper: int | None = None,\n        compression_type: ImageCompressionType = ImageCompressionType.JPEG,\n        quality_range: tuple[int, int] = (99, 100),\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p, always_apply)\n        self.quality_range = quality_range\n        self.compression_type = compression_type\n\n    def apply(self, img: np.ndarray, quality: int, image_type: Literal[\".jpg\", \".webp\"], **params: Any) -&gt; np.ndarray:\n        if img.ndim != MONO_CHANNEL_DIMENSIONS and img.shape[-1] not in (1, 3, 4):\n            msg = \"ImageCompression transformation expects 1, 3 or 4 channel images.\"\n            raise TypeError(msg)\n        return fmain.image_compression(img, quality, image_type)\n\n    def get_params(self) -&gt; dict[str, int | str]:\n        if self.compression_type == ImageCompressionType.JPEG:\n            image_type = \".jpg\"\n        elif self.compression_type == ImageCompressionType.WEBP:\n            image_type = \".webp\"\n        else:\n            raise ValueError(f\"Unknown image compression type: {self.compression_type}\")\n\n        return {\n            \"quality\": random.randint(self.quality_range[0], self.quality_range[1]),\n            \"image_type\": image_type,\n        }\n\n    def get_transform_init_args(self) -&gt; dict[str, Any]:\n        return {\n            \"quality_range\": self.quality_range,\n            \"compression_type\": self.compression_type.value,\n        }\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.ImageCompression.apply","title":"<code>apply (self, img, quality, image_type, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def apply(self, img: np.ndarray, quality: int, image_type: Literal[\".jpg\", \".webp\"], **params: Any) -&gt; np.ndarray:\n    if img.ndim != MONO_CHANNEL_DIMENSIONS and img.shape[-1] not in (1, 3, 4):\n        msg = \"ImageCompression transformation expects 1, 3 or 4 channel images.\"\n        raise TypeError(msg)\n    return fmain.image_compression(img, quality, image_type)\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.ImageCompression.get_params","title":"<code>get_params (self)</code>","text":"<p>Returns parameters independent of input.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_params(self) -&gt; dict[str, int | str]:\n    if self.compression_type == ImageCompressionType.JPEG:\n        image_type = \".jpg\"\n    elif self.compression_type == ImageCompressionType.WEBP:\n        image_type = \".webp\"\n    else:\n        raise ValueError(f\"Unknown image compression type: {self.compression_type}\")\n\n    return {\n        \"quality\": random.randint(self.quality_range[0], self.quality_range[1]),\n        \"image_type\": image_type,\n    }\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.InvertImg","title":"<code>class  InvertImg</code> <code> </code>  [view source on GitHub]","text":"<p>Invert the input image by subtracting pixel values from max values of the image types, i.e., 255 for uint8 and 1.0 for float32.</p> <p>Parameters:</p> Name Type Description <code>p</code> <p>probability of applying the transform. Default: 0.5.</p> <p>Targets</p> <p>image</p> <p>Image types:     uint8, float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>class InvertImg(ImageOnlyTransform):\n    \"\"\"Invert the input image by subtracting pixel values from max values of the image types,\n    i.e., 255 for uint8 and 1.0 for float32.\n\n    Args:\n        p: probability of applying the transform. Default: 0.5.\n\n    Targets:\n        image\n\n    Image types:\n        uint8, float32\n\n    \"\"\"\n\n    def apply(self, img: np.ndarray, **params: Any) -&gt; np.ndarray:\n        return fmain.invert(img)\n\n    def get_transform_init_args_names(self) -&gt; tuple[()]:\n        return ()\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.InvertImg.apply","title":"<code>apply (self, img, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def apply(self, img: np.ndarray, **params: Any) -&gt; np.ndarray:\n    return fmain.invert(img)\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.InvertImg.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[()]:\n    return ()\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.Lambda","title":"<code>class  Lambda</code> <code>     (image=None, mask=None, keypoint=None, bbox=None, global_label=None, name=None, always_apply=None, p=1.0)                 </code>  [view source on GitHub]","text":"<p>A flexible transformation class for using user-defined transformation functions per targets. Function signature must include **kwargs to accept optional arguments like interpolation method, image size, etc:</p> <p>Parameters:</p> Name Type Description <code>image</code> <code>Callable[..., Any] | None</code> <p>Image transformation function.</p> <code>mask</code> <code>Callable[..., Any] | None</code> <p>Mask transformation function.</p> <code>keypoint</code> <code>Callable[..., Any] | None</code> <p>Keypoint transformation function.</p> <code>bbox</code> <code>Callable[..., Any] | None</code> <p>BBox transformation function.</p> <code>global_label</code> <code>Callable[..., Any] | None</code> <p>Global label transformation function.</p> <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 1.0.</p> <p>Targets</p> <p>image, mask, bboxes, keypoints, global_label</p> <p>Image types:     Any</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>class Lambda(NoOp):\n    \"\"\"A flexible transformation class for using user-defined transformation functions per targets.\n    Function signature must include **kwargs to accept optional arguments like interpolation method, image size, etc:\n\n    Args:\n        image: Image transformation function.\n        mask: Mask transformation function.\n        keypoint: Keypoint transformation function.\n        bbox: BBox transformation function.\n        global_label: Global label transformation function.\n        p: probability of applying the transform. Default: 1.0.\n\n    Targets:\n        image, mask, bboxes, keypoints, global_label\n\n    Image types:\n        Any\n\n    \"\"\"\n\n    def __init__(\n        self,\n        image: Callable[..., Any] | None = None,\n        mask: Callable[..., Any] | None = None,\n        keypoint: Callable[..., Any] | None = None,\n        bbox: Callable[..., Any] | None = None,\n        global_label: Callable[..., Any] | None = None,\n        name: str | None = None,\n        always_apply: bool | None = None,\n        p: float = 1.0,\n    ):\n        super().__init__(p, always_apply)\n\n        self.name = name\n        self.custom_apply_fns = {\n            target_name: fmain.noop for target_name in (\"image\", \"mask\", \"keypoint\", \"bbox\", \"global_label\")\n        }\n        for target_name, custom_apply_fn in {\n            \"image\": image,\n            \"mask\": mask,\n            \"keypoint\": keypoint,\n            \"bbox\": bbox,\n            \"global_label\": global_label,\n        }.items():\n            if custom_apply_fn is not None:\n                if isinstance(custom_apply_fn, LambdaType) and custom_apply_fn.__name__ == \"&lt;lambda&gt;\":\n                    warnings.warn(\n                        \"Using lambda is incompatible with multiprocessing. \"\n                        \"Consider using regular functions or partial().\",\n                        stacklevel=2,\n                    )\n\n                self.custom_apply_fns[target_name] = custom_apply_fn\n\n    def apply(self, img: np.ndarray, **params: Any) -&gt; np.ndarray:\n        fn = self.custom_apply_fns[\"image\"]\n        return fn(img, **params)\n\n    def apply_to_mask(self, mask: np.ndarray, **params: Any) -&gt; np.ndarray:\n        fn = self.custom_apply_fns[\"mask\"]\n        return fn(mask, **params)\n\n    def apply_to_bbox(self, bbox: BoxInternalType, **params: Any) -&gt; BoxInternalType:\n        fn = self.custom_apply_fns[\"bbox\"]\n        return fn(bbox, **params)\n\n    def apply_to_keypoint(self, keypoint: KeypointInternalType, **params: Any) -&gt; KeypointInternalType:\n        fn = self.custom_apply_fns[\"keypoint\"]\n        return fn(keypoint, **params)\n\n    def apply_to_global_label(self, label: np.ndarray, **params: Any) -&gt; np.ndarray:\n        fn = self.custom_apply_fns[\"global_label\"]\n        return fn(label, **params)\n\n    @classmethod\n    def is_serializable(cls) -&gt; bool:\n        return False\n\n    def to_dict_private(self) -&gt; dict[str, Any]:\n        if self.name is None:\n            msg = (\n                \"To make a Lambda transform serializable you should provide the `name` argument, \"\n                \"e.g. `Lambda(name='my_transform', image=&lt;some func&gt;, ...)`.\"\n            )\n            raise ValueError(msg)\n        return {\"__class_fullname__\": self.get_class_fullname(), \"__name__\": self.name}\n\n    def __repr__(self) -&gt; str:\n        state = {\"name\": self.name}\n        state.update(self.custom_apply_fns.items())  # type: ignore[arg-type]\n        state.update(self.get_base_init_args())\n        return f\"{self.__class__.__name__}({format_args(state)})\"\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.Lambda.__init__","title":"<code>__init__ (self, image=None, mask=None, keypoint=None, bbox=None, global_label=None, name=None, always_apply=None, p=1.0)</code>  <code>special</code>","text":"<p>Initialize self.  See help(type(self)) for accurate signature.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def __init__(\n    self,\n    image: Callable[..., Any] | None = None,\n    mask: Callable[..., Any] | None = None,\n    keypoint: Callable[..., Any] | None = None,\n    bbox: Callable[..., Any] | None = None,\n    global_label: Callable[..., Any] | None = None,\n    name: str | None = None,\n    always_apply: bool | None = None,\n    p: float = 1.0,\n):\n    super().__init__(p, always_apply)\n\n    self.name = name\n    self.custom_apply_fns = {\n        target_name: fmain.noop for target_name in (\"image\", \"mask\", \"keypoint\", \"bbox\", \"global_label\")\n    }\n    for target_name, custom_apply_fn in {\n        \"image\": image,\n        \"mask\": mask,\n        \"keypoint\": keypoint,\n        \"bbox\": bbox,\n        \"global_label\": global_label,\n    }.items():\n        if custom_apply_fn is not None:\n            if isinstance(custom_apply_fn, LambdaType) and custom_apply_fn.__name__ == \"&lt;lambda&gt;\":\n                warnings.warn(\n                    \"Using lambda is incompatible with multiprocessing. \"\n                    \"Consider using regular functions or partial().\",\n                    stacklevel=2,\n                )\n\n            self.custom_apply_fns[target_name] = custom_apply_fn\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.Lambda.apply","title":"<code>apply (self, img, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def apply(self, img: np.ndarray, **params: Any) -&gt; np.ndarray:\n    fn = self.custom_apply_fns[\"image\"]\n    return fn(img, **params)\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.Morphological","title":"<code>class  Morphological</code> <code>     (scale=(2, 3), operation='dilation', always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Apply a morphological operation (dilation or erosion) to an image, with particular value for enhancing document scans.</p> <p>Morphological operations modify the structure of the image. Dilation expands the white (foreground) regions in a binary or grayscale image, while erosion shrinks them. These operations are beneficial in document processing, for example: - Dilation helps in closing up gaps within text or making thin lines thicker,     enhancing legibility for OCR (Optical Character Recognition). - Erosion can remove small white noise and detach connected objects,     making the structure of larger objects more pronounced.</p> <p>Parameters:</p> Name Type Description <code>scale</code> <code>int or tuple/list of int</code> <p>Specifies the size of the structuring element (kernel) used for the operation. - If an integer is provided, a square kernel of that size will be used. - If a tuple or list is provided, it should contain two integers representing the minimum     and maximum sizes for the dilation kernel.</p> <code>operation</code> <code>str</code> <p>The morphological operation to apply. Options are 'dilation' or 'erosion'. Default is 'dilation'.</p> <code>p</code> <code>float</code> <p>The probability of applying this transformation. Default is 0.5.</p> <p>Targets</p> <p>image, mask</p> <p>Image types:     uint8, float32</p> <p>Reference</p> <p>https://github.com/facebookresearch/nougat</p> <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; import albumentations as A\n&gt;&gt;&gt; transform = A.Compose([\n&gt;&gt;&gt;     A.Morphological(scale=(2, 3), operation='dilation', p=0.5)\n&gt;&gt;&gt; ])\n&gt;&gt;&gt; image = transform(image=image)[\"image\"]\n</code></pre> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>class Morphological(DualTransform):\n    \"\"\"Apply a morphological operation (dilation or erosion) to an image,\n    with particular value for enhancing document scans.\n\n    Morphological operations modify the structure of the image.\n    Dilation expands the white (foreground) regions in a binary or grayscale image, while erosion shrinks them.\n    These operations are beneficial in document processing, for example:\n    - Dilation helps in closing up gaps within text or making thin lines thicker,\n        enhancing legibility for OCR (Optical Character Recognition).\n    - Erosion can remove small white noise and detach connected objects,\n        making the structure of larger objects more pronounced.\n\n    Args:\n        scale (int or tuple/list of int): Specifies the size of the structuring element (kernel) used for the operation.\n            - If an integer is provided, a square kernel of that size will be used.\n            - If a tuple or list is provided, it should contain two integers representing the minimum\n                and maximum sizes for the dilation kernel.\n        operation (str, optional): The morphological operation to apply. Options are 'dilation' or 'erosion'.\n            Default is 'dilation'.\n        p (float, optional): The probability of applying this transformation. Default is 0.5.\n\n    Targets:\n        image, mask\n\n    Image types:\n        uint8, float32\n\n    Reference:\n        https://github.com/facebookresearch/nougat\n\n    Example:\n        &gt;&gt;&gt; import albumentations as A\n        &gt;&gt;&gt; transform = A.Compose([\n        &gt;&gt;&gt;     A.Morphological(scale=(2, 3), operation='dilation', p=0.5)\n        &gt;&gt;&gt; ])\n        &gt;&gt;&gt; image = transform(image=image)[\"image\"]\n    \"\"\"\n\n    _targets = (Targets.IMAGE, Targets.MASK)\n\n    class InitSchema(BaseTransformInitSchema):\n        scale: OnePlusIntRangeType = (2, 3)\n        operation: MorphologyMode = \"dilation\"\n\n    def __init__(\n        self,\n        scale: ScaleIntType = (2, 3),\n        operation: MorphologyMode = \"dilation\",\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p, always_apply)\n        self.scale = cast(Tuple[int, int], scale)\n        self.operation = operation\n\n    def apply(self, img: np.ndarray, kernel: tuple[int, int], **params: Any) -&gt; np.ndarray:\n        return fmain.morphology(img, kernel, self.operation)\n\n    def apply_to_mask(self, mask: np.ndarray, kernel: tuple[int, int], **params: Any) -&gt; np.ndarray:\n        return fmain.morphology(mask, kernel, self.operation)\n\n    def get_params(self) -&gt; dict[str, float]:\n        return {\n            \"kernel\": cv2.getStructuringElement(cv2.MORPH_ELLIPSE, self.scale),\n        }\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return (\"scale\", \"operation\")\n\n    @property\n    def targets(self) -&gt; dict[str, Callable[..., Any]]:\n        return {\n            \"image\": self.apply,\n            \"mask\": self.apply_to_mask,\n            \"masks\": self.apply_to_masks,\n        }\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.Morphological.apply","title":"<code>apply (self, img, kernel, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def apply(self, img: np.ndarray, kernel: tuple[int, int], **params: Any) -&gt; np.ndarray:\n    return fmain.morphology(img, kernel, self.operation)\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.Morphological.get_params","title":"<code>get_params (self)</code>","text":"<p>Returns parameters independent of input.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_params(self) -&gt; dict[str, float]:\n    return {\n        \"kernel\": cv2.getStructuringElement(cv2.MORPH_ELLIPSE, self.scale),\n    }\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.Morphological.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return (\"scale\", \"operation\")\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.MultiplicativeNoise","title":"<code>class  MultiplicativeNoise</code> <code>     (multiplier=(0.9, 1.1), per_channel=None, elementwise=False, always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Multiply image by a random number or array of numbers.</p> <p>Parameters:</p> Name Type Description <code>multiplier</code> <code>ScaleFloatType</code> <p>If a single float, the image will be multiplied by this number. If a tuple of floats, the multiplier will be a random number in the range <code>[multiplier[0], multiplier[1])</code>. Default: (0.9, 1.1).</p> <code>elementwise</code> <code>bool</code> <p>If <code>False</code>, multiply all pixels in the image by a single random value sampled once. If <code>True</code>, multiply image pixels by values that are pixelwise randomly sampled. Default: False.</p> <code>p</code> <code>float</code> <p>Probability of applying the transform. Default: 0.5.</p> <p>Targets</p> <p>image</p> <p>Image types:     uint8, np.float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>class MultiplicativeNoise(ImageOnlyTransform):\n    \"\"\"Multiply image by a random number or array of numbers.\n\n    Args:\n        multiplier: If a single float, the image will be multiplied by this number.\n            If a tuple of floats, the multiplier will be a random number in the range `[multiplier[0], multiplier[1])`.\n            Default: (0.9, 1.1).\n        elementwise: If `False`, multiply all pixels in the image by a single random value sampled once.\n            If `True`, multiply image pixels by values that are pixelwise randomly sampled. Default: False.\n        p: Probability of applying the transform. Default: 0.5.\n\n    Targets:\n        image\n\n    Image types:\n        uint8, np.float32\n\n    \"\"\"\n\n    class InitSchema(BaseTransformInitSchema):\n        multiplier: Annotated[tuple[float, float], AfterValidator(check_0plus), AfterValidator(nondecreasing)] = (\n            0.9,\n            1.1,\n        )\n        per_channel: bool | None = Field(\n            default=False,\n            description=\"Apply multiplier per channel.\",\n            deprecated=\"Does not have any effect. Will be removed in future releases.\",\n        )\n        elementwise: bool = Field(default=False, description=\"Apply multiplier element-wise to pixels.\")\n\n    def __init__(\n        self,\n        multiplier: ScaleFloatType = (0.9, 1.1),\n        per_channel: bool | None = None,\n        elementwise: bool = False,\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p=p, always_apply=always_apply)\n        self.multiplier = cast(Tuple[float, float], multiplier)\n        self.elementwise = elementwise\n\n    def apply(\n        self,\n        img: np.ndarray,\n        multiplier: float | np.ndarray,\n        **kwargs: Any,\n    ) -&gt; np.ndarray:\n        return multiply(img, multiplier)\n\n    def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n        if self.multiplier[0] == self.multiplier[1]:\n            return {\"multiplier\": self.multiplier[0]}\n\n        img = data[\"image\"] if \"image\" in data else data[\"images\"][0]\n        shape = img.shape if self.elementwise else get_num_channels(img)\n\n        multiplier = random_utils.uniform(self.multiplier[0], self.multiplier[1], shape).astype(np.float32)\n\n        return {\"multiplier\": multiplier}\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return \"multiplier\", \"elementwise\"\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.MultiplicativeNoise.apply","title":"<code>apply (self, img, multiplier, **kwargs)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def apply(\n    self,\n    img: np.ndarray,\n    multiplier: float | np.ndarray,\n    **kwargs: Any,\n) -&gt; np.ndarray:\n    return multiply(img, multiplier)\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.MultiplicativeNoise.get_params_dependent_on_data","title":"<code>get_params_dependent_on_data (self, params, data)</code>","text":"<p>Returns parameters dependent on input.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n    if self.multiplier[0] == self.multiplier[1]:\n        return {\"multiplier\": self.multiplier[0]}\n\n    img = data[\"image\"] if \"image\" in data else data[\"images\"][0]\n    shape = img.shape if self.elementwise else get_num_channels(img)\n\n    multiplier = random_utils.uniform(self.multiplier[0], self.multiplier[1], shape).astype(np.float32)\n\n    return {\"multiplier\": multiplier}\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.MultiplicativeNoise.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return \"multiplier\", \"elementwise\"\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.Normalize","title":"<code>class  Normalize</code> <code>     (mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, normalization='standard', always_apply=None, p=1.0)                 </code>  [view source on GitHub]","text":"<p>Applies various normalization techniques to an image. The specific normalization technique can be selected     with the <code>normalization</code> parameter.</p> <p>Standard normalization is applied using the formula:     <code>img = (img - mean * max_pixel_value) / (std * max_pixel_value)</code>.     Other normalization techniques adjust the image based on global or per-channel statistics,     or scale pixel values to a specified range.</p> <p>Parameters:</p> Name Type Description <code>mean</code> <code>ColorType | None</code> <p>Mean values for standard normalization. For \"standard\" normalization, the default values are ImageNet mean values: (0.485, 0.456, 0.406). For \"inception\" normalization, use mean values of (0.5, 0.5, 0.5).</p> <code>std</code> <code>ColorType | None</code> <p>Standard deviation values for standard normalization. For \"standard\" normalization, the default values are ImageNet standard deviation :(0.229, 0.224, 0.225). For \"inception\" normalization, use standard deviation values of (0.5, 0.5, 0.5).</p> <code>max_pixel_value</code> <code>float | None</code> <p>Maximum possible pixel value, used for scaling in standard normalization. Defaults to 255.0.</p> <code>normalization</code> <code>Literal[\"standard\", \"image\", \"image_per_channel\", \"min_max\", \"min_max_per_channel\", \"inception\"]) Specifies the normalization technique to apply. Defaults to \"standard\". - \"standard\"</code> <p>Applies the formula <code>(img - mean * max_pixel_value) / (std * max_pixel_value)</code>.     The default mean and std are based on ImageNet. - \"image\": Normalizes the whole image based on its global mean and standard deviation. - \"image_per_channel\": Normalizes the image per channel based on each channel's mean and standard deviation. - \"min_max\": Scales the image pixel values to a [0, 1] range based on the global     minimum and maximum pixel values. - \"min_max_per_channel\": Scales each channel of the image pixel values to a [0, 1]     range based on the per-channel minimum and maximum pixel values.</p> <code>p</code> <code>float</code> <p>Probability of applying the transform. Defaults to 1.0.</p> <p>Targets</p> <p>image</p> <p>Image types:     uint8, float32</p> <p>Note</p> <p>For \"standard\" normalization, <code>mean</code>, <code>std</code>, and <code>max_pixel_value</code> must be provided. For other normalization types, these parameters are ignored.</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>class Normalize(ImageOnlyTransform):\n    \"\"\"Applies various normalization techniques to an image. The specific normalization technique can be selected\n        with the `normalization` parameter.\n\n    Standard normalization is applied using the formula:\n        `img = (img - mean * max_pixel_value) / (std * max_pixel_value)`.\n        Other normalization techniques adjust the image based on global or per-channel statistics,\n        or scale pixel values to a specified range.\n\n    Args:\n        mean (ColorType | None): Mean values for standard normalization.\n            For \"standard\" normalization, the default values are ImageNet mean values: (0.485, 0.456, 0.406).\n            For \"inception\" normalization, use mean values of (0.5, 0.5, 0.5).\n        std (ColorType | None): Standard deviation values for standard normalization.\n            For \"standard\" normalization, the default values are ImageNet standard deviation :(0.229, 0.224, 0.225).\n            For \"inception\" normalization, use standard deviation values of (0.5, 0.5, 0.5).\n        max_pixel_value (float | None): Maximum possible pixel value, used for scaling in standard normalization.\n            Defaults to 255.0.\n        normalization (Literal[\"standard\", \"image\", \"image_per_channel\", \"min_max\", \"min_max_per_channel\", \"inception\"])\n            Specifies the normalization technique to apply. Defaults to \"standard\".\n            - \"standard\": Applies the formula `(img - mean * max_pixel_value) / (std * max_pixel_value)`.\n                The default mean and std are based on ImageNet.\n            - \"image\": Normalizes the whole image based on its global mean and standard deviation.\n            - \"image_per_channel\": Normalizes the image per channel based on each channel's mean and standard deviation.\n            - \"min_max\": Scales the image pixel values to a [0, 1] range based on the global\n                minimum and maximum pixel values.\n            - \"min_max_per_channel\": Scales each channel of the image pixel values to a [0, 1]\n                range based on the per-channel minimum and maximum pixel values.\n\n        p (float): Probability of applying the transform. Defaults to 1.0.\n\n    Targets:\n        image\n\n    Image types:\n        uint8, float32\n\n    Note:\n        For \"standard\" normalization, `mean`, `std`, and `max_pixel_value` must be provided.\n        For other normalization types, these parameters are ignored.\n    \"\"\"\n\n    class InitSchema(BaseTransformInitSchema):\n        mean: ColorType | None = Field(\n            default=(0.485, 0.456, 0.406),\n            description=\"Mean values for normalization, defaulting to ImageNet mean values.\",\n        )\n        std: ColorType | None = Field(\n            default=(0.229, 0.224, 0.225),\n            description=\"Standard deviation values for normalization, defaulting to ImageNet std values.\",\n        )\n        max_pixel_value: float | None = Field(default=255.0, description=\"Maximum possible pixel value.\")\n        normalization: Literal[\n            \"standard\",\n            \"image\",\n            \"image_per_channel\",\n            \"min_max\",\n            \"min_max_per_channel\",\n        ] = \"standard\"\n        p: ProbabilityType = 1\n\n        @model_validator(mode=\"after\")\n        def validate_normalization(self) -&gt; Self:\n            if (\n                self.mean is None\n                or self.std is None\n                or self.max_pixel_value is None\n                and self.normalization == \"standard\"\n            ):\n                raise ValueError(\"mean, std, and max_pixel_value must be provided for standard normalization.\")\n            return self\n\n    def __init__(\n        self,\n        mean: ColorType | None = (0.485, 0.456, 0.406),\n        std: ColorType | None = (0.229, 0.224, 0.225),\n        max_pixel_value: float | None = 255.0,\n        normalization: Literal[\"standard\", \"image\", \"image_per_channel\", \"min_max\", \"min_max_per_channel\"] = \"standard\",\n        always_apply: bool | None = None,\n        p: float = 1.0,\n    ):\n        super().__init__(p=p, always_apply=always_apply)\n        self.mean = mean\n        self.mean_np = np.array(mean, dtype=np.float32) * max_pixel_value\n        self.std = std\n        self.denominator = np.reciprocal(np.array(std, dtype=np.float32) * max_pixel_value)\n        self.max_pixel_value = max_pixel_value\n        if normalization not in {\"standard\", \"image\", \"image_per_channel\", \"min_max\", \"min_max_per_channel\"}:\n            raise ValueError(\n                f\"Error during Normalize initialization. Unknown normalization type: {normalization}\",\n            )\n        self.normalization = normalization\n\n    def apply(self, img: np.ndarray, **params: Any) -&gt; np.ndarray:\n        if self.normalization == \"standard\":\n            return normalize(\n                img,\n                self.mean_np,\n                self.denominator,\n            )\n        return normalize_per_image(img, self.normalization)\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return \"mean\", \"std\", \"max_pixel_value\", \"normalization\"\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.Normalize.apply","title":"<code>apply (self, img, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def apply(self, img: np.ndarray, **params: Any) -&gt; np.ndarray:\n    if self.normalization == \"standard\":\n        return normalize(\n            img,\n            self.mean_np,\n            self.denominator,\n        )\n    return normalize_per_image(img, self.normalization)\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.Normalize.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return \"mean\", \"std\", \"max_pixel_value\", \"normalization\"\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.PixelDropout","title":"<code>class  PixelDropout</code> <code>     (dropout_prob=0.01, per_channel=False, drop_value=0, mask_drop_value=None, always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Set pixels to 0 with some probability.</p> <p>Parameters:</p> Name Type Description <code>dropout_prob</code> <code>float</code> <p>pixel drop probability. Default: 0.01</p> <code>per_channel</code> <code>bool</code> <p>if set to <code>True</code> drop mask will be sampled for each channel, otherwise the same mask will be sampled for all channels. Default: False</p> <code>drop_value</code> <code>number or sequence of numbers or None</code> <p>Value that will be set in dropped place. If set to None value will be sampled randomly, default ranges will be used:     - uint8 - [0, 255]     - uint16 - [0, 65535]     - uint32 - [0, 4294967295]     - float, double - [0, 1] Default: 0</p> <code>mask_drop_value</code> <code>number or sequence of numbers or None</code> <p>Value that will be set in dropped place in masks. If set to None masks will be unchanged. Default: 0</p> <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 0.5.</p> <p>Targets</p> <p>image, mask</p> <p>Image types:     any</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>class PixelDropout(DualTransform):\n    \"\"\"Set pixels to 0 with some probability.\n\n    Args:\n        dropout_prob (float): pixel drop probability. Default: 0.01\n        per_channel (bool): if set to `True` drop mask will be sampled for each channel,\n            otherwise the same mask will be sampled for all channels. Default: False\n        drop_value (number or sequence of numbers or None): Value that will be set in dropped place.\n            If set to None value will be sampled randomly, default ranges will be used:\n                - uint8 - [0, 255]\n                - uint16 - [0, 65535]\n                - uint32 - [0, 4294967295]\n                - float, double - [0, 1]\n            Default: 0\n        mask_drop_value (number or sequence of numbers or None): Value that will be set in dropped place in masks.\n            If set to None masks will be unchanged. Default: 0\n        p (float): probability of applying the transform. Default: 0.5.\n\n    Targets:\n        image, mask\n    Image types:\n        any\n\n    \"\"\"\n\n    class InitSchema(BaseTransformInitSchema):\n        dropout_prob: ProbabilityType = 0.01\n        per_channel: bool = Field(default=False, description=\"Sample drop mask per channel.\")\n        drop_value: ScaleFloatType | None = Field(\n            default=0,\n            description=\"Value to set in dropped pixels. None for random sampling.\",\n        )\n        mask_drop_value: ScaleFloatType | None = Field(\n            default=None,\n            description=\"Value to set in dropped pixels in masks. None to leave masks unchanged.\",\n        )\n\n        @model_validator(mode=\"after\")\n        def validate_mask_drop_value(self) -&gt; Self:\n            if self.mask_drop_value is not None and self.per_channel:\n                msg = \"PixelDropout supports mask only with per_channel=False.\"\n                raise ValueError(msg)\n            return self\n\n    _targets = (Targets.IMAGE, Targets.MASK)\n\n    def __init__(\n        self,\n        dropout_prob: float = 0.01,\n        per_channel: bool = False,\n        drop_value: ScaleFloatType | None = 0,\n        mask_drop_value: ScaleFloatType | None = None,\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p=p, always_apply=always_apply)\n        self.dropout_prob = dropout_prob\n        self.per_channel = per_channel\n        self.drop_value = drop_value\n        self.mask_drop_value = mask_drop_value\n\n    def apply(\n        self,\n        img: np.ndarray,\n        drop_mask: np.ndarray,\n        drop_value: float | Sequence[float],\n        **params: Any,\n    ) -&gt; np.ndarray:\n        return fmain.pixel_dropout(img, drop_mask, drop_value)\n\n    def apply_to_mask(self, mask: np.ndarray, drop_mask: np.ndarray, **params: Any) -&gt; np.ndarray:\n        if self.mask_drop_value is None:\n            return mask\n\n        if mask.ndim == MONO_CHANNEL_DIMENSIONS:\n            drop_mask = np.squeeze(drop_mask)\n\n        return fmain.pixel_dropout(mask, drop_mask, self.mask_drop_value)\n\n    def apply_to_bbox(self, bbox: BoxInternalType, **params: Any) -&gt; BoxInternalType:\n        return bbox\n\n    def apply_to_keypoint(self, keypoint: KeypointInternalType, **params: Any) -&gt; KeypointInternalType:\n        return keypoint\n\n    def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n        img = data[\"image\"] if \"image\" in data else data[\"images\"][0]\n        shape = img.shape if self.per_channel else img.shape[:2]\n\n        rnd = np.random.RandomState(random.randint(0, 1 &lt;&lt; 31))\n        # Use choice to create boolean matrix, if we will use binomial after that we will need type conversion\n        drop_mask = rnd.choice([True, False], shape, p=[self.dropout_prob, 1 - self.dropout_prob])\n\n        drop_value: float | Sequence[float] | np.ndarray\n        if drop_mask.ndim != img.ndim:\n            drop_mask = np.expand_dims(drop_mask, -1)\n        if self.drop_value is None:\n            drop_shape = 1 if is_grayscale_image(img) else int(img.shape[-1])\n\n            if img.dtype in (np.uint8, np.uint16, np.uint32):\n                drop_value = rnd.randint(0, int(MAX_VALUES_BY_DTYPE[img.dtype]), drop_shape, img.dtype)\n            elif img.dtype in [np.float32, np.double]:\n                drop_value = rnd.uniform(0, 1, drop_shape).astype(img.dtype)\n            else:\n                raise ValueError(f\"Unsupported dtype: {img.dtype}\")\n        else:\n            drop_value = self.drop_value\n\n        return {\"drop_mask\": drop_mask, \"drop_value\": drop_value}\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, str, str, str]:\n        return (\"dropout_prob\", \"per_channel\", \"drop_value\", \"mask_drop_value\")\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.PixelDropout.apply","title":"<code>apply (self, img, drop_mask, drop_value, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def apply(\n    self,\n    img: np.ndarray,\n    drop_mask: np.ndarray,\n    drop_value: float | Sequence[float],\n    **params: Any,\n) -&gt; np.ndarray:\n    return fmain.pixel_dropout(img, drop_mask, drop_value)\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.PixelDropout.get_params_dependent_on_data","title":"<code>get_params_dependent_on_data (self, params, data)</code>","text":"<p>Returns parameters dependent on input.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n    img = data[\"image\"] if \"image\" in data else data[\"images\"][0]\n    shape = img.shape if self.per_channel else img.shape[:2]\n\n    rnd = np.random.RandomState(random.randint(0, 1 &lt;&lt; 31))\n    # Use choice to create boolean matrix, if we will use binomial after that we will need type conversion\n    drop_mask = rnd.choice([True, False], shape, p=[self.dropout_prob, 1 - self.dropout_prob])\n\n    drop_value: float | Sequence[float] | np.ndarray\n    if drop_mask.ndim != img.ndim:\n        drop_mask = np.expand_dims(drop_mask, -1)\n    if self.drop_value is None:\n        drop_shape = 1 if is_grayscale_image(img) else int(img.shape[-1])\n\n        if img.dtype in (np.uint8, np.uint16, np.uint32):\n            drop_value = rnd.randint(0, int(MAX_VALUES_BY_DTYPE[img.dtype]), drop_shape, img.dtype)\n        elif img.dtype in [np.float32, np.double]:\n            drop_value = rnd.uniform(0, 1, drop_shape).astype(img.dtype)\n        else:\n            raise ValueError(f\"Unsupported dtype: {img.dtype}\")\n    else:\n        drop_value = self.drop_value\n\n    return {\"drop_mask\": drop_mask, \"drop_value\": drop_value}\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.PixelDropout.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, str, str, str]:\n    return (\"dropout_prob\", \"per_channel\", \"drop_value\", \"mask_drop_value\")\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.PlanckianJitter","title":"<code>class  PlanckianJitter</code> <code>     (mode='blackbody', temperature_limit=None, sampling_method='uniform', always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Randomly jitter the image illuminant along the Planckian locus.</p> <p>Physics-based color augmentation creates realistic variations in chromaticity, simulating illumination changes in a scene.</p> <p>Parameters:</p> Name Type Description <code>mode</code> <code>Literal[\"blackbody\", \"cied\"]</code> <p>The mode of the transformation. <code>blackbody</code> simulates blackbody radiation, and <code>cied</code> uses the CIED illuminant series.</p> <code>temperature_limit</code> <code>tuple[int, int]</code> <p>Temperature range to sample from. For <code>blackbody</code> mode, the range should be within <code>[3000K, 15000K]</code>. For \"cied\" mode, the range should be within <code>[4000K, 15000K]</code>. Range should include white temperature <code>6000</code> Higher temperatures produce cooler (bluish) images. If not defined, it defaults to: - <code>[3000, 15000]</code> for <code>blackbody</code> mode - <code>[4000, 15000]</code> for <code>cied</code> mode</p> <code>p</code> <code>float</code> <p>Probability of applying the transform. Defaults to 0.5.</p> <code>sampling_method</code> <code>Literal[\"uniform\", \"gaussian\"]</code> <p>Method to sample the temperature. \"uniform\" samples uniformly across the range, while \"gaussian\" samples from a Gaussian distribution.</p> <code>p</code> <code>float</code> <p>Probability of applying the transform. Defaults to 0.5.</p> <p>If <code>temperature_limit</code> is not defined, it defaults to:     - <code>[3000, 15000]</code> for <code>blackbody</code> mode     - <code>[4000, 15000]</code> for <code>cied</code> mode</p> <p>Targets</p> <p>image</p> <p>Image types:     uint8, float32</p> <p>References</p> <ul> <li>https://github.com/TheZino/PlanckianJitter</li> <li>https://arxiv.org/pdf/2202.07993.pdf</li> </ul> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>class PlanckianJitter(ImageOnlyTransform):\n    r\"\"\"Randomly jitter the image illuminant along the Planckian locus.\n\n    Physics-based color augmentation creates realistic variations in chromaticity, simulating illumination changes\n    in a scene.\n\n    Args:\n        mode (Literal[\"blackbody\", \"cied\"]): The mode of the transformation. `blackbody` simulates blackbody radiation,\n            and `cied` uses the CIED illuminant series.\n        temperature_limit (tuple[int, int]): Temperature range to sample from. For `blackbody` mode, the range should\n            be within `[3000K, 15000K]`. For \"cied\" mode, the range should be within `[4000K, 15000K]`. Range should\n            include white temperature `6000`\n            Higher temperatures produce cooler (bluish) images. If not defined, it defaults to:\n            - `[3000, 15000]` for `blackbody` mode\n            - `[4000, 15000]` for `cied` mode\n        p (float): Probability of applying the transform. Defaults to 0.5.\n        sampling_method (Literal[\"uniform\", \"gaussian\"]): Method to sample the temperature.\n            \"uniform\" samples uniformly across the range, while \"gaussian\" samples from a Gaussian distribution.\n        p (float): Probability of applying the transform. Defaults to 0.5.\n\n    If `temperature_limit` is not defined, it defaults to:\n        - `[3000, 15000]` for `blackbody` mode\n        - `[4000, 15000]` for `cied` mode\n\n    Targets:\n        image\n\n    Image types:\n        uint8, float32\n\n    References:\n        - https://github.com/TheZino/PlanckianJitter\n        - https://arxiv.org/pdf/2202.07993.pdf\n\n    \"\"\"\n\n    class InitSchema(BaseTransformInitSchema):\n        mode: PlanckianJitterMode = \"blackbody\"\n        temperature_limit: Annotated[tuple[int, int], AfterValidator(nondecreasing)] | None = None\n        sampling_method: Literal[\"uniform\", \"gaussian\"] = \"uniform\"\n\n        @model_validator(mode=\"after\")\n        def validate_temperature(self) -&gt; Self:\n            max_temp = int(PLANKIAN_JITTER_CONST[\"MAX_TEMP\"])\n\n            if self.temperature_limit is None:\n                if self.mode == \"blackbody\":\n                    self.temperature_limit = int(PLANKIAN_JITTER_CONST[\"MIN_BLACKBODY_TEMP\"]), max_temp\n                elif self.mode == \"cied\":\n                    self.temperature_limit = int(PLANKIAN_JITTER_CONST[\"MIN_CIED_TEMP\"]), max_temp\n            else:\n                if self.mode == \"blackbody\" and (\n                    min(self.temperature_limit) &lt; PLANKIAN_JITTER_CONST[\"MIN_BLACKBODY_TEMP\"]\n                    or max(self.temperature_limit) &gt; max_temp\n                ):\n                    raise ValueError(\"Temperature limits for blackbody should be in [3000, 15000] range\")\n                if self.mode == \"cied\" and (\n                    min(self.temperature_limit) &lt; PLANKIAN_JITTER_CONST[\"MIN_CIED_TEMP\"]\n                    or max(self.temperature_limit) &gt; max_temp\n                ):\n                    raise ValueError(\"Temperature limits for CIED should be in [4000, 15000] range\")\n\n                if not self.temperature_limit[0] &lt;= PLANKIAN_JITTER_CONST[\"WHITE_TEMP\"] &lt;= self.temperature_limit[1]:\n                    raise ValueError(\"White temperature should be within the temperature limits\")\n\n            return self\n\n    def __init__(\n        self,\n        mode: PlanckianJitterMode = \"blackbody\",\n        temperature_limit: tuple[int, int] | None = None,\n        sampling_method: Literal[\"uniform\", \"gaussian\"] = \"uniform\",\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ) -&gt; None:\n        super().__init__(p=p, always_apply=always_apply)\n\n        self.mode = mode\n        self.temperature_limit = cast(Tuple[int, int], temperature_limit)\n        self.sampling_method = sampling_method\n\n    def apply(self, img: np.ndarray, temperature: int, **params: Any) -&gt; np.ndarray:\n        if not is_rgb_image(img):\n            raise TypeError(\"PlanckianJitter transformation expects 3-channel images.\")\n        return fmain.planckian_jitter(img, temperature, mode=self.mode)\n\n    def get_params(self) -&gt; dict[str, Any]:\n        sampling_prob_boundary = PLANKIAN_JITTER_CONST[\"SAMPLING_TEMP_PROB\"]\n        sampling_temp_boundary = PLANKIAN_JITTER_CONST[\"WHITE_TEMP\"]\n\n        if self.sampling_method == \"uniform\":\n            # Split into 2 cases to avoid selecting cold temperatures (&gt;6000) too often\n            if random.random() &lt; sampling_prob_boundary:\n                temperature = (\n                    random.uniform(\n                        self.temperature_limit[0],\n                        sampling_temp_boundary,\n                    ),\n                )\n            else:\n                temperature = (\n                    random.uniform(\n                        sampling_temp_boundary,\n                        self.temperature_limit[1],\n                    ),\n                )\n        elif self.sampling_method == \"gaussian\":\n            # Sample values from asymmetric gaussian distribution\n            if random.random() &lt; sampling_prob_boundary:\n                # Left side\n                shift = np.abs(\n                    random.gauss(\n                        0,\n                        np.abs(sampling_temp_boundary - self.temperature_limit[0]) / 3,\n                    ),\n                )\n            else:\n                # Right side\n                shift = -np.abs(\n                    random.gauss(\n                        0,\n                        np.abs(self.temperature_limit[1] - sampling_temp_boundary) / 3,\n                    ),\n                )\n\n            temperature = sampling_temp_boundary - shift\n        else:\n            raise ValueError(f\"Unknown sampling method: {self.sampling_method}\")\n\n        return {\"temperature\": int(np.clip(temperature, self.temperature_limit[0], self.temperature_limit[1]))}\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return \"mode\", \"temperature_limit\", \"sampling_method\"\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.PlanckianJitter.apply","title":"<code>apply (self, img, temperature, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def apply(self, img: np.ndarray, temperature: int, **params: Any) -&gt; np.ndarray:\n    if not is_rgb_image(img):\n        raise TypeError(\"PlanckianJitter transformation expects 3-channel images.\")\n    return fmain.planckian_jitter(img, temperature, mode=self.mode)\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.PlanckianJitter.get_params","title":"<code>get_params (self)</code>","text":"<p>Returns parameters independent of input.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_params(self) -&gt; dict[str, Any]:\n    sampling_prob_boundary = PLANKIAN_JITTER_CONST[\"SAMPLING_TEMP_PROB\"]\n    sampling_temp_boundary = PLANKIAN_JITTER_CONST[\"WHITE_TEMP\"]\n\n    if self.sampling_method == \"uniform\":\n        # Split into 2 cases to avoid selecting cold temperatures (&gt;6000) too often\n        if random.random() &lt; sampling_prob_boundary:\n            temperature = (\n                random.uniform(\n                    self.temperature_limit[0],\n                    sampling_temp_boundary,\n                ),\n            )\n        else:\n            temperature = (\n                random.uniform(\n                    sampling_temp_boundary,\n                    self.temperature_limit[1],\n                ),\n            )\n    elif self.sampling_method == \"gaussian\":\n        # Sample values from asymmetric gaussian distribution\n        if random.random() &lt; sampling_prob_boundary:\n            # Left side\n            shift = np.abs(\n                random.gauss(\n                    0,\n                    np.abs(sampling_temp_boundary - self.temperature_limit[0]) / 3,\n                ),\n            )\n        else:\n            # Right side\n            shift = -np.abs(\n                random.gauss(\n                    0,\n                    np.abs(self.temperature_limit[1] - sampling_temp_boundary) / 3,\n                ),\n            )\n\n        temperature = sampling_temp_boundary - shift\n    else:\n        raise ValueError(f\"Unknown sampling method: {self.sampling_method}\")\n\n    return {\"temperature\": int(np.clip(temperature, self.temperature_limit[0], self.temperature_limit[1]))}\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.PlanckianJitter.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return \"mode\", \"temperature_limit\", \"sampling_method\"\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.Posterize","title":"<code>class  Posterize</code> <code>     (num_bits=4, always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Reduce the number of bits for each color channel.</p> <p>Parameters:</p> Name Type Description <code>num_bits</code> <code>int, int) or int,       or list of ints [r, g, b],       or list of ints [[r1, r1], [g1, g2], [b1, b2]]</code> <p>number of high bits. If num_bits is a single value, the range will be [num_bits, num_bits]. Must be in range [0, 8]. Default: 4.</p> <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 0.5.</p> <p>Targets: image</p> <p>Image types:     uint8</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>class Posterize(ImageOnlyTransform):\n    \"\"\"Reduce the number of bits for each color channel.\n\n    Args:\n        num_bits ((int, int) or int,\n                  or list of ints [r, g, b],\n                  or list of ints [[r1, r1], [g1, g2], [b1, b2]]): number of high bits.\n            If num_bits is a single value, the range will be [num_bits, num_bits].\n            Must be in range [0, 8]. Default: 4.\n        p: probability of applying the transform. Default: 0.5.\n\n    Targets:\n    image\n\n    Image types:\n        uint8\n\n    \"\"\"\n\n    class InitSchema(BaseTransformInitSchema):\n        num_bits: Annotated[\n            int | tuple[int, int] | tuple[int, int, int],\n            Field(default=4, description=\"Number of high bits\"),\n        ]\n\n        @field_validator(\"num_bits\")\n        @classmethod\n        def validate_num_bits(cls, num_bits: Any) -&gt; tuple[int, int] | list[tuple[int, int]]:\n            if isinstance(num_bits, int):\n                return cast(Tuple[int, int], to_tuple(num_bits, num_bits))\n            if isinstance(num_bits, Sequence) and len(num_bits) == NUM_BITS_ARRAY_LENGTH:\n                return [cast(Tuple[int, int], to_tuple(i, 0)) for i in num_bits]\n            return cast(Tuple[int, int], to_tuple(num_bits, 0))\n\n    def __init__(\n        self,\n        num_bits: int | tuple[int, int] | tuple[int, int, int] = 4,\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p=p, always_apply=always_apply)\n        self.num_bits = cast(Union[Tuple[int, ...], List[Tuple[int, ...]]], num_bits)\n\n    def apply(self, img: np.ndarray, num_bits: int, **params: Any) -&gt; np.ndarray:\n        return fmain.posterize(img, num_bits)\n\n    def get_params(self) -&gt; dict[str, Any]:\n        if len(self.num_bits) == NUM_BITS_ARRAY_LENGTH:\n            return {\"num_bits\": [random.randint(int(i[0]), int(i[1])) for i in self.num_bits]}  # type: ignore[index]\n        num_bits = self.num_bits\n        return {\"num_bits\": random.randint(int(num_bits[0]), int(num_bits[1]))}  # type: ignore[arg-type]\n\n    def get_transform_init_args_names(self) -&gt; tuple[str]:\n        return (\"num_bits\",)\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.Posterize.apply","title":"<code>apply (self, img, num_bits, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def apply(self, img: np.ndarray, num_bits: int, **params: Any) -&gt; np.ndarray:\n    return fmain.posterize(img, num_bits)\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.Posterize.get_params","title":"<code>get_params (self)</code>","text":"<p>Returns parameters independent of input.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_params(self) -&gt; dict[str, Any]:\n    if len(self.num_bits) == NUM_BITS_ARRAY_LENGTH:\n        return {\"num_bits\": [random.randint(int(i[0]), int(i[1])) for i in self.num_bits]}  # type: ignore[index]\n    num_bits = self.num_bits\n    return {\"num_bits\": random.randint(int(num_bits[0]), int(num_bits[1]))}  # type: ignore[arg-type]\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.Posterize.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str]:\n    return (\"num_bits\",)\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.RGBShift","title":"<code>class  RGBShift</code> <code>     (r_shift_limit=(-20, 20), g_shift_limit=(-20, 20), b_shift_limit=(-20, 20), always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Randomly shift values for each channel of the input RGB image.</p> <p>Parameters:</p> Name Type Description <code>r_shift_limit</code> <code>ScaleIntType</code> <p>range for changing values for the red channel. If r_shift_limit is a single int, the range will be (-r_shift_limit, r_shift_limit). Default: (-20, 20).</p> <code>g_shift_limit</code> <code>ScaleIntType</code> <p>range for changing values for the green channel. If g_shift_limit is a single int, the range  will be (-g_shift_limit, g_shift_limit). Default: (-20, 20).</p> <code>b_shift_limit</code> <code>ScaleIntType</code> <p>range for changing values for the blue channel. If b_shift_limit is a single int, the range will be (-b_shift_limit, b_shift_limit). Default: (-20, 20).</p> <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 0.5.</p> <p>Targets</p> <p>image</p> <p>Image types:     uint8, float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>class RGBShift(ImageOnlyTransform):\n    \"\"\"Randomly shift values for each channel of the input RGB image.\n\n    Args:\n        r_shift_limit: range for changing values for the red channel. If r_shift_limit is a single\n            int, the range will be (-r_shift_limit, r_shift_limit). Default: (-20, 20).\n        g_shift_limit: range for changing values for the green channel. If g_shift_limit is a\n            single int, the range  will be (-g_shift_limit, g_shift_limit). Default: (-20, 20).\n        b_shift_limit: range for changing values for the blue channel. If b_shift_limit is a single\n            int, the range will be (-b_shift_limit, b_shift_limit). Default: (-20, 20).\n        p: probability of applying the transform. Default: 0.5.\n\n    Targets:\n        image\n\n    Image types:\n        uint8, float32\n\n    \"\"\"\n\n    class InitSchema(BaseTransformInitSchema):\n        r_shift_limit: SymmetricRangeType = (-20, 20)\n        g_shift_limit: SymmetricRangeType = (-20, 20)\n        b_shift_limit: SymmetricRangeType = (-20, 20)\n\n    def __init__(\n        self,\n        r_shift_limit: ScaleIntType = (-20, 20),\n        g_shift_limit: ScaleIntType = (-20, 20),\n        b_shift_limit: ScaleIntType = (-20, 20),\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p=p, always_apply=always_apply)\n        self.r_shift_limit = cast(Tuple[float, float], r_shift_limit)\n        self.g_shift_limit = cast(Tuple[float, float], g_shift_limit)\n        self.b_shift_limit = cast(Tuple[float, float], b_shift_limit)\n\n    def apply(self, img: np.ndarray, shift: np.ndarray, **params: Any) -&gt; np.ndarray:\n        if not is_rgb_image(img):\n            msg = \"RGBShift transformation expects 3-channel images.\"\n            raise TypeError(msg)\n\n        return albucore.add_vector(img, shift)\n\n    def get_params(self) -&gt; dict[str, Any]:\n        return {\n            \"shift\": np.array(\n                [\n                    random.uniform(self.r_shift_limit[0], self.r_shift_limit[1]),\n                    random.uniform(self.g_shift_limit[0], self.g_shift_limit[1]),\n                    random.uniform(self.b_shift_limit[0], self.b_shift_limit[1]),\n                ],\n            ),\n        }\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return \"r_shift_limit\", \"g_shift_limit\", \"b_shift_limit\"\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.RGBShift.apply","title":"<code>apply (self, img, shift, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def apply(self, img: np.ndarray, shift: np.ndarray, **params: Any) -&gt; np.ndarray:\n    if not is_rgb_image(img):\n        msg = \"RGBShift transformation expects 3-channel images.\"\n        raise TypeError(msg)\n\n    return albucore.add_vector(img, shift)\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.RGBShift.get_params","title":"<code>get_params (self)</code>","text":"<p>Returns parameters independent of input.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_params(self) -&gt; dict[str, Any]:\n    return {\n        \"shift\": np.array(\n            [\n                random.uniform(self.r_shift_limit[0], self.r_shift_limit[1]),\n                random.uniform(self.g_shift_limit[0], self.g_shift_limit[1]),\n                random.uniform(self.b_shift_limit[0], self.b_shift_limit[1]),\n            ],\n        ),\n    }\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.RGBShift.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return \"r_shift_limit\", \"g_shift_limit\", \"b_shift_limit\"\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.RandomBrightnessContrast","title":"<code>class  RandomBrightnessContrast</code> <code>     (brightness_limit=(-0.2, 0.2), contrast_limit=(-0.2, 0.2), brightness_by_max=True, always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Randomly change brightness and contrast of the input image.</p> <p>Parameters:</p> Name Type Description <code>brightness_limit</code> <code>ScaleFloatType</code> <p>factor range for changing brightness. If limit is a single float, the range will be (-limit, limit). Default: (-0.2, 0.2).</p> <code>contrast_limit</code> <code>ScaleFloatType</code> <p>factor range for changing contrast. If limit is a single float, the range will be (-limit, limit). Default: (-0.2, 0.2).</p> <code>brightness_by_max</code> <code>bool</code> <p>If True adjust contrast by image dtype maximum, else adjust contrast by image mean.</p> <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 0.5.</p> <p>Targets</p> <p>image</p> <p>Image types:     uint8, float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>class RandomBrightnessContrast(ImageOnlyTransform):\n    \"\"\"Randomly change brightness and contrast of the input image.\n\n    Args:\n        brightness_limit: factor range for changing brightness.\n            If limit is a single float, the range will be (-limit, limit). Default: (-0.2, 0.2).\n        contrast_limit: factor range for changing contrast.\n            If limit is a single float, the range will be (-limit, limit). Default: (-0.2, 0.2).\n        brightness_by_max: If True adjust contrast by image dtype maximum,\n            else adjust contrast by image mean.\n        p: probability of applying the transform. Default: 0.5.\n\n    Targets:\n        image\n\n    Image types:\n        uint8, float32\n\n    \"\"\"\n\n    class InitSchema(BaseTransformInitSchema):\n        brightness_limit: SymmetricRangeType = (-0.2, 0.2)\n        contrast_limit: SymmetricRangeType = (-0.2, 0.2)\n        brightness_by_max: bool = Field(default=True, description=\"Adjust brightness by image dtype maximum if True.\")\n\n    def __init__(\n        self,\n        brightness_limit: ScaleFloatType = (-0.2, 0.2),\n        contrast_limit: ScaleFloatType = (-0.2, 0.2),\n        brightness_by_max: bool = True,\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p=p, always_apply=always_apply)\n        self.brightness_limit = cast(Tuple[float, float], brightness_limit)\n        self.contrast_limit = cast(Tuple[float, float], contrast_limit)\n        self.brightness_by_max = brightness_by_max\n\n    def apply(self, img: np.ndarray, alpha: float, beta: float, **params: Any) -&gt; np.ndarray:\n        return fmain.brightness_contrast_adjust(img, alpha, beta, self.brightness_by_max)\n\n    def get_params(self) -&gt; dict[str, float]:\n        return {\n            \"alpha\": 1.0 + random.uniform(self.contrast_limit[0], self.contrast_limit[1]),\n            \"beta\": 0.0 + random.uniform(self.brightness_limit[0], self.brightness_limit[1]),\n        }\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, str, str]:\n        return (\"brightness_limit\", \"contrast_limit\", \"brightness_by_max\")\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.RandomBrightnessContrast.apply","title":"<code>apply (self, img, alpha, beta, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def apply(self, img: np.ndarray, alpha: float, beta: float, **params: Any) -&gt; np.ndarray:\n    return fmain.brightness_contrast_adjust(img, alpha, beta, self.brightness_by_max)\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.RandomBrightnessContrast.get_params","title":"<code>get_params (self)</code>","text":"<p>Returns parameters independent of input.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_params(self) -&gt; dict[str, float]:\n    return {\n        \"alpha\": 1.0 + random.uniform(self.contrast_limit[0], self.contrast_limit[1]),\n        \"beta\": 0.0 + random.uniform(self.brightness_limit[0], self.brightness_limit[1]),\n    }\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.RandomBrightnessContrast.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, str, str]:\n    return (\"brightness_limit\", \"contrast_limit\", \"brightness_by_max\")\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.RandomFog","title":"<code>class  RandomFog</code> <code>     (fog_coef_lower=None, fog_coef_upper=None, alpha_coef=0.08, fog_coef_range=(0.3, 1), always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Simulates fog for the image.</p> <p>Parameters:</p> Name Type Description <code>fog_coef_range</code> <code>tuple</code> <p>tuple of bounds on the fog intensity coefficient (fog_coef_lower, fog_coef_upper). Default: (0.3, 1).</p> <code>alpha_coef</code> <code>float</code> <p>Transparency of the fog circles. Should be in [0, 1] range. Default: 0.08.</p> <code>p</code> <code>float</code> <p>Probability of applying the transform. Default: 0.5.</p> <p>Targets</p> <p>image</p> <p>Image types:     uint8, float32</p> <p>Reference</p> <p>https://github.com/UjjwalSaxena/Automold--Road-Augmentation-Library</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>class RandomFog(ImageOnlyTransform):\n    \"\"\"Simulates fog for the image.\n\n    Args:\n        fog_coef_range (tuple): tuple of bounds on the fog intensity coefficient (fog_coef_lower, fog_coef_upper).\n            Default: (0.3, 1).\n        alpha_coef (float): Transparency of the fog circles. Should be in [0, 1] range. Default: 0.08.\n        p (float): Probability of applying the transform. Default: 0.5.\n\n    Targets:\n        image\n\n    Image types:\n        uint8, float32\n\n    Reference:\n        https://github.com/UjjwalSaxena/Automold--Road-Augmentation-Library\n    \"\"\"\n\n    class InitSchema(BaseTransformInitSchema):\n        fog_coef_lower: float | None = Field(\n            default=None,\n            description=\"Lower limit for fog intensity coefficient\",\n            ge=0,\n            le=1,\n        )\n        fog_coef_upper: float | None = Field(\n            default=None,\n            description=\"Upper limit for fog intensity coefficient\",\n            ge=0,\n            le=1,\n        )\n        fog_coef_range: Annotated[tuple[float, float], AfterValidator(check_01), AfterValidator(nondecreasing)] = (\n            0.3,\n            1,\n        )\n\n        alpha_coef: float = Field(default=0.08, description=\"Transparency of the fog circles\", ge=0, le=1)\n\n        @model_validator(mode=\"after\")\n        def validate_fog_coefficients(self) -&gt; Self:\n            if self.fog_coef_lower is not None:\n                warn(\"`fog_coef_lower` is deprecated, use `fog_coef_range` instead.\", DeprecationWarning, stacklevel=2)\n            if self.fog_coef_upper is not None:\n                warn(\"`fog_coef_upper` is deprecated, use `fog_coef_range` instead.\", DeprecationWarning, stacklevel=2)\n\n            lower = self.fog_coef_lower if self.fog_coef_lower is not None else self.fog_coef_range[0]\n            upper = self.fog_coef_upper if self.fog_coef_upper is not None else self.fog_coef_range[1]\n            self.fog_coef_range = (lower, upper)\n\n            self.fog_coef_lower = None\n            self.fog_coef_upper = None\n\n            return self\n\n    def __init__(\n        self,\n        fog_coef_lower: float | None = None,\n        fog_coef_upper: float | None = None,\n        alpha_coef: float = 0.08,\n        fog_coef_range: tuple[float, float] = (0.3, 1),\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p=p, always_apply=always_apply)\n        self.fog_coef_range = fog_coef_range\n        self.alpha_coef = alpha_coef\n\n    def apply(\n        self,\n        img: np.ndarray,\n        fog_coef: np.ndarray,\n        haze_list: list[tuple[int, int]],\n        **params: Any,\n    ) -&gt; np.ndarray:\n        return fmain.add_fog(img, fog_coef, self.alpha_coef, haze_list)\n\n    def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n        fog_coef = random.uniform(*self.fog_coef_range)\n\n        height, width = imshape = params[\"shape\"][:2]\n\n        hw = max(1, int(width // 3 * fog_coef))\n\n        haze_list = []\n        midx = width // 2 - 2 * hw\n        midy = height // 2 - hw\n        index = 1\n\n        while midx &gt; -hw or midy &gt; -hw:\n            for _ in range(hw // 10 * index):\n                x = random.randint(midx, width - midx - hw)\n                y = random.randint(midy, height - midy - hw)\n                haze_list.append((x, y))\n\n            midx -= 3 * hw * width // sum(imshape)\n            midy -= 3 * hw * height // sum(imshape)\n            index += 1\n\n        return {\"haze_list\": haze_list, \"fog_coef\": fog_coef}\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, str]:\n        return \"fog_coef_range\", \"alpha_coef\"\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.RandomFog.apply","title":"<code>apply (self, img, fog_coef, haze_list, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def apply(\n    self,\n    img: np.ndarray,\n    fog_coef: np.ndarray,\n    haze_list: list[tuple[int, int]],\n    **params: Any,\n) -&gt; np.ndarray:\n    return fmain.add_fog(img, fog_coef, self.alpha_coef, haze_list)\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.RandomFog.get_params_dependent_on_data","title":"<code>get_params_dependent_on_data (self, params, data)</code>","text":"<p>Returns parameters dependent on input.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n    fog_coef = random.uniform(*self.fog_coef_range)\n\n    height, width = imshape = params[\"shape\"][:2]\n\n    hw = max(1, int(width // 3 * fog_coef))\n\n    haze_list = []\n    midx = width // 2 - 2 * hw\n    midy = height // 2 - hw\n    index = 1\n\n    while midx &gt; -hw or midy &gt; -hw:\n        for _ in range(hw // 10 * index):\n            x = random.randint(midx, width - midx - hw)\n            y = random.randint(midy, height - midy - hw)\n            haze_list.append((x, y))\n\n        midx -= 3 * hw * width // sum(imshape)\n        midy -= 3 * hw * height // sum(imshape)\n        index += 1\n\n    return {\"haze_list\": haze_list, \"fog_coef\": fog_coef}\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.RandomFog.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, str]:\n    return \"fog_coef_range\", \"alpha_coef\"\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.RandomGamma","title":"<code>class  RandomGamma</code> <code>     (gamma_limit=(80, 120), always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Applies random gamma correction to an image as a form of data augmentation.</p> <p>This class adjusts the luminance of an image by applying gamma correction with a randomly selected gamma value from a specified range. Gamma correction can simulate various lighting conditions, potentially enhancing model generalization.</p> <p>Attributes:</p> Name Type Description <code>gamma_limit</code> <code>Union[int, tuple[int, int]]</code> <p>The range for gamma adjustment. If <code>gamma_limit</code> is a single int, the range will be interpreted as (-gamma_limit, gamma_limit), defining how much to adjust the image's gamma. Default is (80, 120).</p> <code>always_apply</code> <p>Depreciated. Use <code>p=1</code> instead.</p> <code>p</code> <code>float</code> <p>The probability that the transform will be applied. Default is 0.5.</p> <p>Targets</p> <p>image</p> <p>Image types:     uint8, float32</p> <p>Reference</p> <p>https://en.wikipedia.org/wiki/Gamma_correction</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>class RandomGamma(ImageOnlyTransform):\n    \"\"\"Applies random gamma correction to an image as a form of data augmentation.\n\n    This class adjusts the luminance of an image by applying gamma correction with a randomly\n    selected gamma value from a specified range. Gamma correction can simulate various lighting\n    conditions, potentially enhancing model generalization.\n\n    Attributes:\n        gamma_limit (Union[int, tuple[int, int]]): The range for gamma adjustment. If `gamma_limit` is a single\n            int, the range will be interpreted as (-gamma_limit, gamma_limit), defining how much\n            to adjust the image's gamma. Default is (80, 120).\n        always_apply: Depreciated. Use `p=1` instead.\n        p (float): The probability that the transform will be applied. Default is 0.5.\n\n    Targets:\n        image\n\n    Image types:\n        uint8, float32\n\n    Reference:\n         https://en.wikipedia.org/wiki/Gamma_correction\n\n    \"\"\"\n\n    class InitSchema(BaseTransformInitSchema):\n        gamma_limit: OnePlusFloatRangeType = (80, 120)\n\n    def __init__(\n        self,\n        gamma_limit: ScaleIntType = (80, 120),\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p, always_apply)\n        self.gamma_limit = cast(Tuple[float, float], gamma_limit)\n\n    def apply(self, img: np.ndarray, gamma: float, **params: Any) -&gt; np.ndarray:\n        return fmain.gamma_transform(img, gamma=gamma)\n\n    def get_params(self) -&gt; dict[str, float]:\n        return {\"gamma\": random.uniform(self.gamma_limit[0], self.gamma_limit[1]) / 100.0}\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return (\"gamma_limit\",)\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.RandomGamma.apply","title":"<code>apply (self, img, gamma, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def apply(self, img: np.ndarray, gamma: float, **params: Any) -&gt; np.ndarray:\n    return fmain.gamma_transform(img, gamma=gamma)\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.RandomGamma.get_params","title":"<code>get_params (self)</code>","text":"<p>Returns parameters independent of input.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_params(self) -&gt; dict[str, float]:\n    return {\"gamma\": random.uniform(self.gamma_limit[0], self.gamma_limit[1]) / 100.0}\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.RandomGamma.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return (\"gamma_limit\",)\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.RandomGravel","title":"<code>class  RandomGravel</code> <code>     (gravel_roi=(0.1, 0.4, 0.9, 0.9), number_of_patches=2, always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Add gravels.</p> <p>Parameters:</p> Name Type Description <code>gravel_roi</code> <code>tuple[float, float, float, float]</code> <p>(top-left x, top-left y, bottom-right x, bottom right y). Should be in [0, 1] range</p> <code>number_of_patches</code> <code>int</code> <p>no. of gravel patches required</p> <p>Targets</p> <p>image</p> <p>Image types:     uint8, float32</p> <p>Reference</p> <p>https://github.com/UjjwalSaxena/Automold--Road-Augmentation-Library</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>class RandomGravel(ImageOnlyTransform):\n    \"\"\"Add gravels.\n\n    Args:\n        gravel_roi: (top-left x, top-left y,\n            bottom-right x, bottom right y). Should be in [0, 1] range\n        number_of_patches: no. of gravel patches required\n\n    Targets:\n        image\n\n    Image types:\n        uint8, float32\n\n    Reference:\n        https://github.com/UjjwalSaxena/Automold--Road-Augmentation-Library\n\n    \"\"\"\n\n    class InitSchema(BaseTransformInitSchema):\n        gravel_roi: tuple[float, float, float, float] = Field(\n            default=(0.1, 0.4, 0.9, 0.9),\n            description=\"Region of interest for gravel placement\",\n        )\n        number_of_patches: int = Field(default=2, description=\"Number of gravel patches\", ge=1)\n\n        @model_validator(mode=\"after\")\n        def validate_gravel_roi(self) -&gt; Self:\n            gravel_lower_x, gravel_lower_y, gravel_upper_x, gravel_upper_y = self.gravel_roi\n            if not 0 &lt;= gravel_lower_x &lt; gravel_upper_x &lt;= 1 or not 0 &lt;= gravel_lower_y &lt; gravel_upper_y &lt;= 1:\n                raise ValueError(f\"Invalid gravel_roi. Got: {self.gravel_roi}.\")\n            return self\n\n    def __init__(\n        self,\n        gravel_roi: tuple[float, float, float, float] = (0.1, 0.4, 0.9, 0.9),\n        number_of_patches: int = 2,\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p, always_apply)\n        self.gravel_roi = gravel_roi\n        self.number_of_patches = number_of_patches\n\n    def generate_gravel_patch(self, rectangular_roi: tuple[int, int, int, int]) -&gt; np.ndarray:\n        x1, y1, x2, y2 = rectangular_roi\n        area = abs((x2 - x1) * (y2 - y1))\n        count = area // 10\n        gravels = np.empty([count, 2], dtype=np.int64)\n        gravels[:, 0] = random_utils.randint(x1, x2, count)\n        gravels[:, 1] = random_utils.randint(y1, y2, count)\n        return gravels\n\n    def apply(self, img: np.ndarray, gravels_infos: list[Any], **params: Any) -&gt; np.ndarray:\n        if gravels_infos is None:\n            gravels_infos = []\n        return fmain.add_gravel(img, gravels_infos)\n\n    def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, np.ndarray]:\n        height, width = params[\"shape\"][:2]\n\n        x_min, y_min, x_max, y_max = self.gravel_roi\n        x_min = int(x_min * width)\n        x_max = int(x_max * width)\n        y_min = int(y_min * height)\n        y_max = int(y_max * height)\n\n        max_height = 200\n        max_width = 30\n\n        rectangular_rois = np.zeros([self.number_of_patches, 4], dtype=np.int64)\n        xx1 = random_utils.randint(x_min + 1, x_max, self.number_of_patches)  # xmax\n        xx2 = random_utils.randint(x_min, xx1)  # xmin\n        yy1 = random_utils.randint(y_min + 1, y_max, self.number_of_patches)  # ymax\n        yy2 = random_utils.randint(y_min, yy1)  # ymin\n\n        rectangular_rois[:, 0] = xx2\n        rectangular_rois[:, 1] = yy2\n        rectangular_rois[:, 2] = [min(tup) for tup in zip(xx1, xx2 + max_height)]\n        rectangular_rois[:, 3] = [min(tup) for tup in zip(yy1, yy2 + max_width)]\n\n        minx = []\n        maxx = []\n        miny = []\n        maxy = []\n        val = []\n        for roi in rectangular_rois:\n            gravels = self.generate_gravel_patch(roi)\n            x = gravels[:, 0]\n            y = gravels[:, 1]\n            r = random_utils.randint(1, 4, len(gravels))\n            sat = random_utils.randint(0, 255, len(gravels))\n            miny.append(np.maximum(y - r, 0))\n            maxy.append(np.minimum(y + r, y))\n            minx.append(np.maximum(x - r, 0))\n            maxx.append(np.minimum(x + r, x))\n            val.append(sat)\n\n        return {\n            \"gravels_infos\": np.stack(\n                [\n                    np.concatenate(miny),\n                    np.concatenate(maxy),\n                    np.concatenate(minx),\n                    np.concatenate(maxx),\n                    np.concatenate(val),\n                ],\n                1,\n            ),\n        }\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, str]:\n        return \"gravel_roi\", \"number_of_patches\"\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.RandomGravel.apply","title":"<code>apply (self, img, gravels_infos, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def apply(self, img: np.ndarray, gravels_infos: list[Any], **params: Any) -&gt; np.ndarray:\n    if gravels_infos is None:\n        gravels_infos = []\n    return fmain.add_gravel(img, gravels_infos)\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.RandomGravel.get_params_dependent_on_data","title":"<code>get_params_dependent_on_data (self, params, data)</code>","text":"<p>Returns parameters dependent on input.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, np.ndarray]:\n    height, width = params[\"shape\"][:2]\n\n    x_min, y_min, x_max, y_max = self.gravel_roi\n    x_min = int(x_min * width)\n    x_max = int(x_max * width)\n    y_min = int(y_min * height)\n    y_max = int(y_max * height)\n\n    max_height = 200\n    max_width = 30\n\n    rectangular_rois = np.zeros([self.number_of_patches, 4], dtype=np.int64)\n    xx1 = random_utils.randint(x_min + 1, x_max, self.number_of_patches)  # xmax\n    xx2 = random_utils.randint(x_min, xx1)  # xmin\n    yy1 = random_utils.randint(y_min + 1, y_max, self.number_of_patches)  # ymax\n    yy2 = random_utils.randint(y_min, yy1)  # ymin\n\n    rectangular_rois[:, 0] = xx2\n    rectangular_rois[:, 1] = yy2\n    rectangular_rois[:, 2] = [min(tup) for tup in zip(xx1, xx2 + max_height)]\n    rectangular_rois[:, 3] = [min(tup) for tup in zip(yy1, yy2 + max_width)]\n\n    minx = []\n    maxx = []\n    miny = []\n    maxy = []\n    val = []\n    for roi in rectangular_rois:\n        gravels = self.generate_gravel_patch(roi)\n        x = gravels[:, 0]\n        y = gravels[:, 1]\n        r = random_utils.randint(1, 4, len(gravels))\n        sat = random_utils.randint(0, 255, len(gravels))\n        miny.append(np.maximum(y - r, 0))\n        maxy.append(np.minimum(y + r, y))\n        minx.append(np.maximum(x - r, 0))\n        maxx.append(np.minimum(x + r, x))\n        val.append(sat)\n\n    return {\n        \"gravels_infos\": np.stack(\n            [\n                np.concatenate(miny),\n                np.concatenate(maxy),\n                np.concatenate(minx),\n                np.concatenate(maxx),\n                np.concatenate(val),\n            ],\n            1,\n        ),\n    }\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.RandomGravel.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, str]:\n    return \"gravel_roi\", \"number_of_patches\"\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.RandomGridShuffle","title":"<code>class  RandomGridShuffle</code> <code>     (grid=(3, 3), p=0.5, always_apply=None)                 </code>  [view source on GitHub]","text":"<p>Randomly shuffles the grid's cells on an image, mask, or keypoints, effectively rearranging patches within the image. This transformation divides the image into a grid and then permutes these grid cells based on a random mapping.</p> <p>Parameters:</p> Name Type Description <code>grid</code> <code>tuple[int, int]</code> <p>Size of the grid for splitting the image into cells. Each cell is shuffled randomly.</p> <code>p</code> <code>float</code> <p>Probability that the transform will be applied.</p> <p>Targets</p> <p>image, mask, keypoints</p> <p>Image types:     uint8, float32</p> <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; import albumentations as A\n&gt;&gt;&gt; transform = A.Compose([\n    A.RandomGridShuffle(grid=(3, 3), p=1.0)\n])\n&gt;&gt;&gt; transformed = transform(image=my_image, mask=my_mask)\n&gt;&gt;&gt; image, mask = transformed['image'], transformed['mask']\n# This will shuffle the 3x3 grid cells of `my_image` and `my_mask` randomly.\n# Mask and image are shuffled in a consistent way\n</code></pre> <p>Note</p> <p>This transform could be useful when only micro features are important for the model, and memorizing the global structure could be harmful. For example: - Identifying the type of cell phone used to take a picture based on micro artifacts generated by phone post-processing algorithms, rather than the semantic features of the photo. See more at https://ieeexplore.ieee.org/abstract/document/8622031 - Identifying stress, glucose, hydration levels based on skin images.</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>class RandomGridShuffle(DualTransform):\n    \"\"\"Randomly shuffles the grid's cells on an image, mask, or keypoints,\n    effectively rearranging patches within the image.\n    This transformation divides the image into a grid and then permutes these grid cells based on a random mapping.\n\n\n    Args:\n        grid (tuple[int, int]): Size of the grid for splitting the image into cells. Each cell is shuffled randomly.\n        p (float): Probability that the transform will be applied.\n\n    Targets:\n        image, mask, keypoints\n\n    Image types:\n        uint8, float32\n\n    Examples:\n        &gt;&gt;&gt; import albumentations as A\n        &gt;&gt;&gt; transform = A.Compose([\n            A.RandomGridShuffle(grid=(3, 3), p=1.0)\n        ])\n        &gt;&gt;&gt; transformed = transform(image=my_image, mask=my_mask)\n        &gt;&gt;&gt; image, mask = transformed['image'], transformed['mask']\n        # This will shuffle the 3x3 grid cells of `my_image` and `my_mask` randomly.\n        # Mask and image are shuffled in a consistent way\n    Note:\n        This transform could be useful when only micro features are important for the model, and memorizing\n        the global structure could be harmful. For example:\n        - Identifying the type of cell phone used to take a picture based on micro artifacts generated by\n        phone post-processing algorithms, rather than the semantic features of the photo.\n        See more at https://ieeexplore.ieee.org/abstract/document/8622031\n        - Identifying stress, glucose, hydration levels based on skin images.\n    \"\"\"\n\n    class InitSchema(BaseTransformInitSchema):\n        grid: Annotated[tuple[int, int], AfterValidator(check_1plus)] = (3, 3)\n\n    _targets = (Targets.IMAGE, Targets.MASK, Targets.KEYPOINTS)\n\n    def __init__(self, grid: tuple[int, int] = (3, 3), p: float = 0.5, always_apply: bool | None = None):\n        super().__init__(p=p, always_apply=always_apply)\n        self.grid = grid\n\n    def apply(self, img: np.ndarray, tiles: np.ndarray, mapping: list[int], **params: Any) -&gt; np.ndarray:\n        return fmain.swap_tiles_on_image(img, tiles, mapping)\n\n    def apply_to_mask(self, mask: np.ndarray, tiles: np.ndarray, mapping: list[int], **params: Any) -&gt; np.ndarray:\n        return fmain.swap_tiles_on_image(mask, tiles, mapping)\n\n    def apply_to_keypoint(\n        self,\n        keypoint: KeypointInternalType,\n        tiles: np.ndarray,\n        mapping: list[int],\n        **params: Any,\n    ) -&gt; KeypointInternalType:\n        x, y = keypoint[:2]\n\n        # Find which original tile the keypoint belongs to\n        for original_index, new_index in enumerate(mapping):\n            start_y, start_x, end_y, end_x = tiles[original_index]\n            # check if the keypoint is in this tile\n            if start_y &lt;= y &lt; end_y and start_x &lt;= x &lt; end_x:\n                # Get the new tile's coordinates\n                new_start_y, new_start_x = tiles[new_index][:2]\n\n                # Map the keypoint to the new tile's position\n                new_x = (x - start_x) + new_start_x\n                new_y = (y - start_y) + new_start_y\n\n                return (new_x, new_y, *keypoint[2:])\n\n        # If the keypoint wasn't in any tile (shouldn't happen), log a warning for debugging purposes\n        warn(\n            \"Keypoint not in any tile, returning it unchanged. This is unexpected and should be investigated.\",\n            RuntimeWarning,\n            stacklevel=2,\n        )\n        return keypoint\n\n    def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, np.ndarray]:\n        height, width = params[\"shape\"][:2]\n        random_state = random_utils.get_random_state()\n        original_tiles = fmain.split_uniform_grid(\n            (height, width),\n            self.grid,\n            random_state=random_state,\n        )\n        shape_groups = fmain.create_shape_groups(original_tiles)\n        mapping = fmain.shuffle_tiles_within_shape_groups(shape_groups, random_state=random_state)\n\n        return {\"tiles\": original_tiles, \"mapping\": mapping}\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return (\"grid\",)\n\n    @property\n    def targets(self) -&gt; dict[str, Callable[..., Any]]:\n        return {\n            \"image\": self.apply,\n            \"mask\": self.apply_to_mask,\n            \"masks\": self.apply_to_masks,\n            \"keypoints\": self.apply_to_keypoints,\n        }\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.RandomGridShuffle.apply","title":"<code>apply (self, img, tiles, mapping, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def apply(self, img: np.ndarray, tiles: np.ndarray, mapping: list[int], **params: Any) -&gt; np.ndarray:\n    return fmain.swap_tiles_on_image(img, tiles, mapping)\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.RandomGridShuffle.get_params_dependent_on_data","title":"<code>get_params_dependent_on_data (self, params, data)</code>","text":"<p>Returns parameters dependent on input.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, np.ndarray]:\n    height, width = params[\"shape\"][:2]\n    random_state = random_utils.get_random_state()\n    original_tiles = fmain.split_uniform_grid(\n        (height, width),\n        self.grid,\n        random_state=random_state,\n    )\n    shape_groups = fmain.create_shape_groups(original_tiles)\n    mapping = fmain.shuffle_tiles_within_shape_groups(shape_groups, random_state=random_state)\n\n    return {\"tiles\": original_tiles, \"mapping\": mapping}\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.RandomGridShuffle.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return (\"grid\",)\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.RandomRain","title":"<code>class  RandomRain</code> <code>     (slant_lower=None, slant_upper=None, slant_range=(-10, 10), drop_length=20, drop_width=1, drop_color=(200, 200, 200), blur_value=7, brightness_coefficient=0.7, rain_type=None, always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Adds rain effects to an image.</p> <p>Parameters:</p> Name Type Description <code>slant_range</code> <code>tuple[int, int]</code> <p>tuple of type (slant_lower, slant_upper) representing the range for rain slant angle.</p> <code>drop_length</code> <code>int</code> <p>Length of the raindrops.</p> <code>drop_width</code> <code>int</code> <p>Width of the raindrops.</p> <code>drop_color</code> <code>tuple[int, int, int]</code> <p>Color of the rain drops in RGB format.</p> <code>blur_value</code> <code>int</code> <p>Blur value for simulating rain effect. Rainy views are blurry.</p> <code>brightness_coefficient</code> <code>float</code> <p>Coefficient to adjust the brightness of the image. Rainy days are usually shady. Should be in the range (0, 1].</p> <code>rain_type</code> <code>Optional[str]</code> <p>Type of rain to simulate. One of [None, \"drizzle\", \"heavy\", \"torrential\"].</p> <p>Targets</p> <p>image</p> <p>Image types:     uint8, float32</p> <p>Reference</p> <p>https://github.com/UjjwalSaxena/Automold--Road-Augmentation-Library</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>class RandomRain(ImageOnlyTransform):\n    \"\"\"Adds rain effects to an image.\n\n    Args:\n        slant_range (tuple[int, int]): tuple of type (slant_lower, slant_upper) representing the range for\n            rain slant angle.\n        drop_length (int): Length of the raindrops.\n        drop_width (int): Width of the raindrops.\n        drop_color (tuple[int, int, int]): Color of the rain drops in RGB format.\n        blur_value (int): Blur value for simulating rain effect. Rainy views are blurry.\n        brightness_coefficient (float): Coefficient to adjust the brightness of the image.\n            Rainy days are usually shady. Should be in the range (0, 1].\n        rain_type (Optional[str]): Type of rain to simulate. One of [None, \"drizzle\", \"heavy\", \"torrential\"].\n\n\n    Targets:\n        image\n\n    Image types:\n        uint8, float32\n\n    Reference:\n        https://github.com/UjjwalSaxena/Automold--Road-Augmentation-Library\n\n    \"\"\"\n\n    class InitSchema(BaseTransformInitSchema):\n        slant_lower: int | None = Field(\n            default=None,\n            description=\"Lower bound for rain slant angle\",\n        )\n        slant_upper: int | None = Field(\n            default=None,\n            description=\"Upper bound for rain slant angle\",\n        )\n        slant_range: Annotated[tuple[float, float], AfterValidator(nondecreasing)] = Field(\n            default=(-10, 10),\n            description=\"tuple like (slant_lower, slant_upper) for rain slant angle\",\n        )\n        drop_length: int = Field(default=20, description=\"Length of raindrops\", ge=1)\n        drop_width: int = Field(default=1, description=\"Width of raindrops\", ge=1)\n        drop_color: tuple[int, int, int] = Field(default=(200, 200, 200), description=\"Color of raindrops\")\n        blur_value: int = Field(default=7, description=\"Blur value for simulating rain effect\", ge=1)\n        brightness_coefficient: float = Field(\n            default=0.7,\n            description=\"Brightness coefficient for rainy effect\",\n            gt=0,\n            le=1,\n        )\n        rain_type: RainMode | None = Field(default=None, description=\"Type of rain to simulate\")\n\n        @model_validator(mode=\"after\")\n        def validate_ranges(self) -&gt; Self:\n            if self.slant_lower is not None or self.slant_upper is not None:\n                if self.slant_lower is not None:\n                    warn(\n                        \"`slant_lower` deprecated. Use `slant_range` as tuple (slant_lower, slant_upper) instead.\",\n                        DeprecationWarning,\n                        stacklevel=2,\n                    )\n                if self.slant_upper is not None:\n                    warn(\n                        \"`slant_upper` deprecated. Use `slant_range` as tuple (slant_lower, slant_upper) instead.\",\n                        DeprecationWarning,\n                        stacklevel=2,\n                    )\n                lower = self.slant_lower if self.slant_lower is not None else self.slant_range[0]\n                upper = self.slant_upper if self.slant_upper is not None else self.slant_range[1]\n                self.slant_range = (lower, upper)\n                self.slant_lower = None\n                self.slant_upper = None\n\n            # Validate the slant_range\n            if not (-MAX_RAIN_ANGLE &lt;= self.slant_range[0] &lt;= self.slant_range[1] &lt;= MAX_RAIN_ANGLE):\n                raise ValueError(\n                    f\"slant_range values should be increasing within [-{MAX_RAIN_ANGLE}, {MAX_RAIN_ANGLE}] range.\",\n                )\n            return self\n\n    def __init__(\n        self,\n        slant_lower: int | None = None,\n        slant_upper: int | None = None,\n        slant_range: tuple[int, int] = (-10, 10),\n        drop_length: int = 20,\n        drop_width: int = 1,\n        drop_color: tuple[int, int, int] = (200, 200, 200),\n        blur_value: int = 7,\n        brightness_coefficient: float = 0.7,\n        rain_type: RainMode | None = None,\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p=p, always_apply=always_apply)\n        self.slant_range = slant_range\n        self.drop_length = drop_length\n        self.drop_width = drop_width\n        self.drop_color = drop_color\n        self.blur_value = blur_value\n        self.brightness_coefficient = brightness_coefficient\n        self.rain_type = rain_type\n\n    def apply(\n        self,\n        img: np.ndarray,\n        slant: int,\n        drop_length: int,\n        rain_drops: list[tuple[int, int]],\n        **params: Any,\n    ) -&gt; np.ndarray:\n        return fmain.add_rain(\n            img,\n            slant,\n            drop_length,\n            self.drop_width,\n            self.drop_color,\n            self.blur_value,\n            self.brightness_coefficient,\n            rain_drops,\n        )\n\n    def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n        slant = int(random.uniform(*self.slant_range))\n\n        height, width = params[\"shape\"][:2]\n        area = height * width\n\n        if self.rain_type == \"drizzle\":\n            num_drops = area // 770\n            drop_length = 10\n        elif self.rain_type == \"heavy\":\n            num_drops = width * height // 600\n            drop_length = 30\n        elif self.rain_type == \"torrential\":\n            num_drops = area // 500\n            drop_length = 60\n        else:\n            drop_length = self.drop_length\n            num_drops = area // 600\n\n        rain_drops = []\n\n        for _ in range(num_drops):  # If You want heavy rain, try increasing this\n            x = random.randint(slant, width) if slant &lt; 0 else random.randint(0, width - slant)\n\n            y = random.randint(0, height - drop_length)\n\n            rain_drops.append((x, y))\n\n        return {\"drop_length\": drop_length, \"slant\": slant, \"rain_drops\": rain_drops}\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return (\n            \"slant_range\",\n            \"drop_length\",\n            \"drop_width\",\n            \"drop_color\",\n            \"blur_value\",\n            \"brightness_coefficient\",\n            \"rain_type\",\n        )\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.RandomRain.apply","title":"<code>apply (self, img, slant, drop_length, rain_drops, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def apply(\n    self,\n    img: np.ndarray,\n    slant: int,\n    drop_length: int,\n    rain_drops: list[tuple[int, int]],\n    **params: Any,\n) -&gt; np.ndarray:\n    return fmain.add_rain(\n        img,\n        slant,\n        drop_length,\n        self.drop_width,\n        self.drop_color,\n        self.blur_value,\n        self.brightness_coefficient,\n        rain_drops,\n    )\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.RandomRain.get_params_dependent_on_data","title":"<code>get_params_dependent_on_data (self, params, data)</code>","text":"<p>Returns parameters dependent on input.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n    slant = int(random.uniform(*self.slant_range))\n\n    height, width = params[\"shape\"][:2]\n    area = height * width\n\n    if self.rain_type == \"drizzle\":\n        num_drops = area // 770\n        drop_length = 10\n    elif self.rain_type == \"heavy\":\n        num_drops = width * height // 600\n        drop_length = 30\n    elif self.rain_type == \"torrential\":\n        num_drops = area // 500\n        drop_length = 60\n    else:\n        drop_length = self.drop_length\n        num_drops = area // 600\n\n    rain_drops = []\n\n    for _ in range(num_drops):  # If You want heavy rain, try increasing this\n        x = random.randint(slant, width) if slant &lt; 0 else random.randint(0, width - slant)\n\n        y = random.randint(0, height - drop_length)\n\n        rain_drops.append((x, y))\n\n    return {\"drop_length\": drop_length, \"slant\": slant, \"rain_drops\": rain_drops}\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.RandomRain.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return (\n        \"slant_range\",\n        \"drop_length\",\n        \"drop_width\",\n        \"drop_color\",\n        \"blur_value\",\n        \"brightness_coefficient\",\n        \"rain_type\",\n    )\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.RandomShadow","title":"<code>class  RandomShadow</code> <code>     (shadow_roi=(0, 0.5, 1, 1), num_shadows_limit=(1, 2), num_shadows_lower=None, num_shadows_upper=None, shadow_dimension=5, shadow_intensity_range=(0.5, 0.5), always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Simulates shadows for the image by reducing the brightness of the image in shadow regions.</p> <p>Parameters:</p> Name Type Description <code>shadow_roi</code> <code>tuple</code> <p>region of the image where shadows will appear (x_min, y_min, x_max, y_max). All values should be in range [0, 1].</p> <code>num_shadows_limit</code> <code>tuple</code> <p>Lower and upper limits for the possible number of shadows. Default: (1, 2).</p> <code>shadow_dimension</code> <code>int</code> <p>number of edges in the shadow polygons. Default: 5.</p> <code>shadow_intensity_range</code> <code>tuple</code> <p>Range for the shadow intensity. Should be two float values between 0 and 1. Default: (0.5, 0.5).</p> <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 0.5.</p> <p>Targets</p> <p>image</p> <p>Image types:     uint8, float32</p> <p>Reference</p> <p>https://github.com/UjjwalSaxena/Automold--Road-Augmentation-Library</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>class RandomShadow(ImageOnlyTransform):\n    \"\"\"Simulates shadows for the image by reducing the brightness of the image in shadow regions.\n\n    Args:\n        shadow_roi (tuple): region of the image where shadows\n            will appear (x_min, y_min, x_max, y_max). All values should be in range [0, 1].\n        num_shadows_limit (tuple): Lower and upper limits for the possible number of shadows.\n            Default: (1, 2).\n        shadow_dimension (int): number of edges in the shadow polygons. Default: 5.\n        shadow_intensity_range (tuple): Range for the shadow intensity.\n            Should be two float values between 0 and 1. Default: (0.5, 0.5).\n        p (float): probability of applying the transform. Default: 0.5.\n\n    Targets:\n        image\n\n    Image types:\n        uint8, float32\n\n    Reference:\n        https://github.com/UjjwalSaxena/Automold--Road-Augmentation-Library\n    \"\"\"\n\n    class InitSchema(BaseTransformInitSchema):\n        shadow_roi: tuple[float, float, float, float] = Field(\n            default=(0, 0.5, 1, 1),\n            description=\"Region of the image where shadows will appear\",\n        )\n        num_shadows_limit: Annotated[tuple[int, int], AfterValidator(check_1plus), AfterValidator(nondecreasing)] = (\n            1,\n            2,\n        )\n        num_shadows_lower: int | None = Field(\n            default=None,\n            description=\"Lower limit for the possible number of shadows\",\n        )\n        num_shadows_upper: int | None = Field(\n            default=None,\n            description=\"Upper limit for the possible number of shadows\",\n        )\n        shadow_dimension: int = Field(default=5, description=\"Number of edges in the shadow polygons\", ge=1)\n\n        shadow_intensity_range: Annotated[\n            tuple[float, float],\n            AfterValidator(check_01),\n            AfterValidator(nondecreasing),\n        ] = Field(\n            default=(0.5, 0.5),\n            description=\"Range for the shadow intensity\",\n        )\n\n        @model_validator(mode=\"after\")\n        def validate_shadows(self) -&gt; Self:\n            if self.num_shadows_lower is not None:\n                warn(\n                    \"`num_shadows_lower` is deprecated. Use `num_shadows_limit` instead.\",\n                    DeprecationWarning,\n                    stacklevel=2,\n                )\n\n            if self.num_shadows_upper is not None:\n                warn(\n                    \"`num_shadows_upper` is deprecated. Use `num_shadows_limit` instead.\",\n                    DeprecationWarning,\n                    stacklevel=2,\n                )\n\n            if self.num_shadows_lower is not None or self.num_shadows_upper is not None:\n                num_shadows_lower = (\n                    self.num_shadows_lower if self.num_shadows_lower is not None else self.num_shadows_limit[0]\n                )\n                num_shadows_upper = (\n                    self.num_shadows_upper if self.num_shadows_upper is not None else self.num_shadows_limit[1]\n                )\n\n                self.num_shadows_limit = (num_shadows_lower, num_shadows_upper)\n                self.num_shadows_lower = None\n                self.num_shadows_upper = None\n\n            shadow_lower_x, shadow_lower_y, shadow_upper_x, shadow_upper_y = self.shadow_roi\n\n            if not 0 &lt;= shadow_lower_x &lt;= shadow_upper_x &lt;= 1 or not 0 &lt;= shadow_lower_y &lt;= shadow_upper_y &lt;= 1:\n                raise ValueError(f\"Invalid shadow_roi. Got: {self.shadow_roi}\")\n\n            if isinstance(self.shadow_intensity_range, float):\n                if not (0 &lt;= self.shadow_intensity_range &lt;= 1):\n                    raise ValueError(\n                        f\"shadow_intensity_range value should be within [0, 1] range. \"\n                        f\"Got: {self.shadow_intensity_range}\",\n                    )\n            elif isinstance(self.shadow_intensity_range, tuple):\n                if not (0 &lt;= self.shadow_intensity_range[0] &lt;= self.shadow_intensity_range[1] &lt;= 1):\n                    raise ValueError(\n                        f\"shadow_intensity_range values should be within [0, 1] range and increasing. \"\n                        f\"Got: {self.shadow_intensity_range}\",\n                    )\n            else:\n                raise TypeError(\"shadow_intensity_range should be an float or a tuple of floats.\")\n\n            return self\n\n    def __init__(\n        self,\n        shadow_roi: tuple[float, float, float, float] = (0, 0.5, 1, 1),\n        num_shadows_limit: tuple[int, int] = (1, 2),\n        num_shadows_lower: int | None = None,\n        num_shadows_upper: int | None = None,\n        shadow_dimension: int = 5,\n        shadow_intensity_range: tuple[float, float] = (0.5, 0.5),\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p=p, always_apply=always_apply)\n\n        self.shadow_roi = shadow_roi\n        self.shadow_dimension = shadow_dimension\n        self.num_shadows_limit = num_shadows_limit\n        self.shadow_intensity_range = shadow_intensity_range\n\n    def apply(\n        self,\n        img: np.ndarray,\n        vertices_list: list[np.ndarray],\n        intensities: np.ndarray,\n        **params: Any,\n    ) -&gt; np.ndarray:\n        return fmain.add_shadow(img, vertices_list, intensities)\n\n    def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, list[np.ndarray]]:\n        height, width = params[\"shape\"][:2]\n\n        num_shadows = random.randint(self.num_shadows_limit[0], self.num_shadows_limit[1])\n\n        x_min, y_min, x_max, y_max = self.shadow_roi\n\n        x_min = int(x_min * width)\n        x_max = int(x_max * width)\n        y_min = int(y_min * height)\n        y_max = int(y_max * height)\n\n        vertices_list = [\n            np.stack(\n                [\n                    random_utils.randint(x_min, x_max, size=5),\n                    random_utils.randint(y_min, y_max, size=5),\n                ],\n                axis=1,\n            )\n            for _ in range(num_shadows)\n        ]\n\n        # Sample shadow intensity for each shadow\n        intensities = random_utils.uniform(\n            self.shadow_intensity_range[0],\n            self.shadow_intensity_range[1],\n            size=num_shadows,\n        )\n\n        return {\"vertices_list\": vertices_list, \"intensities\": intensities}\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return (\n            \"shadow_roi\",\n            \"num_shadows_limit\",\n            \"shadow_dimension\",\n        )\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.RandomShadow.apply","title":"<code>apply (self, img, vertices_list, intensities, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def apply(\n    self,\n    img: np.ndarray,\n    vertices_list: list[np.ndarray],\n    intensities: np.ndarray,\n    **params: Any,\n) -&gt; np.ndarray:\n    return fmain.add_shadow(img, vertices_list, intensities)\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.RandomShadow.get_params_dependent_on_data","title":"<code>get_params_dependent_on_data (self, params, data)</code>","text":"<p>Returns parameters dependent on input.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, list[np.ndarray]]:\n    height, width = params[\"shape\"][:2]\n\n    num_shadows = random.randint(self.num_shadows_limit[0], self.num_shadows_limit[1])\n\n    x_min, y_min, x_max, y_max = self.shadow_roi\n\n    x_min = int(x_min * width)\n    x_max = int(x_max * width)\n    y_min = int(y_min * height)\n    y_max = int(y_max * height)\n\n    vertices_list = [\n        np.stack(\n            [\n                random_utils.randint(x_min, x_max, size=5),\n                random_utils.randint(y_min, y_max, size=5),\n            ],\n            axis=1,\n        )\n        for _ in range(num_shadows)\n    ]\n\n    # Sample shadow intensity for each shadow\n    intensities = random_utils.uniform(\n        self.shadow_intensity_range[0],\n        self.shadow_intensity_range[1],\n        size=num_shadows,\n    )\n\n    return {\"vertices_list\": vertices_list, \"intensities\": intensities}\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.RandomShadow.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return (\n        \"shadow_roi\",\n        \"num_shadows_limit\",\n        \"shadow_dimension\",\n    )\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.RandomSnow","title":"<code>class  RandomSnow</code> <code>     (snow_point_lower=None, snow_point_upper=None, brightness_coeff=2.5, snow_point_range=(0.1, 0.3), always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Bleach out some pixel values imitating snow.</p> <p>Parameters:</p> Name Type Description <code>snow_point_range</code> <code>tuple</code> <p>tuple of bounds on the amount of snow i.e. (snow_point_lower, snow_point_upper). Both values should be in the (0, 1) range. Default: (0.1, 0.3).</p> <code>brightness_coeff</code> <code>float</code> <p>Coefficient applied to increase the brightness of pixels below the snow_point threshold. Larger values lead to more pronounced snow effects. Should be &gt; 0. Default: 2.5.</p> <code>p</code> <code>float</code> <p>Probability of applying the transform. Default: 0.5.</p> <p>Targets</p> <p>image</p> <p>Image types:     uint8, float32</p> <p>Reference</p> <p>https://github.com/UjjwalSaxena/Automold--Road-Augmentation-Library</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>class RandomSnow(ImageOnlyTransform):\n    \"\"\"Bleach out some pixel values imitating snow.\n\n    Args:\n        snow_point_range (tuple): tuple of bounds on the amount of snow i.e. (snow_point_lower, snow_point_upper).\n            Both values should be in the (0, 1) range. Default: (0.1, 0.3).\n        brightness_coeff (float): Coefficient applied to increase the brightness of pixels\n            below the snow_point threshold. Larger values lead to more pronounced snow effects.\n            Should be &gt; 0. Default: 2.5.\n        p (float): Probability of applying the transform. Default: 0.5.\n\n    Targets:\n        image\n\n    Image types:\n        uint8, float32\n\n    Reference:\n        https://github.com/UjjwalSaxena/Automold--Road-Augmentation-Library\n\n    \"\"\"\n\n    class InitSchema(BaseTransformInitSchema):\n        snow_point_range: Annotated[tuple[float, float], AfterValidator(check_01), AfterValidator(nondecreasing)] = (\n            Field(\n                default=(0.1, 0.3),\n                description=\"lower and upper bound on the amount of snow as tuple (snow_point_lower, snow_point_upper)\",\n            )\n        )\n        snow_point_lower: float | None = Field(\n            default=None,\n            description=\"Lower bound of the amount of snow\",\n            gt=0,\n            lt=1,\n        )\n        snow_point_upper: float | None = Field(\n            default=None,\n            description=\"Upper bound of the amount of snow\",\n            gt=0,\n            lt=1,\n        )\n        brightness_coeff: float = Field(default=2.5, description=\"Brightness coefficient, must be &gt; 0\", gt=0)\n\n        @model_validator(mode=\"after\")\n        def validate_ranges(self) -&gt; Self:\n            if self.snow_point_lower is not None or self.snow_point_upper is not None:\n                if self.snow_point_lower is not None:\n                    warn(\n                        \"`snow_point_lower` deprecated. Use `snow_point_range` as tuple\"\n                        \" (snow_point_lower, snow_point_upper) instead.\",\n                        DeprecationWarning,\n                        stacklevel=2,\n                    )\n                if self.snow_point_upper is not None:\n                    warn(\n                        \"`snow_point_upper` deprecated. Use `snow_point_range` as tuple\"\n                        \"(snow_point_lower, snow_point_upper) instead.\",\n                        DeprecationWarning,\n                        stacklevel=2,\n                    )\n                lower = self.snow_point_lower if self.snow_point_lower is not None else self.snow_point_range[0]\n                upper = self.snow_point_upper if self.snow_point_upper is not None else self.snow_point_range[1]\n                self.snow_point_range = (lower, upper)\n                self.snow_point_lower = None\n                self.snow_point_upper = None\n\n            # Validate the snow_point_range\n            if not (0 &lt; self.snow_point_range[0] &lt;= self.snow_point_range[1] &lt; 1):\n                raise ValueError(\"snow_point_range values should be increasing within (0, 1) range.\")\n\n            return self\n\n    def __init__(\n        self,\n        snow_point_lower: float | None = None,\n        snow_point_upper: float | None = None,\n        brightness_coeff: float = 2.5,\n        snow_point_range: tuple[float, float] = (0.1, 0.3),\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p, always_apply)\n\n        self.snow_point_range = snow_point_range\n        self.brightness_coeff = brightness_coeff\n\n    def apply(self, img: np.ndarray, snow_point: float, **params: Any) -&gt; np.ndarray:\n        return fmain.add_snow(img, snow_point, self.brightness_coeff)\n\n    def get_params(self) -&gt; dict[str, np.ndarray]:\n        return {\"snow_point\": random.uniform(*self.snow_point_range)}\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, str]:\n        return \"snow_point_range\", \"brightness_coeff\"\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.RandomSnow.apply","title":"<code>apply (self, img, snow_point, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def apply(self, img: np.ndarray, snow_point: float, **params: Any) -&gt; np.ndarray:\n    return fmain.add_snow(img, snow_point, self.brightness_coeff)\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.RandomSnow.get_params","title":"<code>get_params (self)</code>","text":"<p>Returns parameters independent of input.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_params(self) -&gt; dict[str, np.ndarray]:\n    return {\"snow_point\": random.uniform(*self.snow_point_range)}\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.RandomSnow.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, str]:\n    return \"snow_point_range\", \"brightness_coeff\"\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.RandomSunFlare","title":"<code>class  RandomSunFlare</code> <code>     (flare_roi=(0, 0, 1, 0.5), angle_lower=None, angle_upper=None, num_flare_circles_lower=None, num_flare_circles_upper=None, src_radius=400, src_color=(255, 255, 255), angle_range=(0, 1), num_flare_circles_range=(6, 10), always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Simulates Sun Flare for the image</p> <p>Parameters:</p> Name Type Description <code>flare_roi</code> <code>tuple[float, float, float, float]</code> <p>Tuple specifying the region of the image where flare will appear (x_min, y_min, x_max, y_max). All values should be in range [0, 1].</p> <code>src_radius</code> <code>int</code> <p>Radius of the source for the flare.</p> <code>src_color</code> <code>tuple[int, int, int]</code> <p>Color of the flare as an (R, G, B) tuple.</p> <code>angle_range</code> <code>tuple[float, float]</code> <p>tuple specifying the range of angles for the flare. Both ends of the range are in the [0, 1] interval.</p> <code>num_flare_circles_range</code> <code>tuple[int, int]</code> <p>tuple specifying the range for the number of flare circles.</p> <code>p</code> <code>float</code> <p>Probability of applying the transform.</p> <p>Targets</p> <p>image</p> <p>Image types:     uint8</p> <p>Reference</p> <p>https://github.com/UjjwalSaxena/Automold--Road-Augmentation-Library</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>class RandomSunFlare(ImageOnlyTransform):\n    \"\"\"Simulates Sun Flare for the image\n\n    Args:\n        flare_roi (tuple[float, float, float, float]): Tuple specifying the region of the image where flare will\n            appear (x_min, y_min, x_max, y_max). All values should be in range [0, 1].\n        src_radius (int): Radius of the source for the flare.\n        src_color (tuple[int, int, int]): Color of the flare as an (R, G, B) tuple.\n        angle_range (tuple[float, float]): tuple specifying the range of angles for the flare.\n            Both ends of the range are in the [0, 1] interval.\n        num_flare_circles_range (tuple[int, int]): tuple specifying the range for the number of flare circles.\n        p (float): Probability of applying the transform.\n\n    Targets:\n        image\n\n    Image types:\n        uint8\n\n    Reference:\n        https://github.com/UjjwalSaxena/Automold--Road-Augmentation-Library\n    \"\"\"\n\n    class InitSchema(BaseTransformInitSchema):\n        flare_roi: tuple[float, float, float, float] = Field(\n            default=(0, 0, 1, 0.5),\n            description=\"Region of the image where flare will appear\",\n        )\n        angle_lower: float | None = Field(default=None, description=\"Lower bound for the angle\", ge=0, le=1)\n        angle_upper: float | None = Field(default=None, description=\"Upper bound for the angle\", ge=0, le=1)\n\n        num_flare_circles_lower: int | None = Field(\n            default=6,\n            description=\"Lower limit for the number of flare circles\",\n            ge=0,\n        )\n        num_flare_circles_upper: int | None = Field(\n            default=10,\n            description=\"Upper limit for the number of flare circles\",\n            gt=0,\n        )\n        src_radius: int = Field(default=400, description=\"Source radius for the flare\")\n        src_color: tuple[int, ...] = Field(default=(255, 255, 255), description=\"Color of the flare\")\n\n        angle_range: Annotated[tuple[float, float], AfterValidator(check_01), AfterValidator(nondecreasing)] = Field(\n            default=(0, 1),\n            description=\"Angle range\",\n        )\n\n        num_flare_circles_range: Annotated[\n            tuple[int, int],\n            AfterValidator(check_1plus),\n            AfterValidator(nondecreasing),\n        ] = Field(default=(6, 10), description=\"Number of flare circles range\")\n\n        @model_validator(mode=\"after\")\n        def validate_parameters(self) -&gt; Self:\n            flare_center_lower_x, flare_center_lower_y, flare_center_upper_x, flare_center_upper_y = self.flare_roi\n            if (\n                not 0 &lt;= flare_center_lower_x &lt; flare_center_upper_x &lt;= 1\n                or not 0 &lt;= flare_center_lower_y &lt; flare_center_upper_y &lt;= 1\n            ):\n                raise ValueError(f\"Invalid flare_roi. Got: {self.flare_roi}\")\n\n            if self.angle_lower is not None or self.angle_upper is not None:\n                if self.angle_lower is not None:\n                    warn(\n                        \"`angle_lower` deprecated. Use `angle_range` as tuple (angle_lower, angle_upper) instead.\",\n                        DeprecationWarning,\n                        stacklevel=2,\n                    )\n                if self.angle_upper is not None:\n                    warn(\n                        \"`angle_upper` deprecated. Use `angle_range` as tuple(angle_lower, angle_upper) instead.\",\n                        DeprecationWarning,\n                        stacklevel=2,\n                    )\n                lower = self.angle_lower if self.angle_lower is not None else self.angle_range[0]\n                upper = self.angle_upper if self.angle_upper is not None else self.angle_range[1]\n                self.angle_range = (lower, upper)\n\n            if self.num_flare_circles_lower is not None or self.num_flare_circles_upper is not None:\n                if self.num_flare_circles_lower is not None:\n                    warn(\n                        \"`num_flare_circles_lower` deprecated. Use `num_flare_circles_range` as tuple\"\n                        \" (num_flare_circles_lower, num_flare_circles_upper) instead.\",\n                        DeprecationWarning,\n                        stacklevel=2,\n                    )\n                if self.num_flare_circles_upper is not None:\n                    warn(\n                        \"`num_flare_circles_upper` deprecated. Use `num_flare_circles_range` as tuple\"\n                        \" (num_flare_circles_lower, num_flare_circles_upper) instead.\",\n                        DeprecationWarning,\n                        stacklevel=2,\n                    )\n                lower = (\n                    self.num_flare_circles_lower\n                    if self.num_flare_circles_lower is not None\n                    else self.num_flare_circles_range[0]\n                )\n                upper = (\n                    self.num_flare_circles_upper\n                    if self.num_flare_circles_upper is not None\n                    else self.num_flare_circles_range[1]\n                )\n                self.num_flare_circles_range = (lower, upper)\n\n            return self\n\n    def __init__(\n        self,\n        flare_roi: tuple[float, float, float, float] = (0, 0, 1, 0.5),\n        angle_lower: float | None = None,\n        angle_upper: float | None = None,\n        num_flare_circles_lower: int | None = None,\n        num_flare_circles_upper: int | None = None,\n        src_radius: int = 400,\n        src_color: tuple[int, ...] = (255, 255, 255),\n        angle_range: tuple[float, float] = (0, 1),\n        num_flare_circles_range: tuple[int, int] = (6, 10),\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p=p, always_apply=always_apply)\n\n        self.angle_range = angle_range\n        self.num_flare_circles_range = num_flare_circles_range\n\n        self.src_radius = src_radius\n        self.src_color = src_color\n        self.flare_roi = flare_roi\n\n    def apply(\n        self,\n        img: np.ndarray,\n        flare_center: tuple[float, float],\n        circles: list[Any],\n        **params: Any,\n    ) -&gt; np.ndarray:\n        if circles is None:\n            circles = []\n        return fmain.add_sun_flare(\n            img,\n            flare_center,\n            self.src_radius,\n            self.src_color,\n            circles,\n        )\n\n    def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n        height, width = params[\"shape\"][:2]\n\n        angle = 2 * math.pi * random.uniform(*self.angle_range)\n\n        (flare_center_lower_x, flare_center_lower_y, flare_center_upper_x, flare_center_upper_y) = self.flare_roi\n\n        flare_center_x = random.uniform(flare_center_lower_x, flare_center_upper_x)\n        flare_center_y = random.uniform(flare_center_lower_y, flare_center_upper_y)\n\n        flare_center_x = int(width * flare_center_x)\n        flare_center_y = int(height * flare_center_y)\n\n        num_circles = random.randint(*self.num_flare_circles_range)\n\n        circles = []\n\n        x = []\n        y = []\n\n        def line(t: float) -&gt; tuple[float, float]:\n            return (flare_center_x + t * math.cos(angle), flare_center_y + t * math.sin(angle))\n\n        for t_val in range(-flare_center_x, width - flare_center_x, 10):\n            rand_x, rand_y = line(t_val)\n            x.append(rand_x)\n            y.append(rand_y)\n\n        for _ in range(num_circles):\n            alpha = random.uniform(0.05, 0.2)\n            r = random.randint(0, len(x) - 1)\n            rad = random.randint(1, max(height // 100 - 2, 2))\n\n            r_color = random.randint(max(self.src_color[0] - 50, 0), self.src_color[0])\n            g_color = random.randint(max(self.src_color[1] - 50, 0), self.src_color[1])\n            b_color = random.randint(max(self.src_color[2] - 50, 0), self.src_color[2])\n\n            circles += [\n                (\n                    alpha,\n                    (int(x[r]), int(y[r])),\n                    pow(rad, 3),\n                    (r_color, g_color, b_color),\n                ),\n            ]\n\n        return {\n            \"circles\": circles,\n            \"flare_center\": (flare_center_x, flare_center_y),\n        }\n\n    def get_transform_init_args(self) -&gt; dict[str, Any]:\n        return {\n            \"flare_roi\": self.flare_roi,\n            \"angle_range\": self.angle_range,\n            \"num_flare_circles_range\": self.num_flare_circles_range,\n            \"src_radius\": self.src_radius,\n            \"src_color\": self.src_color,\n        }\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.RandomSunFlare.apply","title":"<code>apply (self, img, flare_center, circles, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def apply(\n    self,\n    img: np.ndarray,\n    flare_center: tuple[float, float],\n    circles: list[Any],\n    **params: Any,\n) -&gt; np.ndarray:\n    if circles is None:\n        circles = []\n    return fmain.add_sun_flare(\n        img,\n        flare_center,\n        self.src_radius,\n        self.src_color,\n        circles,\n    )\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.RandomSunFlare.get_params_dependent_on_data","title":"<code>get_params_dependent_on_data (self, params, data)</code>","text":"<p>Returns parameters dependent on input.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n    height, width = params[\"shape\"][:2]\n\n    angle = 2 * math.pi * random.uniform(*self.angle_range)\n\n    (flare_center_lower_x, flare_center_lower_y, flare_center_upper_x, flare_center_upper_y) = self.flare_roi\n\n    flare_center_x = random.uniform(flare_center_lower_x, flare_center_upper_x)\n    flare_center_y = random.uniform(flare_center_lower_y, flare_center_upper_y)\n\n    flare_center_x = int(width * flare_center_x)\n    flare_center_y = int(height * flare_center_y)\n\n    num_circles = random.randint(*self.num_flare_circles_range)\n\n    circles = []\n\n    x = []\n    y = []\n\n    def line(t: float) -&gt; tuple[float, float]:\n        return (flare_center_x + t * math.cos(angle), flare_center_y + t * math.sin(angle))\n\n    for t_val in range(-flare_center_x, width - flare_center_x, 10):\n        rand_x, rand_y = line(t_val)\n        x.append(rand_x)\n        y.append(rand_y)\n\n    for _ in range(num_circles):\n        alpha = random.uniform(0.05, 0.2)\n        r = random.randint(0, len(x) - 1)\n        rad = random.randint(1, max(height // 100 - 2, 2))\n\n        r_color = random.randint(max(self.src_color[0] - 50, 0), self.src_color[0])\n        g_color = random.randint(max(self.src_color[1] - 50, 0), self.src_color[1])\n        b_color = random.randint(max(self.src_color[2] - 50, 0), self.src_color[2])\n\n        circles += [\n            (\n                alpha,\n                (int(x[r]), int(y[r])),\n                pow(rad, 3),\n                (r_color, g_color, b_color),\n            ),\n        ]\n\n    return {\n        \"circles\": circles,\n        \"flare_center\": (flare_center_x, flare_center_y),\n    }\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.RandomToneCurve","title":"<code>class  RandomToneCurve</code> <code>     (scale=0.1, per_channel=False, always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Randomly change the relationship between bright and dark areas of the image by manipulating its tone curve.</p> <p>Parameters:</p> Name Type Description <code>scale</code> <code>float</code> <p>Standard deviation of the normal distribution used to sample random distances to move two control points that modify the image's curve. Values should be in range [0, 1]. Default: 0.1</p> <code>per_channel</code> <code>bool</code> <p>If <code>True</code>, the tone curve will be applied to each channel of the input image separately, which can lead to color distortion. Default: False.</p> <code>p</code> <code>float</code> <p>Probability of applying the transform. Default: 0.5</p> <p>Targets</p> <p>image</p> <p>Image types:     uint8, float32</p> <p>Reference</p> <ul> <li>\"What Else Can Fool Deep Learning? Addressing Color Constancy Errors on Deep Neural Network Performance\"   by Mahmoud Afifi and Michael S. Brown, ICCV 2019.</li> <li>GitHub repository: https://github.com/mahmoudnafifi/WB_color_augmenter</li> </ul> <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from albumentations import RandomToneCurve\n&gt;&gt;&gt; img = np.random.randint(0, 256, (100, 100, 3), dtype=np.uint8)\n&gt;&gt;&gt; transform = RandomToneCurve(scale=0.1, per_channel=True, p=1.0)\n&gt;&gt;&gt; transformed_img = transform(image=img)['image']\n</code></pre> <p>This transform applies a random tone curve to the input image by adjusting the relationship between bright and dark areas. When <code>per_channel</code> is set to True, each channel is adjusted separately, potentially causing color distortions. Otherwise, the same adjustment is applied to all channels, preserving the original color relationships.</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>class RandomToneCurve(ImageOnlyTransform):\n    \"\"\"Randomly change the relationship between bright and dark areas of the image by manipulating its tone curve.\n\n    Args:\n        scale (float): Standard deviation of the normal distribution used to sample random distances\n            to move two control points that modify the image's curve. Values should be in range [0, 1]. Default: 0.1\n        per_channel (bool): If `True`, the tone curve will be applied to each channel of the input image separately,\n            which can lead to color distortion. Default: False.\n        p (float): Probability of applying the transform. Default: 0.5\n\n    Targets:\n        image\n\n    Image types:\n        uint8, float32\n\n    Reference:\n        - \"What Else Can Fool Deep Learning? Addressing Color Constancy Errors on Deep Neural Network Performance\"\n          by Mahmoud Afifi and Michael S. Brown, ICCV 2019.\n        - GitHub repository: https://github.com/mahmoudnafifi/WB_color_augmenter\n\n    Example:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from albumentations import RandomToneCurve\n        &gt;&gt;&gt; img = np.random.randint(0, 256, (100, 100, 3), dtype=np.uint8)\n        &gt;&gt;&gt; transform = RandomToneCurve(scale=0.1, per_channel=True, p=1.0)\n        &gt;&gt;&gt; transformed_img = transform(image=img)['image']\n\n    This transform applies a random tone curve to the input image by adjusting the relationship between bright and\n    dark areas. When `per_channel` is set to True, each channel is adjusted separately, potentially causing color\n    distortions. Otherwise, the same adjustment is applied to all channels, preserving the original color relationships.\n    \"\"\"\n\n    class InitSchema(BaseTransformInitSchema):\n        scale: float = Field(\n            default=0.1,\n            description=\"Standard deviation of the normal distribution used to sample random distances\",\n            ge=0,\n            le=1,\n        )\n        per_channel: bool = Field(default=False, description=\"Apply the tone curve to each channel separately\")\n\n    def __init__(\n        self,\n        scale: float = 0.1,\n        per_channel: bool = False,\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p=p, always_apply=always_apply)\n        self.scale = scale\n        self.per_channel = per_channel\n\n    def apply(\n        self,\n        img: np.ndarray,\n        low_y: float | np.ndarray,\n        high_y: float | np.ndarray,\n        **params: Any,\n    ) -&gt; np.ndarray:\n        return fmain.move_tone_curve(img, low_y, high_y)\n\n    def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n        image = data[\"image\"] if \"image\" in data else data[\"images\"][0]\n        num_channels = get_num_channels(image)\n\n        if self.per_channel and num_channels != 1:\n            return {\n                \"low_y\": np.clip(random_utils.normal(loc=0.25, scale=self.scale, size=[num_channels]), 0, 1),\n                \"high_y\": np.clip(random_utils.normal(loc=0.75, scale=self.scale, size=[num_channels]), 0, 1),\n            }\n        # Same values for all channels\n        low_y = np.clip(random_utils.normal(loc=0.25, scale=self.scale), 0, 1)\n        high_y = np.clip(random_utils.normal(loc=0.75, scale=self.scale), 0, 1)\n\n        return {\"low_y\": low_y, \"high_y\": high_y}\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return \"scale\", \"per_channel\"\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.RandomToneCurve.apply","title":"<code>apply (self, img, low_y, high_y, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def apply(\n    self,\n    img: np.ndarray,\n    low_y: float | np.ndarray,\n    high_y: float | np.ndarray,\n    **params: Any,\n) -&gt; np.ndarray:\n    return fmain.move_tone_curve(img, low_y, high_y)\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.RandomToneCurve.get_params_dependent_on_data","title":"<code>get_params_dependent_on_data (self, params, data)</code>","text":"<p>Returns parameters dependent on input.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n    image = data[\"image\"] if \"image\" in data else data[\"images\"][0]\n    num_channels = get_num_channels(image)\n\n    if self.per_channel and num_channels != 1:\n        return {\n            \"low_y\": np.clip(random_utils.normal(loc=0.25, scale=self.scale, size=[num_channels]), 0, 1),\n            \"high_y\": np.clip(random_utils.normal(loc=0.75, scale=self.scale, size=[num_channels]), 0, 1),\n        }\n    # Same values for all channels\n    low_y = np.clip(random_utils.normal(loc=0.25, scale=self.scale), 0, 1)\n    high_y = np.clip(random_utils.normal(loc=0.75, scale=self.scale), 0, 1)\n\n    return {\"low_y\": low_y, \"high_y\": high_y}\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.RandomToneCurve.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return \"scale\", \"per_channel\"\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.RingingOvershoot","title":"<code>class  RingingOvershoot</code> <code>     (blur_limit=(7, 15), cutoff=(0.7853981633974483, 1.5707963267948966), always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Create ringing or overshoot artefacts by conlvolving image with 2D sinc filter.</p> <p>Parameters:</p> Name Type Description <code>blur_limit</code> <code>ScaleIntType</code> <p>maximum kernel size for sinc filter. Should be in range [3, inf). Default: (7, 15).</p> <code>cutoff</code> <code>ScaleFloatType</code> <p>range to choose the cutoff frequency in radians. Should be in range (0, np.pi) Default: (np.pi / 4, np.pi / 2).</p> <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 0.5.</p> <p>Reference</p> <p>dsp.stackexchange.com/questions/58301/2-d-circularly-symmetric-low-pass-filter https://arxiv.org/abs/2107.10833</p> <p>Targets</p> <p>image</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>class RingingOvershoot(ImageOnlyTransform):\n    \"\"\"Create ringing or overshoot artefacts by conlvolving image with 2D sinc filter.\n\n    Args:\n        blur_limit: maximum kernel size for sinc filter.\n            Should be in range [3, inf). Default: (7, 15).\n        cutoff: range to choose the cutoff frequency in radians.\n            Should be in range (0, np.pi)\n            Default: (np.pi / 4, np.pi / 2).\n        p: probability of applying the transform. Default: 0.5.\n\n    Reference:\n        dsp.stackexchange.com/questions/58301/2-d-circularly-symmetric-low-pass-filter\n        https://arxiv.org/abs/2107.10833\n\n    Targets:\n        image\n\n    \"\"\"\n\n    class InitSchema(BlurInitSchema):\n        blur_limit: ScaleIntType = Field(default=(7, 15), description=\"Maximum kernel size for sinc filter.\")\n        cutoff: ScaleFloatType = Field(default=(np.pi / 4, np.pi / 2), description=\"Cutoff frequency range in radians.\")\n\n        @field_validator(\"cutoff\")\n        @classmethod\n        def check_cutoff(cls, v: ScaleFloatType, info: ValidationInfo) -&gt; tuple[float, float]:\n            bounds = 0, np.pi\n            result = to_tuple(v, v)\n            check_range(result, *bounds, info.field_name)\n            return result\n\n    def __init__(\n        self,\n        blur_limit: ScaleIntType = (7, 15),\n        cutoff: ScaleFloatType = (np.pi / 4, np.pi / 2),\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p=p, always_apply=always_apply)\n        self.blur_limit = cast(Tuple[int, int], blur_limit)\n        self.cutoff = cast(Tuple[float, float], cutoff)\n\n    def get_params(self) -&gt; dict[str, np.ndarray]:\n        ksize = random.randrange(self.blur_limit[0], self.blur_limit[1] + 1, 2)\n        if ksize % 2 == 0:\n            raise ValueError(f\"Kernel size must be odd. Got: {ksize}\")\n\n        cutoff = random.uniform(*self.cutoff)\n\n        # From dsp.stackexchange.com/questions/58301/2-d-circularly-symmetric-low-pass-filter\n        with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n            kernel = np.fromfunction(\n                lambda x, y: cutoff\n                * special.j1(cutoff * np.sqrt((x - (ksize - 1) / 2) ** 2 + (y - (ksize - 1) / 2) ** 2))\n                / (2 * np.pi * np.sqrt((x - (ksize - 1) / 2) ** 2 + (y - (ksize - 1) / 2) ** 2)),\n                [ksize, ksize],\n            )\n        kernel[(ksize - 1) // 2, (ksize - 1) // 2] = cutoff**2 / (4 * np.pi)\n\n        # Normalize kernel\n        kernel = kernel.astype(np.float32) / np.sum(kernel)\n\n        return {\"kernel\": kernel}\n\n    def apply(self, img: np.ndarray, kernel: int, **params: Any) -&gt; np.ndarray:\n        return fmain.convolve(img, kernel)\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, str]:\n        return (\"blur_limit\", \"cutoff\")\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.RingingOvershoot.apply","title":"<code>apply (self, img, kernel, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def apply(self, img: np.ndarray, kernel: int, **params: Any) -&gt; np.ndarray:\n    return fmain.convolve(img, kernel)\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.RingingOvershoot.get_params","title":"<code>get_params (self)</code>","text":"<p>Returns parameters independent of input.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_params(self) -&gt; dict[str, np.ndarray]:\n    ksize = random.randrange(self.blur_limit[0], self.blur_limit[1] + 1, 2)\n    if ksize % 2 == 0:\n        raise ValueError(f\"Kernel size must be odd. Got: {ksize}\")\n\n    cutoff = random.uniform(*self.cutoff)\n\n    # From dsp.stackexchange.com/questions/58301/2-d-circularly-symmetric-low-pass-filter\n    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n        kernel = np.fromfunction(\n            lambda x, y: cutoff\n            * special.j1(cutoff * np.sqrt((x - (ksize - 1) / 2) ** 2 + (y - (ksize - 1) / 2) ** 2))\n            / (2 * np.pi * np.sqrt((x - (ksize - 1) / 2) ** 2 + (y - (ksize - 1) / 2) ** 2)),\n            [ksize, ksize],\n        )\n    kernel[(ksize - 1) // 2, (ksize - 1) // 2] = cutoff**2 / (4 * np.pi)\n\n    # Normalize kernel\n    kernel = kernel.astype(np.float32) / np.sum(kernel)\n\n    return {\"kernel\": kernel}\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.RingingOvershoot.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, str]:\n    return (\"blur_limit\", \"cutoff\")\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.Sharpen","title":"<code>class  Sharpen</code> <code>     (alpha=(0.2, 0.5), lightness=(0.5, 1.0), always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Sharpen the input image and overlays the result with the original image.</p> <p>Parameters:</p> Name Type Description <code>alpha</code> <code>tuple[float, float]</code> <p>range to choose the visibility of the sharpened image. At 0, only the original image is visible, at 1.0 only its sharpened version is visible. Default: (0.2, 0.5).</p> <code>lightness</code> <code>tuple[float, float]</code> <p>range to choose the lightness of the sharpened image. Default: (0.5, 1.0).</p> <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 0.5.</p> <p>Targets</p> <p>image</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>class Sharpen(ImageOnlyTransform):\n    \"\"\"Sharpen the input image and overlays the result with the original image.\n\n    Args:\n        alpha: range to choose the visibility of the sharpened image. At 0, only the original image is\n            visible, at 1.0 only its sharpened version is visible. Default: (0.2, 0.5).\n        lightness: range to choose the lightness of the sharpened image. Default: (0.5, 1.0).\n        p: probability of applying the transform. Default: 0.5.\n\n    Targets:\n        image\n\n    \"\"\"\n\n    class InitSchema(BaseTransformInitSchema):\n        alpha: ZeroOneRangeType = (0.2, 0.5)\n        lightness: NonNegativeFloatRangeType = (0.5, 1.0)\n\n    def __init__(\n        self,\n        alpha: tuple[float, float] = (0.2, 0.5),\n        lightness: tuple[float, float] = (0.5, 1.0),\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p=p, always_apply=always_apply)\n        self.alpha = alpha\n        self.lightness = lightness\n\n    @staticmethod\n    def __generate_sharpening_matrix(alpha_sample: np.ndarray, lightness_sample: np.ndarray) -&gt; np.ndarray:\n        matrix_nochange = np.array([[0, 0, 0], [0, 1, 0], [0, 0, 0]], dtype=np.float32)\n        matrix_effect = np.array(\n            [[-1, -1, -1], [-1, 8 + lightness_sample, -1], [-1, -1, -1]],\n            dtype=np.float32,\n        )\n\n        return (1 - alpha_sample) * matrix_nochange + alpha_sample * matrix_effect\n\n    def get_params(self) -&gt; dict[str, np.ndarray]:\n        alpha = random.uniform(*self.alpha)\n        lightness = random.uniform(*self.lightness)\n        sharpening_matrix = self.__generate_sharpening_matrix(alpha_sample=alpha, lightness_sample=lightness)\n        return {\"sharpening_matrix\": sharpening_matrix}\n\n    def apply(self, img: np.ndarray, sharpening_matrix: np.ndarray, **params: Any) -&gt; np.ndarray:\n        return fmain.convolve(img, sharpening_matrix)\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, str]:\n        return (\"alpha\", \"lightness\")\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.Sharpen.apply","title":"<code>apply (self, img, sharpening_matrix, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def apply(self, img: np.ndarray, sharpening_matrix: np.ndarray, **params: Any) -&gt; np.ndarray:\n    return fmain.convolve(img, sharpening_matrix)\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.Sharpen.get_params","title":"<code>get_params (self)</code>","text":"<p>Returns parameters independent of input.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_params(self) -&gt; dict[str, np.ndarray]:\n    alpha = random.uniform(*self.alpha)\n    lightness = random.uniform(*self.lightness)\n    sharpening_matrix = self.__generate_sharpening_matrix(alpha_sample=alpha, lightness_sample=lightness)\n    return {\"sharpening_matrix\": sharpening_matrix}\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.Sharpen.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, str]:\n    return (\"alpha\", \"lightness\")\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.Solarize","title":"<code>class  Solarize</code> <code>     (threshold=(128, 128), p=0.5, always_apply=None)                 </code>  [view source on GitHub]","text":"<p>Invert all pixel values above a threshold.</p> <p>Parameters:</p> Name Type Description <code>threshold</code> <code>ScaleType</code> <p>range for solarizing threshold. If threshold is a single value, the range will be [1, threshold]. Default: 128.</p> <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 0.5.</p> <p>Targets</p> <p>image</p> <p>Image types:     uint8, float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>class Solarize(ImageOnlyTransform):\n    \"\"\"Invert all pixel values above a threshold.\n\n    Args:\n        threshold: range for solarizing threshold.\n            If threshold is a single value, the range will be [1, threshold]. Default: 128.\n        p: probability of applying the transform. Default: 0.5.\n\n    Targets:\n        image\n\n    Image types:\n        uint8, float32\n\n    \"\"\"\n\n    class InitSchema(BaseTransformInitSchema):\n        threshold: OnePlusFloatRangeType = (128, 128)\n\n    def __init__(self, threshold: ScaleType = (128, 128), p: float = 0.5, always_apply: bool | None = None):\n        super().__init__(p=p, always_apply=always_apply)\n        self.threshold = cast(Tuple[float, float], threshold)\n\n    def apply(self, img: np.ndarray, threshold: int, **params: Any) -&gt; np.ndarray:\n        return fmain.solarize(img, threshold)\n\n    def get_params(self) -&gt; dict[str, float]:\n        return {\"threshold\": random.uniform(self.threshold[0], self.threshold[1])}\n\n    def get_transform_init_args_names(self) -&gt; tuple[str]:\n        return (\"threshold\",)\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.Solarize.apply","title":"<code>apply (self, img, threshold, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def apply(self, img: np.ndarray, threshold: int, **params: Any) -&gt; np.ndarray:\n    return fmain.solarize(img, threshold)\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.Solarize.get_params","title":"<code>get_params (self)</code>","text":"<p>Returns parameters independent of input.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_params(self) -&gt; dict[str, float]:\n    return {\"threshold\": random.uniform(self.threshold[0], self.threshold[1])}\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.Solarize.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str]:\n    return (\"threshold\",)\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.Spatter","title":"<code>class  Spatter</code> <code>     (mean=(0.65, 0.65), std=(0.3, 0.3), gauss_sigma=(2, 2), cutout_threshold=(0.68, 0.68), intensity=(0.6, 0.6), mode='rain', color=None, always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Apply spatter transform. It simulates corruption which can occlude a lens in the form of rain or mud.</p> <p>Parameters:</p> Name Type Description <code>mean</code> <code>float, or tuple of floats</code> <p>Mean value of normal distribution for generating liquid layer. If single float mean will be sampled from <code>(0, mean)</code> If tuple of float mean will be sampled from range <code>(mean[0], mean[1])</code>. If you want constant value use (mean, mean). Default (0.65, 0.65)</p> <code>std</code> <code>float, or tuple of floats</code> <p>Standard deviation value of normal distribution for generating liquid layer. If single float the number will be sampled from <code>(0, std)</code>. If tuple of float std will be sampled from range <code>(std[0], std[1])</code>. If you want constant value use (std, std). Default: (0.3, 0.3).</p> <code>gauss_sigma</code> <code>float, or tuple of floats</code> <p>Sigma value for gaussian filtering of liquid layer. If single float the number will be sampled from <code>(0, gauss_sigma)</code>. If tuple of float gauss_sigma will be sampled from range <code>(gauss_sigma[0], gauss_sigma[1])</code>. If you want constant value use (gauss_sigma, gauss_sigma). Default: (2, 3).</p> <code>cutout_threshold</code> <code>float, or tuple of floats</code> <p>Threshold for filtering liqued layer (determines number of drops). If single float it will used as cutout_threshold. If single float the number will be sampled from <code>(0, cutout_threshold)</code>. If tuple of float cutout_threshold will be sampled from range <code>(cutout_threshold[0], cutout_threshold[1])</code>. If you want constant value use <code>(cutout_threshold, cutout_threshold)</code>. Default: (0.68, 0.68).</p> <code>intensity</code> <code>float, or tuple of floats</code> <p>Intensity of corruption. If single float the number will be sampled from <code>(0, intensity)</code>. If tuple of float intensity will be sampled from range <code>(intensity[0], intensity[1])</code>. If you want constant value use <code>(intensity, intensity)</code>. Default: (0.6, 0.6).</p> <code>mode</code> <code>string, or list of strings</code> <p>Type of corruption. Currently, supported options are 'rain' and 'mud'.  If list is provided type of corruption will be sampled list. Default: (\"rain\").</p> <code>color</code> <code>list of (r, g, b) or dict or None</code> <p>Corruption elements color. If list uses provided list as color for specified mode. If dict uses provided color for specified mode. Color for each specified mode should be provided in dict. If None uses default colors (rain: (238, 238, 175), mud: (20, 42, 63)).</p> <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 0.5.</p> <p>Targets</p> <p>image</p> <p>Image types:     uint8, float32</p> <p>Reference</p> <p>https://arxiv.org/abs/1903.12261 https://github.com/hendrycks/robustness/blob/master/ImageNet-C/create_c/make_imagenet_c.py</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>class Spatter(ImageOnlyTransform):\n    \"\"\"Apply spatter transform. It simulates corruption which can occlude a lens in the form of rain or mud.\n\n    Args:\n        mean (float, or tuple of floats): Mean value of normal distribution for generating liquid layer.\n            If single float mean will be sampled from `(0, mean)`\n            If tuple of float mean will be sampled from range `(mean[0], mean[1])`.\n            If you want constant value use (mean, mean).\n            Default (0.65, 0.65)\n        std (float, or tuple of floats): Standard deviation value of normal distribution for generating liquid layer.\n            If single float the number will be sampled from `(0, std)`.\n            If tuple of float std will be sampled from range `(std[0], std[1])`.\n            If you want constant value use (std, std).\n            Default: (0.3, 0.3).\n        gauss_sigma (float, or tuple of floats): Sigma value for gaussian filtering of liquid layer.\n            If single float the number will be sampled from `(0, gauss_sigma)`.\n            If tuple of float gauss_sigma will be sampled from range `(gauss_sigma[0], gauss_sigma[1])`.\n            If you want constant value use (gauss_sigma, gauss_sigma).\n            Default: (2, 3).\n        cutout_threshold (float, or tuple of floats): Threshold for filtering liqued layer\n            (determines number of drops). If single float it will used as cutout_threshold.\n            If single float the number will be sampled from `(0, cutout_threshold)`.\n            If tuple of float cutout_threshold will be sampled from range `(cutout_threshold[0], cutout_threshold[1])`.\n            If you want constant value use `(cutout_threshold, cutout_threshold)`.\n            Default: (0.68, 0.68).\n        intensity (float, or tuple of floats): Intensity of corruption.\n            If single float the number will be sampled from `(0, intensity)`.\n            If tuple of float intensity will be sampled from range `(intensity[0], intensity[1])`.\n            If you want constant value use `(intensity, intensity)`.\n            Default: (0.6, 0.6).\n        mode (string, or list of strings): Type of corruption. Currently, supported options are 'rain' and 'mud'.\n             If list is provided type of corruption will be sampled list. Default: (\"rain\").\n        color (list of (r, g, b) or dict or None): Corruption elements color.\n            If list uses provided list as color for specified mode.\n            If dict uses provided color for specified mode. Color for each specified mode should be provided in dict.\n            If None uses default colors (rain: (238, 238, 175), mud: (20, 42, 63)).\n        p (float): probability of applying the transform. Default: 0.5.\n\n    Targets:\n        image\n\n    Image types:\n        uint8, float32\n\n    Reference:\n        https://arxiv.org/abs/1903.12261\n        https://github.com/hendrycks/robustness/blob/master/ImageNet-C/create_c/make_imagenet_c.py\n\n    \"\"\"\n\n    class InitSchema(BaseTransformInitSchema):\n        mean: ZeroOneRangeType = (0.65, 0.65)\n        std: ZeroOneRangeType = (0.3, 0.3)\n        gauss_sigma: NonNegativeFloatRangeType = (2, 2)\n        cutout_threshold: ZeroOneRangeType = (0.68, 0.68)\n        intensity: ZeroOneRangeType = (0.6, 0.6)\n        mode: SpatterMode | Sequence[SpatterMode] = Field(\n            default=\"rain\",\n            description=\"Type of corruption ('rain', 'mud').\",\n        )\n        color: Sequence[int] | dict[str, Sequence[int]] | None = None\n\n        @field_validator(\"mode\")\n        @classmethod\n        def check_mode(cls, mode: SpatterMode | Sequence[SpatterMode]) -&gt; Sequence[SpatterMode]:\n            if isinstance(mode, str):\n                return [mode]\n            return mode\n\n        @model_validator(mode=\"after\")\n        def check_color(self) -&gt; Self:\n            if self.color is None:\n                self.color = {\"rain\": [238, 238, 175], \"mud\": [20, 42, 63]}\n\n            elif isinstance(self.color, (list, tuple)) and len(self.mode) == 1:\n                if len(self.color) != NUM_RGB_CHANNELS:\n                    msg = \"Color must be a list of three integers for RGB format.\"\n                    raise ValueError(msg)\n                self.color = {self.mode[0]: self.color}\n            elif isinstance(self.color, dict):\n                result = {}\n                for mode in self.mode:\n                    if mode not in self.color:\n                        raise ValueError(f\"Color for mode {mode} is not specified.\")\n                    if len(self.color[mode]) != NUM_RGB_CHANNELS:\n                        raise ValueError(f\"Color for mode {mode} must be in RGB format.\")\n                    result[mode] = self.color[mode]\n            else:\n                msg = \"Color must be a list of RGB values or a dict mapping mode to RGB values.\"\n                raise ValueError(msg)\n            return self\n\n    def __init__(\n        self,\n        mean: ScaleFloatType = (0.65, 0.65),\n        std: ScaleFloatType = (0.3, 0.3),\n        gauss_sigma: ScaleFloatType = (2, 2),\n        cutout_threshold: ScaleFloatType = (0.68, 0.68),\n        intensity: ScaleFloatType = (0.6, 0.6),\n        mode: SpatterMode | Sequence[SpatterMode] = \"rain\",\n        color: Sequence[int] | dict[str, Sequence[int]] | None = None,\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p=p, always_apply=always_apply)\n        self.mean = cast(Tuple[float, float], mean)\n        self.std = cast(Tuple[float, float], std)\n        self.gauss_sigma = cast(Tuple[float, float], gauss_sigma)\n        self.cutout_threshold = cast(Tuple[float, float], cutout_threshold)\n        self.intensity = cast(Tuple[float, float], intensity)\n        self.mode = mode\n        self.color = cast(Dict[str, Sequence[int]], color)\n\n    def apply(\n        self,\n        img: np.ndarray,\n        non_mud: np.ndarray,\n        mud: np.ndarray,\n        drops: np.ndarray,\n        mode: SpatterMode,\n        **params: dict[str, Any],\n    ) -&gt; np.ndarray:\n        return fmain.spatter(img, non_mud, mud, drops, mode)\n\n    def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n        height, width = params[\"shape\"][:2]\n\n        mean = random.uniform(self.mean[0], self.mean[1])\n        std = random.uniform(self.std[0], self.std[1])\n        cutout_threshold = random.uniform(self.cutout_threshold[0], self.cutout_threshold[1])\n        sigma = random.uniform(self.gauss_sigma[0], self.gauss_sigma[1])\n        mode = random.choice(self.mode)\n        intensity = random.uniform(self.intensity[0], self.intensity[1])\n        color = np.array(self.color[mode]) / 255.0\n\n        liquid_layer = random_utils.normal(size=(height, width), loc=mean, scale=std)\n        liquid_layer = gaussian_filter(liquid_layer, sigma=sigma, mode=\"nearest\")\n        liquid_layer[liquid_layer &lt; cutout_threshold] = 0\n\n        if mode == \"rain\":\n            liquid_layer = clip(liquid_layer * 255, np.uint8)\n            dist = 255 - cv2.Canny(liquid_layer, 50, 150)\n            dist = cv2.distanceTransform(dist, cv2.DIST_L2, 5)\n            _, dist = cv2.threshold(dist, 20, 20, cv2.THRESH_TRUNC)\n            dist = clip(blur(dist, 3), np.uint8)\n            dist = fmain.equalize(dist)\n\n            ker = np.array([[-2, -1, 0], [-1, 1, 1], [0, 1, 2]])\n            dist = fmain.convolve(dist, ker)\n            dist = blur(dist, 3).astype(np.float32)\n\n            m = liquid_layer * dist\n            m *= 1 / np.max(m, axis=(0, 1))\n\n            drops = m[:, :, None] * color * intensity\n            mud = None\n            non_mud = None\n        else:\n            m = np.where(liquid_layer &gt; cutout_threshold, 1, 0)\n            m = gaussian_filter(m.astype(np.float32), sigma=sigma, mode=\"nearest\")\n            m[m &lt; 1.2 * cutout_threshold] = 0\n            m = m[..., np.newaxis]\n\n            mud = m * color\n            non_mud = 1 - m\n            drops = None\n\n        return {\n            \"non_mud\": non_mud,\n            \"mud\": mud,\n            \"drops\": drops,\n            \"mode\": mode,\n        }\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, str, str, str, str, str, str]:\n        return \"mean\", \"std\", \"gauss_sigma\", \"intensity\", \"cutout_threshold\", \"mode\", \"color\"\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.Spatter.apply","title":"<code>apply (self, img, non_mud, mud, drops, mode, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def apply(\n    self,\n    img: np.ndarray,\n    non_mud: np.ndarray,\n    mud: np.ndarray,\n    drops: np.ndarray,\n    mode: SpatterMode,\n    **params: dict[str, Any],\n) -&gt; np.ndarray:\n    return fmain.spatter(img, non_mud, mud, drops, mode)\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.Spatter.get_params_dependent_on_data","title":"<code>get_params_dependent_on_data (self, params, data)</code>","text":"<p>Returns parameters dependent on input.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n    height, width = params[\"shape\"][:2]\n\n    mean = random.uniform(self.mean[0], self.mean[1])\n    std = random.uniform(self.std[0], self.std[1])\n    cutout_threshold = random.uniform(self.cutout_threshold[0], self.cutout_threshold[1])\n    sigma = random.uniform(self.gauss_sigma[0], self.gauss_sigma[1])\n    mode = random.choice(self.mode)\n    intensity = random.uniform(self.intensity[0], self.intensity[1])\n    color = np.array(self.color[mode]) / 255.0\n\n    liquid_layer = random_utils.normal(size=(height, width), loc=mean, scale=std)\n    liquid_layer = gaussian_filter(liquid_layer, sigma=sigma, mode=\"nearest\")\n    liquid_layer[liquid_layer &lt; cutout_threshold] = 0\n\n    if mode == \"rain\":\n        liquid_layer = clip(liquid_layer * 255, np.uint8)\n        dist = 255 - cv2.Canny(liquid_layer, 50, 150)\n        dist = cv2.distanceTransform(dist, cv2.DIST_L2, 5)\n        _, dist = cv2.threshold(dist, 20, 20, cv2.THRESH_TRUNC)\n        dist = clip(blur(dist, 3), np.uint8)\n        dist = fmain.equalize(dist)\n\n        ker = np.array([[-2, -1, 0], [-1, 1, 1], [0, 1, 2]])\n        dist = fmain.convolve(dist, ker)\n        dist = blur(dist, 3).astype(np.float32)\n\n        m = liquid_layer * dist\n        m *= 1 / np.max(m, axis=(0, 1))\n\n        drops = m[:, :, None] * color * intensity\n        mud = None\n        non_mud = None\n    else:\n        m = np.where(liquid_layer &gt; cutout_threshold, 1, 0)\n        m = gaussian_filter(m.astype(np.float32), sigma=sigma, mode=\"nearest\")\n        m[m &lt; 1.2 * cutout_threshold] = 0\n        m = m[..., np.newaxis]\n\n        mud = m * color\n        non_mud = 1 - m\n        drops = None\n\n    return {\n        \"non_mud\": non_mud,\n        \"mud\": mud,\n        \"drops\": drops,\n        \"mode\": mode,\n    }\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.Spatter.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, str, str, str, str, str, str]:\n    return \"mean\", \"std\", \"gauss_sigma\", \"intensity\", \"cutout_threshold\", \"mode\", \"color\"\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.Superpixels","title":"<code>class  Superpixels</code> <code>     (p_replace=(0, 0.1), n_segments=(100, 100), max_size=128, interpolation=1, always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Transform images partially/completely to their superpixel representation. This implementation uses skimage's version of the SLIC algorithm.</p> <p>Parameters:</p> Name Type Description <code>p_replace</code> <code>float or tuple of float</code> <p>Defines for any segment the probability that the pixels within that segment are replaced by their average color (otherwise, the pixels are not changed).</p> <p>Examples:</p> <ul> <li>A probability of <code>0.0</code> would mean, that the pixels in no   segment are replaced by their average color (image is not   changed at all).</li> <li>A probability of <code>0.5</code> would mean, that around half of all   segments are replaced by their average color.</li> <li>A probability of <code>1.0</code> would mean, that all segments are   replaced by their average color (resulting in a Voronoi   image).</li> </ul> <pre><code>    Behavior based on chosen data types for this parameter:\n        * If a ``float``, then that ``flat`` will always be used.\n        * If ``tuple`` ``(a, b)``, then a random probability will be\n          sampled from the interval ``[a, b]`` per image.\nn_segments (tuple of int): Rough target number of how many superpixels to generate (the algorithm\n    may deviate from this number). Lower value will lead to coarser superpixels.\n    Higher values are computationally more intensive and will hence lead to a slowdown\n    Then a value from the discrete interval ``[a..b]`` will be sampled per image.\n    If input is a single integer, the range will be ``(1, n_segments)``.\n    If interested in a fixed number of segments, use ``(n_segments, n_segments)``.\nmax_size (int or None): Maximum image size at which the augmentation is performed.\n    If the width or height of an image exceeds this value, it will be\n    downscaled before the augmentation so that the longest side matches `max_size`.\n    This is done to speed up the process. The final output image has the same size as the input image.\n    Note that in case `p_replace` is below ``1.0``,\n    the down-/upscaling will affect the not-replaced pixels too.\n    Use ``None`` to apply no down-/upscaling.\ninterpolation (OpenCV flag): flag that is used to specify the interpolation algorithm. Should be one of:\n    cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.\n    Default: cv2.INTER_LINEAR.\np (float): probability of applying the transform. Default: 0.5.\n</code></pre> <p>Targets</p> <p>image</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>class Superpixels(ImageOnlyTransform):\n    \"\"\"Transform images partially/completely to their superpixel representation.\n    This implementation uses skimage's version of the SLIC algorithm.\n\n    Args:\n        p_replace (float or tuple of float): Defines for any segment the probability that the pixels within that\n            segment are replaced by their average color (otherwise, the pixels are not changed).\n\n    Examples:\n                * A probability of ``0.0`` would mean, that the pixels in no\n                  segment are replaced by their average color (image is not\n                  changed at all).\n                * A probability of ``0.5`` would mean, that around half of all\n                  segments are replaced by their average color.\n                * A probability of ``1.0`` would mean, that all segments are\n                  replaced by their average color (resulting in a Voronoi\n                  image).\n            Behavior based on chosen data types for this parameter:\n                * If a ``float``, then that ``flat`` will always be used.\n                * If ``tuple`` ``(a, b)``, then a random probability will be\n                  sampled from the interval ``[a, b]`` per image.\n        n_segments (tuple of int): Rough target number of how many superpixels to generate (the algorithm\n            may deviate from this number). Lower value will lead to coarser superpixels.\n            Higher values are computationally more intensive and will hence lead to a slowdown\n            Then a value from the discrete interval ``[a..b]`` will be sampled per image.\n            If input is a single integer, the range will be ``(1, n_segments)``.\n            If interested in a fixed number of segments, use ``(n_segments, n_segments)``.\n        max_size (int or None): Maximum image size at which the augmentation is performed.\n            If the width or height of an image exceeds this value, it will be\n            downscaled before the augmentation so that the longest side matches `max_size`.\n            This is done to speed up the process. The final output image has the same size as the input image.\n            Note that in case `p_replace` is below ``1.0``,\n            the down-/upscaling will affect the not-replaced pixels too.\n            Use ``None`` to apply no down-/upscaling.\n        interpolation (OpenCV flag): flag that is used to specify the interpolation algorithm. Should be one of:\n            cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.\n            Default: cv2.INTER_LINEAR.\n        p (float): probability of applying the transform. Default: 0.5.\n\n    Targets:\n        image\n\n    \"\"\"\n\n    class InitSchema(BaseTransformInitSchema):\n        p_replace: ZeroOneRangeType = (0, 0.1)\n        n_segments: OnePlusIntRangeType = (100, 100)\n        max_size: int | None = Field(default=128, ge=1, description=\"Maximum image size for the transformation.\")\n        interpolation: InterpolationType = cv2.INTER_LINEAR\n\n    def __init__(\n        self,\n        p_replace: ScaleFloatType = (0, 0.1),\n        n_segments: ScaleIntType = (100, 100),\n        max_size: int | None = 128,\n        interpolation: int = cv2.INTER_LINEAR,\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p=p, always_apply=always_apply)\n        self.p_replace = cast(Tuple[float, float], p_replace)\n        self.n_segments = cast(Tuple[int, int], n_segments)\n        self.max_size = max_size\n        self.interpolation = interpolation\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return \"p_replace\", \"n_segments\", \"max_size\", \"interpolation\"\n\n    def get_params(self) -&gt; dict[str, Any]:\n        n_segments = random.randint(self.n_segments[0], self.n_segments[1])\n        p = random.uniform(*self.p_replace)\n        return {\"replace_samples\": random_utils.random(n_segments) &lt; p, \"n_segments\": n_segments}\n\n    def apply(\n        self,\n        img: np.ndarray,\n        replace_samples: Sequence[bool],\n        n_segments: int,\n        **kwargs: Any,\n    ) -&gt; np.ndarray:\n        return fmain.superpixels(img, n_segments, replace_samples, self.max_size, self.interpolation)\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.Superpixels.apply","title":"<code>apply (self, img, replace_samples, n_segments, **kwargs)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def apply(\n    self,\n    img: np.ndarray,\n    replace_samples: Sequence[bool],\n    n_segments: int,\n    **kwargs: Any,\n) -&gt; np.ndarray:\n    return fmain.superpixels(img, n_segments, replace_samples, self.max_size, self.interpolation)\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.Superpixels.get_params","title":"<code>get_params (self)</code>","text":"<p>Returns parameters independent of input.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_params(self) -&gt; dict[str, Any]:\n    n_segments = random.randint(self.n_segments[0], self.n_segments[1])\n    p = random.uniform(*self.p_replace)\n    return {\"replace_samples\": random_utils.random(n_segments) &lt; p, \"n_segments\": n_segments}\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.Superpixels.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return \"p_replace\", \"n_segments\", \"max_size\", \"interpolation\"\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.TemplateTransform","title":"<code>class  TemplateTransform</code> <code>     (templates, img_weight=(0.5, 0.5), template_weight=(0.5, 0.5), template_transform=None, name=None, always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Apply blending of input image with specified templates</p> <p>Parameters:</p> Name Type Description <code>templates</code> <code>numpy array or list of numpy arrays</code> <p>Images as template for transform.</p> <code>img_weight</code> <code>ScaleFloatType</code> <p>If single float weight will be sampled from (0, img_weight). If tuple of float img_weight will be in range <code>[img_weight[0], img_weight[1])</code>. If you want fixed weight, use (img_weight, img_weight) Default: (0.5, 0.5).</p> <code>template_weight</code> <code>ScaleFloatType</code> <p>If single float weight will be sampled from (0, template_weight). If tuple of float template_weight will be in range <code>[template_weight[0], template_weight[1])</code>. If you want fixed weight, use (template_weight, template_weight) Default: (0.5, 0.5).</p> <code>template_transform</code> <code>Callable[..., Any] | None</code> <p>transformation object which could be applied to template, must produce template the same size as input image.</p> <code>name</code> <code>str | None</code> <p>(Optional) Name of transform, used only for deserialization.</p> <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 0.5.</p> <p>Targets</p> <p>image</p> <p>Image types:     uint8, float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>class TemplateTransform(ImageOnlyTransform):\n    \"\"\"Apply blending of input image with specified templates\n    Args:\n        templates (numpy array or list of numpy arrays): Images as template for transform.\n        img_weight: If single float weight will be sampled from (0, img_weight).\n            If tuple of float img_weight will be in range `[img_weight[0], img_weight[1])`.\n            If you want fixed weight, use (img_weight, img_weight)\n            Default: (0.5, 0.5).\n        template_weight: If single float weight will be sampled from (0, template_weight).\n            If tuple of float template_weight will be in range `[template_weight[0], template_weight[1])`.\n            If you want fixed weight, use (template_weight, template_weight)\n            Default: (0.5, 0.5).\n        template_transform: transformation object which could be applied to template,\n            must produce template the same size as input image.\n        name: (Optional) Name of transform, used only for deserialization.\n        p: probability of applying the transform. Default: 0.5.\n    Targets:\n        image\n    Image types:\n        uint8, float32\n    \"\"\"\n\n    class InitSchema(BaseTransformInitSchema):\n        templates: np.ndarray | Sequence[np.ndarray] = Field(..., description=\"Images as template for transform.\")\n        img_weight: ZeroOneRangeType = (0.5, 0.5)\n        template_weight: ZeroOneRangeType = (0.5, 0.5)\n        template_transform: Callable[..., Any] | None = Field(\n            default=None,\n            description=\"Transformation object applied to template.\",\n        )\n        name: str | None = Field(default=None, description=\"Name of transform, used only for deserialization.\")\n\n        @field_validator(\"templates\")\n        @classmethod\n        def validate_templates(cls, v: np.ndarray | list[np.ndarray]) -&gt; list[np.ndarray]:\n            if isinstance(v, np.ndarray):\n                return [v]\n            if isinstance(v, list):\n                if not all(isinstance(item, np.ndarray) for item in v):\n                    msg = \"All templates must be numpy arrays.\"\n                    raise ValueError(msg)\n                return v\n            msg = \"Templates must be a numpy array or a list of numpy arrays.\"\n            raise TypeError(msg)\n\n    def __init__(\n        self,\n        templates: np.ndarray | list[np.ndarray],\n        img_weight: ScaleFloatType = (0.5, 0.5),\n        template_weight: ScaleFloatType = (0.5, 0.5),\n        template_transform: Callable[..., Any] | None = None,\n        name: str | None = None,\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p=p, always_apply=always_apply)\n        self.templates = templates\n        self.img_weight = cast(Tuple[float, float], img_weight)\n        self.template_weight = cast(Tuple[float, float], template_weight)\n        self.template_transform = template_transform\n        self.name = name\n\n    def apply(\n        self,\n        img: np.ndarray,\n        template: np.ndarray,\n        img_weight: float,\n        template_weight: float,\n        **params: Any,\n    ) -&gt; np.ndarray:\n        return add_weighted(img, img_weight, template, template_weight)\n\n    def get_params(self) -&gt; dict[str, float]:\n        return {\n            \"img_weight\": random.uniform(self.img_weight[0], self.img_weight[1]),\n            \"template_weight\": random.uniform(self.template_weight[0], self.template_weight[1]),\n        }\n\n    def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n        img = data[\"image\"] if \"image\" in data else data[\"images\"][0]\n        template = random.choice(self.templates)\n\n        if self.template_transform is not None:\n            template = self.template_transform(image=template)[\"image\"]\n\n        if get_num_channels(template) not in [1, get_num_channels(img)]:\n            msg = (\n                \"Template must be a single channel or \"\n                \"has the same number of channels as input \"\n                f\"image ({get_num_channels(img)}), got {get_num_channels(template)}\"\n            )\n            raise ValueError(msg)\n\n        if template.dtype != img.dtype:\n            msg = \"Image and template must be the same image type\"\n            raise ValueError(msg)\n\n        if img.shape[:2] != template.shape[:2]:\n            raise ValueError(f\"Image and template must be the same size, got {img.shape[:2]} and {template.shape[:2]}\")\n\n        if get_num_channels(template) == 1 and get_num_channels(img) &gt; 1:\n            template = np.stack((template,) * get_num_channels(img), axis=-1)\n\n        # in order to support grayscale image with dummy dim\n        template = template.reshape(img.shape)\n\n        return {\"template\": template}\n\n    @classmethod\n    def is_serializable(cls) -&gt; bool:\n        return False\n\n    def to_dict_private(self) -&gt; dict[str, Any]:\n        if self.name is None:\n            msg = (\n                \"To make a TemplateTransform serializable you should provide the `name` argument, \"\n                \"e.g. `TemplateTransform(name='my_transform', ...)`.\"\n            )\n            raise ValueError(msg)\n        return {\"__class_fullname__\": self.get_class_fullname(), \"__name__\": self.name}\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.TemplateTransform.apply","title":"<code>apply (self, img, template, img_weight, template_weight, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def apply(\n    self,\n    img: np.ndarray,\n    template: np.ndarray,\n    img_weight: float,\n    template_weight: float,\n    **params: Any,\n) -&gt; np.ndarray:\n    return add_weighted(img, img_weight, template, template_weight)\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.TemplateTransform.get_params","title":"<code>get_params (self)</code>","text":"<p>Returns parameters independent of input.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_params(self) -&gt; dict[str, float]:\n    return {\n        \"img_weight\": random.uniform(self.img_weight[0], self.img_weight[1]),\n        \"template_weight\": random.uniform(self.template_weight[0], self.template_weight[1]),\n    }\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.TemplateTransform.get_params_dependent_on_data","title":"<code>get_params_dependent_on_data (self, params, data)</code>","text":"<p>Returns parameters dependent on input.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n    img = data[\"image\"] if \"image\" in data else data[\"images\"][0]\n    template = random.choice(self.templates)\n\n    if self.template_transform is not None:\n        template = self.template_transform(image=template)[\"image\"]\n\n    if get_num_channels(template) not in [1, get_num_channels(img)]:\n        msg = (\n            \"Template must be a single channel or \"\n            \"has the same number of channels as input \"\n            f\"image ({get_num_channels(img)}), got {get_num_channels(template)}\"\n        )\n        raise ValueError(msg)\n\n    if template.dtype != img.dtype:\n        msg = \"Image and template must be the same image type\"\n        raise ValueError(msg)\n\n    if img.shape[:2] != template.shape[:2]:\n        raise ValueError(f\"Image and template must be the same size, got {img.shape[:2]} and {template.shape[:2]}\")\n\n    if get_num_channels(template) == 1 and get_num_channels(img) &gt; 1:\n        template = np.stack((template,) * get_num_channels(img), axis=-1)\n\n    # in order to support grayscale image with dummy dim\n    template = template.reshape(img.shape)\n\n    return {\"template\": template}\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.ToFloat","title":"<code>class  ToFloat</code> <code>     (max_value=None, p=1.0, always_apply=None)                 </code>  [view source on GitHub]","text":"<p>Divide pixel values by <code>max_value</code> to get a float32 output array where all values lie in the range [0, 1.0]. If <code>max_value</code> is None the transform will try to infer the maximum value by inspecting the data type of the input image.</p> <p>See Also:     :class:<code>~albumentations.augmentations.transforms.FromFloat</code></p> <p>Parameters:</p> Name Type Description <code>max_value</code> <code>float | None</code> <p>maximum possible input value. Default: None.</p> <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 1.0.</p> <p>Targets</p> <p>image</p> <p>Image types:     any type</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>class ToFloat(ImageOnlyTransform):\n    \"\"\"Divide pixel values by `max_value` to get a float32 output array where all values lie in the range [0, 1.0].\n    If `max_value` is None the transform will try to infer the maximum value by inspecting the data type of the input\n    image.\n\n    See Also:\n        :class:`~albumentations.augmentations.transforms.FromFloat`\n\n    Args:\n        max_value: maximum possible input value. Default: None.\n        p: probability of applying the transform. Default: 1.0.\n\n    Targets:\n        image\n\n    Image types:\n        any type\n\n    \"\"\"\n\n    class InitSchema(BaseTransformInitSchema):\n        max_value: float | None = Field(default=None, description=\"Maximum possible input value.\")\n        p: ProbabilityType = 1\n\n    def __init__(self, max_value: float | None = None, p: float = 1.0, always_apply: bool | None = None):\n        super().__init__(p, always_apply)\n        self.max_value = max_value\n\n    def apply(self, img: np.ndarray, **params: Any) -&gt; np.ndarray:\n        return fmain.to_float(img, self.max_value)\n\n    def get_transform_init_args_names(self) -&gt; tuple[str]:\n        return (\"max_value\",)\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.ToFloat.apply","title":"<code>apply (self, img, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def apply(self, img: np.ndarray, **params: Any) -&gt; np.ndarray:\n    return fmain.to_float(img, self.max_value)\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.ToFloat.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str]:\n    return (\"max_value\",)\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.ToGray","title":"<code>class  ToGray</code> <code>     (num_output_channels=3, method='weighted_average', always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Convert an image to grayscale and optionally replicate the grayscale channel.</p> <p>This transform first converts a color image to a single-channel grayscale image using various methods, then replicates the grayscale channel if num_output_channels is greater than 1.</p> <p>Parameters:</p> Name Type Description <code>num_output_channels</code> <code>int</code> <p>The number of channels in the output image. If greater than 1, the grayscale channel will be replicated. Default: 3.</p> <code>method</code> <code>Literal[\"weighted_average\", \"from_lab\", \"desaturation\", \"average\", \"max\", \"pca\"]</code> <p>The method used for grayscale conversion: - \"weighted_average\": Uses a weighted sum of RGB channels (0.299R + 0.587G + 0.114B).   Works only with 3-channel images. Provides realistic results based on human perception. - \"from_lab\": Extracts the L channel from the LAB color space.   Works only with 3-channel images. Gives perceptually uniform results. - \"desaturation\": Averages the maximum and minimum values across channels.   Works with any number of channels. Fast but may not preserve perceived brightness well. - \"average\": Simple average of all channels.   Works with any number of channels. Fast but may not give realistic results. - \"max\": Takes the maximum value across all channels.   Works with any number of channels. Tends to produce brighter results. - \"pca\": Applies Principal Component Analysis to reduce channels.   Works with any number of channels. Can preserve more information but is computationally intensive.</p> <code>p</code> <code>float</code> <p>Probability of applying the transform. Default: 0.5.</p> <p>Exceptions:</p> Type Description <code>TypeError</code> <p>If the input image doesn't have 3 channels for methods that require it.</p> <p>Note</p> <ul> <li>The transform first converts the input image to single-channel grayscale, then replicates   this channel if num_output_channels &gt; 1.</li> <li>\"weighted_average\" and \"from_lab\" are typically used in image processing and computer vision   applications where accurate representation of human perception is important.</li> <li>\"desaturation\" and \"average\" are often used in simple image manipulation tools or when   computational speed is a priority.</li> <li>\"max\" method can be useful in scenarios where preserving bright features is important,   such as in some medical imaging applications.</li> <li>\"pca\" might be used in advanced image analysis tasks or when dealing with hyperspectral images.</li> </ul> <p>Image types:     uint8, float32</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Grayscale image with the specified number of channels.</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>class ToGray(ImageOnlyTransform):\n    \"\"\"Convert an image to grayscale and optionally replicate the grayscale channel.\n\n    This transform first converts a color image to a single-channel grayscale image using various methods,\n    then replicates the grayscale channel if num_output_channels is greater than 1.\n\n    Args:\n        num_output_channels (int): The number of channels in the output image. If greater than 1,\n            the grayscale channel will be replicated. Default: 3.\n        method (Literal[\"weighted_average\", \"from_lab\", \"desaturation\", \"average\", \"max\", \"pca\"]):\n            The method used for grayscale conversion:\n            - \"weighted_average\": Uses a weighted sum of RGB channels (0.299R + 0.587G + 0.114B).\n              Works only with 3-channel images. Provides realistic results based on human perception.\n            - \"from_lab\": Extracts the L channel from the LAB color space.\n              Works only with 3-channel images. Gives perceptually uniform results.\n            - \"desaturation\": Averages the maximum and minimum values across channels.\n              Works with any number of channels. Fast but may not preserve perceived brightness well.\n            - \"average\": Simple average of all channels.\n              Works with any number of channels. Fast but may not give realistic results.\n            - \"max\": Takes the maximum value across all channels.\n              Works with any number of channels. Tends to produce brighter results.\n            - \"pca\": Applies Principal Component Analysis to reduce channels.\n              Works with any number of channels. Can preserve more information but is computationally intensive.\n        p (float): Probability of applying the transform. Default: 0.5.\n\n    Raises:\n        TypeError: If the input image doesn't have 3 channels for methods that require it.\n\n    Note:\n        - The transform first converts the input image to single-channel grayscale, then replicates\n          this channel if num_output_channels &gt; 1.\n        - \"weighted_average\" and \"from_lab\" are typically used in image processing and computer vision\n          applications where accurate representation of human perception is important.\n        - \"desaturation\" and \"average\" are often used in simple image manipulation tools or when\n          computational speed is a priority.\n        - \"max\" method can be useful in scenarios where preserving bright features is important,\n          such as in some medical imaging applications.\n        - \"pca\" might be used in advanced image analysis tasks or when dealing with hyperspectral images.\n\n    Image types:\n        uint8, float32\n\n    Returns:\n        np.ndarray: Grayscale image with the specified number of channels.\n    \"\"\"\n\n    class InitSchema(BaseTransformInitSchema):\n        num_output_channels: int = Field(default=3, description=\"The number of output channels.\", ge=1)\n        method: Literal[\"weighted_average\", \"from_lab\", \"desaturation\", \"average\", \"max\", \"pca\"]\n\n    def __init__(\n        self,\n        num_output_channels: int = 3,\n        method: Literal[\"weighted_average\", \"from_lab\", \"desaturation\", \"average\", \"max\", \"pca\"] = \"weighted_average\",\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p=p, always_apply=always_apply)\n        self.num_output_channels = num_output_channels\n        self.method = method\n\n    def apply(self, img: np.ndarray, **params: Any) -&gt; np.ndarray:\n        if is_grayscale_image(img):\n            warnings.warn(\"The image is already gray.\", stacklevel=2)\n            return img\n\n        num_channels = get_num_channels(img)\n\n        if num_channels != NUM_RGB_CHANNELS and self.method not in {\"desaturation\", \"average\", \"max\", \"pca\"}:\n            msg = \"ToGray transformation expects 3-channel images.\"\n            raise TypeError(msg)\n\n        return fmain.to_gray(img, self.num_output_channels, self.method)\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return \"num_output_channels\", \"method\"\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.ToGray.apply","title":"<code>apply (self, img, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def apply(self, img: np.ndarray, **params: Any) -&gt; np.ndarray:\n    if is_grayscale_image(img):\n        warnings.warn(\"The image is already gray.\", stacklevel=2)\n        return img\n\n    num_channels = get_num_channels(img)\n\n    if num_channels != NUM_RGB_CHANNELS and self.method not in {\"desaturation\", \"average\", \"max\", \"pca\"}:\n        msg = \"ToGray transformation expects 3-channel images.\"\n        raise TypeError(msg)\n\n    return fmain.to_gray(img, self.num_output_channels, self.method)\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.ToGray.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return \"num_output_channels\", \"method\"\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.ToRGB","title":"<code>class  ToRGB</code> <code>     (p=1.0, always_apply=None)                 </code>  [view source on GitHub]","text":"<p>Convert the input grayscale image to RGB.</p> <p>Parameters:</p> Name Type Description <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 1.</p> <p>Targets</p> <p>image</p> <p>Image types:     uint8, float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>class ToRGB(ImageOnlyTransform):\n    \"\"\"Convert the input grayscale image to RGB.\n\n    Args:\n        p: probability of applying the transform. Default: 1.\n\n    Targets:\n        image\n\n    Image types:\n        uint8, float32\n\n    \"\"\"\n\n    def __init__(self, p: float = 1.0, always_apply: bool | None = None):\n        super().__init__(p=p, always_apply=always_apply)\n\n    def apply(self, img: np.ndarray, **params: Any) -&gt; np.ndarray:\n        if is_rgb_image(img):\n            warnings.warn(\"The image is already an RGB.\", stacklevel=2)\n            return np.ascontiguousarray(img)\n        if not is_grayscale_image(img):\n            msg = \"ToRGB transformation expects 2-dim images or 3-dim with the last dimension equal to 1.\"\n            raise TypeError(msg)\n\n        return fmain.grayscale_to_multichannel(img, num_output_channels=3)\n\n    def get_transform_init_args_names(self) -&gt; tuple[()]:\n        return ()\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.ToRGB.__init__","title":"<code>__init__ (self, p=1.0, always_apply=None)</code>  <code>special</code>","text":"<p>Initialize self.  See help(type(self)) for accurate signature.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def __init__(self, p: float = 1.0, always_apply: bool | None = None):\n    super().__init__(p=p, always_apply=always_apply)\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.ToRGB.apply","title":"<code>apply (self, img, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def apply(self, img: np.ndarray, **params: Any) -&gt; np.ndarray:\n    if is_rgb_image(img):\n        warnings.warn(\"The image is already an RGB.\", stacklevel=2)\n        return np.ascontiguousarray(img)\n    if not is_grayscale_image(img):\n        msg = \"ToRGB transformation expects 2-dim images or 3-dim with the last dimension equal to 1.\"\n        raise TypeError(msg)\n\n    return fmain.grayscale_to_multichannel(img, num_output_channels=3)\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.ToRGB.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[()]:\n    return ()\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.ToSepia","title":"<code>class  ToSepia</code> <code>     (p=0.5, always_apply=None)                 </code>  [view source on GitHub]","text":"<p>Applies sepia filter to the input RGB image</p> <p>Parameters:</p> Name Type Description <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 0.5.</p> <p>Targets</p> <p>image</p> <p>Image types:     uint8, float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>class ToSepia(ImageOnlyTransform):\n    \"\"\"Applies sepia filter to the input RGB image\n\n    Args:\n        p: probability of applying the transform. Default: 0.5.\n\n    Targets:\n        image\n\n    Image types:\n        uint8, float32\n\n    \"\"\"\n\n    def __init__(self, p: float = 0.5, always_apply: bool | None = None):\n        super().__init__(p, always_apply)\n        self.sepia_transformation_matrix = np.array(\n            [[0.393, 0.769, 0.189], [0.349, 0.686, 0.168], [0.272, 0.534, 0.131]],\n        )\n\n    def apply(self, img: np.ndarray, **params: Any) -&gt; np.ndarray:\n        if not is_rgb_image(img):\n            msg = \"ToSepia transformation expects 3-channel images.\"\n            raise TypeError(msg)\n        return fmain.linear_transformation_rgb(img, self.sepia_transformation_matrix)\n\n    def get_transform_init_args_names(self) -&gt; tuple[()]:\n        return ()\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.ToSepia.__init__","title":"<code>__init__ (self, p=0.5, always_apply=None)</code>  <code>special</code>","text":"<p>Initialize self.  See help(type(self)) for accurate signature.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def __init__(self, p: float = 0.5, always_apply: bool | None = None):\n    super().__init__(p, always_apply)\n    self.sepia_transformation_matrix = np.array(\n        [[0.393, 0.769, 0.189], [0.349, 0.686, 0.168], [0.272, 0.534, 0.131]],\n    )\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.ToSepia.apply","title":"<code>apply (self, img, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def apply(self, img: np.ndarray, **params: Any) -&gt; np.ndarray:\n    if not is_rgb_image(img):\n        msg = \"ToSepia transformation expects 3-channel images.\"\n        raise TypeError(msg)\n    return fmain.linear_transformation_rgb(img, self.sepia_transformation_matrix)\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.ToSepia.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[()]:\n    return ()\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.UnsharpMask","title":"<code>class  UnsharpMask</code> <code>     (blur_limit=(3, 7), sigma_limit=0.0, alpha=(0.2, 0.5), threshold=10, always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Sharpen the input image using Unsharp Masking processing and overlays the result with the original image.</p> <p>Parameters:</p> Name Type Description <code>blur_limit</code> <code>ScaleIntType</code> <p>maximum Gaussian kernel size for blurring the input image. Must be zero or odd and in range [0, inf). If set to 0 it will be computed from sigma as <code>round(sigma * (3 if img.dtype == np.uint8 else 4) * 2 + 1) + 1</code>. If set single value <code>blur_limit</code> will be in range (0, blur_limit). Default: (3, 7).</p> <code>sigma_limit</code> <code>ScaleFloatType</code> <p>Gaussian kernel standard deviation. Must be in range [0, inf). If set single value <code>sigma_limit</code> will be in range (0, sigma_limit). If set to 0 sigma will be computed as <code>sigma = 0.3*((ksize-1)*0.5 - 1) + 0.8</code>. Default: 0.</p> <code>alpha</code> <code>ScaleFloatType</code> <p>range to choose the visibility of the sharpened image. At 0, only the original image is visible, at 1.0 only its sharpened version is visible. Default: (0.2, 0.5).</p> <code>threshold</code> <code>int</code> <p>Value to limit sharpening only for areas with high pixel difference between original image and it's smoothed version. Higher threshold means less sharpening on flat areas. Must be in range [0, 255]. Default: 10.</p> <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 0.5.</p> <p>Reference</p> <p>arxiv.org/pdf/2107.10833.pdf</p> <p>Targets</p> <p>image</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>class UnsharpMask(ImageOnlyTransform):\n    \"\"\"Sharpen the input image using Unsharp Masking processing and overlays the result with the original image.\n\n    Args:\n        blur_limit: maximum Gaussian kernel size for blurring the input image.\n            Must be zero or odd and in range [0, inf). If set to 0 it will be computed from sigma\n            as `round(sigma * (3 if img.dtype == np.uint8 else 4) * 2 + 1) + 1`.\n            If set single value `blur_limit` will be in range (0, blur_limit).\n            Default: (3, 7).\n        sigma_limit: Gaussian kernel standard deviation. Must be in range [0, inf).\n            If set single value `sigma_limit` will be in range (0, sigma_limit).\n            If set to 0 sigma will be computed as `sigma = 0.3*((ksize-1)*0.5 - 1) + 0.8`. Default: 0.\n        alpha: range to choose the visibility of the sharpened image.\n            At 0, only the original image is visible, at 1.0 only its sharpened version is visible.\n            Default: (0.2, 0.5).\n        threshold: Value to limit sharpening only for areas with high pixel difference between original image\n            and it's smoothed version. Higher threshold means less sharpening on flat areas.\n            Must be in range [0, 255]. Default: 10.\n        p: probability of applying the transform. Default: 0.5.\n\n    Reference:\n        arxiv.org/pdf/2107.10833.pdf\n\n    Targets:\n        image\n\n    \"\"\"\n\n    class InitSchema(BaseTransformInitSchema):\n        sigma_limit: NonNegativeFloatRangeType = 0\n        alpha: ZeroOneRangeType = (0.2, 0.5)\n        threshold: int = Field(default=10, ge=0, le=255, description=\"Threshold for limiting sharpening.\")\n\n        blur_limit: ScaleIntType = Field(\n            default=(3, 7),\n            description=\"Maximum kernel size for blurring the input image.\",\n        )\n\n        @field_validator(\"blur_limit\")\n        @classmethod\n        def process_blur(cls, value: ScaleIntType, info: ValidationInfo) -&gt; tuple[int, int]:\n            return process_blur_limit(value, info, min_value=3)\n\n    def __init__(\n        self,\n        blur_limit: ScaleIntType = (3, 7),\n        sigma_limit: ScaleFloatType = 0.0,\n        alpha: ScaleFloatType = (0.2, 0.5),\n        threshold: int = 10,\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p=p, always_apply=always_apply)\n        self.blur_limit = cast(Tuple[int, int], blur_limit)\n        self.sigma_limit = cast(Tuple[float, float], sigma_limit)\n        self.alpha = cast(Tuple[float, float], alpha)\n        self.threshold = threshold\n\n    def get_params(self) -&gt; dict[str, Any]:\n        return {\n            \"ksize\": random.randrange(self.blur_limit[0], self.blur_limit[1] + 1, 2),\n            \"sigma\": random.uniform(*self.sigma_limit),\n            \"alpha\": random.uniform(*self.alpha),\n        }\n\n    def apply(self, img: np.ndarray, ksize: int, sigma: int, alpha: float, **params: Any) -&gt; np.ndarray:\n        return fmain.unsharp_mask(img, ksize, sigma=sigma, alpha=alpha, threshold=self.threshold)\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return \"blur_limit\", \"sigma_limit\", \"alpha\", \"threshold\"\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.UnsharpMask.apply","title":"<code>apply (self, img, ksize, sigma, alpha, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def apply(self, img: np.ndarray, ksize: int, sigma: int, alpha: float, **params: Any) -&gt; np.ndarray:\n    return fmain.unsharp_mask(img, ksize, sigma=sigma, alpha=alpha, threshold=self.threshold)\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.UnsharpMask.get_params","title":"<code>get_params (self)</code>","text":"<p>Returns parameters independent of input.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_params(self) -&gt; dict[str, Any]:\n    return {\n        \"ksize\": random.randrange(self.blur_limit[0], self.blur_limit[1] + 1, 2),\n        \"sigma\": random.uniform(*self.sigma_limit),\n        \"alpha\": random.uniform(*self.alpha),\n    }\n</code></pre>"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.UnsharpMask.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return \"blur_limit\", \"sigma_limit\", \"alpha\", \"threshold\"\n</code></pre>"},{"location":"api_reference/augmentations/blur/","title":"Index","text":"<ul> <li>Blur transforms (albumentations.augmentations.blur.transforms)</li> </ul>"},{"location":"api_reference/augmentations/blur/functional/","title":"Blur functional transforms (augmentations.blur.functional)","text":""},{"location":"api_reference/augmentations/blur/transforms/","title":"Blur transforms (augmentations.blur.transforms)","text":""},{"location":"api_reference/augmentations/blur/transforms/#albumentations.augmentations.blur.transforms.AdvancedBlur","title":"<code>class  AdvancedBlur</code> <code>     (blur_limit=(3, 7), sigma_x_limit=(0.2, 1.0), sigma_y_limit=(0.2, 1.0), sigmaX_limit=None, sigmaY_limit=None, rotate_limit=90, beta_limit=(0.5, 8.0), noise_limit=(0.9, 1.1), always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Blurs the input image using a Generalized Normal filter with randomly selected parameters.</p> <p>This transform also adds multiplicative noise to the generated kernel before convolution, affecting the image in a unique way that combines blurring and noise injection for enhanced data augmentation.</p> <p>Parameters:</p> Name Type Description <code>blur_limit</code> <code>ScaleIntType</code> <p>Maximum Gaussian kernel size for blurring the input image. Must be zero or odd and in range [0, inf). If set to 0, it will be computed from sigma as <code>round(sigma * (3 if img.dtype == np.uint8 else 4) * 2 + 1) + 1</code>. If a single value is provided, <code>blur_limit</code> will be in the range (0, blur_limit). Defaults to (3, 7).</p> <code>sigma_x_limit</code> <code>ScaleFloatType</code> <p>Gaussian kernel standard deviation for the X dimension. Must be in range [0, inf). If a single value is provided, <code>sigma_x_limit</code> will be in the range (0, sigma_limit). If set to 0, sigma will be computed as <code>sigma = 0.3*((ksize-1)*0.5 - 1) + 0.8</code>. Defaults to (0.2, 1.0).</p> <code>sigma_y_limit</code> <code>ScaleFloatType</code> <p>Gaussian kernel standard deviation for the Y dimension. Must follow the same rules as <code>sigma_x_limit</code>. Defaults to (0.2, 1.0).</p> <code>rotate_limit</code> <code>ScaleIntType</code> <p>Range from which a random angle used to rotate the Gaussian kernel is picked. If limit is a single int, an angle is picked from (-rotate_limit, rotate_limit). Defaults to (-90, 90).</p> <code>beta_limit</code> <code>ScaleFloatType</code> <p>Distribution shape parameter. 1 represents the normal distribution. Values below 1.0 make distribution tails heavier than normal, and values above 1.0 make it lighter than normal. Defaults to (0.5, 8.0).</p> <code>noise_limit</code> <code>ScaleFloatType</code> <p>Multiplicative factor that controls the strength of kernel noise. Must be positive and preferably centered around 1.0. If a single value is provided, <code>noise_limit</code> will be in the range (0, noise_limit). Defaults to (0.75, 1.25).</p> <code>p</code> <code>float</code> <p>Probability of applying the transform. Defaults to 0.5.</p> <p>Reference</p> <p>\"Real-ESRGAN: Training Real-World Blind Super-Resolution with Pure Synthetic Data\", available at https://arxiv.org/abs/2107.10833</p> <p>Targets</p> <p>image</p> <p>Image types:     uint8, float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/blur/transforms.py</code> Python<pre><code>class AdvancedBlur(ImageOnlyTransform):\n    \"\"\"Blurs the input image using a Generalized Normal filter with randomly selected parameters.\n\n    This transform also adds multiplicative noise to the generated kernel before convolution,\n    affecting the image in a unique way that combines blurring and noise injection for enhanced\n    data augmentation.\n\n    Args:\n        blur_limit (ScaleIntType, optional): Maximum Gaussian kernel size for blurring the input image.\n            Must be zero or odd and in range [0, inf). If set to 0, it will be computed from sigma\n            as `round(sigma * (3 if img.dtype == np.uint8 else 4) * 2 + 1) + 1`.\n            If a single value is provided, `blur_limit` will be in the range (0, blur_limit).\n            Defaults to (3, 7).\n        sigma_x_limit ScaleFloatType: Gaussian kernel standard deviation for the X dimension.\n            Must be in range [0, inf). If a single value is provided, `sigma_x_limit` will be in the range\n            (0, sigma_limit). If set to 0, sigma will be computed as `sigma = 0.3*((ksize-1)*0.5 - 1) + 0.8`.\n            Defaults to (0.2, 1.0).\n        sigma_y_limit ScaleFloatType: Gaussian kernel standard deviation for the Y dimension.\n            Must follow the same rules as `sigma_x_limit`.\n            Defaults to (0.2, 1.0).\n        rotate_limit (ScaleIntType, optional): Range from which a random angle used to rotate the Gaussian kernel\n            is picked. If limit is a single int, an angle is picked from (-rotate_limit, rotate_limit).\n            Defaults to (-90, 90).\n        beta_limit (ScaleFloatType, optional): Distribution shape parameter. 1 represents the normal distribution.\n            Values below 1.0 make distribution tails heavier than normal, and values above 1.0 make it\n            lighter than normal.\n            Defaults to (0.5, 8.0).\n        noise_limit (ScaleFloatType, optional): Multiplicative factor that controls the strength of kernel noise.\n            Must be positive and preferably centered around 1.0. If a single value is provided,\n            `noise_limit` will be in the range (0, noise_limit).\n            Defaults to (0.75, 1.25).\n        p (float, optional): Probability of applying the transform.\n            Defaults to 0.5.\n\n    Reference:\n        \"Real-ESRGAN: Training Real-World Blind Super-Resolution with Pure Synthetic Data\",\n        available at https://arxiv.org/abs/2107.10833\n\n    Targets:\n        image\n\n    Image types:\n        uint8, float32\n\n    \"\"\"\n\n    class InitSchema(BlurInitSchema):\n        sigma_x_limit: NonNegativeFloatRangeType = (0.2, 1.0)\n        sigma_y_limit: NonNegativeFloatRangeType = (0.2, 1.0)\n        beta_limit: NonNegativeFloatRangeType = (0.5, 8.0)\n        noise_limit: NonNegativeFloatRangeType = (0.75, 1.25)\n        rotate_limit: SymmetricRangeType = (-90, 90)\n\n        @field_validator(\"beta_limit\")\n        @classmethod\n        def check_beta_limit(cls, value: ScaleFloatType) -&gt; tuple[float, float]:\n            result = to_tuple(value, low=0)\n            if not (result[0] &lt; 1.0 &lt; result[1]):\n                msg = \"beta_limit is expected to include 1.0.\"\n                raise ValueError(msg)\n            return result\n\n        @model_validator(mode=\"after\")\n        def validate_limits(self) -&gt; Self:\n            if (\n                isinstance(self.sigma_x_limit, (tuple, list))\n                and self.sigma_x_limit[0] == 0\n                and isinstance(self.sigma_y_limit, (tuple, list))\n                and self.sigma_y_limit[0] == 0\n            ):\n                msg = \"sigma_x_limit and sigma_y_limit minimum value cannot be both equal to 0.\"\n                raise ValueError(msg)\n            return self\n\n    def __init__(\n        self,\n        blur_limit: ScaleIntType = (3, 7),\n        sigma_x_limit: ScaleFloatType = (0.2, 1.0),\n        sigma_y_limit: ScaleFloatType = (0.2, 1.0),\n        sigmaX_limit: ScaleFloatType | None = None,  # noqa: N803\n        sigmaY_limit: ScaleFloatType | None = None,  # noqa: N803\n        rotate_limit: ScaleIntType = 90,\n        beta_limit: ScaleFloatType = (0.5, 8.0),\n        noise_limit: ScaleFloatType = (0.9, 1.1),\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p, always_apply)\n\n        if sigmaX_limit is not None:\n            warnings.warn(\"sigmaX_limit is deprecated; use sigma_x_limit instead.\", DeprecationWarning, stacklevel=2)\n            sigma_x_limit = sigmaX_limit\n\n        if sigmaY_limit is not None:\n            warnings.warn(\"sigmaY_limit is deprecated; use sigma_y_limit instead.\", DeprecationWarning, stacklevel=2)\n            sigma_y_limit = sigmaY_limit\n\n        self.blur_limit = cast(Tuple[int, int], blur_limit)\n        self.sigma_x_limit = cast(Tuple[float, float], sigma_x_limit)\n        self.sigma_y_limit = cast(Tuple[float, float], sigma_y_limit)\n        self.rotate_limit = cast(Tuple[int, int], rotate_limit)\n        self.beta_limit = cast(Tuple[float, float], beta_limit)\n        self.noise_limit = cast(Tuple[float, float], noise_limit)\n\n    def apply(self, img: np.ndarray, kernel: np.ndarray, **params: Any) -&gt; np.ndarray:\n        return fmain.convolve(img, kernel=kernel)\n\n    def get_params(self) -&gt; dict[str, np.ndarray]:\n        ksize = random.randrange(self.blur_limit[0], self.blur_limit[1] + 1, 2)\n        sigma_x = random.uniform(*self.sigma_x_limit)\n        sigma_y = random.uniform(*self.sigma_y_limit)\n        angle = np.deg2rad(random.uniform(*self.rotate_limit))\n\n        # Split into 2 cases to avoid selection of narrow kernels (beta &gt; 1) too often.\n        beta = (\n            random.uniform(self.beta_limit[0], 1) if random.random() &lt; HALF else random.uniform(1, self.beta_limit[1])\n        )\n\n        noise_matrix = random_utils.uniform(self.noise_limit[0], self.noise_limit[1], size=[ksize, ksize])\n\n        # Generate mesh grid centered at zero.\n        ax = np.arange(-ksize // 2 + 1.0, ksize // 2 + 1.0)\n        # &gt; Shape (ksize, ksize, 2)\n        grid = np.stack(np.meshgrid(ax, ax), axis=-1)\n\n        # Calculate rotated sigma matrix\n        d_matrix = np.array([[sigma_x**2, 0], [0, sigma_y**2]])\n        u_matrix = np.array([[np.cos(angle), -np.sin(angle)], [np.sin(angle), np.cos(angle)]])\n        sigma_matrix = np.dot(u_matrix, np.dot(d_matrix, u_matrix.T))\n\n        inverse_sigma = np.linalg.inv(sigma_matrix)\n        # Described in \"Parameter Estimation For Multivariate Generalized Gaussian Distributions\"\n        kernel = np.exp(-0.5 * np.power(np.sum(np.dot(grid, inverse_sigma) * grid, 2), beta))\n        # Add noise\n        kernel *= noise_matrix\n\n        # Normalize kernel\n        kernel = kernel.astype(np.float32) / np.sum(kernel)\n        return {\"kernel\": kernel}\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, str, str, str, str, str]:\n        return (\n            \"blur_limit\",\n            \"sigma_x_limit\",\n            \"sigma_y_limit\",\n            \"rotate_limit\",\n            \"beta_limit\",\n            \"noise_limit\",\n        )\n</code></pre>"},{"location":"api_reference/augmentations/blur/transforms/#albumentations.augmentations.blur.transforms.AdvancedBlur.apply","title":"<code>apply (self, img, kernel, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/blur/transforms.py</code> Python<pre><code>def apply(self, img: np.ndarray, kernel: np.ndarray, **params: Any) -&gt; np.ndarray:\n    return fmain.convolve(img, kernel=kernel)\n</code></pre>"},{"location":"api_reference/augmentations/blur/transforms/#albumentations.augmentations.blur.transforms.AdvancedBlur.get_params","title":"<code>get_params (self)</code>","text":"<p>Returns parameters independent of input.</p> Source code in <code>albumentations/augmentations/blur/transforms.py</code> Python<pre><code>def get_params(self) -&gt; dict[str, np.ndarray]:\n    ksize = random.randrange(self.blur_limit[0], self.blur_limit[1] + 1, 2)\n    sigma_x = random.uniform(*self.sigma_x_limit)\n    sigma_y = random.uniform(*self.sigma_y_limit)\n    angle = np.deg2rad(random.uniform(*self.rotate_limit))\n\n    # Split into 2 cases to avoid selection of narrow kernels (beta &gt; 1) too often.\n    beta = (\n        random.uniform(self.beta_limit[0], 1) if random.random() &lt; HALF else random.uniform(1, self.beta_limit[1])\n    )\n\n    noise_matrix = random_utils.uniform(self.noise_limit[0], self.noise_limit[1], size=[ksize, ksize])\n\n    # Generate mesh grid centered at zero.\n    ax = np.arange(-ksize // 2 + 1.0, ksize // 2 + 1.0)\n    # &gt; Shape (ksize, ksize, 2)\n    grid = np.stack(np.meshgrid(ax, ax), axis=-1)\n\n    # Calculate rotated sigma matrix\n    d_matrix = np.array([[sigma_x**2, 0], [0, sigma_y**2]])\n    u_matrix = np.array([[np.cos(angle), -np.sin(angle)], [np.sin(angle), np.cos(angle)]])\n    sigma_matrix = np.dot(u_matrix, np.dot(d_matrix, u_matrix.T))\n\n    inverse_sigma = np.linalg.inv(sigma_matrix)\n    # Described in \"Parameter Estimation For Multivariate Generalized Gaussian Distributions\"\n    kernel = np.exp(-0.5 * np.power(np.sum(np.dot(grid, inverse_sigma) * grid, 2), beta))\n    # Add noise\n    kernel *= noise_matrix\n\n    # Normalize kernel\n    kernel = kernel.astype(np.float32) / np.sum(kernel)\n    return {\"kernel\": kernel}\n</code></pre>"},{"location":"api_reference/augmentations/blur/transforms/#albumentations.augmentations.blur.transforms.AdvancedBlur.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/blur/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, str, str, str, str, str]:\n    return (\n        \"blur_limit\",\n        \"sigma_x_limit\",\n        \"sigma_y_limit\",\n        \"rotate_limit\",\n        \"beta_limit\",\n        \"noise_limit\",\n    )\n</code></pre>"},{"location":"api_reference/augmentations/blur/transforms/#albumentations.augmentations.blur.transforms.Blur","title":"<code>class  Blur</code> <code>     (blur_limit=7, p=0.5, always_apply=None)                 </code>  [view source on GitHub]","text":"<p>Blur the input image using a random-sized kernel.</p> <p>Parameters:</p> Name Type Description <code>blur_limit</code> <code>ScaleIntType</code> <p>maximum kernel size for blurring the input image. Should be in range [3, inf). Default: (3, 7).</p> <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 0.5.</p> <p>Targets</p> <p>image</p> <p>Image types:     uint8, float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/blur/transforms.py</code> Python<pre><code>class Blur(ImageOnlyTransform):\n    \"\"\"Blur the input image using a random-sized kernel.\n\n    Args:\n        blur_limit: maximum kernel size for blurring the input image.\n            Should be in range [3, inf). Default: (3, 7).\n        p: probability of applying the transform. Default: 0.5.\n\n    Targets:\n        image\n\n    Image types:\n        uint8, float32\n\n    \"\"\"\n\n    class InitSchema(BlurInitSchema):\n        pass\n\n    def __init__(self, blur_limit: ScaleIntType = 7, p: float = 0.5, always_apply: bool | None = None):\n        super().__init__(p, always_apply)\n        self.blur_limit = cast(Tuple[int, int], blur_limit)\n\n    def apply(self, img: np.ndarray, kernel: int, **params: Any) -&gt; np.ndarray:\n        return fblur.blur(img, kernel)\n\n    def get_params(self) -&gt; dict[str, Any]:\n        return {\"kernel\": random_utils.choice(list(range(self.blur_limit[0], self.blur_limit[1] + 1, 2)))}\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return (\"blur_limit\",)\n</code></pre>"},{"location":"api_reference/augmentations/blur/transforms/#albumentations.augmentations.blur.transforms.Blur.apply","title":"<code>apply (self, img, kernel, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/blur/transforms.py</code> Python<pre><code>def apply(self, img: np.ndarray, kernel: int, **params: Any) -&gt; np.ndarray:\n    return fblur.blur(img, kernel)\n</code></pre>"},{"location":"api_reference/augmentations/blur/transforms/#albumentations.augmentations.blur.transforms.Blur.get_params","title":"<code>get_params (self)</code>","text":"<p>Returns parameters independent of input.</p> Source code in <code>albumentations/augmentations/blur/transforms.py</code> Python<pre><code>def get_params(self) -&gt; dict[str, Any]:\n    return {\"kernel\": random_utils.choice(list(range(self.blur_limit[0], self.blur_limit[1] + 1, 2)))}\n</code></pre>"},{"location":"api_reference/augmentations/blur/transforms/#albumentations.augmentations.blur.transforms.Blur.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/blur/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return (\"blur_limit\",)\n</code></pre>"},{"location":"api_reference/augmentations/blur/transforms/#albumentations.augmentations.blur.transforms.Defocus","title":"<code>class  Defocus</code> <code>     (radius=(3, 10), alias_blur=(0.1, 0.5), always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Apply defocus transform.</p> <p>Parameters:</p> Name Type Description <code>radius</code> <code>int, int) or int</code> <p>range for radius of defocusing. If limit is a single int, the range will be [1, limit]. Default: (3, 10).</p> <code>alias_blur</code> <code>float, float) or float</code> <p>range for alias_blur of defocusing (sigma of gaussian blur). If limit is a single float, the range will be (0, limit). Default: (0.1, 0.5).</p> <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 0.5.</p> <p>Targets</p> <p>image</p> <p>Image types:     unit8, float32</p> <p>Reference</p> <p>https://arxiv.org/abs/1903.12261</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/blur/transforms.py</code> Python<pre><code>class Defocus(ImageOnlyTransform):\n    \"\"\"Apply defocus transform.\n\n    Args:\n        radius ((int, int) or int): range for radius of defocusing.\n            If limit is a single int, the range will be [1, limit]. Default: (3, 10).\n        alias_blur ((float, float) or float): range for alias_blur of defocusing (sigma of gaussian blur).\n            If limit is a single float, the range will be (0, limit). Default: (0.1, 0.5).\n        p (float): probability of applying the transform. Default: 0.5.\n\n    Targets:\n        image\n\n    Image types:\n        unit8, float32\n\n    Reference:\n        https://arxiv.org/abs/1903.12261\n    \"\"\"\n\n    class InitSchema(BaseTransformInitSchema):\n        radius: OnePlusIntRangeType = (3, 10)\n        alias_blur: NonNegativeFloatRangeType = (0.1, 0.5)\n\n    def __init__(\n        self,\n        radius: ScaleIntType = (3, 10),\n        alias_blur: ScaleFloatType = (0.1, 0.5),\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p, always_apply)\n        self.radius = cast(Tuple[int, int], radius)\n        self.alias_blur = cast(Tuple[float, float], alias_blur)\n\n    def apply(self, img: np.ndarray, radius: int, alias_blur: float, **params: Any) -&gt; np.ndarray:\n        return fblur.defocus(img, radius, alias_blur)\n\n    def get_params(self) -&gt; dict[str, Any]:\n        return {\n            \"radius\": random.randint(self.radius[0], self.radius[1]),\n            \"alias_blur\": random.uniform(self.alias_blur[0], self.alias_blur[1]),\n        }\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, str]:\n        return (\"radius\", \"alias_blur\")\n</code></pre>"},{"location":"api_reference/augmentations/blur/transforms/#albumentations.augmentations.blur.transforms.Defocus.apply","title":"<code>apply (self, img, radius, alias_blur, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/blur/transforms.py</code> Python<pre><code>def apply(self, img: np.ndarray, radius: int, alias_blur: float, **params: Any) -&gt; np.ndarray:\n    return fblur.defocus(img, radius, alias_blur)\n</code></pre>"},{"location":"api_reference/augmentations/blur/transforms/#albumentations.augmentations.blur.transforms.Defocus.get_params","title":"<code>get_params (self)</code>","text":"<p>Returns parameters independent of input.</p> Source code in <code>albumentations/augmentations/blur/transforms.py</code> Python<pre><code>def get_params(self) -&gt; dict[str, Any]:\n    return {\n        \"radius\": random.randint(self.radius[0], self.radius[1]),\n        \"alias_blur\": random.uniform(self.alias_blur[0], self.alias_blur[1]),\n    }\n</code></pre>"},{"location":"api_reference/augmentations/blur/transforms/#albumentations.augmentations.blur.transforms.Defocus.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/blur/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, str]:\n    return (\"radius\", \"alias_blur\")\n</code></pre>"},{"location":"api_reference/augmentations/blur/transforms/#albumentations.augmentations.blur.transforms.GaussianBlur","title":"<code>class  GaussianBlur</code> <code>     (blur_limit=(3, 7), sigma_limit=0, always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Blur the input image using a Gaussian filter with a random kernel size.</p> <p>Parameters:</p> Name Type Description <code>blur_limit</code> <code>int, (int, int</code> <p>maximum Gaussian kernel size for blurring the input image. Must be zero or odd and in range [0, inf). If set to 0 it will be computed from sigma as <code>round(sigma * (3 if img.dtype == np.uint8 else 4) * 2 + 1) + 1</code>. If set single value <code>blur_limit</code> will be in range (0, blur_limit). Default: (3, 7).</p> <code>sigma_limit</code> <code>float, (float, float</code> <p>Gaussian kernel standard deviation. Must be in range [0, inf). If set single value <code>sigma_limit</code> will be in range (0, sigma_limit). If set to 0 sigma will be computed as <code>sigma = 0.3*((ksize-1)*0.5 - 1) + 0.8</code>. Default: 0.</p> <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 0.5.</p> <p>Targets</p> <p>image</p> <p>Image types:     uint8, float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/blur/transforms.py</code> Python<pre><code>class GaussianBlur(ImageOnlyTransform):\n    \"\"\"Blur the input image using a Gaussian filter with a random kernel size.\n\n    Args:\n        blur_limit (int, (int, int)): maximum Gaussian kernel size for blurring the input image.\n            Must be zero or odd and in range [0, inf). If set to 0 it will be computed from sigma\n            as `round(sigma * (3 if img.dtype == np.uint8 else 4) * 2 + 1) + 1`.\n            If set single value `blur_limit` will be in range (0, blur_limit).\n            Default: (3, 7).\n        sigma_limit (float, (float, float)): Gaussian kernel standard deviation. Must be in range [0, inf).\n            If set single value `sigma_limit` will be in range (0, sigma_limit).\n            If set to 0 sigma will be computed as `sigma = 0.3*((ksize-1)*0.5 - 1) + 0.8`. Default: 0.\n        p (float): probability of applying the transform. Default: 0.5.\n\n    Targets:\n        image\n\n    Image types:\n        uint8, float32\n\n    \"\"\"\n\n    class InitSchema(BlurInitSchema):\n        sigma_limit: NonNegativeFloatRangeType = 0\n\n        @field_validator(\"blur_limit\")\n        @classmethod\n        def process_blur(cls, value: ScaleIntType, info: ValidationInfo) -&gt; tuple[int, int]:\n            return process_blur_limit(value, info, min_value=0)\n\n        @model_validator(mode=\"after\")\n        def validate_limits(self) -&gt; Self:\n            if (\n                isinstance(self.blur_limit, (tuple, list))\n                and self.blur_limit[0] == 0\n                and isinstance(self.sigma_limit, (tuple, list))\n                and self.sigma_limit[0] == 0\n            ):\n                self.blur_limit = 3, max(3, self.blur_limit[1])\n                warnings.warn(\n                    \"blur_limit and sigma_limit minimum value can not be both equal to 0. \"\n                    \"blur_limit minimum value changed to 3.\",\n                    stacklevel=2,\n                )\n\n            if isinstance(self.blur_limit, tuple):\n                for v in self.blur_limit:\n                    if v != 0 and v % 2 != 1:\n                        raise ValueError(f\"Blur limit must be 0 or odd. Got: {self.blur_limit}\")\n\n            return self\n\n    def __init__(\n        self,\n        blur_limit: ScaleIntType = (3, 7),\n        sigma_limit: ScaleFloatType = 0,\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p, always_apply)\n        self.blur_limit = cast(Tuple[int, int], blur_limit)\n        self.sigma_limit = cast(Tuple[float, float], sigma_limit)\n\n    def apply(self, img: np.ndarray, ksize: int, sigma: float, **params: Any) -&gt; np.ndarray:\n        return fblur.gaussian_blur(img, ksize, sigma=sigma)\n\n    def get_params(self) -&gt; dict[str, float]:\n        ksize = random.randrange(self.blur_limit[0], self.blur_limit[1] + 1)\n        if ksize != 0 and ksize % 2 != 1:\n            ksize = (ksize + 1) % (self.blur_limit[1] + 1)\n\n        return {\"ksize\": ksize, \"sigma\": random.uniform(*self.sigma_limit)}\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, str]:\n        return (\"blur_limit\", \"sigma_limit\")\n</code></pre>"},{"location":"api_reference/augmentations/blur/transforms/#albumentations.augmentations.blur.transforms.GaussianBlur.apply","title":"<code>apply (self, img, ksize, sigma, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/blur/transforms.py</code> Python<pre><code>def apply(self, img: np.ndarray, ksize: int, sigma: float, **params: Any) -&gt; np.ndarray:\n    return fblur.gaussian_blur(img, ksize, sigma=sigma)\n</code></pre>"},{"location":"api_reference/augmentations/blur/transforms/#albumentations.augmentations.blur.transforms.GaussianBlur.get_params","title":"<code>get_params (self)</code>","text":"<p>Returns parameters independent of input.</p> Source code in <code>albumentations/augmentations/blur/transforms.py</code> Python<pre><code>def get_params(self) -&gt; dict[str, float]:\n    ksize = random.randrange(self.blur_limit[0], self.blur_limit[1] + 1)\n    if ksize != 0 and ksize % 2 != 1:\n        ksize = (ksize + 1) % (self.blur_limit[1] + 1)\n\n    return {\"ksize\": ksize, \"sigma\": random.uniform(*self.sigma_limit)}\n</code></pre>"},{"location":"api_reference/augmentations/blur/transforms/#albumentations.augmentations.blur.transforms.GaussianBlur.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/blur/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, str]:\n    return (\"blur_limit\", \"sigma_limit\")\n</code></pre>"},{"location":"api_reference/augmentations/blur/transforms/#albumentations.augmentations.blur.transforms.GlassBlur","title":"<code>class  GlassBlur</code> <code>     (sigma=0.7, max_delta=4, iterations=2, mode='fast', always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Apply glass noise to the input image.</p> <p>Parameters:</p> Name Type Description <code>sigma</code> <code>float</code> <p>standard deviation for Gaussian kernel.</p> <code>max_delta</code> <code>int</code> <p>max distance between pixels which are swapped.</p> <code>iterations</code> <code>int</code> <p>number of repeats. Should be in range [1, inf). Default: (2).</p> <code>mode</code> <code>str</code> <p>mode of computation: fast or exact. Default: \"fast\".</p> <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 0.5.</p> <p>Targets</p> <p>image</p> <p>Image types:     uint8, float32</p> <p>Reference</p> <p>https://arxiv.org/abs/1903.12261 https://github.com/hendrycks/robustness/blob/master/ImageNet-C/create_c/make_imagenet_c.py</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/blur/transforms.py</code> Python<pre><code>class GlassBlur(ImageOnlyTransform):\n    \"\"\"Apply glass noise to the input image.\n\n    Args:\n        sigma (float): standard deviation for Gaussian kernel.\n        max_delta (int): max distance between pixels which are swapped.\n        iterations (int): number of repeats.\n            Should be in range [1, inf). Default: (2).\n        mode (str): mode of computation: fast or exact. Default: \"fast\".\n        p (float): probability of applying the transform. Default: 0.5.\n\n    Targets:\n        image\n\n    Image types:\n        uint8, float32\n\n    Reference:\n        https://arxiv.org/abs/1903.12261\n        https://github.com/hendrycks/robustness/blob/master/ImageNet-C/create_c/make_imagenet_c.py\n\n    \"\"\"\n\n    class InitSchema(BaseTransformInitSchema):\n        sigma: float = Field(default=0.7, ge=0, description=\"Standard deviation for the Gaussian kernel.\")\n        max_delta: int = Field(default=4, ge=1, description=\"Maximum distance between pixels that are swapped.\")\n        iterations: int = Field(default=2, ge=1, description=\"Number of times the glass noise effect is applied.\")\n        mode: Literal[\"fast\", \"exact\"] = \"fast\"\n\n    def __init__(\n        self,\n        sigma: float = 0.7,\n        max_delta: int = 4,\n        iterations: int = 2,\n        mode: Literal[\"fast\", \"exact\"] = \"fast\",\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p=p, always_apply=always_apply)\n        self.sigma = sigma\n        self.max_delta = max_delta\n        self.iterations = iterations\n        self.mode = mode\n\n    def apply(self, img: np.ndarray, *args: Any, dxy: np.ndarray, **params: Any) -&gt; np.ndarray:\n        if dxy is None:\n            msg = \"dxy is None\"\n            raise ValueError(msg)\n\n        return fblur.glass_blur(img, self.sigma, self.max_delta, self.iterations, dxy, self.mode)\n\n    def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, np.ndarray]:\n        height, width = params[\"shape\"][:2]\n\n        # generate array containing all necessary values for transformations\n        width_pixels = height - self.max_delta * 2\n        height_pixels = width - self.max_delta * 2\n        total_pixels = int(width_pixels * height_pixels)\n        dxy = random_utils.randint(-self.max_delta, self.max_delta, size=(total_pixels, self.iterations, 2))\n\n        return {\"dxy\": dxy}\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, str, str, str]:\n        return (\"sigma\", \"max_delta\", \"iterations\", \"mode\")\n</code></pre>"},{"location":"api_reference/augmentations/blur/transforms/#albumentations.augmentations.blur.transforms.GlassBlur.apply","title":"<code>apply (self, img, *args, *, dxy, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/blur/transforms.py</code> Python<pre><code>def apply(self, img: np.ndarray, *args: Any, dxy: np.ndarray, **params: Any) -&gt; np.ndarray:\n    if dxy is None:\n        msg = \"dxy is None\"\n        raise ValueError(msg)\n\n    return fblur.glass_blur(img, self.sigma, self.max_delta, self.iterations, dxy, self.mode)\n</code></pre>"},{"location":"api_reference/augmentations/blur/transforms/#albumentations.augmentations.blur.transforms.GlassBlur.get_params_dependent_on_data","title":"<code>get_params_dependent_on_data (self, params, data)</code>","text":"<p>Returns parameters dependent on input.</p> Source code in <code>albumentations/augmentations/blur/transforms.py</code> Python<pre><code>def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, np.ndarray]:\n    height, width = params[\"shape\"][:2]\n\n    # generate array containing all necessary values for transformations\n    width_pixels = height - self.max_delta * 2\n    height_pixels = width - self.max_delta * 2\n    total_pixels = int(width_pixels * height_pixels)\n    dxy = random_utils.randint(-self.max_delta, self.max_delta, size=(total_pixels, self.iterations, 2))\n\n    return {\"dxy\": dxy}\n</code></pre>"},{"location":"api_reference/augmentations/blur/transforms/#albumentations.augmentations.blur.transforms.GlassBlur.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/blur/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, str, str, str]:\n    return (\"sigma\", \"max_delta\", \"iterations\", \"mode\")\n</code></pre>"},{"location":"api_reference/augmentations/blur/transforms/#albumentations.augmentations.blur.transforms.MedianBlur","title":"<code>class  MedianBlur</code> <code>     (blur_limit=7, p=0.5, always_apply=None)                 </code>  [view source on GitHub]","text":"<p>Blur the input image using a median filter with a random aperture linear size.</p> <p>Parameters:</p> Name Type Description <code>blur_limit</code> <code>int</code> <p>maximum aperture linear size for blurring the input image. Must be odd and in range [3, inf). Default: (3, 7).</p> <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 0.5.</p> <p>Targets</p> <p>image</p> <p>Image types:     uint8, float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/blur/transforms.py</code> Python<pre><code>class MedianBlur(Blur):\n    \"\"\"Blur the input image using a median filter with a random aperture linear size.\n\n    Args:\n        blur_limit (int): maximum aperture linear size for blurring the input image.\n            Must be odd and in range [3, inf). Default: (3, 7).\n        p (float): probability of applying the transform. Default: 0.5.\n\n    Targets:\n        image\n\n    Image types:\n        uint8, float32\n\n    \"\"\"\n\n    def __init__(self, blur_limit: ScaleIntType = 7, p: float = 0.5, always_apply: bool | None = None):\n        super().__init__(blur_limit, p, always_apply)\n\n    def apply(self, img: np.ndarray, kernel: int, **params: Any) -&gt; np.ndarray:\n        return fblur.median_blur(img, kernel)\n</code></pre>"},{"location":"api_reference/augmentations/blur/transforms/#albumentations.augmentations.blur.transforms.MedianBlur.__init__","title":"<code>__init__ (self, blur_limit=7, p=0.5, always_apply=None)</code>  <code>special</code>","text":"<p>Initialize self.  See help(type(self)) for accurate signature.</p> Source code in <code>albumentations/augmentations/blur/transforms.py</code> Python<pre><code>def __init__(self, blur_limit: ScaleIntType = 7, p: float = 0.5, always_apply: bool | None = None):\n    super().__init__(blur_limit, p, always_apply)\n</code></pre>"},{"location":"api_reference/augmentations/blur/transforms/#albumentations.augmentations.blur.transforms.MedianBlur.apply","title":"<code>apply (self, img, kernel, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/blur/transforms.py</code> Python<pre><code>def apply(self, img: np.ndarray, kernel: int, **params: Any) -&gt; np.ndarray:\n    return fblur.median_blur(img, kernel)\n</code></pre>"},{"location":"api_reference/augmentations/blur/transforms/#albumentations.augmentations.blur.transforms.MotionBlur","title":"<code>class  MotionBlur</code> <code>     (blur_limit=7, allow_shifted=True, always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Apply motion blur to the input image using a random-sized kernel.</p> <p>Parameters:</p> Name Type Description <code>blur_limit</code> <code>int</code> <p>maximum kernel size for blurring the input image. Should be in range [3, inf). Default: (3, 7).</p> <code>allow_shifted</code> <code>bool</code> <p>if set to true creates non shifted kernels only, otherwise creates randomly shifted kernels. Default: True.</p> <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 0.5.</p> <p>Targets</p> <p>image</p> <p>Image types:     uint8, float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/blur/transforms.py</code> Python<pre><code>class MotionBlur(Blur):\n    \"\"\"Apply motion blur to the input image using a random-sized kernel.\n\n    Args:\n        blur_limit (int): maximum kernel size for blurring the input image.\n            Should be in range [3, inf). Default: (3, 7).\n        allow_shifted (bool): if set to true creates non shifted kernels only,\n            otherwise creates randomly shifted kernels. Default: True.\n        p (float): probability of applying the transform. Default: 0.5.\n\n    Targets:\n        image\n\n    Image types:\n        uint8, float32\n\n    \"\"\"\n\n    class InitSchema(BaseTransformInitSchema):\n        allow_shifted: bool = Field(\n            default=True,\n            description=\"If set to true creates non-shifted kernels only, otherwise creates randomly shifted kernels.\",\n        )\n        blur_limit: ScaleIntType = Field(\n            default=(3, 7),\n            description=\"Maximum kernel size for blurring the input image.\",\n        )\n\n        @model_validator(mode=\"after\")\n        def process_blur(self) -&gt; Self:\n            self.blur_limit = cast(Tuple[int, int], to_tuple(self.blur_limit, 3))\n\n            if self.allow_shifted and isinstance(self.blur_limit, tuple) and any(x % 2 != 1 for x in self.blur_limit):\n                raise ValueError(f\"Blur limit must be odd when centered=True. Got: {self.blur_limit}\")\n\n            return self\n\n    def __init__(\n        self,\n        blur_limit: ScaleIntType = 7,\n        allow_shifted: bool = True,\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(blur_limit=blur_limit, p=p, always_apply=always_apply)\n        self.allow_shifted = allow_shifted\n        self.blur_limit = cast(Tuple[int, int], blur_limit)\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return (*super().get_transform_init_args_names(), \"allow_shifted\")\n\n    def apply(self, img: np.ndarray, kernel: np.ndarray, **params: Any) -&gt; np.ndarray:\n        return fmain.convolve(img, kernel=kernel)\n\n    def get_params(self) -&gt; dict[str, Any]:\n        ksize = random.choice(list(range(self.blur_limit[0], self.blur_limit[1] + 1, 2)))\n        if ksize &lt;= TWO:\n            raise ValueError(f\"ksize must be &gt; 2. Got: {ksize}\")\n        kernel = np.zeros((ksize, ksize), dtype=np.uint8)\n        x1, x2 = random.randint(0, ksize - 1), random.randint(0, ksize - 1)\n        if x1 == x2:\n            y1, y2 = random.sample(range(ksize), 2)\n        else:\n            y1, y2 = random.randint(0, ksize - 1), random.randint(0, ksize - 1)\n\n        def make_odd_val(v1: int, v2: int) -&gt; tuple[int, int]:\n            len_v = abs(v1 - v2) + 1\n            if len_v % 2 != 1:\n                if v2 &gt; v1:\n                    v2 -= 1\n                else:\n                    v1 -= 1\n            return v1, v2\n\n        if not self.allow_shifted:\n            x1, x2 = make_odd_val(x1, x2)\n            y1, y2 = make_odd_val(y1, y2)\n\n            xc = (x1 + x2) / 2\n            yc = (y1 + y2) / 2\n\n            center = ksize / 2 - 0.5\n            dx = xc - center\n            dy = yc - center\n            x1, x2 = (int(i - dx) for i in [x1, x2])\n            y1, y2 = (int(i - dy) for i in [y1, y2])\n\n        cv2.line(kernel, (x1, y1), (x2, y2), 1, thickness=1)\n\n        # Normalize kernel\n        return {\"kernel\": kernel.astype(np.float32) / np.sum(kernel)}\n</code></pre>"},{"location":"api_reference/augmentations/blur/transforms/#albumentations.augmentations.blur.transforms.MotionBlur.apply","title":"<code>apply (self, img, kernel, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/blur/transforms.py</code> Python<pre><code>def apply(self, img: np.ndarray, kernel: np.ndarray, **params: Any) -&gt; np.ndarray:\n    return fmain.convolve(img, kernel=kernel)\n</code></pre>"},{"location":"api_reference/augmentations/blur/transforms/#albumentations.augmentations.blur.transforms.MotionBlur.get_params","title":"<code>get_params (self)</code>","text":"<p>Returns parameters independent of input.</p> Source code in <code>albumentations/augmentations/blur/transforms.py</code> Python<pre><code>def get_params(self) -&gt; dict[str, Any]:\n    ksize = random.choice(list(range(self.blur_limit[0], self.blur_limit[1] + 1, 2)))\n    if ksize &lt;= TWO:\n        raise ValueError(f\"ksize must be &gt; 2. Got: {ksize}\")\n    kernel = np.zeros((ksize, ksize), dtype=np.uint8)\n    x1, x2 = random.randint(0, ksize - 1), random.randint(0, ksize - 1)\n    if x1 == x2:\n        y1, y2 = random.sample(range(ksize), 2)\n    else:\n        y1, y2 = random.randint(0, ksize - 1), random.randint(0, ksize - 1)\n\n    def make_odd_val(v1: int, v2: int) -&gt; tuple[int, int]:\n        len_v = abs(v1 - v2) + 1\n        if len_v % 2 != 1:\n            if v2 &gt; v1:\n                v2 -= 1\n            else:\n                v1 -= 1\n        return v1, v2\n\n    if not self.allow_shifted:\n        x1, x2 = make_odd_val(x1, x2)\n        y1, y2 = make_odd_val(y1, y2)\n\n        xc = (x1 + x2) / 2\n        yc = (y1 + y2) / 2\n\n        center = ksize / 2 - 0.5\n        dx = xc - center\n        dy = yc - center\n        x1, x2 = (int(i - dx) for i in [x1, x2])\n        y1, y2 = (int(i - dy) for i in [y1, y2])\n\n    cv2.line(kernel, (x1, y1), (x2, y2), 1, thickness=1)\n\n    # Normalize kernel\n    return {\"kernel\": kernel.astype(np.float32) / np.sum(kernel)}\n</code></pre>"},{"location":"api_reference/augmentations/blur/transforms/#albumentations.augmentations.blur.transforms.MotionBlur.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/blur/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return (*super().get_transform_init_args_names(), \"allow_shifted\")\n</code></pre>"},{"location":"api_reference/augmentations/blur/transforms/#albumentations.augmentations.blur.transforms.ZoomBlur","title":"<code>class  ZoomBlur</code> <code>     (max_factor=(1, 1.31), step_factor=(0.01, 0.03), always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Apply zoom blur transform.</p> <p>Parameters:</p> Name Type Description <code>max_factor</code> <code>float, float) or float</code> <p>range for max factor for blurring. If max_factor is a single float, the range will be (1, limit). Default: (1, 1.31). All max_factor values should be larger than 1.</p> <code>step_factor</code> <code>float, float) or float</code> <p>If single float will be used as step parameter for np.arange. If tuple of float step_factor will be in range <code>[step_factor[0], step_factor[1])</code>. Default: (0.01, 0.03). All step_factor values should be positive.</p> <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 0.5.</p> <p>Targets</p> <p>image</p> <p>Image types:     unit8, float32</p> <p>Reference</p> <p>https://arxiv.org/abs/1903.12261</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/blur/transforms.py</code> Python<pre><code>class ZoomBlur(ImageOnlyTransform):\n    \"\"\"Apply zoom blur transform.\n\n    Args:\n        max_factor ((float, float) or float): range for max factor for blurring.\n            If max_factor is a single float, the range will be (1, limit). Default: (1, 1.31).\n            All max_factor values should be larger than 1.\n        step_factor ((float, float) or float): If single float will be used as step parameter for np.arange.\n            If tuple of float step_factor will be in range `[step_factor[0], step_factor[1])`. Default: (0.01, 0.03).\n            All step_factor values should be positive.\n        p (float): probability of applying the transform. Default: 0.5.\n\n    Targets:\n        image\n\n    Image types:\n        unit8, float32\n\n    Reference:\n        https://arxiv.org/abs/1903.12261\n    \"\"\"\n\n    class InitSchema(BaseTransformInitSchema):\n        max_factor: OnePlusFloatRangeType = (1, 1.31)\n        step_factor: NonNegativeFloatRangeType = (0.01, 0.03)\n\n    def __init__(\n        self,\n        max_factor: ScaleFloatType = (1, 1.31),\n        step_factor: ScaleFloatType = (0.01, 0.03),\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p, always_apply)\n        self.max_factor = cast(Tuple[float, float], max_factor)\n        self.step_factor = cast(Tuple[float, float], step_factor)\n\n    def apply(self, img: np.ndarray, zoom_factors: np.ndarray, **params: Any) -&gt; np.ndarray:\n        return fblur.zoom_blur(img, zoom_factors)\n\n    def get_params(self) -&gt; dict[str, Any]:\n        max_factor = random.uniform(self.max_factor[0], self.max_factor[1])\n        step_factor = random.uniform(self.step_factor[0], self.step_factor[1])\n        return {\"zoom_factors\": np.arange(1.0, max_factor, step_factor)}\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, str]:\n        return (\"max_factor\", \"step_factor\")\n</code></pre>"},{"location":"api_reference/augmentations/blur/transforms/#albumentations.augmentations.blur.transforms.ZoomBlur.apply","title":"<code>apply (self, img, zoom_factors, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/blur/transforms.py</code> Python<pre><code>def apply(self, img: np.ndarray, zoom_factors: np.ndarray, **params: Any) -&gt; np.ndarray:\n    return fblur.zoom_blur(img, zoom_factors)\n</code></pre>"},{"location":"api_reference/augmentations/blur/transforms/#albumentations.augmentations.blur.transforms.ZoomBlur.get_params","title":"<code>get_params (self)</code>","text":"<p>Returns parameters independent of input.</p> Source code in <code>albumentations/augmentations/blur/transforms.py</code> Python<pre><code>def get_params(self) -&gt; dict[str, Any]:\n    max_factor = random.uniform(self.max_factor[0], self.max_factor[1])\n    step_factor = random.uniform(self.step_factor[0], self.step_factor[1])\n    return {\"zoom_factors\": np.arange(1.0, max_factor, step_factor)}\n</code></pre>"},{"location":"api_reference/augmentations/blur/transforms/#albumentations.augmentations.blur.transforms.ZoomBlur.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/blur/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, str]:\n    return (\"max_factor\", \"step_factor\")\n</code></pre>"},{"location":"api_reference/augmentations/crops/","title":"Index","text":"<ul> <li>Crop functional transforms (albumentations.augmentations.crops.functional)</li> <li>Crop transforms (albumentations.augmentations.crops.transforms)</li> </ul>"},{"location":"api_reference/augmentations/crops/functional/","title":"Crop functional transforms (augmentations.crops.functional)","text":""},{"location":"api_reference/augmentations/crops/functional/#albumentations.augmentations.crops.functional.crop_keypoint_by_coords","title":"<code>def crop_keypoint_by_coords    (keypoint, crop_coords)    </code> [view source on GitHub]","text":"<p>Crop a keypoint using the provided coordinates of bottom-left and top-right corners in pixels and the required height and width of the crop.</p> <p>Parameters:</p> Name Type Description <code>keypoint</code> <code>tuple</code> <p>A keypoint <code>(x, y, angle, scale)</code>.</p> <code>crop_coords</code> <code>tuple</code> <p>Crop box coords <code>(x1, x2, y1, y2)</code>.</p> <p>Returns:</p> Type Description <code>KeypointInternalType</code> <p>A keypoint <code>(x, y, angle, scale)</code>.</p> Source code in <code>albumentations/augmentations/crops/functional.py</code> Python<pre><code>def crop_keypoint_by_coords(\n    keypoint: KeypointInternalType,\n    crop_coords: tuple[int, int, int, int],\n) -&gt; KeypointInternalType:\n    \"\"\"Crop a keypoint using the provided coordinates of bottom-left and top-right corners in pixels and the\n    required height and width of the crop.\n\n    Args:\n        keypoint (tuple): A keypoint `(x, y, angle, scale)`.\n        crop_coords (tuple): Crop box coords `(x1, x2, y1, y2)`.\n\n    Returns:\n        A keypoint `(x, y, angle, scale)`.\n\n    \"\"\"\n    x, y, angle, scale = keypoint[:4]\n    x1, y1 = crop_coords[:2]\n    return x - x1, y - y1, angle, scale\n</code></pre>"},{"location":"api_reference/augmentations/crops/transforms/","title":"Crop transforms (augmentations.crops.transforms)","text":""},{"location":"api_reference/augmentations/crops/transforms/#albumentations.augmentations.crops.transforms.BBoxSafeRandomCrop","title":"<code>class  BBoxSafeRandomCrop</code> <code>     (erosion_rate=0.0, p=1.0, always_apply=None)                 </code>  [view source on GitHub]","text":"<p>Crop a random part of the input without loss of bboxes.</p> <p>Parameters:</p> Name Type Description <code>erosion_rate</code> <code>float</code> <p>erosion rate applied on input image height before crop.</p> <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 1.</p> <p>Targets</p> <p>image, mask, bboxes, keypoints</p> <p>Image types:     uint8, float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/crops/transforms.py</code> Python<pre><code>class BBoxSafeRandomCrop(_BaseCrop):\n    \"\"\"Crop a random part of the input without loss of bboxes.\n\n    Args:\n        erosion_rate: erosion rate applied on input image height before crop.\n        p: probability of applying the transform. Default: 1.\n\n    Targets:\n        image, mask, bboxes, keypoints\n\n    Image types:\n        uint8, float32\n\n    \"\"\"\n\n    _targets = (Targets.IMAGE, Targets.MASK, Targets.BBOXES, Targets.KEYPOINTS)\n\n    class InitSchema(BaseTransformInitSchema):\n        erosion_rate: float = Field(\n            default=0.0,\n            ge=0.0,\n            le=1.0,\n            description=\"Erosion rate applied on input image height before crop.\",\n        )\n        p: ProbabilityType = 1\n\n    def __init__(self, erosion_rate: float = 0.0, p: float = 1.0, always_apply: bool | None = None):\n        super().__init__(p=p, always_apply=always_apply)\n        self.erosion_rate = erosion_rate\n\n    def _get_coords_no_bbox(self, image_shape: tuple[int, int]) -&gt; tuple[int, int, int, int]:\n        image_height, image_width = image_shape\n\n        erosive_h = int(image_height * (1.0 - self.erosion_rate))\n        crop_height = image_height if erosive_h &gt;= image_height else random.randint(erosive_h, image_height)\n\n        crop_width = int(crop_height * image_width / image_height)\n\n        h_start = random.random()\n        w_start = random.random()\n\n        crop_shape = (crop_height, crop_width)\n\n        return fcrops.get_crop_coords(image_shape, crop_shape, h_start, w_start)\n\n    def get_params_dependent_on_data(\n        self,\n        params: dict[str, Any],\n        data: dict[str, Any],\n    ) -&gt; dict[str, tuple[int, int, int, int]]:\n        image_shape = params[\"shape\"][:2]\n\n        if len(data[\"bboxes\"]) == 0:  # less likely, this class is for use with bboxes.\n            crop_coords = self._get_coords_no_bbox(image_shape)\n            return {\"crop_coords\": crop_coords}\n\n        bbox_union = union_of_bboxes(bboxes=data[\"bboxes\"], erosion_rate=self.erosion_rate)\n\n        if bbox_union is None:\n            crop_coords = self._get_coords_no_bbox(image_shape)\n            return {\"crop_coords\": crop_coords}\n\n        x_min, y_min, x_max, y_max = bbox_union\n\n        x_min = np.clip(x_min, 0, 1)\n        y_min = np.clip(y_min, 0, 1)\n        x_max = np.clip(x_max, x_min, 1)\n        y_max = np.clip(y_max, y_min, 1)\n\n        image_height, image_width = image_shape\n\n        crop_x_min = int(x_min * random.random() * image_width)\n        crop_y_min = int(y_min * random.random() * image_height)\n\n        bbox_xmax = x_max + (1 - x_max) * random.random()\n        bbox_ymax = y_max + (1 - y_max) * random.random()\n        crop_x_max = int(bbox_xmax * image_width)\n        crop_y_max = int(bbox_ymax * image_height)\n\n        return {\"crop_coords\": (crop_x_min, crop_y_min, crop_x_max, crop_y_max)}\n\n    @property\n    def targets_as_params(self) -&gt; list[str]:\n        return [\"bboxes\"]\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return (\"erosion_rate\",)\n</code></pre>"},{"location":"api_reference/augmentations/crops/transforms/#albumentations.augmentations.crops.transforms.BBoxSafeRandomCrop.targets_as_params","title":"<code>targets_as_params: list[str]</code>  <code>property</code> <code>readonly</code>","text":"<p>Targets used to get params dependent on targets. This is used to check input has all required targets.</p>"},{"location":"api_reference/augmentations/crops/transforms/#albumentations.augmentations.crops.transforms.BBoxSafeRandomCrop.get_params_dependent_on_data","title":"<code>get_params_dependent_on_data (self, params, data)</code>","text":"<p>Returns parameters dependent on input.</p> Source code in <code>albumentations/augmentations/crops/transforms.py</code> Python<pre><code>def get_params_dependent_on_data(\n    self,\n    params: dict[str, Any],\n    data: dict[str, Any],\n) -&gt; dict[str, tuple[int, int, int, int]]:\n    image_shape = params[\"shape\"][:2]\n\n    if len(data[\"bboxes\"]) == 0:  # less likely, this class is for use with bboxes.\n        crop_coords = self._get_coords_no_bbox(image_shape)\n        return {\"crop_coords\": crop_coords}\n\n    bbox_union = union_of_bboxes(bboxes=data[\"bboxes\"], erosion_rate=self.erosion_rate)\n\n    if bbox_union is None:\n        crop_coords = self._get_coords_no_bbox(image_shape)\n        return {\"crop_coords\": crop_coords}\n\n    x_min, y_min, x_max, y_max = bbox_union\n\n    x_min = np.clip(x_min, 0, 1)\n    y_min = np.clip(y_min, 0, 1)\n    x_max = np.clip(x_max, x_min, 1)\n    y_max = np.clip(y_max, y_min, 1)\n\n    image_height, image_width = image_shape\n\n    crop_x_min = int(x_min * random.random() * image_width)\n    crop_y_min = int(y_min * random.random() * image_height)\n\n    bbox_xmax = x_max + (1 - x_max) * random.random()\n    bbox_ymax = y_max + (1 - y_max) * random.random()\n    crop_x_max = int(bbox_xmax * image_width)\n    crop_y_max = int(bbox_ymax * image_height)\n\n    return {\"crop_coords\": (crop_x_min, crop_y_min, crop_x_max, crop_y_max)}\n</code></pre>"},{"location":"api_reference/augmentations/crops/transforms/#albumentations.augmentations.crops.transforms.BBoxSafeRandomCrop.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/crops/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return (\"erosion_rate\",)\n</code></pre>"},{"location":"api_reference/augmentations/crops/transforms/#albumentations.augmentations.crops.transforms.CenterCrop","title":"<code>class  CenterCrop</code> <code>     (height, width, p=1.0, always_apply=None)                 </code>  [view source on GitHub]","text":"<p>Crop the central part of the input.</p> <p>Parameters:</p> Name Type Description <code>height</code> <code>int</code> <p>height of the crop.</p> <code>width</code> <code>int</code> <p>width of the crop.</p> <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 1.</p> <p>Targets</p> <p>image, mask, bboxes, keypoints</p> <p>Image types:     uint8, float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/crops/transforms.py</code> Python<pre><code>class CenterCrop(_BaseCrop):\n    \"\"\"Crop the central part of the input.\n\n    Args:\n        height: height of the crop.\n        width: width of the crop.\n        p: probability of applying the transform. Default: 1.\n\n    Targets:\n        image, mask, bboxes, keypoints\n\n    Image types:\n        uint8, float32\n\n    \"\"\"\n\n    class InitSchema(CropInitSchema):\n        pass\n\n    def __init__(self, height: int, width: int, p: float = 1.0, always_apply: bool | None = None):\n        super().__init__(p, always_apply)\n        self.height = height\n        self.width = width\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return \"height\", \"width\"\n\n    def get_params_dependent_on_data(\n        self,\n        params: dict[str, Any],\n        data: dict[str, Any],\n    ) -&gt; dict[str, tuple[int, int, int, int]]:\n        image_shape = params[\"shape\"][:2]\n        crop_coords = fcrops.get_center_crop_coords(image_shape, (self.height, self.width))\n\n        return {\"crop_coords\": crop_coords}\n</code></pre>"},{"location":"api_reference/augmentations/crops/transforms/#albumentations.augmentations.crops.transforms.CenterCrop.get_params_dependent_on_data","title":"<code>get_params_dependent_on_data (self, params, data)</code>","text":"<p>Returns parameters dependent on input.</p> Source code in <code>albumentations/augmentations/crops/transforms.py</code> Python<pre><code>def get_params_dependent_on_data(\n    self,\n    params: dict[str, Any],\n    data: dict[str, Any],\n) -&gt; dict[str, tuple[int, int, int, int]]:\n    image_shape = params[\"shape\"][:2]\n    crop_coords = fcrops.get_center_crop_coords(image_shape, (self.height, self.width))\n\n    return {\"crop_coords\": crop_coords}\n</code></pre>"},{"location":"api_reference/augmentations/crops/transforms/#albumentations.augmentations.crops.transforms.CenterCrop.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/crops/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return \"height\", \"width\"\n</code></pre>"},{"location":"api_reference/augmentations/crops/transforms/#albumentations.augmentations.crops.transforms.Crop","title":"<code>class  Crop</code> <code>     (x_min=0, y_min=0, x_max=1024, y_max=1024, always_apply=None, p=1.0)                 </code>  [view source on GitHub]","text":"<p>Crop region from image.</p> <p>Parameters:</p> Name Type Description <code>x_min</code> <code>int</code> <p>Minimum upper left x coordinate.</p> <code>y_min</code> <code>int</code> <p>Minimum upper left y coordinate.</p> <code>x_max</code> <code>int</code> <p>Maximum lower right x coordinate.</p> <code>y_max</code> <code>int</code> <p>Maximum lower right y coordinate.</p> <p>Targets</p> <p>image, mask, bboxes, keypoints</p> <p>Image types:     uint8, float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/crops/transforms.py</code> Python<pre><code>class Crop(_BaseCrop):\n    \"\"\"Crop region from image.\n\n    Args:\n        x_min: Minimum upper left x coordinate.\n        y_min: Minimum upper left y coordinate.\n        x_max: Maximum lower right x coordinate.\n        y_max: Maximum lower right y coordinate.\n\n    Targets:\n        image, mask, bboxes, keypoints\n\n    Image types:\n        uint8, float32\n\n    \"\"\"\n\n    class InitSchema(BaseTransformInitSchema):\n        x_min: Annotated[int, Field(ge=0, description=\"Minimum upper left x coordinate\")]\n        y_min: Annotated[int, Field(ge=0, description=\"Minimum upper left y coordinate\")]\n        x_max: Annotated[int, Field(gt=0, description=\"Maximum lower right x coordinate\")]\n        y_max: Annotated[int, Field(gt=0, description=\"Maximum lower right y coordinate\")]\n        p: ProbabilityType = 1\n\n        @model_validator(mode=\"after\")\n        def validate_coordinates(self) -&gt; Self:\n            if not self.x_min &lt; self.x_max:\n                msg = \"x_max must be greater than x_min\"\n                raise ValueError(msg)\n            if not self.y_min &lt; self.y_max:\n                msg = \"y_max must be greater than y_min\"\n                raise ValueError(msg)\n            return self\n\n    def __init__(\n        self,\n        x_min: int = 0,\n        y_min: int = 0,\n        x_max: int = 1024,\n        y_max: int = 1024,\n        always_apply: bool | None = None,\n        p: float = 1.0,\n    ):\n        super().__init__(p=p, always_apply=always_apply)\n        self.x_min = x_min\n        self.y_min = y_min\n        self.x_max = x_max\n        self.y_max = y_max\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return \"x_min\", \"y_min\", \"x_max\", \"y_max\"\n\n    def get_params_dependent_on_data(\n        self,\n        params: dict[str, Any],\n        data: dict[str, Any],\n    ) -&gt; dict[str, tuple[int, int, int, int]]:\n        return {\"crop_coords\": (self.x_min, self.y_min, self.x_max, self.y_max)}\n</code></pre>"},{"location":"api_reference/augmentations/crops/transforms/#albumentations.augmentations.crops.transforms.Crop.get_params_dependent_on_data","title":"<code>get_params_dependent_on_data (self, params, data)</code>","text":"<p>Returns parameters dependent on input.</p> Source code in <code>albumentations/augmentations/crops/transforms.py</code> Python<pre><code>def get_params_dependent_on_data(\n    self,\n    params: dict[str, Any],\n    data: dict[str, Any],\n) -&gt; dict[str, tuple[int, int, int, int]]:\n    return {\"crop_coords\": (self.x_min, self.y_min, self.x_max, self.y_max)}\n</code></pre>"},{"location":"api_reference/augmentations/crops/transforms/#albumentations.augmentations.crops.transforms.Crop.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/crops/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return \"x_min\", \"y_min\", \"x_max\", \"y_max\"\n</code></pre>"},{"location":"api_reference/augmentations/crops/transforms/#albumentations.augmentations.crops.transforms.CropAndPad","title":"<code>class  CropAndPad</code> <code>     (px=None, percent=None, pad_mode=0, pad_cval=0, pad_cval_mask=0, keep_size=True, sample_independently=True, interpolation=1, always_apply=None, p=1.0)                 </code>  [view source on GitHub]","text":"<p>Crop and pad images by pixel amounts or fractions of image sizes. Cropping removes pixels at the sides (i.e., extracts a subimage from a given full image). Padding adds pixels to the sides (e.g., black pixels). This transformation will never crop images below a height or width of 1.</p> <p>Note</p> <p>This transformation automatically resizes images back to their original size. To deactivate this, add the parameter <code>keep_size=False</code>.</p> <p>Parameters:</p> Name Type Description <code>px</code> <code>int, tuple[int, int], tuple[int, int, int, int], tuple[Union[int, tuple[int, int], list[int]],       Union[int, tuple[int, int], list[int]],       Union[int, tuple[int, int], list[int]],       Union[int, tuple[int, int], list[int]]]</code> <p>The number of pixels to crop (negative values) or pad (positive values) on each side of the image.     Either this or the parameter <code>percent</code> may be set, not both at the same time.</p> <pre><code>* If `None`, then pixel-based cropping/padding will not be used.\n* If `int`, then that exact number of pixels will always be cropped/padded.\n* If a `tuple` of two `int`s with values `a` and `b`, then each side will be cropped/padded by a\n    random amount sampled uniformly per image and side from the interval `[a, b]`.\n    If `sample_independently` is set to `False`, only one value will be sampled per\n        image and used for all sides.\n* If a `tuple` of four entries, then the entries represent top, right, bottom, and left.\n    Each entry may be:\n    - A single `int` (always crop/pad by exactly that value).\n    - A `tuple` of two `int`s `a` and `b` (crop/pad by an amount within `[a, b]`).\n    - A `list` of `int`s (crop/pad by a random value that is contained in the `list`).\n</code></pre> <code>percent</code> <code>float,      tuple[float, float],      tuple[float, float, float, float],      tuple[Union[float, tuple[float, float], list[float]],            Union[float, tuple[float, float], list[float]],            Union[float, tuple[float, float], list[float]],            Union[float, tuple[float, float], list[float]]]</code> <p>The number of pixels to crop (negative values) or pad (positive values) on each side of the image given     as a fraction of the image height/width. E.g. if this is set to <code>-0.1</code>, the transformation will     always crop away <code>10%</code> of the image's height at both the top and the bottom (both <code>10%</code> each),     as well as <code>10%</code> of the width at the right and left. Expected value range is <code>(-1.0, inf)</code>.     Either this or the parameter <code>px</code> may be set, not both at the same time.</p> <pre><code>* If `None`, then fraction-based cropping/padding will not be used.\n* If `float`, then that fraction will always be cropped/padded.\n* If a `tuple` of two `float`s with values `a` and `b`, then each side will be cropped/padded by a\nrandom fraction sampled uniformly per image and side from the interval `[a, b]`.\nIf `sample_independently` is set to `False`, only one value will be sampled per image and used\nfor all sides.\n* If a `tuple` of four entries, then the entries represent top, right, bottom, and left.\n    Each entry may be:\n    - A single `float` (always crop/pad by exactly that percent value).\n    - A `tuple` of two `float`s `a` and `b` (crop/pad by a fraction from `[a, b]`).\n    - A `list` of `float`s (crop/pad by a random value that is contained in the `list`).\n</code></pre> <code>pad_mode</code> <code>int</code> <p>OpenCV border mode.</p> <code>pad_cval</code> <code>Union[int, float, tuple[Union[int, float], Union[int, float]], list[Union[int, float]]]</code> <p>The constant value to use if the pad mode is <code>BORDER_CONSTANT</code>.     * If <code>number</code>, then that value will be used.     * If a <code>tuple</code> of two numbers and at least one of them is a <code>float</code>, then a random number         will be uniformly sampled per image from the continuous interval <code>[a, b]</code> and used as the value.         If both numbers are <code>int</code>s, the interval is discrete.     * If a <code>list</code> of numbers, then a random value will be chosen from the elements of the <code>list</code> and         used as the value.</p> <code>pad_cval_mask</code> <code>Union[int, float, tuple[Union[int, float], Union[int, float]], list[Union[int, float]]]</code> <p>Same as <code>pad_cval</code> but only for masks.</p> <code>keep_size</code> <code>bool</code> <p>After cropping and padding, the resulting image will usually have a different height/width compared to the original input image. If this parameter is set to <code>True</code>, then the cropped/padded image will be resized to the input image's size, i.e., the output shape is always identical to the input shape.</p> <code>sample_independently</code> <code>bool</code> <p>If <code>False</code> and the values for <code>px</code>/<code>percent</code> result in exactly one probability distribution for all image sides, only one single value will be sampled from that probability distribution and used for all sides. I.e., the crop/pad amount then is the same for all sides. If <code>True</code>, four values will be sampled independently, one per side.</p> <code>interpolation</code> <code>int</code> <p>OpenCV flag that is used to specify the interpolation algorithm for images. Should be one of: <code>cv2.INTER_NEAREST</code>, <code>cv2.INTER_LINEAR</code>, <code>cv2.INTER_CUBIC</code>, <code>cv2.INTER_AREA</code>, <code>cv2.INTER_LANCZOS4</code>. Default: <code>cv2.INTER_LINEAR</code>.</p> <p>Targets</p> <p>image, mask, bboxes, keypoints</p> <p>Image types:     unit8, float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/crops/transforms.py</code> Python<pre><code>class CropAndPad(DualTransform):\n    \"\"\"Crop and pad images by pixel amounts or fractions of image sizes.\n    Cropping removes pixels at the sides (i.e., extracts a subimage from a given full image).\n    Padding adds pixels to the sides (e.g., black pixels).\n    This transformation will never crop images below a height or width of 1.\n\n    Note:\n        This transformation automatically resizes images back to their original size. To deactivate this, add the\n        parameter `keep_size=False`.\n\n    Args:\n        px (int,\n            tuple[int, int],\n            tuple[int, int, int, int],\n            tuple[Union[int, tuple[int, int], list[int]],\n                  Union[int, tuple[int, int], list[int]],\n                  Union[int, tuple[int, int], list[int]],\n                  Union[int, tuple[int, int], list[int]]]):\n            The number of pixels to crop (negative values) or pad (positive values) on each side of the image.\n                Either this or the parameter `percent` may be set, not both at the same time.\n\n                * If `None`, then pixel-based cropping/padding will not be used.\n                * If `int`, then that exact number of pixels will always be cropped/padded.\n                * If a `tuple` of two `int`s with values `a` and `b`, then each side will be cropped/padded by a\n                    random amount sampled uniformly per image and side from the interval `[a, b]`.\n                    If `sample_independently` is set to `False`, only one value will be sampled per\n                        image and used for all sides.\n                * If a `tuple` of four entries, then the entries represent top, right, bottom, and left.\n                    Each entry may be:\n                    - A single `int` (always crop/pad by exactly that value).\n                    - A `tuple` of two `int`s `a` and `b` (crop/pad by an amount within `[a, b]`).\n                    - A `list` of `int`s (crop/pad by a random value that is contained in the `list`).\n\n        percent (float,\n                 tuple[float, float],\n                 tuple[float, float, float, float],\n                 tuple[Union[float, tuple[float, float], list[float]],\n                       Union[float, tuple[float, float], list[float]],\n                       Union[float, tuple[float, float], list[float]],\n                       Union[float, tuple[float, float], list[float]]]):\n            The number of pixels to crop (negative values) or pad (positive values) on each side of the image given\n                as a *fraction* of the image height/width. E.g. if this is set to `-0.1`, the transformation will\n                always crop away `10%` of the image's height at both the top and the bottom (both `10%` each),\n                as well as `10%` of the width at the right and left. Expected value range is `(-1.0, inf)`.\n                Either this or the parameter `px` may be set, not both at the same time.\n\n                * If `None`, then fraction-based cropping/padding will not be used.\n                * If `float`, then that fraction will always be cropped/padded.\n                * If a `tuple` of two `float`s with values `a` and `b`, then each side will be cropped/padded by a\n                random fraction sampled uniformly per image and side from the interval `[a, b]`.\n                If `sample_independently` is set to `False`, only one value will be sampled per image and used\n                for all sides.\n                * If a `tuple` of four entries, then the entries represent top, right, bottom, and left.\n                    Each entry may be:\n                    - A single `float` (always crop/pad by exactly that percent value).\n                    - A `tuple` of two `float`s `a` and `b` (crop/pad by a fraction from `[a, b]`).\n                    - A `list` of `float`s (crop/pad by a random value that is contained in the `list`).\n\n        pad_mode (int): OpenCV border mode.\n        pad_cval (Union[int, float, tuple[Union[int, float], Union[int, float]], list[Union[int, float]]]):\n            The constant value to use if the pad mode is `BORDER_CONSTANT`.\n                * If `number`, then that value will be used.\n                * If a `tuple` of two numbers and at least one of them is a `float`, then a random number\n                    will be uniformly sampled per image from the continuous interval `[a, b]` and used as the value.\n                    If both numbers are `int`s, the interval is discrete.\n                * If a `list` of numbers, then a random value will be chosen from the elements of the `list` and\n                    used as the value.\n\n        pad_cval_mask (Union[int, float, tuple[Union[int, float], Union[int, float]], list[Union[int, float]]]):\n            Same as `pad_cval` but only for masks.\n\n        keep_size (bool):\n            After cropping and padding, the resulting image will usually have a different height/width compared to\n            the original input image. If this parameter is set to `True`, then the cropped/padded image will be\n            resized to the input image's size, i.e., the output shape is always identical to the input shape.\n\n        sample_independently (bool):\n            If `False` and the values for `px`/`percent` result in exactly one probability distribution for all\n            image sides, only one single value will be sampled from that probability distribution and used for\n            all sides. I.e., the crop/pad amount then is the same for all sides. If `True`, four values\n            will be sampled independently, one per side.\n\n        interpolation (int):\n            OpenCV flag that is used to specify the interpolation algorithm for images. Should be one of:\n            `cv2.INTER_NEAREST`, `cv2.INTER_LINEAR`, `cv2.INTER_CUBIC`, `cv2.INTER_AREA`, `cv2.INTER_LANCZOS4`.\n            Default: `cv2.INTER_LINEAR`.\n\n    Targets:\n        image, mask, bboxes, keypoints\n\n    Image types:\n        unit8, float32\n\n    \"\"\"\n\n    _targets = (Targets.IMAGE, Targets.MASK, Targets.BBOXES, Targets.KEYPOINTS)\n\n    class InitSchema(BaseTransformInitSchema):\n        px: PxType | None = Field(\n            default=None,\n            description=\"Number of pixels to crop (negative) or pad (positive).\",\n        )\n        percent: PercentType | None = Field(\n            default=None,\n            description=\"Fraction of image size to crop (negative) or pad (positive).\",\n        )\n        pad_mode: BorderModeType = cv2.BORDER_CONSTANT\n        pad_cval: ScalarType | tuple[ScalarType, ScalarType] | list[ScalarType] = Field(\n            default=0,\n            description=\"Padding value if pad_mode is BORDER_CONSTANT.\",\n        )\n        pad_cval_mask: ScalarType | tuple[ScalarType, ScalarType] | list[ScalarType] = Field(\n            default=0,\n            description=\"Padding value for masks if pad_mode is BORDER_CONSTANT.\",\n        )\n        keep_size: bool = Field(\n            default=True,\n            description=\"Whether to resize the image back to the original size after cropping and padding.\",\n        )\n        sample_independently: bool = Field(\n            default=True,\n            description=\"Whether to sample the crop/pad size independently for each side.\",\n        )\n        interpolation: InterpolationType = cv2.INTER_LINEAR\n        p: ProbabilityType = 1\n\n        @model_validator(mode=\"after\")\n        def check_px_percent(self) -&gt; Self:\n            if self.px is None and self.percent is None:\n                msg = \"Both px and percent parameters cannot be None simultaneously.\"\n                raise ValueError(msg)\n            if self.px is not None and self.percent is not None:\n                msg = \"Only px or percent may be set!\"\n                raise ValueError(msg)\n            return self\n\n    def __init__(\n        self,\n        px: int | list[int] | None = None,\n        percent: float | list[float] | None = None,\n        pad_mode: int = cv2.BORDER_CONSTANT,\n        pad_cval: ScalarType | tuple[ScalarType, ScalarType] | list[ScalarType] = 0,\n        pad_cval_mask: ScalarType | tuple[ScalarType, ScalarType] | list[ScalarType] = 0,\n        keep_size: bool = True,\n        sample_independently: bool = True,\n        interpolation: int = cv2.INTER_LINEAR,\n        always_apply: bool | None = None,\n        p: float = 1.0,\n    ):\n        super().__init__(p=p, always_apply=always_apply)\n\n        self.px = px\n        self.percent = percent\n\n        self.pad_mode = pad_mode\n        self.pad_cval = pad_cval\n        self.pad_cval_mask = pad_cval_mask\n\n        self.keep_size = keep_size\n        self.sample_independently = sample_independently\n\n        self.interpolation = interpolation\n\n    def apply(\n        self,\n        img: np.ndarray,\n        crop_params: Sequence[int],\n        pad_params: Sequence[int],\n        pad_value: ColorType,\n        interpolation: int,\n        **params: Any,\n    ) -&gt; np.ndarray:\n        return fcrops.crop_and_pad(\n            img,\n            crop_params,\n            pad_params,\n            pad_value,\n            params[\"shape\"][:2],\n            interpolation,\n            self.pad_mode,\n            self.keep_size,\n        )\n\n    def apply_to_mask(\n        self,\n        mask: np.ndarray,\n        crop_params: Sequence[int],\n        pad_params: Sequence[int],\n        pad_value_mask: float,\n        interpolation: int,\n        **params: Any,\n    ) -&gt; np.ndarray:\n        return fcrops.crop_and_pad(\n            mask,\n            crop_params,\n            pad_params,\n            pad_value_mask,\n            params[\"shape\"][:2],\n            interpolation,\n            self.pad_mode,\n            self.keep_size,\n        )\n\n    def apply_to_bbox(\n        self,\n        bbox: BoxInternalType,\n        crop_params: Sequence[int],\n        pad_params: Sequence[int],\n        result_shape: tuple[int, int],\n        **params: Any,\n    ) -&gt; BoxInternalType:\n        return fcrops.crop_and_pad_bbox(bbox, crop_params, pad_params, params[\"shape\"][:2], result_shape)\n\n    def apply_to_keypoint(\n        self,\n        keypoint: KeypointInternalType,\n        crop_params: Sequence[int],\n        pad_params: Sequence[int],\n        result_shape: tuple[int, int],\n        **params: Any,\n    ) -&gt; KeypointInternalType:\n        return fcrops.crop_and_pad_keypoint(\n            keypoint,\n            crop_params,\n            pad_params,\n            params[\"shape\"][:2],\n            result_shape,\n            self.keep_size,\n        )\n\n    @staticmethod\n    def __prevent_zero(val1: int, val2: int, max_val: int) -&gt; tuple[int, int]:\n        regain = abs(max_val) + 1\n        regain1 = regain // 2\n        regain2 = regain // 2\n        if regain1 + regain2 &lt; regain:\n            regain1 += 1\n\n        if regain1 &gt; val1:\n            diff = regain1 - val1\n            regain1 = val1\n            regain2 += diff\n        elif regain2 &gt; val2:\n            diff = regain2 - val2\n            regain2 = val2\n            regain1 += diff\n\n        return val1 - regain1, val2 - regain2\n\n    @staticmethod\n    def _prevent_zero(crop_params: list[int], height: int, width: int) -&gt; list[int]:\n        top, right, bottom, left = crop_params\n\n        remaining_height = height - (top + bottom)\n        remaining_width = width - (left + right)\n\n        if remaining_height &lt; 1:\n            top, bottom = CropAndPad.__prevent_zero(top, bottom, height)\n        if remaining_width &lt; 1:\n            left, right = CropAndPad.__prevent_zero(left, right, width)\n\n        return [max(top, 0), max(right, 0), max(bottom, 0), max(left, 0)]\n\n    def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n        height, width = params[\"shape\"][:2]\n\n        if self.px is not None:\n            new_params = self._get_px_params()\n        else:\n            percent_params = self._get_percent_params()\n            new_params = [\n                int(percent_params[0] * height),\n                int(percent_params[1] * width),\n                int(percent_params[2] * height),\n                int(percent_params[3] * width),\n            ]\n\n        pad_params = [max(i, 0) for i in new_params]\n\n        crop_params = self._prevent_zero([-min(i, 0) for i in new_params], height, width)\n\n        top, right, bottom, left = crop_params\n        crop_params = [left, top, width - right, height - bottom]\n        result_rows = crop_params[3] - crop_params[1]\n        result_cols = crop_params[2] - crop_params[0]\n        if result_cols == width and result_rows == height:\n            crop_params = []\n\n        top, right, bottom, left = pad_params\n        pad_params = [top, bottom, left, right]\n        if any(pad_params):\n            result_rows += top + bottom\n            result_cols += left + right\n        else:\n            pad_params = []\n\n        return {\n            \"crop_params\": crop_params or None,\n            \"pad_params\": pad_params or None,\n            \"pad_value\": None if pad_params is None else self._get_pad_value(self.pad_cval),\n            \"pad_value_mask\": None if pad_params is None else self._get_pad_value(self.pad_cval_mask),\n            \"result_shape\": (result_rows, result_cols),\n        }\n\n    def _get_px_params(self) -&gt; list[int]:\n        if self.px is None:\n            msg = \"px is not set\"\n            raise ValueError(msg)\n\n        if isinstance(self.px, int):\n            params = [self.px] * 4\n        elif len(self.px) == PAIR:\n            if self.sample_independently:\n                params = [random.randrange(*self.px) for _ in range(4)]\n            else:\n                px = random.randrange(*self.px)\n                params = [px] * 4\n        elif isinstance(self.px[0], int):\n            params = self.px\n        elif len(self.px[0]) == PAIR:\n            params = [random.randrange(*i) for i in self.px]\n        else:\n            params = [random.choice(i) for i in self.px]\n\n        return params\n\n    def _get_percent_params(self) -&gt; list[float]:\n        if self.percent is None:\n            msg = \"percent is not set\"\n            raise ValueError(msg)\n\n        if isinstance(self.percent, float):\n            params = [self.percent] * 4\n        elif len(self.percent) == PAIR:\n            if self.sample_independently:\n                params = [random.uniform(*self.percent) for _ in range(4)]\n            else:\n                px = random.uniform(*self.percent)\n                params = [px] * 4\n        elif isinstance(self.percent[0], (int, float)):\n            params = self.percent\n        elif len(self.percent[0]) == PAIR:\n            params = [random.uniform(*i) for i in self.percent]\n        else:\n            params = [random.choice(i) for i in self.percent]\n\n        return params  # params = [top, right, bottom, left]\n\n    @staticmethod\n    def _get_pad_value(\n        pad_value: ScalarType | tuple[ScalarType, ScalarType] | list[ScalarType],\n    ) -&gt; ScalarType:\n        if isinstance(pad_value, (int, float)):\n            return pad_value\n\n        if len(pad_value) == PAIR:\n            a, b = pad_value\n            if isinstance(a, int) and isinstance(b, int):\n                return random.randint(a, b)\n\n            return random.uniform(a, b)\n\n        return random.choice(pad_value)\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return (\n            \"px\",\n            \"percent\",\n            \"pad_mode\",\n            \"pad_cval\",\n            \"pad_cval_mask\",\n            \"keep_size\",\n            \"sample_independently\",\n            \"interpolation\",\n        )\n</code></pre>"},{"location":"api_reference/augmentations/crops/transforms/#albumentations.augmentations.crops.transforms.CropAndPad.apply","title":"<code>apply (self, img, crop_params, pad_params, pad_value, interpolation, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/crops/transforms.py</code> Python<pre><code>def apply(\n    self,\n    img: np.ndarray,\n    crop_params: Sequence[int],\n    pad_params: Sequence[int],\n    pad_value: ColorType,\n    interpolation: int,\n    **params: Any,\n) -&gt; np.ndarray:\n    return fcrops.crop_and_pad(\n        img,\n        crop_params,\n        pad_params,\n        pad_value,\n        params[\"shape\"][:2],\n        interpolation,\n        self.pad_mode,\n        self.keep_size,\n    )\n</code></pre>"},{"location":"api_reference/augmentations/crops/transforms/#albumentations.augmentations.crops.transforms.CropAndPad.get_params_dependent_on_data","title":"<code>get_params_dependent_on_data (self, params, data)</code>","text":"<p>Returns parameters dependent on input.</p> Source code in <code>albumentations/augmentations/crops/transforms.py</code> Python<pre><code>def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n    height, width = params[\"shape\"][:2]\n\n    if self.px is not None:\n        new_params = self._get_px_params()\n    else:\n        percent_params = self._get_percent_params()\n        new_params = [\n            int(percent_params[0] * height),\n            int(percent_params[1] * width),\n            int(percent_params[2] * height),\n            int(percent_params[3] * width),\n        ]\n\n    pad_params = [max(i, 0) for i in new_params]\n\n    crop_params = self._prevent_zero([-min(i, 0) for i in new_params], height, width)\n\n    top, right, bottom, left = crop_params\n    crop_params = [left, top, width - right, height - bottom]\n    result_rows = crop_params[3] - crop_params[1]\n    result_cols = crop_params[2] - crop_params[0]\n    if result_cols == width and result_rows == height:\n        crop_params = []\n\n    top, right, bottom, left = pad_params\n    pad_params = [top, bottom, left, right]\n    if any(pad_params):\n        result_rows += top + bottom\n        result_cols += left + right\n    else:\n        pad_params = []\n\n    return {\n        \"crop_params\": crop_params or None,\n        \"pad_params\": pad_params or None,\n        \"pad_value\": None if pad_params is None else self._get_pad_value(self.pad_cval),\n        \"pad_value_mask\": None if pad_params is None else self._get_pad_value(self.pad_cval_mask),\n        \"result_shape\": (result_rows, result_cols),\n    }\n</code></pre>"},{"location":"api_reference/augmentations/crops/transforms/#albumentations.augmentations.crops.transforms.CropAndPad.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/crops/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return (\n        \"px\",\n        \"percent\",\n        \"pad_mode\",\n        \"pad_cval\",\n        \"pad_cval_mask\",\n        \"keep_size\",\n        \"sample_independently\",\n        \"interpolation\",\n    )\n</code></pre>"},{"location":"api_reference/augmentations/crops/transforms/#albumentations.augmentations.crops.transforms.CropNonEmptyMaskIfExists","title":"<code>class  CropNonEmptyMaskIfExists</code> <code>     (height, width, ignore_values=None, ignore_channels=None, always_apply=None, p=1.0)                 </code>  [view source on GitHub]","text":"<p>Crop area with mask if mask is non-empty, else make random crop.</p> <p>Parameters:</p> Name Type Description <code>height</code> <code>int</code> <p>vertical size of crop in pixels</p> <code>width</code> <code>int</code> <p>horizontal size of crop in pixels</p> <code>ignore_values</code> <code>list of int</code> <p>values to ignore in mask, <code>0</code> values are always ignored (e.g. if background value is 5 set <code>ignore_values=[5]</code> to ignore)</p> <code>ignore_channels</code> <code>list of int</code> <p>channels to ignore in mask (e.g. if background is a first channel set <code>ignore_channels=[0]</code> to ignore)</p> <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 1.0.</p> <p>Targets</p> <p>image, mask, bboxes, keypoints</p> <p>Image types:     uint8, float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/crops/transforms.py</code> Python<pre><code>class CropNonEmptyMaskIfExists(_BaseCrop):\n    \"\"\"Crop area with mask if mask is non-empty, else make random crop.\n\n    Args:\n        height: vertical size of crop in pixels\n        width: horizontal size of crop in pixels\n        ignore_values (list of int): values to ignore in mask, `0` values are always ignored\n            (e.g. if background value is 5 set `ignore_values=[5]` to ignore)\n        ignore_channels (list of int): channels to ignore in mask\n            (e.g. if background is a first channel set `ignore_channels=[0]` to ignore)\n        p: probability of applying the transform. Default: 1.0.\n\n    Targets:\n        image, mask, bboxes, keypoints\n\n    Image types:\n        uint8, float32\n\n    \"\"\"\n\n    class InitSchema(CropInitSchema):\n        ignore_values: list[int] | None = Field(\n            default=None,\n            description=\"Values to ignore in mask, `0` values are always ignored\",\n        )\n        ignore_channels: list[int] | None = Field(default=None, description=\"Channels to ignore in mask\")\n\n    def __init__(\n        self,\n        height: int,\n        width: int,\n        ignore_values: list[int] | None = None,\n        ignore_channels: list[int] | None = None,\n        always_apply: bool | None = None,\n        p: float = 1.0,\n    ):\n        super().__init__(p, always_apply)\n\n        self.height = height\n        self.width = width\n        self.ignore_values = ignore_values\n        self.ignore_channels = ignore_channels\n\n    def _preprocess_mask(self, mask: np.ndarray) -&gt; np.ndarray:\n        mask_height, mask_width = mask.shape[:2]\n\n        if self.ignore_values is not None:\n            ignore_values_np = np.array(self.ignore_values)\n            mask = np.where(np.isin(mask, ignore_values_np), 0, mask)\n\n        if mask.ndim == NUM_MULTI_CHANNEL_DIMENSIONS and self.ignore_channels is not None:\n            target_channels = np.array([ch for ch in range(mask.shape[-1]) if ch not in self.ignore_channels])\n            mask = np.take(mask, target_channels, axis=-1)\n\n        if self.height &gt; mask_height or self.width &gt; mask_width:\n            raise ValueError(\n                f\"Crop size ({self.height},{self.width}) is larger than image ({mask_height},{mask_width})\",\n            )\n\n        return mask\n\n    def update_params(self, params: dict[str, Any], **kwargs: Any) -&gt; dict[str, Any]:\n        super().update_params(params, **kwargs)\n        if \"mask\" in kwargs:\n            mask = self._preprocess_mask(kwargs[\"mask\"])\n        elif \"masks\" in kwargs and len(kwargs[\"masks\"]):\n            masks = kwargs[\"masks\"]\n            mask = self._preprocess_mask(np.copy(masks[0]))  # need copy as we perform in-place mod afterwards\n            for m in masks[1:]:\n                mask |= self._preprocess_mask(m)\n        else:\n            msg = \"Can not find mask for CropNonEmptyMaskIfExists\"\n            raise RuntimeError(msg)\n\n        mask_height, mask_width = mask.shape[:2]\n\n        if mask.any():\n            mask = mask.sum(axis=-1) if mask.ndim == NUM_MULTI_CHANNEL_DIMENSIONS else mask\n            non_zero_yx = np.argwhere(mask)\n            y, x = random.choice(non_zero_yx)\n            x_min = x - random.randint(0, self.width - 1)\n            y_min = y - random.randint(0, self.height - 1)\n            x_min = np.clip(x_min, 0, mask_width - self.width)\n            y_min = np.clip(y_min, 0, mask_height - self.height)\n        else:\n            x_min = random.randint(0, mask_width - self.width)\n            y_min = random.randint(0, mask_height - self.height)\n\n        x_max = x_min + self.width\n        y_max = y_min + self.height\n\n        crop_coords = x_min, y_min, x_max, y_max\n\n        params[\"crop_coords\"] = crop_coords\n        return params\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return \"height\", \"width\", \"ignore_values\", \"ignore_channels\"\n</code></pre>"},{"location":"api_reference/augmentations/crops/transforms/#albumentations.augmentations.crops.transforms.CropNonEmptyMaskIfExists.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/crops/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return \"height\", \"width\", \"ignore_values\", \"ignore_channels\"\n</code></pre>"},{"location":"api_reference/augmentations/crops/transforms/#albumentations.augmentations.crops.transforms.CropNonEmptyMaskIfExists.update_params","title":"<code>update_params (self, params, **kwargs)</code>","text":"<p>Update parameters with transform specific params. This method is deprecated, use: - <code>get_params</code> for transform specific params like interpolation and - <code>update_params_shape</code> for data like shape.</p> Source code in <code>albumentations/augmentations/crops/transforms.py</code> Python<pre><code>def update_params(self, params: dict[str, Any], **kwargs: Any) -&gt; dict[str, Any]:\n    super().update_params(params, **kwargs)\n    if \"mask\" in kwargs:\n        mask = self._preprocess_mask(kwargs[\"mask\"])\n    elif \"masks\" in kwargs and len(kwargs[\"masks\"]):\n        masks = kwargs[\"masks\"]\n        mask = self._preprocess_mask(np.copy(masks[0]))  # need copy as we perform in-place mod afterwards\n        for m in masks[1:]:\n            mask |= self._preprocess_mask(m)\n    else:\n        msg = \"Can not find mask for CropNonEmptyMaskIfExists\"\n        raise RuntimeError(msg)\n\n    mask_height, mask_width = mask.shape[:2]\n\n    if mask.any():\n        mask = mask.sum(axis=-1) if mask.ndim == NUM_MULTI_CHANNEL_DIMENSIONS else mask\n        non_zero_yx = np.argwhere(mask)\n        y, x = random.choice(non_zero_yx)\n        x_min = x - random.randint(0, self.width - 1)\n        y_min = y - random.randint(0, self.height - 1)\n        x_min = np.clip(x_min, 0, mask_width - self.width)\n        y_min = np.clip(y_min, 0, mask_height - self.height)\n    else:\n        x_min = random.randint(0, mask_width - self.width)\n        y_min = random.randint(0, mask_height - self.height)\n\n    x_max = x_min + self.width\n    y_max = y_min + self.height\n\n    crop_coords = x_min, y_min, x_max, y_max\n\n    params[\"crop_coords\"] = crop_coords\n    return params\n</code></pre>"},{"location":"api_reference/augmentations/crops/transforms/#albumentations.augmentations.crops.transforms.RandomCrop","title":"<code>class  RandomCrop</code> <code>     (height, width, p=1.0, always_apply=None)                 </code>  [view source on GitHub]","text":"<p>Crop a random part of the input.</p> <p>Parameters:</p> Name Type Description <code>height</code> <code>int</code> <p>height of the crop.</p> <code>width</code> <code>int</code> <p>width of the crop.</p> <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 1.</p> <p>Targets</p> <p>image, mask, bboxes, keypoints</p> <p>Image types:     uint8, float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/crops/transforms.py</code> Python<pre><code>class RandomCrop(_BaseCrop):\n    \"\"\"Crop a random part of the input.\n\n    Args:\n        height: height of the crop.\n        width: width of the crop.\n        p: probability of applying the transform. Default: 1.\n\n    Targets:\n        image, mask, bboxes, keypoints\n\n    Image types:\n        uint8, float32\n\n    \"\"\"\n\n    class InitSchema(CropInitSchema):\n        pass\n\n    def __init__(self, height: int, width: int, p: float = 1.0, always_apply: bool | None = None):\n        super().__init__(p=p, always_apply=always_apply)\n        self.height = height\n        self.width = width\n\n    def get_params_dependent_on_data(\n        self,\n        params: dict[str, Any],\n        data: dict[str, Any],\n    ) -&gt; dict[str, tuple[int, int, int, int]]:\n        image_shape = params[\"shape\"][:2]\n\n        image_height, image_width = image_shape\n\n        if self.height &gt; image_height or self.width &gt; image_width:\n            raise CropSizeError(\n                f\"Crop size (height, width) exceeds image dimensions (height, width):\"\n                f\" {(self.height, self.width)} vs {image_shape[:2]}\",\n            )\n\n        h_start = random.random()\n        w_start = random.random()\n        crop_coords = fcrops.get_crop_coords(image_shape, (self.height, self.width), h_start, w_start)\n        return {\"crop_coords\": crop_coords}\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return \"height\", \"width\"\n</code></pre>"},{"location":"api_reference/augmentations/crops/transforms/#albumentations.augmentations.crops.transforms.RandomCrop.get_params_dependent_on_data","title":"<code>get_params_dependent_on_data (self, params, data)</code>","text":"<p>Returns parameters dependent on input.</p> Source code in <code>albumentations/augmentations/crops/transforms.py</code> Python<pre><code>def get_params_dependent_on_data(\n    self,\n    params: dict[str, Any],\n    data: dict[str, Any],\n) -&gt; dict[str, tuple[int, int, int, int]]:\n    image_shape = params[\"shape\"][:2]\n\n    image_height, image_width = image_shape\n\n    if self.height &gt; image_height or self.width &gt; image_width:\n        raise CropSizeError(\n            f\"Crop size (height, width) exceeds image dimensions (height, width):\"\n            f\" {(self.height, self.width)} vs {image_shape[:2]}\",\n        )\n\n    h_start = random.random()\n    w_start = random.random()\n    crop_coords = fcrops.get_crop_coords(image_shape, (self.height, self.width), h_start, w_start)\n    return {\"crop_coords\": crop_coords}\n</code></pre>"},{"location":"api_reference/augmentations/crops/transforms/#albumentations.augmentations.crops.transforms.RandomCrop.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/crops/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return \"height\", \"width\"\n</code></pre>"},{"location":"api_reference/augmentations/crops/transforms/#albumentations.augmentations.crops.transforms.RandomCropFromBorders","title":"<code>class  RandomCropFromBorders</code> <code>     (crop_left=0.1, crop_right=0.1, crop_top=0.1, crop_bottom=0.1, always_apply=None, p=1.0)                 </code>  [view source on GitHub]","text":"<p>Randomly crops parts of the image from the borders without resizing at the end. The cropped regions are defined as fractions of the original image dimensions, specified for each side of the image (left, right, top, bottom).</p> <p>Parameters:</p> Name Type Description <code>crop_left</code> <code>float</code> <p>Fraction of the width to randomly crop from the left side. Must be in the range [0.0, 1.0].                 Default is 0.1.</p> <code>crop_right</code> <code>float</code> <p>Fraction of the width to randomly crop from the right side. Must be in the range [0.0, 1.0].                 Default is 0.1.</p> <code>crop_top</code> <code>float</code> <p>Fraction of the height to randomly crop from the top side. Must be in the range [0.0, 1.0].               Default is 0.1.</p> <code>crop_bottom</code> <code>float</code> <p>Fraction of the height to randomly crop from the bottom side.                  Must be in the range [0.0, 1.0]. Default is 0.1.</p> <code>p</code> <code>float</code> <p>Probability of applying the transform. Default is 1.</p> <p>Targets</p> <p>image, mask, bboxes, keypoints</p> <p>Image types:     uint8, float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/crops/transforms.py</code> Python<pre><code>class RandomCropFromBorders(_BaseCrop):\n    \"\"\"Randomly crops parts of the image from the borders without resizing at the end. The cropped regions are defined\n    as fractions of the original image dimensions, specified for each side of the image (left, right, top, bottom).\n\n    Args:\n        crop_left (float): Fraction of the width to randomly crop from the left side. Must be in the range [0.0, 1.0].\n                            Default is 0.1.\n        crop_right (float): Fraction of the width to randomly crop from the right side. Must be in the range [0.0, 1.0].\n                            Default is 0.1.\n        crop_top (float): Fraction of the height to randomly crop from the top side. Must be in the range [0.0, 1.0].\n                          Default is 0.1.\n        crop_bottom (float): Fraction of the height to randomly crop from the bottom side.\n                             Must be in the range [0.0, 1.0]. Default is 0.1.\n        p (float): Probability of applying the transform. Default is 1.\n\n    Targets:\n        image, mask, bboxes, keypoints\n\n    Image types:\n        uint8, float32\n    \"\"\"\n\n    _targets = (Targets.IMAGE, Targets.MASK, Targets.BBOXES, Targets.KEYPOINTS)\n\n    class InitSchema(BaseTransformInitSchema):\n        crop_left: float = Field(\n            default=0.1,\n            ge=0.0,\n            le=1.0,\n            description=\"Fraction of width to randomly crop from the left side.\",\n        )\n        crop_right: float = Field(\n            default=0.1,\n            ge=0.0,\n            le=1.0,\n            description=\"Fraction of width to randomly crop from the right side.\",\n        )\n        crop_top: float = Field(\n            default=0.1,\n            ge=0.0,\n            le=1.0,\n            description=\"Fraction of height to randomly crop from the top side.\",\n        )\n        crop_bottom: float = Field(\n            default=0.1,\n            ge=0.0,\n            le=1.0,\n            description=\"Fraction of height to randomly crop from the bottom side.\",\n        )\n        p: ProbabilityType = 1\n\n        @model_validator(mode=\"after\")\n        def validate_crop_values(self) -&gt; Self:\n            if self.crop_left + self.crop_right &gt; 1.0:\n                msg = \"The sum of crop_left and crop_right must be &lt;= 1.\"\n                raise ValueError(msg)\n            if self.crop_top + self.crop_bottom &gt; 1.0:\n                msg = \"The sum of crop_top and crop_bottom must be &lt;= 1.\"\n                raise ValueError(msg)\n            return self\n\n    def __init__(\n        self,\n        crop_left: float = 0.1,\n        crop_right: float = 0.1,\n        crop_top: float = 0.1,\n        crop_bottom: float = 0.1,\n        always_apply: bool | None = None,\n        p: float = 1.0,\n    ):\n        super().__init__(p, always_apply)\n        self.crop_left = crop_left\n        self.crop_right = crop_right\n        self.crop_top = crop_top\n        self.crop_bottom = crop_bottom\n\n    def get_params_dependent_on_data(\n        self,\n        params: dict[str, Any],\n        data: dict[str, Any],\n    ) -&gt; dict[str, tuple[int, int, int, int]]:\n        height, width = params[\"shape\"][:2]\n\n        x_min = random.randint(0, int(self.crop_left * width))\n        x_max = random.randint(max(x_min + 1, int((1 - self.crop_right) * width)), width)\n\n        y_min = random.randint(0, int(self.crop_top * height))\n        y_max = random.randint(max(y_min + 1, int((1 - self.crop_bottom) * height)), height)\n\n        crop_coords = x_min, y_min, x_max, y_max\n\n        return {\"crop_coords\": crop_coords}\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return \"crop_left\", \"crop_right\", \"crop_top\", \"crop_bottom\"\n</code></pre>"},{"location":"api_reference/augmentations/crops/transforms/#albumentations.augmentations.crops.transforms.RandomCropFromBorders.get_params_dependent_on_data","title":"<code>get_params_dependent_on_data (self, params, data)</code>","text":"<p>Returns parameters dependent on input.</p> Source code in <code>albumentations/augmentations/crops/transforms.py</code> Python<pre><code>def get_params_dependent_on_data(\n    self,\n    params: dict[str, Any],\n    data: dict[str, Any],\n) -&gt; dict[str, tuple[int, int, int, int]]:\n    height, width = params[\"shape\"][:2]\n\n    x_min = random.randint(0, int(self.crop_left * width))\n    x_max = random.randint(max(x_min + 1, int((1 - self.crop_right) * width)), width)\n\n    y_min = random.randint(0, int(self.crop_top * height))\n    y_max = random.randint(max(y_min + 1, int((1 - self.crop_bottom) * height)), height)\n\n    crop_coords = x_min, y_min, x_max, y_max\n\n    return {\"crop_coords\": crop_coords}\n</code></pre>"},{"location":"api_reference/augmentations/crops/transforms/#albumentations.augmentations.crops.transforms.RandomCropFromBorders.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/crops/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return \"crop_left\", \"crop_right\", \"crop_top\", \"crop_bottom\"\n</code></pre>"},{"location":"api_reference/augmentations/crops/transforms/#albumentations.augmentations.crops.transforms.RandomCropNearBBox","title":"<code>class  RandomCropNearBBox</code> <code>     (max_part_shift=(0, 0.3), cropping_bbox_key='cropping_bbox', cropping_box_key=None, always_apply=None, p=1.0)                 </code>  [view source on GitHub]","text":"<p>Crop bbox from image with random shift by x,y coordinates</p> <p>Parameters:</p> Name Type Description <code>max_part_shift</code> <code>float, (float, float</code> <p>Max shift in <code>height</code> and <code>width</code> dimensions relative to <code>cropping_bbox</code> dimension. If max_part_shift is a single float, the range will be (0, max_part_shift). Default (0, 0.3).</p> <code>cropping_bbox_key</code> <code>str</code> <p>Additional target key for cropping box. Default <code>cropping_bbox</code>.</p> <code>cropping_box_key</code> <code>str</code> <p>[Deprecated] Use <code>cropping_bbox_key</code> instead.</p> <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 1.</p> <p>Targets</p> <p>image, mask, bboxes, keypoints</p> <p>Image types:     uint8, float32</p> <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; aug = Compose([RandomCropNearBBox(max_part_shift=(0.1, 0.5), cropping_bbox_key='test_bbox')],\n&gt;&gt;&gt;              bbox_params=BboxParams(\"pascal_voc\"))\n&gt;&gt;&gt; result = aug(image=image, bboxes=bboxes, test_bbox=[0, 5, 10, 20])\n</code></pre> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/crops/transforms.py</code> Python<pre><code>class RandomCropNearBBox(_BaseCrop):\n    \"\"\"Crop bbox from image with random shift by x,y coordinates\n\n    Args:\n        max_part_shift (float, (float, float)): Max shift in `height` and `width` dimensions relative\n            to `cropping_bbox` dimension.\n            If max_part_shift is a single float, the range will be (0, max_part_shift).\n            Default (0, 0.3).\n        cropping_bbox_key (str): Additional target key for cropping box. Default `cropping_bbox`.\n        cropping_box_key (str): [Deprecated] Use `cropping_bbox_key` instead.\n        p (float): probability of applying the transform. Default: 1.\n\n    Targets:\n        image, mask, bboxes, keypoints\n\n    Image types:\n        uint8, float32\n\n    Examples:\n        &gt;&gt;&gt; aug = Compose([RandomCropNearBBox(max_part_shift=(0.1, 0.5), cropping_bbox_key='test_bbox')],\n        &gt;&gt;&gt;              bbox_params=BboxParams(\"pascal_voc\"))\n        &gt;&gt;&gt; result = aug(image=image, bboxes=bboxes, test_bbox=[0, 5, 10, 20])\n\n    \"\"\"\n\n    _targets = (Targets.IMAGE, Targets.MASK, Targets.BBOXES, Targets.KEYPOINTS)\n\n    class InitSchema(BaseTransformInitSchema):\n        max_part_shift: ZeroOneRangeType = (0, 0.3)\n        cropping_bbox_key: str = Field(default=\"cropping_bbox\", description=\"Additional target key for cropping box.\")\n        p: ProbabilityType = 1\n\n    def __init__(\n        self,\n        max_part_shift: ScaleFloatType = (0, 0.3),\n        cropping_bbox_key: str = \"cropping_bbox\",\n        cropping_box_key: str | None = None,  # Deprecated\n        always_apply: bool | None = None,\n        p: float = 1.0,\n    ):\n        super().__init__(p=p, always_apply=always_apply)\n        # Check for deprecated parameter and issue warning\n        if cropping_box_key is not None:\n            warn(\n                \"The parameter 'cropping_box_key' is deprecated and will be removed in future versions. \"\n                \"Use 'cropping_bbox_key' instead.\",\n                DeprecationWarning,\n                stacklevel=2,\n            )\n            # Ensure the new parameter is used even if the old one is passed\n            cropping_bbox_key = cropping_box_key\n\n        self.max_part_shift = cast(Tuple[float, float], max_part_shift)\n        self.cropping_bbox_key = cropping_bbox_key\n\n    @staticmethod\n    def _clip_bbox(bbox: BoxInternalType, image_shape: tuple[int, int]) -&gt; BoxInternalType:\n        height, width = image_shape[:2]\n        x_min, y_min, x_max, y_max = bbox\n        x_min = np.clip(x_min, 0, width)\n        y_min = np.clip(y_min, 0, height)\n\n        x_max = np.clip(x_max, x_min, width)\n        y_max = np.clip(y_max, y_min, height)\n        return x_min, y_min, x_max, y_max\n\n    def get_params_dependent_on_data(\n        self,\n        params: dict[str, Any],\n        data: dict[str, Any],\n    ) -&gt; dict[str, tuple[float, ...]]:\n        bbox = data[self.cropping_bbox_key]\n\n        image_shape = params[\"shape\"][:2]\n\n        bbox = self._clip_bbox(bbox, image_shape)\n\n        h_max_shift = round((bbox[3] - bbox[1]) * self.max_part_shift[0])\n        w_max_shift = round((bbox[2] - bbox[0]) * self.max_part_shift[1])\n\n        x_min = bbox[0] - random.randint(-w_max_shift, w_max_shift)\n        x_max = bbox[2] + random.randint(-w_max_shift, w_max_shift)\n\n        y_min = bbox[1] - random.randint(-h_max_shift, h_max_shift)\n        y_max = bbox[3] + random.randint(-h_max_shift, h_max_shift)\n\n        crop_coords = self._clip_bbox((x_min, y_min, x_max, y_max), image_shape)\n\n        if crop_coords[0] == crop_coords[2] or crop_coords[1] == crop_coords[3]:\n            crop_shape = (bbox[3] - bbox[1], bbox[2] - bbox[0])\n            crop_coords = fcrops.get_center_crop_coords(image_shape, crop_shape)\n\n        return {\"crop_coords\": crop_coords}\n\n    @property\n    def targets_as_params(self) -&gt; list[str]:\n        return [self.cropping_bbox_key]\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return \"max_part_shift\", \"cropping_bbox_key\"\n</code></pre>"},{"location":"api_reference/augmentations/crops/transforms/#albumentations.augmentations.crops.transforms.RandomCropNearBBox.targets_as_params","title":"<code>targets_as_params: list[str]</code>  <code>property</code> <code>readonly</code>","text":"<p>Targets used to get params dependent on targets. This is used to check input has all required targets.</p>"},{"location":"api_reference/augmentations/crops/transforms/#albumentations.augmentations.crops.transforms.RandomCropNearBBox.get_params_dependent_on_data","title":"<code>get_params_dependent_on_data (self, params, data)</code>","text":"<p>Returns parameters dependent on input.</p> Source code in <code>albumentations/augmentations/crops/transforms.py</code> Python<pre><code>def get_params_dependent_on_data(\n    self,\n    params: dict[str, Any],\n    data: dict[str, Any],\n) -&gt; dict[str, tuple[float, ...]]:\n    bbox = data[self.cropping_bbox_key]\n\n    image_shape = params[\"shape\"][:2]\n\n    bbox = self._clip_bbox(bbox, image_shape)\n\n    h_max_shift = round((bbox[3] - bbox[1]) * self.max_part_shift[0])\n    w_max_shift = round((bbox[2] - bbox[0]) * self.max_part_shift[1])\n\n    x_min = bbox[0] - random.randint(-w_max_shift, w_max_shift)\n    x_max = bbox[2] + random.randint(-w_max_shift, w_max_shift)\n\n    y_min = bbox[1] - random.randint(-h_max_shift, h_max_shift)\n    y_max = bbox[3] + random.randint(-h_max_shift, h_max_shift)\n\n    crop_coords = self._clip_bbox((x_min, y_min, x_max, y_max), image_shape)\n\n    if crop_coords[0] == crop_coords[2] or crop_coords[1] == crop_coords[3]:\n        crop_shape = (bbox[3] - bbox[1], bbox[2] - bbox[0])\n        crop_coords = fcrops.get_center_crop_coords(image_shape, crop_shape)\n\n    return {\"crop_coords\": crop_coords}\n</code></pre>"},{"location":"api_reference/augmentations/crops/transforms/#albumentations.augmentations.crops.transforms.RandomCropNearBBox.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/crops/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return \"max_part_shift\", \"cropping_bbox_key\"\n</code></pre>"},{"location":"api_reference/augmentations/crops/transforms/#albumentations.augmentations.crops.transforms.RandomResizedCrop","title":"<code>class  RandomResizedCrop</code> <code>     (size=None, width=None, height=None, *, scale=(0.08, 1.0), ratio=(0.75, 1.3333333333333333), interpolation=1, always_apply=None, p=1.0)                 </code>  [view source on GitHub]","text":"<p>Torchvision's variant of crop a random part of the input and rescale it to some size.</p> <p>Parameters:</p> Name Type Description <code>size</code> <code>int, int</code> <p>expected output size of the crop, for each edge. If size is an int instead of sequence like (height, width), a square output size (size, size) is made. If provided a sequence of length 1, it will be interpreted as (size[0], size[0]).</p> <code>scale</code> <code>float, float</code> <p>Specifies the lower and upper bounds for the random area of the crop, before resizing. The scale is defined with respect to the area of the original image.</p> <code>ratio</code> <code>float, float</code> <p>lower and upper bounds for the random aspect ratio of the crop, before resizing.</p> <code>interpolation</code> <code>OpenCV flag</code> <p>flag that is used to specify the interpolation algorithm. Should be one of: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4. Default: cv2.INTER_LINEAR.</p> <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 1.</p> <p>Targets</p> <p>image, mask, bboxes, keypoints</p> <p>Image types:     uint8, float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/crops/transforms.py</code> Python<pre><code>class RandomResizedCrop(_BaseRandomSizedCrop):\n    \"\"\"Torchvision's variant of crop a random part of the input and rescale it to some size.\n\n    Args:\n        size (int, int): expected output size of the crop, for each edge. If size is an int instead of sequence\n            like (height, width), a square output size (size, size) is made. If provided a sequence of length 1,\n            it will be interpreted as (size[0], size[0]).\n        scale ((float, float)): Specifies the lower and upper bounds for the random area of the crop, before resizing.\n            The scale is defined with respect to the area of the original image.\n        ratio ((float, float)): lower and upper bounds for the random aspect ratio of the crop, before resizing.\n        interpolation (OpenCV flag): flag that is used to specify the interpolation algorithm. Should be one of:\n            cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.\n            Default: cv2.INTER_LINEAR.\n        p (float): probability of applying the transform. Default: 1.\n\n    Targets:\n        image, mask, bboxes, keypoints\n\n    Image types:\n        uint8, float32\n\n    \"\"\"\n\n    _targets = (Targets.IMAGE, Targets.MASK, Targets.BBOXES, Targets.KEYPOINTS)\n\n    class InitSchema(BaseTransformInitSchema):\n        scale: Annotated[tuple[float, float], AfterValidator(check_01)] = (0.08, 1.0)\n        ratio: Annotated[tuple[float, float], AfterValidator(check_0plus)] = (0.75, 1.3333333333333333)\n        width: int | None = Field(\n            None,\n            deprecated=\"Initializing with 'height' and 'width' is deprecated. Use size instead.\",\n        )\n        height: int | None = Field(\n            None,\n            deprecated=\"Initializing with 'height' and 'width' is deprecated. Use size instead.\",\n        )\n        size: ScaleIntType | None = None\n        p: ProbabilityType = 1\n        interpolation: InterpolationType = cv2.INTER_LINEAR\n\n        @model_validator(mode=\"after\")\n        def process(self) -&gt; Self:\n            if isinstance(self.size, int):\n                if isinstance(self.width, int):\n                    self.size = (self.size, self.width)\n                else:\n                    msg = \"If size is an integer, width as integer must be specified.\"\n                    raise TypeError(msg)\n\n            if self.size is None:\n                if self.height is None or self.width is None:\n                    message = \"If 'size' is not provided, both 'height' and 'width' must be specified.\"\n                    raise ValueError(message)\n                self.size = (self.height, self.width)\n\n            return self\n\n    def __init__(\n        self,\n        # NOTE @zetyquickly: when (width, height) are deprecated, make 'size' non optional\n        size: ScaleIntType | None = None,\n        width: int | None = None,\n        height: int | None = None,\n        *,\n        scale: tuple[float, float] = (0.08, 1.0),\n        ratio: tuple[float, float] = (0.75, 1.3333333333333333),\n        interpolation: int = cv2.INTER_LINEAR,\n        always_apply: bool | None = None,\n        p: float = 1.0,\n    ):\n        super().__init__(size=cast(Tuple[int, int], size), interpolation=interpolation, p=p, always_apply=always_apply)\n        self.scale = scale\n        self.ratio = ratio\n\n    def get_params_dependent_on_data(\n        self,\n        params: dict[str, Any],\n        data: dict[str, Any],\n    ) -&gt; dict[str, tuple[int, int, int, int]]:\n        image_shape = params[\"shape\"][:2]\n        image_height, image_width = image_shape\n\n        area = image_height * image_width\n\n        for _ in range(10):\n            target_area = random.uniform(*self.scale) * area\n            log_ratio = (math.log(self.ratio[0]), math.log(self.ratio[1]))\n            aspect_ratio = math.exp(random.uniform(*log_ratio))\n\n            width = int(round(math.sqrt(target_area * aspect_ratio)))\n            height = int(round(math.sqrt(target_area / aspect_ratio)))\n\n            if 0 &lt; width &lt;= image_width and 0 &lt; height &lt;= image_height:\n                i = random.randint(0, image_height - height)\n                j = random.randint(0, image_width - width)\n\n                h_start = i * 1.0 / (image_height - height + 1e-10)\n                w_start = j * 1.0 / (image_width - width + 1e-10)\n\n                crop_shape = (height, width)\n\n                crop_coords = fcrops.get_crop_coords(image_shape, crop_shape, h_start, w_start)\n\n                return {\"crop_coords\": crop_coords}\n\n        # Fallback to central crop\n        in_ratio = image_width / image_height\n        if in_ratio &lt; min(self.ratio):\n            width = image_width\n            height = int(round(image_width / min(self.ratio)))\n        elif in_ratio &gt; max(self.ratio):\n            height = image_height\n            width = int(round(height * max(self.ratio)))\n        else:  # whole image\n            width = image_width\n            height = image_height\n\n        i = (image_height - height) // 2\n        j = (image_width - width) // 2\n\n        h_start = i * 1.0 / (image_height - height + 1e-10)\n        w_start = j * 1.0 / (image_width - width + 1e-10)\n\n        crop_shape = (height, width)\n\n        crop_coords = fcrops.get_crop_coords(image_shape, crop_shape, h_start, w_start)\n\n        return {\"crop_coords\": crop_coords}\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return \"size\", \"scale\", \"ratio\", \"interpolation\"\n</code></pre>"},{"location":"api_reference/augmentations/crops/transforms/#albumentations.augmentations.crops.transforms.RandomResizedCrop.get_params_dependent_on_data","title":"<code>get_params_dependent_on_data (self, params, data)</code>","text":"<p>Returns parameters dependent on input.</p> Source code in <code>albumentations/augmentations/crops/transforms.py</code> Python<pre><code>def get_params_dependent_on_data(\n    self,\n    params: dict[str, Any],\n    data: dict[str, Any],\n) -&gt; dict[str, tuple[int, int, int, int]]:\n    image_shape = params[\"shape\"][:2]\n    image_height, image_width = image_shape\n\n    area = image_height * image_width\n\n    for _ in range(10):\n        target_area = random.uniform(*self.scale) * area\n        log_ratio = (math.log(self.ratio[0]), math.log(self.ratio[1]))\n        aspect_ratio = math.exp(random.uniform(*log_ratio))\n\n        width = int(round(math.sqrt(target_area * aspect_ratio)))\n        height = int(round(math.sqrt(target_area / aspect_ratio)))\n\n        if 0 &lt; width &lt;= image_width and 0 &lt; height &lt;= image_height:\n            i = random.randint(0, image_height - height)\n            j = random.randint(0, image_width - width)\n\n            h_start = i * 1.0 / (image_height - height + 1e-10)\n            w_start = j * 1.0 / (image_width - width + 1e-10)\n\n            crop_shape = (height, width)\n\n            crop_coords = fcrops.get_crop_coords(image_shape, crop_shape, h_start, w_start)\n\n            return {\"crop_coords\": crop_coords}\n\n    # Fallback to central crop\n    in_ratio = image_width / image_height\n    if in_ratio &lt; min(self.ratio):\n        width = image_width\n        height = int(round(image_width / min(self.ratio)))\n    elif in_ratio &gt; max(self.ratio):\n        height = image_height\n        width = int(round(height * max(self.ratio)))\n    else:  # whole image\n        width = image_width\n        height = image_height\n\n    i = (image_height - height) // 2\n    j = (image_width - width) // 2\n\n    h_start = i * 1.0 / (image_height - height + 1e-10)\n    w_start = j * 1.0 / (image_width - width + 1e-10)\n\n    crop_shape = (height, width)\n\n    crop_coords = fcrops.get_crop_coords(image_shape, crop_shape, h_start, w_start)\n\n    return {\"crop_coords\": crop_coords}\n</code></pre>"},{"location":"api_reference/augmentations/crops/transforms/#albumentations.augmentations.crops.transforms.RandomResizedCrop.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/crops/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return \"size\", \"scale\", \"ratio\", \"interpolation\"\n</code></pre>"},{"location":"api_reference/augmentations/crops/transforms/#albumentations.augmentations.crops.transforms.RandomSizedBBoxSafeCrop","title":"<code>class  RandomSizedBBoxSafeCrop</code> <code>     (height, width, erosion_rate=0.0, interpolation=1, always_apply=None, p=1.0)                 </code>  [view source on GitHub]","text":"<p>Crop a random part of the input and rescale it to some size without loss of bboxes.</p> <p>Parameters:</p> Name Type Description <code>height</code> <code>int</code> <p>height after crop and resize.</p> <code>width</code> <code>int</code> <p>width after crop and resize.</p> <code>erosion_rate</code> <code>float</code> <p>erosion rate applied on input image height before crop.</p> <code>interpolation</code> <code>OpenCV flag</code> <p>flag that is used to specify the interpolation algorithm. Should be one of: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4. Default: cv2.INTER_LINEAR.</p> <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 1.</p> <p>Targets</p> <p>image, mask, bboxes, keypoints</p> <p>Image types:     uint8, float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/crops/transforms.py</code> Python<pre><code>class RandomSizedBBoxSafeCrop(BBoxSafeRandomCrop):\n    \"\"\"Crop a random part of the input and rescale it to some size without loss of bboxes.\n\n    Args:\n        height: height after crop and resize.\n        width: width after crop and resize.\n        erosion_rate: erosion rate applied on input image height before crop.\n        interpolation (OpenCV flag): flag that is used to specify the interpolation algorithm. Should be one of:\n            cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.\n            Default: cv2.INTER_LINEAR.\n        p (float): probability of applying the transform. Default: 1.\n\n    Targets:\n        image, mask, bboxes, keypoints\n\n    Image types:\n        uint8, float32\n\n    \"\"\"\n\n    _targets = (Targets.IMAGE, Targets.MASK, Targets.BBOXES, Targets.KEYPOINTS)\n\n    class InitSchema(CropInitSchema):\n        erosion_rate: float = Field(\n            default=0.0,\n            ge=0.0,\n            le=1.0,\n            description=\"Erosion rate applied on input image height before crop.\",\n        )\n        interpolation: InterpolationType = cv2.INTER_LINEAR\n\n    def __init__(\n        self,\n        height: int,\n        width: int,\n        erosion_rate: float = 0.0,\n        interpolation: int = cv2.INTER_LINEAR,\n        always_apply: bool | None = None,\n        p: float = 1.0,\n    ):\n        super().__init__(erosion_rate=erosion_rate, p=p, always_apply=always_apply)\n        self.height = height\n        self.width = width\n        self.interpolation = interpolation\n\n    def apply(\n        self,\n        img: np.ndarray,\n        crop_coords: tuple[int, int, int, int],\n        **params: Any,\n    ) -&gt; np.ndarray:\n        crop = fcrops.crop(img, *crop_coords)\n        return fgeometric.resize(crop, (self.height, self.width), self.interpolation)\n\n    def apply_to_keypoint(\n        self,\n        keypoint: KeypointInternalType,\n        crop_coords: tuple[int, int, int, int],\n        **params: Any,\n    ) -&gt; KeypointInternalType:\n        keypoint = fcrops.crop_keypoint_by_coords(keypoint, crop_coords)\n\n        crop_height = crop_coords[3] - crop_coords[1]\n        crop_width = crop_coords[2] - crop_coords[0]\n\n        scale_y = self.height / crop_height\n        scale_x = self.width / crop_width\n        return fgeometric.keypoint_scale(keypoint, scale_x=scale_x, scale_y=scale_y)\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return (*super().get_transform_init_args_names(), \"height\", \"width\", \"interpolation\")\n</code></pre>"},{"location":"api_reference/augmentations/crops/transforms/#albumentations.augmentations.crops.transforms.RandomSizedBBoxSafeCrop.apply","title":"<code>apply (self, img, crop_coords, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/crops/transforms.py</code> Python<pre><code>def apply(\n    self,\n    img: np.ndarray,\n    crop_coords: tuple[int, int, int, int],\n    **params: Any,\n) -&gt; np.ndarray:\n    crop = fcrops.crop(img, *crop_coords)\n    return fgeometric.resize(crop, (self.height, self.width), self.interpolation)\n</code></pre>"},{"location":"api_reference/augmentations/crops/transforms/#albumentations.augmentations.crops.transforms.RandomSizedBBoxSafeCrop.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/crops/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return (*super().get_transform_init_args_names(), \"height\", \"width\", \"interpolation\")\n</code></pre>"},{"location":"api_reference/augmentations/crops/transforms/#albumentations.augmentations.crops.transforms.RandomSizedCrop","title":"<code>class  RandomSizedCrop</code> <code>     (min_max_height, size=None, width=None, height=None, *, w2h_ratio=1.0, interpolation=1, always_apply=None, p=1.0)                 </code>  [view source on GitHub]","text":"<p>Crop a random portion of the input and rescale it to a specific size.</p> <p>Parameters:</p> Name Type Description <code>min_max_height</code> <code>int, int</code> <p>crop size limits.</p> <code>size</code> <code>int, int</code> <p>target size for the output image, i.e. (height, width) after crop and resize</p> <code>w2h_ratio</code> <code>float</code> <p>aspect ratio of crop.</p> <code>interpolation</code> <code>OpenCV flag</code> <p>flag that is used to specify the interpolation algorithm. Should be one of: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4. Default: cv2.INTER_LINEAR.</p> <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 1.</p> <p>Targets</p> <p>image, mask, bboxes, keypoints</p> <p>Image types:     uint8, float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/crops/transforms.py</code> Python<pre><code>class RandomSizedCrop(_BaseRandomSizedCrop):\n    \"\"\"Crop a random portion of the input and rescale it to a specific size.\n\n    Args:\n        min_max_height ((int, int)): crop size limits.\n        size ((int, int)): target size for the output image, i.e. (height, width) after crop and resize\n        w2h_ratio (float): aspect ratio of crop.\n        interpolation (OpenCV flag): flag that is used to specify the interpolation algorithm. Should be one of:\n            cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.\n            Default: cv2.INTER_LINEAR.\n        p (float): probability of applying the transform. Default: 1.\n\n    Targets:\n        image, mask, bboxes, keypoints\n\n    Image types:\n        uint8, float32\n\n    \"\"\"\n\n    _targets = (Targets.IMAGE, Targets.MASK, Targets.BBOXES, Targets.KEYPOINTS)\n\n    class InitSchema(BaseTransformInitSchema):\n        interpolation: InterpolationType = cv2.INTER_LINEAR\n        p: ProbabilityType = 1\n        min_max_height: OnePlusIntRangeType\n        w2h_ratio: Annotated[float, Field(gt=0, description=\"Aspect ratio of crop.\")]\n        width: int | None = Field(\n            None,\n            deprecated=(\n                \"Initializing with 'size' as an integer and a separate 'width' is deprecated. \"\n                \"Please use a tuple (height, width) for the 'size' argument.\"\n            ),\n        )\n        height: int | None = Field(\n            None,\n            deprecated=(\n                \"Initializing with 'height' and 'width' is deprecated. \"\n                \"Please use a tuple (height, width) for the 'size' argument.\"\n            ),\n        )\n        size: ScaleIntType | None = None\n\n        @model_validator(mode=\"after\")\n        def process(self) -&gt; Self:\n            if isinstance(self.size, int):\n                if isinstance(self.width, int):\n                    self.size = (self.size, self.width)\n                else:\n                    msg = \"If size is an integer, width as integer must be specified.\"\n                    raise TypeError(msg)\n\n            if self.size is None:\n                if self.height is None or self.width is None:\n                    message = \"If 'size' is not provided, both 'height' and 'width' must be specified.\"\n                    raise ValueError(message)\n                self.size = (self.height, self.width)\n            return self\n\n    def __init__(\n        self,\n        min_max_height: tuple[int, int],\n        # NOTE @zetyquickly: when (width, height) are deprecated, make 'size' non optional\n        size: ScaleIntType | None = None,\n        width: int | None = None,\n        height: int | None = None,\n        *,\n        w2h_ratio: float = 1.0,\n        interpolation: int = cv2.INTER_LINEAR,\n        always_apply: bool | None = None,\n        p: float = 1.0,\n    ):\n        super().__init__(size=cast(Tuple[int, int], size), interpolation=interpolation, p=p, always_apply=always_apply)\n        self.min_max_height = min_max_height\n        self.w2h_ratio = w2h_ratio\n\n    def get_params_dependent_on_data(\n        self,\n        params: dict[str, Any],\n        data: dict[str, Any],\n    ) -&gt; dict[str, tuple[int, int, int, int]]:\n        image_shape = params[\"shape\"][:2]\n\n        crop_height = random.randint(self.min_max_height[0], self.min_max_height[1])\n        crop_width = int(crop_height * self.w2h_ratio)\n\n        crop_shape = (crop_height, crop_width)\n\n        h_start = random.random()\n        w_start = random.random()\n\n        crop_coords = fcrops.get_crop_coords(image_shape, crop_shape, h_start, w_start)\n\n        return {\"crop_coords\": crop_coords}\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return \"min_max_height\", \"size\", \"w2h_ratio\", \"interpolation\"\n</code></pre>"},{"location":"api_reference/augmentations/crops/transforms/#albumentations.augmentations.crops.transforms.RandomSizedCrop.get_params_dependent_on_data","title":"<code>get_params_dependent_on_data (self, params, data)</code>","text":"<p>Returns parameters dependent on input.</p> Source code in <code>albumentations/augmentations/crops/transforms.py</code> Python<pre><code>def get_params_dependent_on_data(\n    self,\n    params: dict[str, Any],\n    data: dict[str, Any],\n) -&gt; dict[str, tuple[int, int, int, int]]:\n    image_shape = params[\"shape\"][:2]\n\n    crop_height = random.randint(self.min_max_height[0], self.min_max_height[1])\n    crop_width = int(crop_height * self.w2h_ratio)\n\n    crop_shape = (crop_height, crop_width)\n\n    h_start = random.random()\n    w_start = random.random()\n\n    crop_coords = fcrops.get_crop_coords(image_shape, crop_shape, h_start, w_start)\n\n    return {\"crop_coords\": crop_coords}\n</code></pre>"},{"location":"api_reference/augmentations/crops/transforms/#albumentations.augmentations.crops.transforms.RandomSizedCrop.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/crops/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return \"min_max_height\", \"size\", \"w2h_ratio\", \"interpolation\"\n</code></pre>"},{"location":"api_reference/augmentations/dropout/","title":"Index","text":"<ul> <li>ChannelDropout augmentation (albumentations.augmentations.dropout.channel_dropout)</li> <li>CoarseDropout augmentation (albumentations.augmentations.dropout.coarse_dropout)</li> <li>GridDropout augmentation (albumentations.augmentations.dropout.grid_dropout)</li> <li>MaskDropout augmentation (albumentations.augmentations.dropout.mask_dropout)</li> </ul>"},{"location":"api_reference/augmentations/dropout/channel_dropout/","title":"ChannelDropout augmentation (augmentations.dropout.channel_dropout)","text":""},{"location":"api_reference/augmentations/dropout/channel_dropout/#albumentations.augmentations.dropout.channel_dropout.ChannelDropout","title":"<code>class  ChannelDropout</code> <code>     (channel_drop_range=(1, 1), fill_value=0, always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Randomly Drop Channels in the input Image.</p> <p>Parameters:</p> Name Type Description <code>channel_drop_range</code> <code>int, int</code> <p>range from which we choose the number of channels to drop.</p> <code>fill_value</code> <code>int, float</code> <p>pixel value for the dropped channel.</p> <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 0.5.</p> <p>Targets</p> <p>image</p> <p>Image types:     uint8, uint16, unit32, float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/dropout/channel_dropout.py</code> Python<pre><code>class ChannelDropout(ImageOnlyTransform):\n    \"\"\"Randomly Drop Channels in the input Image.\n\n    Args:\n        channel_drop_range (int, int): range from which we choose the number of channels to drop.\n        fill_value (int, float): pixel value for the dropped channel.\n        p (float): probability of applying the transform. Default: 0.5.\n\n    Targets:\n        image\n\n    Image types:\n        uint8, uint16, unit32, float32\n\n    \"\"\"\n\n    class InitSchema(BaseTransformInitSchema):\n        channel_drop_range: OnePlusIntRangeType = (1, 1)\n        fill_value: Annotated[ColorType, Field(description=\"Pixel value for the dropped channel.\")]\n\n    def __init__(\n        self,\n        channel_drop_range: tuple[int, int] = (1, 1),\n        fill_value: float = 0,\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p=p, always_apply=always_apply)\n\n        self.channel_drop_range = channel_drop_range\n        self.fill_value = fill_value\n\n    def apply(self, img: np.ndarray, channels_to_drop: tuple[int, ...], **params: Any) -&gt; np.ndarray:\n        return channel_dropout(img, channels_to_drop, self.fill_value)\n\n    def get_params_dependent_on_data(self, params: Mapping[str, Any], data: Mapping[str, Any]) -&gt; dict[str, Any]:\n        image = data[\"image\"] if \"image\" in data else data[\"images\"][0]\n        num_channels = get_num_channels(image)\n\n        if num_channels == 1:\n            msg = \"Images has one channel. ChannelDropout is not defined.\"\n            raise NotImplementedError(msg)\n\n        if self.channel_drop_range[1] &gt;= num_channels:\n            msg = \"Can not drop all channels in ChannelDropout.\"\n            raise ValueError(msg)\n\n        num_drop_channels = random.randint(self.channel_drop_range[0], self.channel_drop_range[1])\n\n        channels_to_drop = random.sample(range(num_channels), k=num_drop_channels)\n\n        return {\"channels_to_drop\": channels_to_drop}\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return \"channel_drop_range\", \"fill_value\"\n</code></pre>"},{"location":"api_reference/augmentations/dropout/channel_dropout/#albumentations.augmentations.dropout.channel_dropout.ChannelDropout.apply","title":"<code>apply (self, img, channels_to_drop, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/dropout/channel_dropout.py</code> Python<pre><code>def apply(self, img: np.ndarray, channels_to_drop: tuple[int, ...], **params: Any) -&gt; np.ndarray:\n    return channel_dropout(img, channels_to_drop, self.fill_value)\n</code></pre>"},{"location":"api_reference/augmentations/dropout/channel_dropout/#albumentations.augmentations.dropout.channel_dropout.ChannelDropout.get_params_dependent_on_data","title":"<code>get_params_dependent_on_data (self, params, data)</code>","text":"<p>Returns parameters dependent on input.</p> Source code in <code>albumentations/augmentations/dropout/channel_dropout.py</code> Python<pre><code>def get_params_dependent_on_data(self, params: Mapping[str, Any], data: Mapping[str, Any]) -&gt; dict[str, Any]:\n    image = data[\"image\"] if \"image\" in data else data[\"images\"][0]\n    num_channels = get_num_channels(image)\n\n    if num_channels == 1:\n        msg = \"Images has one channel. ChannelDropout is not defined.\"\n        raise NotImplementedError(msg)\n\n    if self.channel_drop_range[1] &gt;= num_channels:\n        msg = \"Can not drop all channels in ChannelDropout.\"\n        raise ValueError(msg)\n\n    num_drop_channels = random.randint(self.channel_drop_range[0], self.channel_drop_range[1])\n\n    channels_to_drop = random.sample(range(num_channels), k=num_drop_channels)\n\n    return {\"channels_to_drop\": channels_to_drop}\n</code></pre>"},{"location":"api_reference/augmentations/dropout/channel_dropout/#albumentations.augmentations.dropout.channel_dropout.ChannelDropout.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/dropout/channel_dropout.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return \"channel_drop_range\", \"fill_value\"\n</code></pre>"},{"location":"api_reference/augmentations/dropout/coarse_dropout/","title":"CoarseDropout augmentation (augmentations.dropout.coarse_dropout)","text":""},{"location":"api_reference/augmentations/dropout/coarse_dropout/#albumentations.augmentations.dropout.coarse_dropout.CoarseDropout","title":"<code>class  CoarseDropout</code> <code>     (max_holes=None, max_height=None, max_width=None, min_holes=None, min_height=None, min_width=None, fill_value=0, mask_fill_value=None, num_holes_range=(1, 1), hole_height_range=(8, 8), hole_width_range=(8, 8), always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>CoarseDropout randomly drops out rectangular regions from the image and optionally, the corresponding regions in an associated mask, to simulate the occlusion and varied object sizes found in real-world settings. This transformation is an evolution of CutOut and RandomErasing, offering more flexibility in the size, number of dropout regions, and fill values.</p> <p>Parameters:</p> Name Type Description <code>num_holes_range</code> <code>tuple[int, int]</code> <p>Specifies the range (minimum and maximum) of the number of rectangular regions to zero out. This allows for dynamic variation in the number of regions removed per transformation instance.</p> <code>hole_height_range</code> <code>tuple[ScalarType, ScalarType]</code> <p>Defines the minimum and maximum heights of the dropout regions, providing variability in their vertical dimensions.</p> <code>hole_width_range</code> <code>tuple[ScalarType, ScalarType]</code> <p>Defines the minimum and maximum widths of the dropout regions, providing variability in their horizontal dimensions.</p> <code>fill_value</code> <code>ColorType, Literal[\"random\"]</code> <p>Specifies the value used to fill the dropout regions. This can be a constant value, a tuple specifying pixel intensity across channels, or 'random' which fills the region with random noise.</p> <code>mask_fill_value</code> <code>ColorType | None</code> <p>Specifies the fill value for dropout regions in the mask. If set to <code>None</code>, the mask regions corresponding to the image dropout regions are left unchanged.</p> <p>Targets</p> <p>image, mask, keypoints</p> <p>Image types:     uint8, float32</p> <p>Reference</p> <p>https://arxiv.org/abs/1708.04552 https://github.com/uoguelph-mlrg/Cutout/blob/master/util/cutout.py https://github.com/aleju/imgaug/blob/master/imgaug/augmenters/arithmetic.py</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/dropout/coarse_dropout.py</code> Python<pre><code>class CoarseDropout(DualTransform):\n    \"\"\"CoarseDropout randomly drops out rectangular regions from the image and optionally,\n    the corresponding regions in an associated mask, to simulate the occlusion and\n    varied object sizes found in real-world settings. This transformation is an\n    evolution of CutOut and RandomErasing, offering more flexibility in the size,\n    number of dropout regions, and fill values.\n\n    Args:\n        num_holes_range (tuple[int, int]): Specifies the range (minimum and maximum)\n            of the number of rectangular regions to zero out. This allows for dynamic\n            variation in the number of regions removed per transformation instance.\n        hole_height_range (tuple[ScalarType, ScalarType]): Defines the minimum and\n            maximum heights of the dropout regions, providing variability in their vertical dimensions.\n        hole_width_range (tuple[ScalarType, ScalarType]): Defines the minimum and\n            maximum widths of the dropout regions, providing variability in their horizontal dimensions.\n        fill_value (ColorType, Literal[\"random\"]): Specifies the value used to fill the dropout regions.\n            This can be a constant value, a tuple specifying pixel intensity across channels, or 'random'\n            which fills the region with random noise.\n        mask_fill_value (ColorType | None): Specifies the fill value for dropout regions in the mask.\n            If set to `None`, the mask regions corresponding to the image dropout regions are left unchanged.\n\n\n    Targets:\n        image, mask, keypoints\n\n    Image types:\n        uint8, float32\n\n    Reference:\n        https://arxiv.org/abs/1708.04552\n        https://github.com/uoguelph-mlrg/Cutout/blob/master/util/cutout.py\n        https://github.com/aleju/imgaug/blob/master/imgaug/augmenters/arithmetic.py\n\n    \"\"\"\n\n    _targets = (Targets.IMAGE, Targets.MASK, Targets.KEYPOINTS)\n\n    class InitSchema(BaseTransformInitSchema):\n        min_holes: int | None = Field(\n            default=None,\n            ge=0,\n            description=\"Minimum number of regions to zero out.\",\n        )\n        max_holes: int | None = Field(\n            default=8,\n            ge=0,\n            description=\"Maximum number of regions to zero out.\",\n        )\n        num_holes_range: Annotated[tuple[int, int], AfterValidator(check_1plus), AfterValidator(nondecreasing)] = (1, 1)\n\n        min_height: ScalarType | None = Field(\n            default=None,\n            ge=0,\n            description=\"Minimum height of the hole.\",\n        )\n        max_height: ScalarType | None = Field(\n            default=8,\n            ge=0,\n            description=\"Maximum height of the hole.\",\n        )\n        hole_height_range: tuple[ScalarType, ScalarType] = (8, 8)\n\n        min_width: ScalarType | None = Field(\n            default=None,\n            ge=0,\n            description=\"Minimum width of the hole.\",\n        )\n        max_width: ScalarType | None = Field(\n            default=8,\n            ge=0,\n            description=\"Maximum width of the hole.\",\n        )\n        hole_width_range: tuple[ScalarType, ScalarType] = (8, 8)\n\n        fill_value: ColorType | Literal[\"random\"] = Field(default=0, description=\"Value for dropped pixels.\")\n        mask_fill_value: ColorType | None = Field(default=None, description=\"Fill value for dropped pixels in mask.\")\n\n        @staticmethod\n        def update_range(\n            min_value: NumericType | None,\n            max_value: NumericType | None,\n            default_range: tuple[NumericType, NumericType],\n        ) -&gt; tuple[NumericType, NumericType]:\n            if max_value is not None:\n                return (min_value or max_value, max_value)\n\n            return default_range\n\n        @staticmethod\n        # Validation for hole dimensions ranges\n        def validate_range(range_value: tuple[ScalarType, ScalarType], range_name: str, minimum: float = 0) -&gt; None:\n            if not minimum &lt;= range_value[0] &lt;= range_value[1]:\n                raise ValueError(\n                    f\"First value in {range_name} should be less or equal than the second value \"\n                    f\"and at least {minimum}. Got: {range_value}\",\n                )\n            if isinstance(range_value[0], float) and not all(0 &lt;= x &lt;= 1 for x in range_value):\n                raise ValueError(f\"All values in {range_name} should be in [0, 1] range. Got: {range_value}\")\n\n        @model_validator(mode=\"after\")\n        def check_num_holes_and_dimensions(self) -&gt; Self:\n            if self.min_holes is not None:\n                warn(\"`min_holes` is deprecated. Use num_holes_range instead.\", DeprecationWarning, stacklevel=2)\n\n            if self.max_holes is not None:\n                warn(\"`max_holes` is deprecated. Use num_holes_range instead.\", DeprecationWarning, stacklevel=2)\n\n            if self.min_height is not None:\n                warn(\"`min_height` is deprecated. Use hole_height_range instead.\", DeprecationWarning, stacklevel=2)\n\n            if self.max_height is not None:\n                warn(\"`max_height` is deprecated. Use hole_height_range instead.\", DeprecationWarning, stacklevel=2)\n\n            if self.min_width is not None:\n                warn(\"`min_width` is deprecated. Use hole_width_range instead.\", DeprecationWarning, stacklevel=2)\n\n            if self.max_width is not None:\n                warn(\"`max_width` is deprecated. Use hole_width_range instead.\", DeprecationWarning, stacklevel=2)\n\n            if self.max_holes is not None:\n                # Update ranges for holes, heights, and widths\n                self.num_holes_range = self.update_range(self.min_holes, self.max_holes, self.num_holes_range)\n\n            self.validate_range(self.num_holes_range, \"num_holes_range\", minimum=1)\n\n            if self.max_height is not None:\n                self.hole_height_range = self.update_range(self.min_height, self.max_height, self.hole_height_range)\n            self.validate_range(self.hole_height_range, \"hole_height_range\")\n\n            if self.max_width is not None:\n                self.hole_width_range = self.update_range(self.min_width, self.max_width, self.hole_width_range)\n            self.validate_range(self.hole_width_range, \"hole_width_range\")\n\n            return self\n\n    def __init__(\n        self,\n        max_holes: int | None = None,\n        max_height: ScalarType | None = None,\n        max_width: ScalarType | None = None,\n        min_holes: int | None = None,\n        min_height: ScalarType | None = None,\n        min_width: ScalarType | None = None,\n        fill_value: ColorType | Literal[\"random\"] = 0,\n        mask_fill_value: ColorType | None = None,\n        num_holes_range: tuple[int, int] = (1, 1),\n        hole_height_range: tuple[ScalarType, ScalarType] = (8, 8),\n        hole_width_range: tuple[ScalarType, ScalarType] = (8, 8),\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p, always_apply)\n        self.num_holes_range = num_holes_range\n        self.hole_height_range = hole_height_range\n        self.hole_width_range = hole_width_range\n\n        self.fill_value = fill_value  # type: ignore[assignment]\n        self.mask_fill_value = mask_fill_value\n\n    def apply(\n        self,\n        img: np.ndarray,\n        fill_value: ColorType | Literal[\"random\"],\n        holes: Iterable[tuple[int, int, int, int]],\n        **params: Any,\n    ) -&gt; np.ndarray:\n        return cutout(img, holes, fill_value)\n\n    def apply_to_mask(\n        self,\n        mask: np.ndarray,\n        mask_fill_value: ScalarType,\n        holes: Iterable[tuple[int, int, int, int]],\n        **params: Any,\n    ) -&gt; np.ndarray:\n        if mask_fill_value is None:\n            return mask\n        return cutout(mask, holes, mask_fill_value)\n\n    @staticmethod\n    def calculate_hole_dimensions(\n        height: int,\n        width: int,\n        height_range: tuple[ScalarType, ScalarType],\n        width_range: tuple[ScalarType, ScalarType],\n        size: int,\n    ) -&gt; tuple[np.ndarray, np.ndarray]:\n        \"\"\"Calculate random hole dimensions based on the provided ranges.\"\"\"\n        if isinstance(height_range[0], int):\n            min_height = height_range[0]\n            max_height = min(height_range[1], height)\n\n            min_width = width_range[0]\n            max_width = min(width_range[1], width)\n\n            hole_heights = randint(np.int64(min_height), np.int64(max_height + 1), size=size)\n            hole_widths = randint(np.int64(min_width), np.int64(max_width + 1), size=size)\n\n        else:  # Assume float\n            hole_heights = (height * uniform(height_range[0], height_range[1], size=size)).astype(int)\n            hole_widths = (width * uniform(width_range[0], width_range[1], size=size)).astype(int)\n\n        return hole_heights, hole_widths\n\n    def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n        height, width = params[\"shape\"][:2]\n\n        num_holes = randint(self.num_holes_range[0], self.num_holes_range[1] + 1)\n\n        hole_heights, hole_widths = self.calculate_hole_dimensions(\n            height,\n            width,\n            self.hole_height_range,\n            self.hole_width_range,\n            size=num_holes,\n        )\n\n        y1 = randint(np.int8(0), height - hole_heights + 1, size=num_holes)\n        x1 = randint(np.int8(0), width - hole_widths + 1, size=num_holes)\n        y2 = y1 + hole_heights\n        x2 = x1 + hole_widths\n\n        holes = np.stack([x1, y1, x2, y2], axis=-1)\n\n        return {\"holes\": holes.tolist()}\n\n    def apply_to_keypoints(\n        self,\n        keypoints: Sequence[KeypointType],\n        holes: Iterable[tuple[int, int, int, int]],\n        **params: Any,\n    ) -&gt; list[KeypointType]:\n        return [keypoint for keypoint in keypoints if not any(keypoint_in_hole(keypoint, hole) for hole in holes)]\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return (\n            \"num_holes_range\",\n            \"hole_height_range\",\n            \"hole_width_range\",\n            \"fill_value\",\n            \"mask_fill_value\",\n        )\n\n    @property\n    def targets(self) -&gt; dict[str, Callable[..., Any]]:\n        return {\n            \"image\": self.apply,\n            \"mask\": self.apply_to_mask,\n            \"masks\": self.apply_to_masks,\n            \"keypoints\": self.apply_to_keypoints,\n        }\n</code></pre>"},{"location":"api_reference/augmentations/dropout/coarse_dropout/#albumentations.augmentations.dropout.coarse_dropout.CoarseDropout.apply","title":"<code>apply (self, img, fill_value, holes, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/dropout/coarse_dropout.py</code> Python<pre><code>def apply(\n    self,\n    img: np.ndarray,\n    fill_value: ColorType | Literal[\"random\"],\n    holes: Iterable[tuple[int, int, int, int]],\n    **params: Any,\n) -&gt; np.ndarray:\n    return cutout(img, holes, fill_value)\n</code></pre>"},{"location":"api_reference/augmentations/dropout/coarse_dropout/#albumentations.augmentations.dropout.coarse_dropout.CoarseDropout.calculate_hole_dimensions","title":"<code>calculate_hole_dimensions (height, width, height_range, width_range, size)</code>  <code>staticmethod</code>","text":"<p>Calculate random hole dimensions based on the provided ranges.</p> Source code in <code>albumentations/augmentations/dropout/coarse_dropout.py</code> Python<pre><code>@staticmethod\ndef calculate_hole_dimensions(\n    height: int,\n    width: int,\n    height_range: tuple[ScalarType, ScalarType],\n    width_range: tuple[ScalarType, ScalarType],\n    size: int,\n) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"Calculate random hole dimensions based on the provided ranges.\"\"\"\n    if isinstance(height_range[0], int):\n        min_height = height_range[0]\n        max_height = min(height_range[1], height)\n\n        min_width = width_range[0]\n        max_width = min(width_range[1], width)\n\n        hole_heights = randint(np.int64(min_height), np.int64(max_height + 1), size=size)\n        hole_widths = randint(np.int64(min_width), np.int64(max_width + 1), size=size)\n\n    else:  # Assume float\n        hole_heights = (height * uniform(height_range[0], height_range[1], size=size)).astype(int)\n        hole_widths = (width * uniform(width_range[0], width_range[1], size=size)).astype(int)\n\n    return hole_heights, hole_widths\n</code></pre>"},{"location":"api_reference/augmentations/dropout/coarse_dropout/#albumentations.augmentations.dropout.coarse_dropout.CoarseDropout.get_params_dependent_on_data","title":"<code>get_params_dependent_on_data (self, params, data)</code>","text":"<p>Returns parameters dependent on input.</p> Source code in <code>albumentations/augmentations/dropout/coarse_dropout.py</code> Python<pre><code>def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n    height, width = params[\"shape\"][:2]\n\n    num_holes = randint(self.num_holes_range[0], self.num_holes_range[1] + 1)\n\n    hole_heights, hole_widths = self.calculate_hole_dimensions(\n        height,\n        width,\n        self.hole_height_range,\n        self.hole_width_range,\n        size=num_holes,\n    )\n\n    y1 = randint(np.int8(0), height - hole_heights + 1, size=num_holes)\n    x1 = randint(np.int8(0), width - hole_widths + 1, size=num_holes)\n    y2 = y1 + hole_heights\n    x2 = x1 + hole_widths\n\n    holes = np.stack([x1, y1, x2, y2], axis=-1)\n\n    return {\"holes\": holes.tolist()}\n</code></pre>"},{"location":"api_reference/augmentations/dropout/coarse_dropout/#albumentations.augmentations.dropout.coarse_dropout.CoarseDropout.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/dropout/coarse_dropout.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return (\n        \"num_holes_range\",\n        \"hole_height_range\",\n        \"hole_width_range\",\n        \"fill_value\",\n        \"mask_fill_value\",\n    )\n</code></pre>"},{"location":"api_reference/augmentations/dropout/functional/","title":"Geometric functional transforms (augmentations.dropout.functional)","text":""},{"location":"api_reference/augmentations/dropout/functional/#albumentations.augmentations.dropout.functional.cutout","title":"<code>def cutout    (img, holes, fill_value=0)    </code> [view source on GitHub]","text":"<p>Apply cutout augmentation to the image by cutting out holes and filling them with either a given value or random noise.</p> <p>Parameters:</p> Name Type Description <code>img</code> <code>np.ndarray</code> <p>The image to augment.</p> <code>holes</code> <code>Iterable[tuple[int, int, int, int]]</code> <p>An iterable of tuples where each tuple contains the coordinates of the top-left and bottom-right corners of the rectangular hole (x1, y1, x2, y2).</p> <code>fill_value</code> <code>Union[ColorType, Literal[\"random\"]]</code> <p>The fill value to use for the hole. Can be a single integer, a tuple or list of numbers for multichannel, or the string \"random\" to fill with random noise.</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>The augmented image.</p> Source code in <code>albumentations/augmentations/dropout/functional.py</code> Python<pre><code>def cutout(\n    img: np.ndarray,\n    holes: Iterable[tuple[int, int, int, int]],\n    fill_value: ColorType | Literal[\"random\"] = 0,\n) -&gt; np.ndarray:\n    \"\"\"Apply cutout augmentation to the image by cutting out holes and filling them\n    with either a given value or random noise.\n\n    Args:\n        img (np.ndarray): The image to augment.\n        holes (Iterable[tuple[int, int, int, int]]): An iterable of tuples where each\n            tuple contains the coordinates of the top-left and bottom-right corners of\n            the rectangular hole (x1, y1, x2, y2).\n        fill_value (Union[ColorType, Literal[\"random\"]]): The fill value to use for the hole. Can be\n            a single integer, a tuple or list of numbers for multichannel,\n            or the string \"random\" to fill with random noise.\n\n    Returns:\n        np.ndarray: The augmented image.\n    \"\"\"\n    img = img.copy()\n\n    if isinstance(fill_value, (int, float, tuple, list)):\n        fill_value = np.array(fill_value, dtype=img.dtype)\n\n    for x1, y1, x2, y2 in holes:\n        if isinstance(fill_value, str) and fill_value == \"random\":\n            shape = (y2 - y1, x2 - x1) if img.ndim == MONO_CHANNEL_DIMENSIONS else (y2 - y1, x2 - x1, img.shape[2])\n            random_fill = generate_random_fill(img.dtype, shape)\n            img[y1:y2, x1:x2] = random_fill\n        else:\n            img[y1:y2, x1:x2] = fill_value\n\n    return img\n</code></pre>"},{"location":"api_reference/augmentations/dropout/functional/#albumentations.augmentations.dropout.functional.generate_random_fill","title":"<code>def generate_random_fill    (dtype, shape)    </code> [view source on GitHub]","text":"<p>Generate a random fill based on dtype and target shape.</p> Source code in <code>albumentations/augmentations/dropout/functional.py</code> Python<pre><code>def generate_random_fill(dtype: np.dtype, shape: tuple[int, ...]) -&gt; np.ndarray:\n    \"\"\"Generate a random fill based on dtype and target shape.\"\"\"\n    max_value = MAX_VALUES_BY_DTYPE[dtype]\n    if np.issubdtype(dtype, np.integer):\n        return random_utils.randint(0, max_value + 1, size=shape, dtype=dtype)\n    if np.issubdtype(dtype, np.floating):\n        return random_utils.uniform(0, max_value, size=shape).astype(dtype)\n    raise ValueError(f\"Unsupported dtype: {dtype}\")\n</code></pre>"},{"location":"api_reference/augmentations/dropout/grid_dropout/","title":"GridDropout augmentation (augmentations.dropout.grid_dropout)","text":""},{"location":"api_reference/augmentations/dropout/grid_dropout/#albumentations.augmentations.dropout.grid_dropout.GridDropout","title":"<code>class  GridDropout</code> <code>     (ratio=0.5, unit_size_min=None, unit_size_max=None, holes_number_x=None, holes_number_y=None, shift_x=None, shift_y=None, random_offset=False, fill_value=0, mask_fill_value=None, unit_size_range=None, holes_number_xy=None, shift_xy=(0, 0), always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>GridDropout, drops out rectangular regions of an image and the corresponding mask in a grid fashion.</p> <p>Parameters:</p> Name Type Description <code>ratio</code> <code>float</code> <p>The ratio of the mask holes to the unit_size (same for horizontal and vertical directions). Must be between 0 and 1. Default: 0.5.</p> <code>random_offset</code> <code>bool</code> <p>Whether to offset the grid randomly between 0 and grid unit size - hole size. If True, entered shift_x and shift_y are ignored and set randomly. Default: False.</p> <code>fill_value</code> <code>Optional[ColorType]</code> <p>Value for the dropped pixels. Default: 0.</p> <code>mask_fill_value</code> <code>Optional[ColorType]</code> <p>Value for the dropped pixels in mask. If None, transformation is not applied to the mask. Default: None.</p> <code>unit_size_range</code> <code>Optional[tuple[int, int]]</code> <p>Range from which to sample grid size. Default: None.  Must be between 2 and the image shorter edge.</p> <code>holes_number_xy</code> <code>Optional[tuple[int, int]]</code> <p>The number of grid units in x and y directions. First value should be between 1 and image width//2, Second value should be between 1 and image height//2. Default: None.</p> <code>shift_xy</code> <code>tuple[int, int]</code> <p>Offsets of the grid start in x and y directions. Offsets of the grid start in x and y directions from (0,0) coordinate. Default: (0, 0).</p> <code>p</code> <code>float</code> <p>Probability of applying the transform. Default: 0.5.</p> <p>Targets</p> <p>image, mask</p> <p>Image types:     uint8, float32</p> <p>Reference</p> <p>https://arxiv.org/abs/2001.04086</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/dropout/grid_dropout.py</code> Python<pre><code>class GridDropout(DualTransform):\n    \"\"\"GridDropout, drops out rectangular regions of an image and the corresponding mask in a grid fashion.\n\n    Args:\n        ratio (float): The ratio of the mask holes to the unit_size (same for horizontal and vertical directions).\n            Must be between 0 and 1. Default: 0.5.\n        random_offset (bool): Whether to offset the grid randomly between 0 and grid unit size - hole size.\n            If True, entered shift_x and shift_y are ignored and set randomly. Default: False.\n        fill_value (Optional[ColorType]): Value for the dropped pixels. Default: 0.\n        mask_fill_value (Optional[ColorType]): Value for the dropped pixels in mask.\n            If None, transformation is not applied to the mask. Default: None.\n        unit_size_range (Optional[tuple[int, int]]): Range from which to sample grid size. Default: None.\n             Must be between 2 and the image shorter edge.\n        holes_number_xy (Optional[tuple[int, int]]): The number of grid units in x and y directions.\n            First value should be between 1 and image width//2,\n            Second value should be between 1 and image height//2.\n            Default: None.\n        shift_xy (tuple[int, int]): Offsets of the grid start in x and y directions.\n            Offsets of the grid start in x and y directions from (0,0) coordinate.\n            Default: (0, 0).\n\n        p (float): Probability of applying the transform. Default: 0.5.\n\n    Targets:\n        image, mask\n\n    Image types:\n        uint8, float32\n\n    Reference:\n        https://arxiv.org/abs/2001.04086\n\n    \"\"\"\n\n    _targets = (Targets.IMAGE, Targets.MASK)\n\n    class InitSchema(BaseTransformInitSchema):\n        ratio: float = Field(description=\"The ratio of the mask holes to the unit_size.\", gt=0, le=1)\n\n        unit_size_min: int | None = Field(None, description=\"Minimum size of the grid unit.\", ge=2)\n        unit_size_max: int | None = Field(None, description=\"Maximum size of the grid unit.\", ge=2)\n\n        holes_number_x: int | None = Field(None, description=\"The number of grid units in x direction.\", ge=1)\n        holes_number_y: int | None = Field(None, description=\"The number of grid units in y direction.\", ge=1)\n\n        shift_x: int | None = Field(0, description=\"Offsets of the grid start in x direction.\", ge=0)\n        shift_y: int | None = Field(0, description=\"Offsets of the grid start in y direction.\", ge=0)\n\n        random_offset: bool = Field(False, description=\"Whether to offset the grid randomly.\")\n        fill_value: ColorType | None = Field(0, description=\"Value for the dropped pixels.\")\n        mask_fill_value: ColorType | None = Field(None, description=\"Value for the dropped pixels in mask.\")\n        unit_size_range: (\n            Annotated[tuple[int, int], AfterValidator(check_1plus), AfterValidator(nondecreasing)] | None\n        ) = None\n        shift_xy: Annotated[tuple[int, int], AfterValidator(check_0plus)] = Field(\n            (0, 0),\n            description=\"Offsets of the grid start in x and y directions.\",\n        )\n        holes_number_xy: Annotated[tuple[int, int], AfterValidator(check_1plus)] | None = Field(\n            None,\n            description=\"The number of grid units in x and y directions.\",\n        )\n\n        @model_validator(mode=\"after\")\n        def validate_normalization(self) -&gt; Self:\n            if self.unit_size_min is not None and self.unit_size_max is not None:\n                self.unit_size_range = self.unit_size_min, self.unit_size_max\n                warn(\n                    \"unit_size_min and unit_size_max are deprecated. Use unit_size_range instead.\",\n                    DeprecationWarning,\n                    stacklevel=2,\n                )\n\n            if self.shift_x is not None and self.shift_y is not None:\n                self.shift_xy = self.shift_x, self.shift_y\n                warn(\"shift_x and shift_y are deprecated. Use shift_xy instead.\", DeprecationWarning, stacklevel=2)\n\n            if self.holes_number_x is not None and self.holes_number_y is not None:\n                self.holes_number_xy = self.holes_number_x, self.holes_number_y\n                warn(\n                    \"holes_number_x and holes_number_y are deprecated. Use holes_number_xy instead.\",\n                    DeprecationWarning,\n                    stacklevel=2,\n                )\n\n            if self.unit_size_range and not MIN_UNIT_SIZE &lt;= self.unit_size_range[0] &lt;= self.unit_size_range[1]:\n                raise ValueError(\"Max unit size should be &gt;= min size, both at least 2 pixels.\")\n\n            return self\n\n    def __init__(\n        self,\n        ratio: float = 0.5,\n        unit_size_min: int | None = None,\n        unit_size_max: int | None = None,\n        holes_number_x: int | None = None,\n        holes_number_y: int | None = None,\n        shift_x: int | None = None,\n        shift_y: int | None = None,\n        random_offset: bool = False,\n        fill_value: ColorType = 0,\n        mask_fill_value: ColorType | None = None,\n        unit_size_range: tuple[int, int] | None = None,\n        holes_number_xy: tuple[int, int] | None = None,\n        shift_xy: tuple[int, int] = (0, 0),\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p, always_apply)\n        self.ratio = ratio\n        self.unit_size_range = unit_size_range\n        self.holes_number_xy = holes_number_xy\n        self.random_offset = random_offset\n        self.fill_value = fill_value\n        self.mask_fill_value = mask_fill_value\n        self.shift_xy = shift_xy\n\n    def apply(self, img: np.ndarray, holes: Iterable[tuple[int, int, int, int]], **params: Any) -&gt; np.ndarray:\n        return fdropout.cutout(img, holes, self.fill_value)\n\n    def apply_to_mask(\n        self,\n        mask: np.ndarray,\n        holes: Iterable[tuple[int, int, int, int]],\n        **params: Any,\n    ) -&gt; np.ndarray:\n        if self.mask_fill_value is None:\n            return mask\n\n        return fdropout.cutout(mask, holes, self.mask_fill_value)\n\n    def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n        height, width = params[\"shape\"][:2]\n        unit_width, unit_height = self._calculate_unit_dimensions(width, height)\n        hole_width, hole_height = self._calculate_hole_dimensions(unit_width, unit_height)\n        shift_x, shift_y = self._calculate_shifts(unit_width, unit_height, hole_width, hole_height)\n        holes = self._generate_holes(width, height, unit_width, unit_height, hole_width, hole_height, shift_x, shift_y)\n        return {\"holes\": holes}\n\n    def _calculate_unit_dimensions(self, width: int, height: int) -&gt; tuple[int, int]:\n        \"\"\"Calculates the dimensions of the grid units.\"\"\"\n        if self.unit_size_range is not None:\n            self._validate_unit_sizes(height, width)\n            unit_size = random.randint(*self.unit_size_range)\n            return unit_size, unit_size\n\n        return self._calculate_dimensions_based_on_holes(width, height)\n\n    def _validate_unit_sizes(self, height: int, width: int) -&gt; None:\n        \"\"\"Validates the minimum and maximum unit sizes.\"\"\"\n        if self.unit_size_range is None:\n            raise ValueError(\"unit_size_range must not be None.\")\n        if self.unit_size_range[1] &gt; min(height, width):\n            msg = \"Grid size limits must be within the shortest image edge.\"\n            raise ValueError(msg)\n\n    def _calculate_dimensions_based_on_holes(self, width: int, height: int) -&gt; tuple[int, int]:\n        \"\"\"Calculates dimensions based on the number of holes specified.\"\"\"\n        holes_number_x, holes_number_y = self.holes_number_xy or (None, None)\n        unit_width = self._calculate_dimension(width, holes_number_x, 10)\n        unit_height = self._calculate_dimension(height, holes_number_y, unit_width)\n        return unit_width, unit_height\n\n    @staticmethod\n    def _calculate_dimension(dimension: int, holes_number: int | None, fallback: int) -&gt; int:\n        \"\"\"Helper function to calculate unit width or height.\"\"\"\n        if holes_number is None:\n            return max(2, dimension // fallback)\n\n        if not 1 &lt;= holes_number &lt;= dimension // 2:\n            raise ValueError(f\"The number of holes must be between 1 and {dimension // 2}.\")\n        return dimension // holes_number\n\n    def _calculate_hole_dimensions(self, unit_width: int, unit_height: int) -&gt; tuple[int, int]:\n        \"\"\"Calculates the dimensions of the holes to be dropped out.\"\"\"\n        hole_width = int(unit_width * self.ratio)\n        hole_height = int(unit_height * self.ratio)\n        hole_width = min(max(hole_width, 1), unit_width - 1)\n        hole_height = min(max(hole_height, 1), unit_height - 1)\n        return hole_width, hole_height\n\n    def _calculate_shifts(\n        self,\n        unit_width: int,\n        unit_height: int,\n        hole_width: int,\n        hole_height: int,\n    ) -&gt; tuple[int, int]:\n        \"\"\"Calculates the shifts for the grid start.\"\"\"\n        if self.random_offset:\n            shift_x = random.randint(0, unit_width - hole_width)\n            shift_y = random.randint(0, unit_height - hole_height)\n            return shift_x, shift_y\n\n        if isinstance(self.shift_xy, Sequence) and len(self.shift_xy) == PAIR:\n            shift_x = min(max(0, self.shift_xy[0]), unit_width - hole_width)\n            shift_y = min(max(0, self.shift_xy[1]), unit_height - hole_height)\n            return shift_x, shift_y\n\n        return 0, 0\n\n    def _generate_holes(\n        self,\n        width: int,\n        height: int,\n        unit_width: int,\n        unit_height: int,\n        hole_width: int,\n        hole_height: int,\n        shift_x: int,\n        shift_y: int,\n    ) -&gt; list[tuple[int, int, int, int]]:\n        \"\"\"Generates the list of holes to be dropped out.\"\"\"\n        holes = []\n        for i in range(width // unit_width + 1):\n            for j in range(height // unit_height + 1):\n                x1 = min(shift_x + unit_width * i, width)\n                y1 = min(shift_y + unit_height * j, height)\n                x2 = min(x1 + hole_width, width)\n                y2 = min(y1 + hole_height, height)\n                holes.append((x1, y1, x2, y2))\n        return holes\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return (\n            \"ratio\",\n            \"unit_size_range\",\n            \"holes_number_xy\",\n            \"shift_xy\",\n            \"random_offset\",\n            \"fill_value\",\n            \"mask_fill_value\",\n        )\n</code></pre>"},{"location":"api_reference/augmentations/dropout/grid_dropout/#albumentations.augmentations.dropout.grid_dropout.GridDropout.apply","title":"<code>apply (self, img, holes, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/dropout/grid_dropout.py</code> Python<pre><code>def apply(self, img: np.ndarray, holes: Iterable[tuple[int, int, int, int]], **params: Any) -&gt; np.ndarray:\n    return fdropout.cutout(img, holes, self.fill_value)\n</code></pre>"},{"location":"api_reference/augmentations/dropout/grid_dropout/#albumentations.augmentations.dropout.grid_dropout.GridDropout.get_params_dependent_on_data","title":"<code>get_params_dependent_on_data (self, params, data)</code>","text":"<p>Returns parameters dependent on input.</p> Source code in <code>albumentations/augmentations/dropout/grid_dropout.py</code> Python<pre><code>def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n    height, width = params[\"shape\"][:2]\n    unit_width, unit_height = self._calculate_unit_dimensions(width, height)\n    hole_width, hole_height = self._calculate_hole_dimensions(unit_width, unit_height)\n    shift_x, shift_y = self._calculate_shifts(unit_width, unit_height, hole_width, hole_height)\n    holes = self._generate_holes(width, height, unit_width, unit_height, hole_width, hole_height, shift_x, shift_y)\n    return {\"holes\": holes}\n</code></pre>"},{"location":"api_reference/augmentations/dropout/grid_dropout/#albumentations.augmentations.dropout.grid_dropout.GridDropout.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/dropout/grid_dropout.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return (\n        \"ratio\",\n        \"unit_size_range\",\n        \"holes_number_xy\",\n        \"shift_xy\",\n        \"random_offset\",\n        \"fill_value\",\n        \"mask_fill_value\",\n    )\n</code></pre>"},{"location":"api_reference/augmentations/dropout/mask_dropout/","title":"MaskDropout augmentation (augmentations.dropout.mask_dropout)","text":""},{"location":"api_reference/augmentations/dropout/mask_dropout/#albumentations.augmentations.dropout.mask_dropout.MaskDropout","title":"<code>class  MaskDropout</code> <code>     (max_objects=(1, 1), image_fill_value=0, mask_fill_value=0, always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Image &amp; mask augmentation that zero out mask and image regions corresponding to randomly chosen object instance from mask.</p> <p>Mask must be single-channel image, zero values treated as background. Image can be any number of channels.</p> <p>Parameters:</p> Name Type Description <code>max_objects</code> <code>ScaleIntType</code> <p>Maximum number of labels that can be zeroed out. Can be tuple, in this case it's [min, max]</p> <code>image_fill_value</code> <code>float | Literal['inpaint']</code> <p>Fill value to use when filling image. Can be 'inpaint' to apply inpainting (works only  for 3-channel images)</p> <code>mask_fill_value</code> <code>ScalarType</code> <p>Fill value to use when filling mask.</p> <p>Targets</p> <p>image, mask</p> <p>Image types:     uint8, float32</p> <p>Reference</p> <p>https://www.kaggle.com/c/severstal-steel-defect-detection/discussion/114254</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/dropout/mask_dropout.py</code> Python<pre><code>class MaskDropout(DualTransform):\n    \"\"\"Image &amp; mask augmentation that zero out mask and image regions corresponding\n    to randomly chosen object instance from mask.\n\n    Mask must be single-channel image, zero values treated as background.\n    Image can be any number of channels.\n\n    Args:\n        max_objects: Maximum number of labels that can be zeroed out. Can be tuple, in this case it's [min, max]\n        image_fill_value: Fill value to use when filling image.\n            Can be 'inpaint' to apply inpainting (works only  for 3-channel images)\n        mask_fill_value: Fill value to use when filling mask.\n\n    Targets:\n        image, mask\n\n    Image types:\n        uint8, float32\n\n    Reference:\n        https://www.kaggle.com/c/severstal-steel-defect-detection/discussion/114254\n\n    \"\"\"\n\n    _targets = (Targets.IMAGE, Targets.MASK)\n\n    class InitSchema(BaseTransformInitSchema):\n        max_objects: OnePlusIntRangeType = (1, 1)\n\n        image_fill_value: float | Literal[\"inpaint\"] = Field(\n            default=0,\n            description=(\n                \"Fill value to use when filling image. \"\n                \"Can be 'inpaint' to apply inpainting (works only for 3-channel images).\"\n            ),\n        )\n        mask_fill_value: float = Field(default=0, description=\"Fill value to use when filling mask.\")\n\n    def __init__(\n        self,\n        max_objects: ScaleIntType = (1, 1),\n        image_fill_value: float | Literal[\"inpaint\"] = 0,\n        mask_fill_value: ScalarType = 0,\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p=p, always_apply=always_apply)\n        self.max_objects = cast(Tuple[int, int], max_objects)\n        self.image_fill_value = image_fill_value\n        self.mask_fill_value = mask_fill_value\n\n    @property\n    def targets_as_params(self) -&gt; list[str]:\n        return [\"mask\"]\n\n    def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n        mask = data[\"mask\"]\n\n        label_image, num_labels = label(mask, return_num=True)\n\n        if num_labels == 0:\n            dropout_mask = None\n        else:\n            objects_to_drop = random.randint(self.max_objects[0], self.max_objects[1])\n            objects_to_drop = min(num_labels, objects_to_drop)\n\n            if objects_to_drop == num_labels:\n                dropout_mask = mask &gt; 0\n            else:\n                labels_index = random.sample(range(1, num_labels + 1), objects_to_drop)\n                dropout_mask = np.zeros((mask.shape[0], mask.shape[1]), dtype=bool)\n                for label_index in labels_index:\n                    dropout_mask |= label_image == label_index\n\n        params.update({\"dropout_mask\": dropout_mask})\n        return params\n\n    def apply(self, img: np.ndarray, dropout_mask: np.ndarray, **params: Any) -&gt; np.ndarray:\n        if dropout_mask is None:\n            return img\n\n        if self.image_fill_value == \"inpaint\":\n            dropout_mask = dropout_mask.astype(np.uint8)\n            _, _, width, height = cv2.boundingRect(dropout_mask)\n            radius = min(3, max(width, height) // 2)\n            return cv2.inpaint(img, dropout_mask, radius, cv2.INPAINT_NS)\n\n        img = img.copy()\n        img[dropout_mask] = self.image_fill_value\n\n        return img\n\n    def apply_to_mask(self, mask: np.ndarray, dropout_mask: np.ndarray, **params: Any) -&gt; np.ndarray:\n        if dropout_mask is None:\n            return mask\n\n        mask = mask.copy()\n        mask[dropout_mask] = self.mask_fill_value\n        return mask\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return \"max_objects\", \"image_fill_value\", \"mask_fill_value\"\n\n    @property\n    def targets(self) -&gt; dict[str, Callable[..., Any]]:\n        return {\n            \"image\": self.apply,\n            \"mask\": self.apply_to_mask,\n            \"masks\": self.apply_to_masks,\n        }\n</code></pre>"},{"location":"api_reference/augmentations/dropout/mask_dropout/#albumentations.augmentations.dropout.mask_dropout.MaskDropout.targets_as_params","title":"<code>targets_as_params: list[str]</code>  <code>property</code> <code>readonly</code>","text":"<p>Targets used to get params dependent on targets. This is used to check input has all required targets.</p>"},{"location":"api_reference/augmentations/dropout/mask_dropout/#albumentations.augmentations.dropout.mask_dropout.MaskDropout.apply","title":"<code>apply (self, img, dropout_mask, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/dropout/mask_dropout.py</code> Python<pre><code>def apply(self, img: np.ndarray, dropout_mask: np.ndarray, **params: Any) -&gt; np.ndarray:\n    if dropout_mask is None:\n        return img\n\n    if self.image_fill_value == \"inpaint\":\n        dropout_mask = dropout_mask.astype(np.uint8)\n        _, _, width, height = cv2.boundingRect(dropout_mask)\n        radius = min(3, max(width, height) // 2)\n        return cv2.inpaint(img, dropout_mask, radius, cv2.INPAINT_NS)\n\n    img = img.copy()\n    img[dropout_mask] = self.image_fill_value\n\n    return img\n</code></pre>"},{"location":"api_reference/augmentations/dropout/mask_dropout/#albumentations.augmentations.dropout.mask_dropout.MaskDropout.get_params_dependent_on_data","title":"<code>get_params_dependent_on_data (self, params, data)</code>","text":"<p>Returns parameters dependent on input.</p> Source code in <code>albumentations/augmentations/dropout/mask_dropout.py</code> Python<pre><code>def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n    mask = data[\"mask\"]\n\n    label_image, num_labels = label(mask, return_num=True)\n\n    if num_labels == 0:\n        dropout_mask = None\n    else:\n        objects_to_drop = random.randint(self.max_objects[0], self.max_objects[1])\n        objects_to_drop = min(num_labels, objects_to_drop)\n\n        if objects_to_drop == num_labels:\n            dropout_mask = mask &gt; 0\n        else:\n            labels_index = random.sample(range(1, num_labels + 1), objects_to_drop)\n            dropout_mask = np.zeros((mask.shape[0], mask.shape[1]), dtype=bool)\n            for label_index in labels_index:\n                dropout_mask |= label_image == label_index\n\n    params.update({\"dropout_mask\": dropout_mask})\n    return params\n</code></pre>"},{"location":"api_reference/augmentations/dropout/mask_dropout/#albumentations.augmentations.dropout.mask_dropout.MaskDropout.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/dropout/mask_dropout.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return \"max_objects\", \"image_fill_value\", \"mask_fill_value\"\n</code></pre>"},{"location":"api_reference/augmentations/dropout/xy_masking/","title":"XYMasking augmentation (augmentations.dropout.xy_masking)","text":""},{"location":"api_reference/augmentations/dropout/xy_masking/#albumentations.augmentations.dropout.xy_masking.XYMasking","title":"<code>class  XYMasking</code> <code>     (num_masks_x=0, num_masks_y=0, mask_x_length=0, mask_y_length=0, fill_value=0, mask_fill_value=0, always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Applies masking strips to an image, either horizontally (X axis) or vertically (Y axis), simulating occlusions. This transform is useful for training models to recognize images with varied visibility conditions. It's particularly effective for spectrogram images, allowing spectral and frequency masking to improve model robustness.</p> <p>At least one of <code>max_x_length</code> or <code>max_y_length</code> must be specified, dictating the mask's maximum size along each axis.</p> <p>Parameters:</p> Name Type Description <code>num_masks_x</code> <code>Union[int, tuple[int, int]]</code> <p>Number or range of horizontal regions to mask. Defaults to 0.</p> <code>num_masks_y</code> <code>Union[int, tuple[int, int]]</code> <p>Number or range of vertical regions to mask. Defaults to 0.</p> <code>mask_x_length</code> <code>[Union[int, tuple[int, int]]</code> <p>Specifies the length of the masks along the X (horizontal) axis. If an integer is provided, it sets a fixed mask length. If a tuple of two integers (min, max) is provided, the mask length is randomly chosen within this range for each mask. This allows for variable-length masks in the horizontal direction.</p> <code>mask_y_length</code> <code>Union[int, tuple[int, int]]</code> <p>Specifies the height of the masks along the Y (vertical) axis. Similar to <code>mask_x_length</code>, an integer sets a fixed mask height, while a tuple (min, max) allows for variable-height masks, chosen randomly within the specified range for each mask. This flexibility facilitates creating masks of various sizes in the vertical direction.</p> <code>fill_value</code> <code>Union[int, float, list[int], list[float]]</code> <p>Value to fill image masks. Defaults to 0.</p> <code>mask_fill_value</code> <code>Optional[Union[int, float, list[int], list[float]]]</code> <p>Value to fill masks in the mask. If <code>None</code>, uses mask is not affected. Default: <code>None</code>.</p> <code>p</code> <code>float</code> <p>Probability of applying the transform. Defaults to 0.5.</p> <p>Targets</p> <p>image, mask, keypoints</p> <p>Image types:     uint8, float32</p> <p>Note: Either <code>max_x_length</code> or <code>max_y_length</code> or both must be defined.</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/dropout/xy_masking.py</code> Python<pre><code>class XYMasking(DualTransform):\n    \"\"\"Applies masking strips to an image, either horizontally (X axis) or vertically (Y axis),\n    simulating occlusions. This transform is useful for training models to recognize images\n    with varied visibility conditions. It's particularly effective for spectrogram images,\n    allowing spectral and frequency masking to improve model robustness.\n\n    At least one of `max_x_length` or `max_y_length` must be specified, dictating the mask's\n    maximum size along each axis.\n\n    Args:\n        num_masks_x (Union[int, tuple[int, int]]): Number or range of horizontal regions to mask. Defaults to 0.\n        num_masks_y (Union[int, tuple[int, int]]): Number or range of vertical regions to mask. Defaults to 0.\n        mask_x_length ([Union[int, tuple[int, int]]): Specifies the length of the masks along\n            the X (horizontal) axis. If an integer is provided, it sets a fixed mask length.\n            If a tuple of two integers (min, max) is provided,\n            the mask length is randomly chosen within this range for each mask.\n            This allows for variable-length masks in the horizontal direction.\n        mask_y_length (Union[int, tuple[int, int]]): Specifies the height of the masks along\n            the Y (vertical) axis. Similar to `mask_x_length`, an integer sets a fixed mask height,\n            while a tuple (min, max) allows for variable-height masks, chosen randomly\n            within the specified range for each mask. This flexibility facilitates creating masks of various\n            sizes in the vertical direction.\n        fill_value (Union[int, float, list[int], list[float]]): Value to fill image masks. Defaults to 0.\n        mask_fill_value (Optional[Union[int, float, list[int], list[float]]]): Value to fill masks in the mask.\n            If `None`, uses mask is not affected. Default: `None`.\n        p (float): Probability of applying the transform. Defaults to 0.5.\n\n    Targets:\n        image, mask, keypoints\n\n    Image types:\n        uint8, float32\n\n    Note: Either `max_x_length` or `max_y_length` or both must be defined.\n\n    \"\"\"\n\n    _targets = (Targets.IMAGE, Targets.MASK, Targets.KEYPOINTS)\n\n    class InitSchema(BaseTransformInitSchema):\n        num_masks_x: NonNegativeIntRangeType = 0\n        num_masks_y: NonNegativeIntRangeType = 0\n        mask_x_length: NonNegativeIntRangeType = 0\n        mask_y_length: NonNegativeIntRangeType = 0\n\n        fill_value: ColorType = Field(default=0, description=\"Value to fill image masks.\")\n        mask_fill_value: ColorType = Field(default=0, description=\"Value to fill masks in the mask.\")\n\n        @model_validator(mode=\"after\")\n        def check_mask_length(self) -&gt; Self:\n            if (\n                isinstance(self.mask_x_length, int)\n                and self.mask_x_length &lt;= 0\n                and isinstance(self.mask_y_length, int)\n                and self.mask_y_length &lt;= 0\n            ):\n                msg = \"At least one of `mask_x_length` or `mask_y_length` Should be a positive number.\"\n                raise ValueError(msg)\n            return self\n\n    def __init__(\n        self,\n        num_masks_x: ScaleIntType = 0,\n        num_masks_y: ScaleIntType = 0,\n        mask_x_length: ScaleIntType = 0,\n        mask_y_length: ScaleIntType = 0,\n        fill_value: ColorType = 0,\n        mask_fill_value: ColorType = 0,\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p, always_apply)\n        self.num_masks_x = cast(Tuple[int, int], num_masks_x)\n        self.num_masks_y = cast(Tuple[int, int], num_masks_y)\n\n        self.mask_x_length = cast(Tuple[int, int], mask_x_length)\n        self.mask_y_length = cast(Tuple[int, int], mask_y_length)\n        self.fill_value = fill_value\n        self.mask_fill_value = mask_fill_value\n\n    def apply(\n        self,\n        img: np.ndarray,\n        masks_x: list[tuple[int, int, int, int]],\n        masks_y: list[tuple[int, int, int, int]],\n        **params: Any,\n    ) -&gt; np.ndarray:\n        return cutout(img, masks_x + masks_y, self.fill_value)\n\n    def apply_to_mask(\n        self,\n        mask: np.ndarray,\n        masks_x: list[tuple[int, int, int, int]],\n        masks_y: list[tuple[int, int, int, int]],\n        **params: Any,\n    ) -&gt; np.ndarray:\n        if self.mask_fill_value is None:\n            return mask\n        return cutout(mask, masks_x + masks_y, self.mask_fill_value)\n\n    def validate_mask_length(\n        self,\n        mask_length: tuple[int, int] | None,\n        dimension_size: int,\n        dimension_name: str,\n    ) -&gt; None:\n        \"\"\"Validate the mask length against the corresponding image dimension size.\n\n        Args:\n            mask_length (Optional[tuple[int, int]]): The length of the mask to be validated.\n            dimension_size (int): The size of the image dimension (width or height)\n                against which to validate the mask length.\n            dimension_name (str): The name of the dimension ('width' or 'height') for error messaging.\n\n        \"\"\"\n        if mask_length is not None:\n            if isinstance(mask_length, (tuple, list)):\n                if mask_length[0] &lt; 0 or mask_length[1] &gt; dimension_size:\n                    raise ValueError(\n                        f\"{dimension_name} range {mask_length} is out of valid range [0, {dimension_size}]\",\n                    )\n            elif mask_length &lt; 0 or mask_length &gt; dimension_size:\n                raise ValueError(f\"{dimension_name} {mask_length} exceeds image {dimension_name} {dimension_size}\")\n\n    def get_params_dependent_on_data(\n        self,\n        params: dict[str, Any],\n        data: dict[str, Any],\n    ) -&gt; dict[str, list[tuple[int, int, int, int]]]:\n        height, width = params[\"shape\"][:2]\n\n        # Use the helper method to validate mask lengths against image dimensions\n        self.validate_mask_length(self.mask_x_length, width, \"mask_x_length\")\n        self.validate_mask_length(self.mask_y_length, height, \"mask_y_length\")\n\n        masks_x = self.generate_masks(self.num_masks_x, width, height, self.mask_x_length, axis=\"x\")\n        masks_y = self.generate_masks(self.num_masks_y, width, height, self.mask_y_length, axis=\"y\")\n\n        return {\"masks_x\": masks_x, \"masks_y\": masks_y}\n\n    @staticmethod\n    def generate_mask_size(mask_length: tuple[int, int]) -&gt; int:\n        return random.randint(mask_length[0], mask_length[1])\n\n    def generate_masks(\n        self,\n        num_masks: tuple[int, int],\n        width: int,\n        height: int,\n        max_length: tuple[int, int] | None,\n        axis: str,\n    ) -&gt; list[tuple[int, int, int, int]]:\n        if max_length is None or max_length == 0 or isinstance(num_masks, (int, float)) and num_masks == 0:\n            return []\n\n        masks = []\n\n        num_masks_integer = num_masks if isinstance(num_masks, int) else random.randint(num_masks[0], num_masks[1])\n\n        for _ in range(num_masks_integer):\n            length = self.generate_mask_size(max_length)\n\n            if axis == \"x\":\n                x1 = random.randint(0, width - length)\n                y1 = 0\n                x2, y2 = x1 + length, height\n            else:  # axis == 'y'\n                y1 = random.randint(0, height - length)\n                x1 = 0\n                x2, y2 = width, y1 + length\n\n            masks.append((x1, y1, x2, y2))\n        return masks\n\n    def apply_to_keypoints(\n        self,\n        keypoints: Sequence[KeypointType],\n        masks_x: list[tuple[int, int, int, int]],\n        masks_y: list[tuple[int, int, int, int]],\n        **params: Any,\n    ) -&gt; list[KeypointType]:\n        return [\n            keypoint\n            for keypoint in keypoints\n            if not any(keypoint_in_hole(keypoint, hole) for hole in masks_x + masks_y)\n        ]\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return (\n            \"num_masks_x\",\n            \"num_masks_y\",\n            \"mask_x_length\",\n            \"mask_y_length\",\n            \"fill_value\",\n            \"mask_fill_value\",\n        )\n\n    @property\n    def targets(self) -&gt; dict[str, Callable[..., Any]]:\n        return {\n            \"image\": self.apply,\n            \"mask\": self.apply_to_mask,\n            \"masks\": self.apply_to_masks,\n            \"keypoints\": self.apply_to_keypoints,\n        }\n</code></pre>"},{"location":"api_reference/augmentations/dropout/xy_masking/#albumentations.augmentations.dropout.xy_masking.XYMasking.apply","title":"<code>apply (self, img, masks_x, masks_y, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/dropout/xy_masking.py</code> Python<pre><code>def apply(\n    self,\n    img: np.ndarray,\n    masks_x: list[tuple[int, int, int, int]],\n    masks_y: list[tuple[int, int, int, int]],\n    **params: Any,\n) -&gt; np.ndarray:\n    return cutout(img, masks_x + masks_y, self.fill_value)\n</code></pre>"},{"location":"api_reference/augmentations/dropout/xy_masking/#albumentations.augmentations.dropout.xy_masking.XYMasking.get_params_dependent_on_data","title":"<code>get_params_dependent_on_data (self, params, data)</code>","text":"<p>Returns parameters dependent on input.</p> Source code in <code>albumentations/augmentations/dropout/xy_masking.py</code> Python<pre><code>def get_params_dependent_on_data(\n    self,\n    params: dict[str, Any],\n    data: dict[str, Any],\n) -&gt; dict[str, list[tuple[int, int, int, int]]]:\n    height, width = params[\"shape\"][:2]\n\n    # Use the helper method to validate mask lengths against image dimensions\n    self.validate_mask_length(self.mask_x_length, width, \"mask_x_length\")\n    self.validate_mask_length(self.mask_y_length, height, \"mask_y_length\")\n\n    masks_x = self.generate_masks(self.num_masks_x, width, height, self.mask_x_length, axis=\"x\")\n    masks_y = self.generate_masks(self.num_masks_y, width, height, self.mask_y_length, axis=\"y\")\n\n    return {\"masks_x\": masks_x, \"masks_y\": masks_y}\n</code></pre>"},{"location":"api_reference/augmentations/dropout/xy_masking/#albumentations.augmentations.dropout.xy_masking.XYMasking.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/dropout/xy_masking.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return (\n        \"num_masks_x\",\n        \"num_masks_y\",\n        \"mask_x_length\",\n        \"mask_y_length\",\n        \"fill_value\",\n        \"mask_fill_value\",\n    )\n</code></pre>"},{"location":"api_reference/augmentations/dropout/xy_masking/#albumentations.augmentations.dropout.xy_masking.XYMasking.validate_mask_length","title":"<code>validate_mask_length (self, mask_length, dimension_size, dimension_name)</code>","text":"<p>Validate the mask length against the corresponding image dimension size.</p> <p>Parameters:</p> Name Type Description <code>mask_length</code> <code>Optional[tuple[int, int]]</code> <p>The length of the mask to be validated.</p> <code>dimension_size</code> <code>int</code> <p>The size of the image dimension (width or height) against which to validate the mask length.</p> <code>dimension_name</code> <code>str</code> <p>The name of the dimension ('width' or 'height') for error messaging.</p> Source code in <code>albumentations/augmentations/dropout/xy_masking.py</code> Python<pre><code>def validate_mask_length(\n    self,\n    mask_length: tuple[int, int] | None,\n    dimension_size: int,\n    dimension_name: str,\n) -&gt; None:\n    \"\"\"Validate the mask length against the corresponding image dimension size.\n\n    Args:\n        mask_length (Optional[tuple[int, int]]): The length of the mask to be validated.\n        dimension_size (int): The size of the image dimension (width or height)\n            against which to validate the mask length.\n        dimension_name (str): The name of the dimension ('width' or 'height') for error messaging.\n\n    \"\"\"\n    if mask_length is not None:\n        if isinstance(mask_length, (tuple, list)):\n            if mask_length[0] &lt; 0 or mask_length[1] &gt; dimension_size:\n                raise ValueError(\n                    f\"{dimension_name} range {mask_length} is out of valid range [0, {dimension_size}]\",\n                )\n        elif mask_length &lt; 0 or mask_length &gt; dimension_size:\n            raise ValueError(f\"{dimension_name} {mask_length} exceeds image {dimension_name} {dimension_size}\")\n</code></pre>"},{"location":"api_reference/augmentations/geometric/","title":"Index","text":"<ul> <li>Geometric functional transforms (albumentations.augmentations.geometric.functional)</li> <li>Resizing transforms (augmentations.geometric.resize)</li> <li>Rotation transforms (augmentations.geometric.functional)</li> <li>Geometric transforms (augmentations.geometric.transforms)</li> </ul>"},{"location":"api_reference/augmentations/geometric/functional/","title":"Geometric functional transforms (augmentations.geometric.functional)","text":""},{"location":"api_reference/augmentations/geometric/functional/#albumentations.augmentations.geometric.functional.bbox_d4","title":"<code>def bbox_d4    (bbox, group_member)    </code> [view source on GitHub]","text":"<p>Applies a <code>D_4</code> symmetry group transformation to a bounding box.</p> <p>The function transforms a bounding box according to the specified group member from the <code>D_4</code> group. These transformations include rotations and reflections, specified to work on an image's bounding box given its dimensions.</p> <ul> <li>bbox (BoxInternalType): The bounding box to transform. This should be a structure specifying coordinates     like (xmin, ymin, xmax, ymax).</li> <li>group_member (D4Type): A string identifier for the <code>D_4</code> group transformation to apply.     Valid values are 'e', 'r90', 'r180', 'r270', 'v', 'hvt', 'h', 't'.</li> </ul> <ul> <li>BoxInternalType: The transformed bounding box.</li> </ul> <ul> <li>ValueError: If an invalid group member is specified.</li> </ul> <p>Examples:</p> <ul> <li>Applying a 90-degree rotation:   <code>bbox_d4((10, 20, 110, 120), 'r90')</code>   This would rotate the bounding box 90 degrees within a 100x100 image.</li> </ul> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>def bbox_d4(\n    bbox: BoxInternalType,\n    group_member: D4Type,\n) -&gt; BoxInternalType:\n    \"\"\"Applies a `D_4` symmetry group transformation to a bounding box.\n\n    The function transforms a bounding box according to the specified group member from the `D_4` group.\n    These transformations include rotations and reflections, specified to work on an image's bounding box given\n    its dimensions.\n\n    Parameters:\n    - bbox (BoxInternalType): The bounding box to transform. This should be a structure specifying coordinates\n        like (xmin, ymin, xmax, ymax).\n    - group_member (D4Type): A string identifier for the `D_4` group transformation to apply.\n        Valid values are 'e', 'r90', 'r180', 'r270', 'v', 'hvt', 'h', 't'.\n\n    Returns:\n    - BoxInternalType: The transformed bounding box.\n\n    Raises:\n    - ValueError: If an invalid group member is specified.\n\n    Examples:\n    - Applying a 90-degree rotation:\n      `bbox_d4((10, 20, 110, 120), 'r90')`\n      This would rotate the bounding box 90 degrees within a 100x100 image.\n    \"\"\"\n    transformations = {\n        \"e\": lambda x: x,  # Identity transformation\n        \"r90\": lambda x: bbox_rot90(x, 1),  # Rotate 90 degrees\n        \"r180\": lambda x: bbox_rot90(x, 2),  # Rotate 180 degrees\n        \"r270\": lambda x: bbox_rot90(x, 3),  # Rotate 270 degrees\n        \"v\": lambda x: bbox_vflip(x),  # Vertical flip\n        \"hvt\": lambda x: bbox_transpose(bbox_rot90(x, 2)),  # Reflect over anti-diagonal\n        \"h\": lambda x: bbox_hflip(x),  # Horizontal flip\n        \"t\": lambda x: bbox_transpose(x),  # Transpose (reflect over main diagonal)\n    }\n\n    # Execute the appropriate transformation\n    if group_member in transformations:\n        return transformations[group_member](bbox)\n\n    raise ValueError(f\"Invalid group member: {group_member}\")\n</code></pre>"},{"location":"api_reference/augmentations/geometric/functional/#albumentations.augmentations.geometric.functional.bbox_flip","title":"<code>def bbox_flip    (bbox, d)    </code> [view source on GitHub]","text":"<p>Flip a bounding box either vertically, horizontally or both depending on the value of <code>d</code>.</p> <p>Parameters:</p> Name Type Description <code>bbox</code> <code>BoxInternalType</code> <p>A bounding box <code>(x_min, y_min, x_max, y_max)</code>.</p> <code>d</code> <code>int</code> <p>dimension. 0 for vertical flip, 1 for horizontal, -1 for transpose</p> <p>Returns:</p> Type Description <code>BoxInternalType</code> <p>A bounding box <code>(x_min, y_min, x_max, y_max)</code>.</p> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>if value of <code>d</code> is not -1, 0 or 1.</p> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>def bbox_flip(bbox: BoxInternalType, d: int) -&gt; BoxInternalType:\n    \"\"\"Flip a bounding box either vertically, horizontally or both depending on the value of `d`.\n\n    Args:\n        bbox: A bounding box `(x_min, y_min, x_max, y_max)`.\n        d: dimension. 0 for vertical flip, 1 for horizontal, -1 for transpose\n\n    Returns:\n        A bounding box `(x_min, y_min, x_max, y_max)`.\n\n    Raises:\n        ValueError: if value of `d` is not -1, 0 or 1.\n\n    \"\"\"\n    if d == 0:\n        bbox = bbox_vflip(bbox)\n    elif d == 1:\n        bbox = bbox_hflip(bbox)\n    elif d == -1:\n        bbox = bbox_hflip(bbox)\n        bbox = bbox_vflip(bbox)\n    else:\n        raise ValueError(f\"Invalid d value {d}. Valid values are -1, 0 and 1\")\n    return bbox\n</code></pre>"},{"location":"api_reference/augmentations/geometric/functional/#albumentations.augmentations.geometric.functional.bbox_hflip","title":"<code>def bbox_hflip    (bbox)    </code> [view source on GitHub]","text":"<p>Flip a bounding box horizontally around the y-axis.</p> <p>Parameters:</p> Name Type Description <code>bbox</code> <code>BoxInternalType</code> <p>A bounding box <code>(x_min, y_min, x_max, y_max)</code>.</p> <p>Returns:</p> Type Description <code>BoxInternalType</code> <p>A bounding box <code>(x_min, y_min, x_max, y_max)</code>.</p> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>def bbox_hflip(bbox: BoxInternalType) -&gt; BoxInternalType:\n    \"\"\"Flip a bounding box horizontally around the y-axis.\n\n    Args:\n        bbox: A bounding box `(x_min, y_min, x_max, y_max)`.\n\n    Returns:\n        A bounding box `(x_min, y_min, x_max, y_max)`.\n\n    \"\"\"\n    x_min, y_min, x_max, y_max = bbox[:4]\n    return 1 - x_max, y_min, 1 - x_min, y_max\n</code></pre>"},{"location":"api_reference/augmentations/geometric/functional/#albumentations.augmentations.geometric.functional.bbox_rot90","title":"<code>def bbox_rot90    (bbox, factor)    </code> [view source on GitHub]","text":"<p>Rotates a bounding box by 90 degrees CCW (see np.rot90)</p> <p>Parameters:</p> Name Type Description <code>bbox</code> <code>BoxInternalType</code> <p>A bounding box tuple (x_min, y_min, x_max, y_max).</p> <code>factor</code> <code>int</code> <p>Number of CCW rotations. Must be in set {0, 1, 2, 3} See np.rot90.</p> <p>Returns:</p> Type Description <code>tuple</code> <p>A bounding box tuple (x_min, y_min, x_max, y_max).</p> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>def bbox_rot90(bbox: BoxInternalType, factor: int) -&gt; BoxInternalType:\n    \"\"\"Rotates a bounding box by 90 degrees CCW (see np.rot90)\n\n    Args:\n        bbox: A bounding box tuple (x_min, y_min, x_max, y_max).\n        factor: Number of CCW rotations. Must be in set {0, 1, 2, 3} See np.rot90.\n\n    Returns:\n        tuple: A bounding box tuple (x_min, y_min, x_max, y_max).\n\n    \"\"\"\n    if factor not in {0, 1, 2, 3}:\n        msg = \"Parameter n must be in set {0, 1, 2, 3}\"\n        raise ValueError(msg)\n    x_min, y_min, x_max, y_max = bbox[:4]\n    if factor == 1:\n        bbox = y_min, 1 - x_max, y_max, 1 - x_min\n    elif factor == ROT90_180_FACTOR:\n        bbox = 1 - x_max, 1 - y_max, 1 - x_min, 1 - y_min\n    elif factor == ROT90_270_FACTOR:\n        bbox = 1 - y_max, x_min, 1 - y_min, x_max\n    return bbox\n</code></pre>"},{"location":"api_reference/augmentations/geometric/functional/#albumentations.augmentations.geometric.functional.bbox_rotate","title":"<code>def bbox_rotate    (bbox, angle, method, image_shape)    </code> [view source on GitHub]","text":"<p>Rotates a bounding box by angle degrees.</p> <p>Parameters:</p> Name Type Description <code>bbox</code> <code>BoxInternalType</code> <p>A bounding box <code>(x_min, y_min, x_max, y_max)</code>.</p> <code>angle</code> <code>float</code> <p>Angle of rotation in degrees.</p> <code>method</code> <code>str</code> <p>Rotation method used. Should be one of: \"largest_box\", \"ellipse\". Default: \"largest_box\".</p> <code>image_shape</code> <code>tuple[int, int]</code> <p>Image shape <code>(height, width)</code>.</p> <p>Returns:</p> Type Description <code>BoxInternalType</code> <p>A bounding box <code>(x_min, y_min, x_max, y_max)</code>.</p> <p>Reference</p> <p>https://arxiv.org/abs/2109.13488</p> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>def bbox_rotate(bbox: BoxInternalType, angle: float, method: str, image_shape: tuple[int, int]) -&gt; BoxInternalType:\n    \"\"\"Rotates a bounding box by angle degrees.\n\n    Args:\n        bbox: A bounding box `(x_min, y_min, x_max, y_max)`.\n        angle: Angle of rotation in degrees.\n        method: Rotation method used. Should be one of: \"largest_box\", \"ellipse\". Default: \"largest_box\".\n        image_shape: Image shape `(height, width)`.\n\n    Returns:\n        A bounding box `(x_min, y_min, x_max, y_max)`.\n\n    Reference:\n        https://arxiv.org/abs/2109.13488\n\n    \"\"\"\n    rows, cols = image_shape\n    x_min, y_min, x_max, y_max = bbox[:4]\n    scale = cols / float(rows)\n    if method == \"largest_box\":\n        x = np.array([x_min, x_max, x_max, x_min]) - 0.5\n        y = np.array([y_min, y_min, y_max, y_max]) - 0.5\n    elif method == \"ellipse\":\n        w = (x_max - x_min) / 2\n        h = (y_max - y_min) / 2\n        data = np.arange(0, 360, dtype=np.float32)\n        x = w * np.sin(np.radians(data)) + (w + x_min - 0.5)\n        y = h * np.cos(np.radians(data)) + (h + y_min - 0.5)\n    else:\n        raise ValueError(f\"Method {method} is not a valid rotation method.\")\n    angle = np.deg2rad(angle)\n    x_t = (np.cos(angle) * x * scale + np.sin(angle) * y) / scale\n    y_t = -np.sin(angle) * x * scale + np.cos(angle) * y\n    x_t = x_t + 0.5\n    y_t = y_t + 0.5\n\n    x_min, x_max = min(x_t), max(x_t)\n    y_min, y_max = min(y_t), max(y_t)\n\n    return x_min, y_min, x_max, y_max\n</code></pre>"},{"location":"api_reference/augmentations/geometric/functional/#albumentations.augmentations.geometric.functional.bbox_transpose","title":"<code>def bbox_transpose    (bbox)    </code> [view source on GitHub]","text":"<p>Transposes a bounding box along given axis.</p> <p>Parameters:</p> Name Type Description <code>bbox</code> <code>KeypointInternalType</code> <p>A bounding box <code>(x_min, y_min, x_max, y_max)</code>.</p> <p>Returns:</p> Type Description <code>KeypointInternalType</code> <p>A bounding box tuple <code>(x_min, y_min, x_max, y_max)</code>.</p> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If axis not equal to 0 or 1.</p> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>def bbox_transpose(\n    bbox: KeypointInternalType,\n) -&gt; KeypointInternalType:\n    \"\"\"Transposes a bounding box along given axis.\n\n    Args:\n        bbox: A bounding box `(x_min, y_min, x_max, y_max)`.\n\n    Returns:\n        A bounding box tuple `(x_min, y_min, x_max, y_max)`.\n\n    Raises:\n        ValueError: If axis not equal to 0 or 1.\n\n    \"\"\"\n    x_min, y_min, x_max, y_max = bbox[:4]\n    return (y_min, x_min, y_max, x_max)\n</code></pre>"},{"location":"api_reference/augmentations/geometric/functional/#albumentations.augmentations.geometric.functional.bbox_vflip","title":"<code>def bbox_vflip    (bbox)    </code> [view source on GitHub]","text":"<p>Flip a bounding box vertically around the x-axis.</p> <p>Parameters:</p> Name Type Description <code>bbox</code> <code>BoxInternalType</code> <p>A bounding box <code>(x_min, y_min, x_max, y_max)</code>.</p> <p>Returns:</p> Type Description <code>tuple</code> <p>A bounding box <code>(x_min, y_min, x_max, y_max)</code>.</p> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>def bbox_vflip(bbox: BoxInternalType) -&gt; BoxInternalType:\n    \"\"\"Flip a bounding box vertically around the x-axis.\n\n    Args:\n        bbox: A bounding box `(x_min, y_min, x_max, y_max)`.\n\n    Returns:\n        tuple: A bounding box `(x_min, y_min, x_max, y_max)`.\n\n    \"\"\"\n    x_min, y_min, x_max, y_max = bbox[:4]\n    return x_min, 1 - y_max, x_max, 1 - y_min\n</code></pre>"},{"location":"api_reference/augmentations/geometric/functional/#albumentations.augmentations.geometric.functional.bboxes_affine","title":"<code>def bboxes_affine    (bboxes, matrix, rotate_method, image_shape, border_mode, output_shape)    </code> [view source on GitHub]","text":"<p>Apply an affine transformation to bounding boxes.</p> <p>For reflection border modes (cv2.BORDER_REFLECT_101, cv2.BORDER_REFLECT), this function: 1. Calculates necessary padding to avoid information loss 2. Applies padding to the bounding boxes 3. Adjusts the transformation matrix to account for padding 4. Applies the affine transformation 5. Validates the transformed bounding boxes</p> <p>For other border modes, it directly applies the affine transformation without padding.</p> <p>Parameters:</p> Name Type Description <code>bboxes</code> <code>np.ndarray</code> <p>Input bounding boxes</p> <code>matrix</code> <code>skimage.transform.ProjectiveTransform</code> <p>Affine transformation matrix</p> <code>rotate_method</code> <code>str</code> <p>Method for rotating bounding boxes ('largest_box' or 'ellipse')</p> <code>image_shape</code> <code>Sequence[int]</code> <p>Shape of the input image</p> <code>border_mode</code> <code>int</code> <p>OpenCV border mode</p> <code>output_shape</code> <code>Sequence[int]</code> <p>Shape of the output image</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Transformed and normalized bounding boxes</p> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>def bboxes_affine(\n    bboxes: np.ndarray,\n    matrix: skimage.transform.ProjectiveTransform,\n    rotate_method: Literal[\"largest_box\", \"ellipse\"],\n    image_shape: tuple[int, int],\n    border_mode: int,\n    output_shape: Sequence[int],\n) -&gt; np.ndarray:\n    \"\"\"Apply an affine transformation to bounding boxes.\n\n    For reflection border modes (cv2.BORDER_REFLECT_101, cv2.BORDER_REFLECT), this function:\n    1. Calculates necessary padding to avoid information loss\n    2. Applies padding to the bounding boxes\n    3. Adjusts the transformation matrix to account for padding\n    4. Applies the affine transformation\n    5. Validates the transformed bounding boxes\n\n    For other border modes, it directly applies the affine transformation without padding.\n\n    Args:\n        bboxes (np.ndarray): Input bounding boxes\n        matrix (skimage.transform.ProjectiveTransform): Affine transformation matrix\n        rotate_method (str): Method for rotating bounding boxes ('largest_box' or 'ellipse')\n        image_shape (Sequence[int]): Shape of the input image\n        border_mode (int): OpenCV border mode\n        output_shape (Sequence[int]): Shape of the output image\n\n    Returns:\n        np.ndarray: Transformed and normalized bounding boxes\n    \"\"\"\n    if is_identity_matrix(matrix):\n        return bboxes\n\n    bboxes = denormalize_bboxes(bboxes, image_shape)\n\n    if border_mode in REFLECT_BORDER_MODES:\n        # Step 1: Compute affine transform padding\n        pad_left, pad_right, pad_top, pad_bottom = calculate_affine_transform_padding(matrix, image_shape)\n        grid_dimensions = get_pad_grid_dimensions(pad_top, pad_bottom, pad_left, pad_right, image_shape)\n        bboxes = generate_reflected_bboxes(bboxes, grid_dimensions, image_shape, center_in_origin=True)\n\n    # Apply affine transform\n    if rotate_method == \"largest_box\":\n        transformed_bboxes = bboxes_affine_largest_box(bboxes, matrix)\n    elif rotate_method == \"ellipse\":\n        transformed_bboxes = bboxes_affine_ellipse(bboxes, matrix)\n    else:\n        raise ValueError(f\"Method {rotate_method} is not a valid rotation method.\")\n\n    # Validate and normalize bboxes\n    validated_bboxes = validate_bboxes(transformed_bboxes, output_shape)\n\n    return normalize_bboxes(validated_bboxes, output_shape)\n</code></pre>"},{"location":"api_reference/augmentations/geometric/functional/#albumentations.augmentations.geometric.functional.bboxes_affine_ellipse","title":"<code>def bboxes_affine_ellipse    (bboxes, matrix)    </code> [view source on GitHub]","text":"<p>Apply an affine transformation to bounding boxes using an ellipse approximation method.</p> <p>This function transforms bounding boxes by approximating each box with an ellipse, transforming points along the ellipse's circumference, and then computing the new bounding box that encloses the transformed ellipse.</p> <p>Parameters:</p> Name Type Description <code>bboxes</code> <code>np.ndarray</code> <p>An array of bounding boxes with shape (N, 4+) where N is the number of                  bounding boxes. Each row should contain [x_min, y_min, x_max, y_max]                  followed by any additional attributes (e.g., class labels).</p> <code>matrix</code> <code>skimage.transform.ProjectiveTransform</code> <p>The affine transformation matrix to apply.</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>An array of transformed bounding boxes with the same shape as the input.             Each row contains [new_x_min, new_y_min, new_x_max, new_y_max] followed by             any additional attributes from the input bounding boxes.</p> <p>Note</p> <ul> <li>This function assumes that the input bounding boxes are in the format [x_min, y_min, x_max, y_max].</li> <li>The ellipse approximation method can provide a tighter bounding box compared to the   largest box method, especially for rotations.</li> <li>360 points are used to approximate each ellipse, which provides a good balance between   accuracy and computational efficiency.</li> <li>Any additional attributes beyond the first 4 coordinates are preserved unchanged.</li> <li>This method may be more suitable for objects that are roughly elliptical in shape.</li> </ul> <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; bboxes = np.array([[10, 10, 30, 20, 1], [40, 40, 60, 60, 2]])  # Two boxes with class labels\n&gt;&gt;&gt; matrix = skimage.transform.AffineTransform(rotation=np.pi/4)  # 45-degree rotation\n&gt;&gt;&gt; transformed_bboxes = bboxes_affine_ellipse(bboxes, matrix)\n&gt;&gt;&gt; print(transformed_bboxes)\n[[ 5.86  5.86 34.14 24.14  1.  ]\n [30.   30.   70.   70.    2.  ]]\n</code></pre> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>def bboxes_affine_ellipse(bboxes: np.ndarray, matrix: skimage.transform.ProjectiveTransform) -&gt; np.ndarray:\n    \"\"\"Apply an affine transformation to bounding boxes using an ellipse approximation method.\n\n    This function transforms bounding boxes by approximating each box with an ellipse,\n    transforming points along the ellipse's circumference, and then computing the\n    new bounding box that encloses the transformed ellipse.\n\n    Args:\n        bboxes (np.ndarray): An array of bounding boxes with shape (N, 4+) where N is the number of\n                             bounding boxes. Each row should contain [x_min, y_min, x_max, y_max]\n                             followed by any additional attributes (e.g., class labels).\n        matrix (skimage.transform.ProjectiveTransform): The affine transformation matrix to apply.\n\n    Returns:\n        np.ndarray: An array of transformed bounding boxes with the same shape as the input.\n                    Each row contains [new_x_min, new_y_min, new_x_max, new_y_max] followed by\n                    any additional attributes from the input bounding boxes.\n\n    Note:\n        - This function assumes that the input bounding boxes are in the format [x_min, y_min, x_max, y_max].\n        - The ellipse approximation method can provide a tighter bounding box compared to the\n          largest box method, especially for rotations.\n        - 360 points are used to approximate each ellipse, which provides a good balance between\n          accuracy and computational efficiency.\n        - Any additional attributes beyond the first 4 coordinates are preserved unchanged.\n        - This method may be more suitable for objects that are roughly elliptical in shape.\n\n    Example:\n        &gt;&gt;&gt; bboxes = np.array([[10, 10, 30, 20, 1], [40, 40, 60, 60, 2]])  # Two boxes with class labels\n        &gt;&gt;&gt; matrix = skimage.transform.AffineTransform(rotation=np.pi/4)  # 45-degree rotation\n        &gt;&gt;&gt; transformed_bboxes = bboxes_affine_ellipse(bboxes, matrix)\n        &gt;&gt;&gt; print(transformed_bboxes)\n        [[ 5.86  5.86 34.14 24.14  1.  ]\n         [30.   30.   70.   70.    2.  ]]\n    \"\"\"\n    x_min, y_min, x_max, y_max = bboxes[:, 0], bboxes[:, 1], bboxes[:, 2], bboxes[:, 3]\n    bbox_width = (x_max - x_min) / 2\n    bbox_height = (y_max - y_min) / 2\n    center_x = x_min + bbox_width\n    center_y = y_min + bbox_height\n\n    angles = np.arange(0, 360, dtype=np.float32)\n    cos_angles = np.cos(np.radians(angles))\n    sin_angles = np.sin(np.radians(angles))\n\n    # Generate points for all ellipses at once\n    x = bbox_width[:, np.newaxis] * sin_angles + center_x[:, np.newaxis]\n    y = bbox_height[:, np.newaxis] * cos_angles + center_y[:, np.newaxis]\n    points = np.stack([x, y], axis=-1).reshape(-1, 2)\n\n    # Transform all points at once\n    transformed_points = skimage.transform.matrix_transform(points, matrix.params)\n    transformed_points = transformed_points.reshape(len(bboxes), -1, 2)\n\n    # Compute new bounding boxes\n    new_x_min = np.min(transformed_points[:, :, 0], axis=1)\n    new_x_max = np.max(transformed_points[:, :, 0], axis=1)\n    new_y_min = np.min(transformed_points[:, :, 1], axis=1)\n    new_y_max = np.max(transformed_points[:, :, 1], axis=1)\n\n    return np.column_stack([new_x_min, new_y_min, new_x_max, new_y_max, bboxes[:, 4:]])\n</code></pre>"},{"location":"api_reference/augmentations/geometric/functional/#albumentations.augmentations.geometric.functional.bboxes_affine_largest_box","title":"<code>def bboxes_affine_largest_box    (bboxes, matrix)    </code> [view source on GitHub]","text":"<p>Apply an affine transformation to bounding boxes and return the largest enclosing boxes.</p> <p>This function transforms each corner of every bounding box using the given affine transformation matrix, then computes the new bounding boxes that fully enclose the transformed corners.</p> <p>Parameters:</p> Name Type Description <code>bboxes</code> <code>np.ndarray</code> <p>An array of bounding boxes with shape (N, 4+) where N is the number of                  bounding boxes. Each row should contain [x_min, y_min, x_max, y_max]                  followed by any additional attributes (e.g., class labels).</p> <code>matrix</code> <code>skimage.transform.ProjectiveTransform</code> <p>The affine transformation matrix to apply.</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>An array of transformed bounding boxes with the same shape as the input.             Each row contains [new_x_min, new_y_min, new_x_max, new_y_max] followed by             any additional attributes from the input bounding boxes.</p> <p>Note</p> <ul> <li>This function assumes that the input bounding boxes are in the format [x_min, y_min, x_max, y_max].</li> <li>The resulting bounding boxes are the smallest axis-aligned boxes that completely   enclose the transformed original boxes. They may be larger than the minimal possible   bounding box if the original box becomes rotated.</li> <li>Any additional attributes beyond the first 4 coordinates are preserved unchanged.</li> <li>This method is called \"largest box\" because it returns the largest axis-aligned box   that encloses all corners of the transformed bounding box.</li> </ul> <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; bboxes = np.array([[10, 10, 20, 20, 1], [30, 30, 40, 40, 2]])  # Two boxes with class labels\n&gt;&gt;&gt; matrix = skimage.transform.AffineTransform(scale=(2, 2), translation=(5, 5))\n&gt;&gt;&gt; transformed_bboxes = bboxes_affine_largest_box(bboxes, matrix)\n&gt;&gt;&gt; print(transformed_bboxes)\n[[ 25.  25.  45.  45.   1.]\n [ 65.  65.  85.  85.   2.]]\n</code></pre> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>def bboxes_affine_largest_box(bboxes: np.ndarray, matrix: skimage.transform.ProjectiveTransform) -&gt; np.ndarray:\n    \"\"\"Apply an affine transformation to bounding boxes and return the largest enclosing boxes.\n\n    This function transforms each corner of every bounding box using the given affine transformation\n    matrix, then computes the new bounding boxes that fully enclose the transformed corners.\n\n    Args:\n        bboxes (np.ndarray): An array of bounding boxes with shape (N, 4+) where N is the number of\n                             bounding boxes. Each row should contain [x_min, y_min, x_max, y_max]\n                             followed by any additional attributes (e.g., class labels).\n        matrix (skimage.transform.ProjectiveTransform): The affine transformation matrix to apply.\n\n    Returns:\n        np.ndarray: An array of transformed bounding boxes with the same shape as the input.\n                    Each row contains [new_x_min, new_y_min, new_x_max, new_y_max] followed by\n                    any additional attributes from the input bounding boxes.\n\n    Note:\n        - This function assumes that the input bounding boxes are in the format [x_min, y_min, x_max, y_max].\n        - The resulting bounding boxes are the smallest axis-aligned boxes that completely\n          enclose the transformed original boxes. They may be larger than the minimal possible\n          bounding box if the original box becomes rotated.\n        - Any additional attributes beyond the first 4 coordinates are preserved unchanged.\n        - This method is called \"largest box\" because it returns the largest axis-aligned box\n          that encloses all corners of the transformed bounding box.\n\n    Example:\n        &gt;&gt;&gt; bboxes = np.array([[10, 10, 20, 20, 1], [30, 30, 40, 40, 2]])  # Two boxes with class labels\n        &gt;&gt;&gt; matrix = skimage.transform.AffineTransform(scale=(2, 2), translation=(5, 5))\n        &gt;&gt;&gt; transformed_bboxes = bboxes_affine_largest_box(bboxes, matrix)\n        &gt;&gt;&gt; print(transformed_bboxes)\n        [[ 25.  25.  45.  45.   1.]\n         [ 65.  65.  85.  85.   2.]]\n    \"\"\"\n    # Extract corners of all bboxes\n    x_min, y_min, x_max, y_max = bboxes[:, 0], bboxes[:, 1], bboxes[:, 2], bboxes[:, 3]\n    corners = np.array([[x_min, y_min], [x_max, y_min], [x_max, y_max], [x_min, y_max]]).transpose(\n        2,\n        0,\n        1,\n    )  # Shape: (num_bboxes, 4, 2)\n\n    # Transform all corners at once\n    transformed_corners = skimage.transform.matrix_transform(corners.reshape(-1, 2), matrix.params)\n    transformed_corners = transformed_corners.reshape(-1, 4, 2)\n\n    # Compute new bounding boxes\n    new_x_min = np.min(transformed_corners[:, :, 0], axis=1)\n    new_x_max = np.max(transformed_corners[:, :, 0], axis=1)\n    new_y_min = np.min(transformed_corners[:, :, 1], axis=1)\n    new_y_max = np.max(transformed_corners[:, :, 1], axis=1)\n\n    return np.column_stack([new_x_min, new_y_min, new_x_max, new_y_max, bboxes[:, 4:]])\n</code></pre>"},{"location":"api_reference/augmentations/geometric/functional/#albumentations.augmentations.geometric.functional.calculate_affine_transform_padding","title":"<code>def calculate_affine_transform_padding    (matrix, image_shape)    </code> [view source on GitHub]","text":"<p>Calculate the necessary padding for an affine transformation to avoid empty spaces.</p> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>def calculate_affine_transform_padding(\n    matrix: skimage.transform.ProjectiveTransform,\n    image_shape: Sequence[int],\n) -&gt; tuple[int, int, int, int]:\n    \"\"\"Calculate the necessary padding for an affine transformation to avoid empty spaces.\"\"\"\n    height, width = image_shape[:2]\n\n    # Check for identity transform\n    if is_identity_matrix(matrix):\n        return (0, 0, 0, 0)\n\n    # Original corners\n    corners = np.array([[0, 0], [width, 0], [width, height], [0, height]])\n\n    # Transform corners\n    transformed_corners = matrix(corners)\n\n    # Find box that includes both original and transformed corners\n    all_corners = np.vstack((corners, transformed_corners))\n    min_x, min_y = all_corners.min(axis=0)\n    max_x, max_y = all_corners.max(axis=0)\n    # Compute the inverse transform\n    inverse_matrix = matrix.inverse\n\n    # Apply inverse transform to all corners of the bounding box\n    bbox_corners = np.array([[min_x, min_y], [max_x, min_y], [max_x, max_y], [min_x, max_y]])\n\n    inverse_corners = inverse_matrix(bbox_corners)\n\n    min_x, min_y = inverse_corners.min(axis=0)\n    max_x, max_y = inverse_corners.max(axis=0)\n\n    pad_left = max(0, math.ceil(0 - min_x))\n    pad_right = max(0, math.ceil(max_x - width))\n    pad_top = max(0, math.ceil(0 - min_y))\n    pad_bottom = max(0, math.ceil(max_y - height))\n\n    return pad_left, pad_right, pad_top, pad_bottom\n</code></pre>"},{"location":"api_reference/augmentations/geometric/functional/#albumentations.augmentations.geometric.functional.calculate_grid_dimensions","title":"<code>def calculate_grid_dimensions    (image_shape, num_grid_xy)    </code> [view source on GitHub]","text":"<p>Calculate the dimensions of a grid overlay on an image using vectorized operations.</p> <p>This function divides an image into a grid and calculates the dimensions (x_min, y_min, x_max, y_max) for each cell in the grid without using loops.</p> <p>Parameters:</p> Name Type Description <code>image_shape</code> <code>tuple[int, int]</code> <p>The shape of the image (height, width).</p> <code>num_grid_xy</code> <code>tuple[int, int]</code> <p>The number of grid cells in (x, y) directions.</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>A 3D array of shape (grid_height, grid_width, 4) where each element             is [x_min, y_min, x_max, y_max] for a grid cell.</p> <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; image_shape = (100, 150)\n&gt;&gt;&gt; num_grid_xy = (3, 2)\n&gt;&gt;&gt; dimensions = calculate_grid_dimensions(image_shape, num_grid_xy)\n&gt;&gt;&gt; print(dimensions.shape)\n(2, 3, 4)\n&gt;&gt;&gt; print(dimensions[0, 0])  # First cell\n[  0   0  50  50]\n</code></pre> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>def calculate_grid_dimensions(\n    image_shape: tuple[int, int],\n    num_grid_xy: tuple[int, int],\n) -&gt; np.ndarray:\n    \"\"\"Calculate the dimensions of a grid overlay on an image using vectorized operations.\n\n    This function divides an image into a grid and calculates the dimensions\n    (x_min, y_min, x_max, y_max) for each cell in the grid without using loops.\n\n    Args:\n        image_shape (tuple[int, int]): The shape of the image (height, width).\n        num_grid_xy (tuple[int, int]): The number of grid cells in (x, y) directions.\n\n    Returns:\n        np.ndarray: A 3D array of shape (grid_height, grid_width, 4) where each element\n                    is [x_min, y_min, x_max, y_max] for a grid cell.\n\n    Example:\n        &gt;&gt;&gt; image_shape = (100, 150)\n        &gt;&gt;&gt; num_grid_xy = (3, 2)\n        &gt;&gt;&gt; dimensions = calculate_grid_dimensions(image_shape, num_grid_xy)\n        &gt;&gt;&gt; print(dimensions.shape)\n        (2, 3, 4)\n        &gt;&gt;&gt; print(dimensions[0, 0])  # First cell\n        [  0   0  50  50]\n    \"\"\"\n    num_grid_yx = np.array(num_grid_xy[::-1])  # Reverse to match image_shape order\n    image_shape = np.array(image_shape)\n\n    square_shape = image_shape // num_grid_yx\n    last_square_shape = image_shape - (square_shape * (num_grid_yx - 1))\n\n    grid_width, grid_height = num_grid_xy\n\n    # Create meshgrid for row and column indices\n    col_indices, row_indices = np.meshgrid(np.arange(grid_width), np.arange(grid_height))\n\n    # Calculate x_min and y_min\n    x_min = col_indices * square_shape[1]\n    y_min = row_indices * square_shape[0]\n\n    # Calculate x_max and y_max\n    x_max = np.where(col_indices == grid_width - 1, x_min + last_square_shape[1], x_min + square_shape[1])\n    y_max = np.where(row_indices == grid_height - 1, y_min + last_square_shape[0], y_min + square_shape[0])\n\n    # Stack the dimensions\n    return np.stack([x_min, y_min, x_max, y_max], axis=-1).astype(np.int16)\n</code></pre>"},{"location":"api_reference/augmentations/geometric/functional/#albumentations.augmentations.geometric.functional.compute_transformed_image_bounds","title":"<code>def compute_transformed_image_bounds    (matrix, image_shape)    </code> [view source on GitHub]","text":"<p>Compute the bounds of an image after applying an affine transformation.</p> <p>Parameters:</p> Name Type Description <code>matrix</code> <code>skimage.transform.ProjectiveTransform</code> <p>The affine transformation matrix.</p> <code>image_shape</code> <code>tuple[int, int]</code> <p>The shape of the image as (height, width).</p> <p>Returns:</p> Type Description <code>tuple[np.ndarray, np.ndarray]</code> <p>A tuple containing:     - min_coords: An array with the minimum x and y coordinates.     - max_coords: An array with the maximum x and y coordinates.</p> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>def compute_transformed_image_bounds(\n    matrix: skimage.transform.ProjectiveTransform,\n    image_shape: tuple[int, int],\n) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"Compute the bounds of an image after applying an affine transformation.\n\n    Args:\n        matrix (skimage.transform.ProjectiveTransform): The affine transformation matrix.\n        image_shape (tuple[int, int]): The shape of the image as (height, width).\n\n    Returns:\n        tuple[np.ndarray, np.ndarray]: A tuple containing:\n            - min_coords: An array with the minimum x and y coordinates.\n            - max_coords: An array with the maximum x and y coordinates.\n    \"\"\"\n    height, width = image_shape[:2]\n\n    # Define the corners of the image\n    corners = np.array([[0, 0], [width, 0], [width, height], [0, height]])\n\n    # Transform the corners\n    transformed_corners = matrix(corners)\n\n    # Calculate the bounding box of the transformed corners\n    min_coords = np.floor(transformed_corners.min(axis=0)).astype(int)\n    max_coords = np.ceil(transformed_corners.max(axis=0)).astype(int)\n\n    return min_coords, max_coords\n</code></pre>"},{"location":"api_reference/augmentations/geometric/functional/#albumentations.augmentations.geometric.functional.create_affine_transformation_matrix","title":"<code>def create_affine_transformation_matrix    (translate, shear, scale, rotate, shift)    </code> [view source on GitHub]","text":"<p>Create an affine transformation matrix combining translation, shear, scale, and rotation.</p> <p>This function creates a complex affine transformation by combining multiple transformations in a specific order. The transformations are applied as follows: 1. Shift to top-left: Moves the center of transformation to (0, 0) 2. Apply main transformations: scale, rotation, shear, and translation 3. Shift back to center: Moves the center of transformation back to its original position</p> <p>The order of these transformations is crucial as matrix multiplications are not commutative.</p> <p>Parameters:</p> Name Type Description <code>translate</code> <code>TranslateDict</code> <p>Translation in x and y directions.                        Keys: 'x', 'y'. Values: translation amounts in pixels.</p> <code>shear</code> <code>ShearDict</code> <p>Shear in x and y directions.                Keys: 'x', 'y'. Values: shear angles in degrees.</p> <code>scale</code> <code>ScaleDict</code> <p>Scale factors for x and y directions.                Keys: 'x', 'y'. Values: scale factors (1.0 means no scaling).</p> <code>rotate</code> <code>float</code> <p>Rotation angle in degrees. Positive values rotate counter-clockwise.</p> <code>shift</code> <code>tuple[float, float]</code> <p>Shift to apply before and after transformations.                          Typically the image center (width/2, height/2).</p> <p>Returns:</p> Type Description <code>skimage.transform.ProjectiveTransform</code> <p>The resulting affine transformation matrix.</p> <p>Note</p> <ul> <li>All angle inputs (rotate, shear) are in degrees and are converted to radians internally.</li> <li>The order of transformations in the AffineTransform is: scale, rotation, shear, translation.</li> <li>The resulting transformation can be applied to coordinates using the call method.</li> </ul> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>def create_affine_transformation_matrix(\n    translate: TranslateDict,\n    shear: ShearDict,\n    scale: ScaleDict,\n    rotate: float,\n    shift: tuple[float, float],\n) -&gt; skimage.transform.ProjectiveTransform:\n    \"\"\"Create an affine transformation matrix combining translation, shear, scale, and rotation.\n\n    This function creates a complex affine transformation by combining multiple transformations\n    in a specific order. The transformations are applied as follows:\n    1. Shift to top-left: Moves the center of transformation to (0, 0)\n    2. Apply main transformations: scale, rotation, shear, and translation\n    3. Shift back to center: Moves the center of transformation back to its original position\n\n    The order of these transformations is crucial as matrix multiplications are not commutative.\n\n    Args:\n        translate (TranslateDict): Translation in x and y directions.\n                                   Keys: 'x', 'y'. Values: translation amounts in pixels.\n        shear (ShearDict): Shear in x and y directions.\n                           Keys: 'x', 'y'. Values: shear angles in degrees.\n        scale (ScaleDict): Scale factors for x and y directions.\n                           Keys: 'x', 'y'. Values: scale factors (1.0 means no scaling).\n        rotate (float): Rotation angle in degrees. Positive values rotate counter-clockwise.\n        shift (tuple[float, float]): Shift to apply before and after transformations.\n                                     Typically the image center (width/2, height/2).\n\n    Returns:\n        skimage.transform.ProjectiveTransform: The resulting affine transformation matrix.\n\n    Note:\n        - All angle inputs (rotate, shear) are in degrees and are converted to radians internally.\n        - The order of transformations in the AffineTransform is: scale, rotation, shear, translation.\n        - The resulting transformation can be applied to coordinates using the __call__ method.\n    \"\"\"\n    # Step 1: Create matrix to shift to top-left\n    # This moves the center of transformation to (0, 0)\n    matrix_to_topleft = skimage.transform.SimilarityTransform(translation=[shift[0], shift[1]])\n\n    # Step 2: Create matrix for main transformations\n    # This includes scaling, translation, rotation, and x-shear\n    matrix_transforms = skimage.transform.AffineTransform(\n        scale=(scale[\"x\"], scale[\"y\"]),\n        rotation=np.deg2rad(rotate),\n        shear=(np.deg2rad(shear[\"x\"]), np.deg2rad(shear[\"y\"])),  # Both x and y shear\n        translation=(translate[\"x\"], translate[\"y\"]),\n    )\n\n    # Step 3: Create matrix to shift back to center\n    # This is the inverse of the top-left shift\n    matrix_to_center = matrix_to_topleft.inverse\n\n    # Combine all transformations\n    # The order is important: transformations are applied from right to left\n    return (\n        matrix_to_center  # 3. Shift back to original center\n        + matrix_transforms  # 2. Apply main transformations\n        + matrix_to_topleft  # 1. Shift to top-left\n    )\n</code></pre>"},{"location":"api_reference/augmentations/geometric/functional/#albumentations.augmentations.geometric.functional.d4","title":"<code>def d4    (img, group_member)    </code> [view source on GitHub]","text":"<p>Applies a <code>D_4</code> symmetry group transformation to an image array.</p> <p>This function manipulates an image using transformations such as rotations and flips, corresponding to the <code>D_4</code> dihedral group symmetry operations. Each transformation is identified by a unique group member code.</p> <ul> <li>img (np.ndarray): The input image array to transform.</li> <li>group_member (D4Type): A string identifier indicating the specific transformation to apply. Valid codes include:</li> <li>'e': Identity (no transformation).</li> <li>'r90': Rotate 90 degrees counterclockwise.</li> <li>'r180': Rotate 180 degrees.</li> <li>'r270': Rotate 270 degrees counterclockwise.</li> <li>'v': Vertical flip.</li> <li>'hvt': Transpose over second diagonal</li> <li>'h': Horizontal flip.</li> <li>'t': Transpose (reflect over the main diagonal).</li> </ul> <ul> <li>np.ndarray: The transformed image array.</li> </ul> <ul> <li>ValueError: If an invalid group member is specified.</li> </ul> <p>Examples:</p> <ul> <li>Rotating an image by 90 degrees:   <code>transformed_image = d4(original_image, 'r90')</code></li> <li>Applying a horizontal flip to an image:   <code>transformed_image = d4(original_image, 'h')</code></li> </ul> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>def d4(img: np.ndarray, group_member: D4Type) -&gt; np.ndarray:\n    \"\"\"Applies a `D_4` symmetry group transformation to an image array.\n\n    This function manipulates an image using transformations such as rotations and flips,\n    corresponding to the `D_4` dihedral group symmetry operations.\n    Each transformation is identified by a unique group member code.\n\n    Parameters:\n    - img (np.ndarray): The input image array to transform.\n    - group_member (D4Type): A string identifier indicating the specific transformation to apply. Valid codes include:\n      - 'e': Identity (no transformation).\n      - 'r90': Rotate 90 degrees counterclockwise.\n      - 'r180': Rotate 180 degrees.\n      - 'r270': Rotate 270 degrees counterclockwise.\n      - 'v': Vertical flip.\n      - 'hvt': Transpose over second diagonal\n      - 'h': Horizontal flip.\n      - 't': Transpose (reflect over the main diagonal).\n\n    Returns:\n    - np.ndarray: The transformed image array.\n\n    Raises:\n    - ValueError: If an invalid group member is specified.\n\n    Examples:\n    - Rotating an image by 90 degrees:\n      `transformed_image = d4(original_image, 'r90')`\n    - Applying a horizontal flip to an image:\n      `transformed_image = d4(original_image, 'h')`\n    \"\"\"\n    transformations = {\n        \"e\": lambda x: x,  # Identity transformation\n        \"r90\": lambda x: rot90(x, 1),  # Rotate 90 degrees\n        \"r180\": lambda x: rot90(x, 2),  # Rotate 180 degrees\n        \"r270\": lambda x: rot90(x, 3),  # Rotate 270 degrees\n        \"v\": vflip,  # Vertical flip\n        \"hvt\": lambda x: transpose(rot90(x, 2)),  # Reflect over anti-diagonal\n        \"h\": hflip,  # Horizontal flip\n        \"t\": transpose,  # Transpose (reflect over main diagonal)\n    }\n\n    # Execute the appropriate transformation\n    if group_member in transformations:\n        return transformations[group_member](img)\n\n    raise ValueError(f\"Invalid group member: {group_member}\")\n</code></pre>"},{"location":"api_reference/augmentations/geometric/functional/#albumentations.augmentations.geometric.functional.distort_image","title":"<code>def distort_image    (image, generated_mesh, interpolation)    </code> [view source on GitHub]","text":"<p>Apply perspective distortion to an image based on a generated mesh.</p> <p>This function applies a perspective transformation to each cell of the image defined by the generated mesh. The distortion is applied using OpenCV's perspective transformation and blending techniques.</p> <p>Parameters:</p> Name Type Description <code>image</code> <code>np.ndarray</code> <p>The input image to be distorted. Can be a 2D grayscale image or a                 3D color image.</p> <code>generated_mesh</code> <code>np.ndarray</code> <p>A 2D array where each row represents a quadrilateral cell                         as [x1, y1, x2, y2, dst_x1, dst_y1, dst_x2, dst_y2, dst_x3, dst_y3, dst_x4, dst_y4].                         The first four values define the source rectangle, and the last eight values                         define the destination quadrilateral.</p> <code>interpolation</code> <code>int</code> <p>Interpolation method to be used in the perspective transformation.                  Should be one of the OpenCV interpolation flags (e.g., cv2.INTER_LINEAR).</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>The distorted image with the same shape and dtype as the input image.</p> <p>Note</p> <ul> <li>The function preserves the channel dimension of the input image.</li> <li>Each cell of the generated mesh is transformed independently and then blended into the output image.</li> <li>The distortion is applied using perspective transformation, which allows for more complex   distortions compared to affine transformations.</li> </ul> <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; image = np.random.randint(0, 255, (100, 100, 3), dtype=np.uint8)\n&gt;&gt;&gt; mesh = np.array([[0, 0, 50, 50, 5, 5, 45, 5, 45, 45, 5, 45]])\n&gt;&gt;&gt; distorted = distort_image(image, mesh, cv2.INTER_LINEAR)\n&gt;&gt;&gt; distorted.shape\n(100, 100, 3)\n</code></pre> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>@preserve_channel_dim\ndef distort_image(image: np.ndarray, generated_mesh: np.ndarray, interpolation: int) -&gt; np.ndarray:\n    \"\"\"Apply perspective distortion to an image based on a generated mesh.\n\n    This function applies a perspective transformation to each cell of the image defined by the\n    generated mesh. The distortion is applied using OpenCV's perspective transformation and\n    blending techniques.\n\n    Args:\n        image (np.ndarray): The input image to be distorted. Can be a 2D grayscale image or a\n                            3D color image.\n        generated_mesh (np.ndarray): A 2D array where each row represents a quadrilateral cell\n                                    as [x1, y1, x2, y2, dst_x1, dst_y1, dst_x2, dst_y2, dst_x3, dst_y3, dst_x4, dst_y4].\n                                    The first four values define the source rectangle, and the last eight values\n                                    define the destination quadrilateral.\n        interpolation (int): Interpolation method to be used in the perspective transformation.\n                             Should be one of the OpenCV interpolation flags (e.g., cv2.INTER_LINEAR).\n\n    Returns:\n        np.ndarray: The distorted image with the same shape and dtype as the input image.\n\n    Note:\n        - The function preserves the channel dimension of the input image.\n        - Each cell of the generated mesh is transformed independently and then blended into the output image.\n        - The distortion is applied using perspective transformation, which allows for more complex\n          distortions compared to affine transformations.\n\n    Example:\n        &gt;&gt;&gt; image = np.random.randint(0, 255, (100, 100, 3), dtype=np.uint8)\n        &gt;&gt;&gt; mesh = np.array([[0, 0, 50, 50, 5, 5, 45, 5, 45, 45, 5, 45]])\n        &gt;&gt;&gt; distorted = distort_image(image, mesh, cv2.INTER_LINEAR)\n        &gt;&gt;&gt; distorted.shape\n        (100, 100, 3)\n    \"\"\"\n    distorted_image = np.zeros_like(image)\n\n    for mesh in generated_mesh:\n        # Extract source rectangle and destination quadrilateral\n        x1, y1, x2, y2 = mesh[:4]  # Source rectangle\n        dst_quad = mesh[4:].reshape(4, 2)  # Destination quadrilateral\n\n        # Convert source rectangle to quadrilateral\n        src_quad = np.array(\n            [\n                [x1, y1],  # Top-left\n                [x2, y1],  # Top-right\n                [x2, y2],  # Bottom-right\n                [x1, y2],  # Bottom-left\n            ],\n            dtype=np.float32,\n        )\n\n        # Calculate Perspective transformation matrix\n        perspective_mat = cv2.getPerspectiveTransform(src_quad, dst_quad)\n\n        # Apply Perspective transformation\n        warped = cv2.warpPerspective(image, perspective_mat, (image.shape[1], image.shape[0]), flags=interpolation)\n\n        # Create mask for the transformed region\n        mask = np.zeros(image.shape[:2], dtype=np.uint8)\n        cv2.fillConvexPoly(mask, np.int32(dst_quad), 255)\n\n        # Copy only the warped quadrilateral area to the output image\n        distorted_image = cv2.copyTo(warped, mask, distorted_image)\n\n    return distorted_image\n</code></pre>"},{"location":"api_reference/augmentations/geometric/functional/#albumentations.augmentations.geometric.functional.elastic_transform","title":"<code>def elastic_transform    (img, alpha, sigma, interpolation, border_mode, value=None, random_state=None, approximate=False, same_dxdy=False)    </code> [view source on GitHub]","text":"<p>Apply an elastic transformation to an image.</p> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>@preserve_channel_dim\ndef elastic_transform(\n    img: np.ndarray,\n    alpha: float,\n    sigma: float,\n    interpolation: int,\n    border_mode: int,\n    value: ColorType | None = None,\n    random_state: np.random.RandomState | None = None,\n    approximate: bool = False,\n    same_dxdy: bool = False,\n) -&gt; np.ndarray:\n    \"\"\"Apply an elastic transformation to an image.\"\"\"\n    if approximate:\n        return elastic_transform_approximate(\n            img,\n            alpha,\n            sigma,\n            interpolation,\n            border_mode,\n            value,\n            random_state,\n            same_dxdy,\n        )\n    return elastic_transform_precise(\n        img,\n        alpha,\n        sigma,\n        interpolation,\n        border_mode,\n        value,\n        random_state,\n        same_dxdy,\n    )\n</code></pre>"},{"location":"api_reference/augmentations/geometric/functional/#albumentations.augmentations.geometric.functional.elastic_transform_approximate","title":"<code>def elastic_transform_approximate    (img, alpha, sigma, interpolation, border_mode, value, random_state, same_dxdy=False)    </code> [view source on GitHub]","text":"<p>Apply an approximate elastic transformation to an image.</p> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>def elastic_transform_approximate(\n    img: np.ndarray,\n    alpha: float,\n    sigma: float,\n    interpolation: int,\n    border_mode: int,\n    value: ColorType | None,\n    random_state: np.random.RandomState | None,\n    same_dxdy: bool = False,\n) -&gt; np.ndarray:\n    \"\"\"Apply an approximate elastic transformation to an image.\"\"\"\n    return elastic_transform_helper(\n        img,\n        alpha,\n        sigma,\n        interpolation,\n        border_mode,\n        value,\n        random_state,\n        same_dxdy,\n        kernel_size=(17, 17),\n    )\n</code></pre>"},{"location":"api_reference/augmentations/geometric/functional/#albumentations.augmentations.geometric.functional.elastic_transform_precise","title":"<code>def elastic_transform_precise    (img, alpha, sigma, interpolation, border_mode, value, random_state, same_dxdy=False)    </code> [view source on GitHub]","text":"<p>Apply a precise elastic transformation to an image.</p> <p>This function applies an elastic deformation to the input image using a precise method. The transformation involves creating random displacement fields, smoothing them using Gaussian blur with adaptive kernel size, and then remapping the image according to the smoothed displacement fields.</p> <p>Parameters:</p> Name Type Description <code>img</code> <code>np.ndarray</code> <p>Input image.</p> <code>alpha</code> <code>float</code> <p>Scaling factor for the random displacement fields.</p> <code>sigma</code> <code>float</code> <p>Standard deviation for Gaussian blur applied to the displacement fields.</p> <code>interpolation</code> <code>int</code> <p>Interpolation method to be used (e.g., cv2.INTER_LINEAR).</p> <code>border_mode</code> <code>int</code> <p>Pixel extrapolation method (e.g., cv2.BORDER_CONSTANT).</p> <code>value</code> <code>ColorType | None</code> <p>Border value if border_mode is cv2.BORDER_CONSTANT.</p> <code>random_state</code> <code>np.random.RandomState | None</code> <p>Random state for reproducibility.</p> <code>same_dxdy</code> <code>bool</code> <p>If True, use the same displacement field for both x and y directions.</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Transformed image with precise elastic deformation applied.</p> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>def elastic_transform_precise(\n    img: np.ndarray,\n    alpha: float,\n    sigma: float,\n    interpolation: int,\n    border_mode: int,\n    value: ColorType | None,\n    random_state: np.random.RandomState | None,\n    same_dxdy: bool = False,\n) -&gt; np.ndarray:\n    \"\"\"Apply a precise elastic transformation to an image.\n\n    This function applies an elastic deformation to the input image using a precise method.\n    The transformation involves creating random displacement fields, smoothing them using Gaussian\n    blur with adaptive kernel size, and then remapping the image according to the smoothed displacement fields.\n\n    Args:\n        img (np.ndarray): Input image.\n        alpha (float): Scaling factor for the random displacement fields.\n        sigma (float): Standard deviation for Gaussian blur applied to the displacement fields.\n        interpolation (int): Interpolation method to be used (e.g., cv2.INTER_LINEAR).\n        border_mode (int): Pixel extrapolation method (e.g., cv2.BORDER_CONSTANT).\n        value (ColorType | None): Border value if border_mode is cv2.BORDER_CONSTANT.\n        random_state (np.random.RandomState | None): Random state for reproducibility.\n        same_dxdy (bool, optional): If True, use the same displacement field for both x and y directions.\n\n    Returns:\n        np.ndarray: Transformed image with precise elastic deformation applied.\n    \"\"\"\n    return elastic_transform_helper(\n        img,\n        alpha,\n        sigma,\n        interpolation,\n        border_mode,\n        value,\n        random_state,\n        same_dxdy,\n        kernel_size=(0, 0),\n    )\n</code></pre>"},{"location":"api_reference/augmentations/geometric/functional/#albumentations.augmentations.geometric.functional.find_keypoint","title":"<code>def find_keypoint    (position, distance_map, threshold, inverted)    </code> [view source on GitHub]","text":"<p>Determine if a valid keypoint can be found at the given position.</p> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>def find_keypoint(\n    position: tuple[int, int],\n    distance_map: np.ndarray,\n    threshold: float | None,\n    inverted: bool,\n) -&gt; tuple[float, float] | None:\n    \"\"\"Determine if a valid keypoint can be found at the given position.\"\"\"\n    y, x = position\n    value = distance_map[y, x]\n    if not inverted and threshold is not None and value &gt;= threshold:\n        return None\n    if inverted and threshold is not None and value &lt; threshold:\n        return None\n    return float(x), float(y)\n</code></pre>"},{"location":"api_reference/augmentations/geometric/functional/#albumentations.augmentations.geometric.functional.flip_bboxes","title":"<code>def flip_bboxes    (bboxes, flip_horizontal=False, flip_vertical=False, image_shape=(0, 0))    </code> [view source on GitHub]","text":"<p>Flip bounding boxes horizontally and/or vertically.</p> <p>Parameters:</p> Name Type Description <code>bboxes</code> <code>np.ndarray</code> <p>Array of bounding boxes with shape (n, m) where each row is [x_min, y_min, x_max, y_max, ...].</p> <code>flip_horizontal</code> <code>bool</code> <p>Whether to flip horizontally.</p> <code>flip_vertical</code> <code>bool</code> <p>Whether to flip vertically.</p> <code>image_shape</code> <code>tuple[int, int]</code> <p>Shape of the image as (height, width).</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Flipped bounding boxes.</p> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>def flip_bboxes(\n    bboxes: np.ndarray,\n    flip_horizontal: bool = False,\n    flip_vertical: bool = False,\n    image_shape: tuple[int, int] = (0, 0),\n) -&gt; np.ndarray:\n    \"\"\"Flip bounding boxes horizontally and/or vertically.\n\n    Args:\n        bboxes (np.ndarray): Array of bounding boxes with shape (n, m) where each row is\n            [x_min, y_min, x_max, y_max, ...].\n        flip_horizontal (bool): Whether to flip horizontally.\n        flip_vertical (bool): Whether to flip vertically.\n        image_shape (tuple[int, int]): Shape of the image as (height, width).\n\n    Returns:\n        np.ndarray: Flipped bounding boxes.\n    \"\"\"\n    rows, cols = image_shape[:2]\n    flipped_bboxes = bboxes.copy()\n    if flip_horizontal:\n        flipped_bboxes[:, [0, 2]] = cols - flipped_bboxes[:, [2, 0]]\n    if flip_vertical:\n        flipped_bboxes[:, [1, 3]] = rows - flipped_bboxes[:, [3, 1]]\n    return flipped_bboxes\n</code></pre>"},{"location":"api_reference/augmentations/geometric/functional/#albumentations.augmentations.geometric.functional.from_distance_maps","title":"<code>def from_distance_maps    (distance_maps, inverted, if_not_found_coords, threshold)    </code> [view source on GitHub]","text":"<p>Convert outputs of <code>to_distance_maps</code> to <code>KeypointsOnImage</code>. This is the inverse of <code>to_distance_maps</code>.</p> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>def from_distance_maps(\n    distance_maps: np.ndarray,\n    inverted: bool,\n    if_not_found_coords: Sequence[int] | dict[str, Any] | None,\n    threshold: float | None,\n) -&gt; list[tuple[float, float]]:\n    \"\"\"Convert outputs of `to_distance_maps` to `KeypointsOnImage`.\n    This is the inverse of `to_distance_maps`.\n    \"\"\"\n    if distance_maps.ndim != NUM_MULTI_CHANNEL_DIMENSIONS:\n        msg = f\"Expected three-dimensional input, got {distance_maps.ndim} dimensions and shape {distance_maps.shape}.\"\n        raise ValueError(msg)\n    height, width, nb_keypoints = distance_maps.shape\n\n    drop_if_not_found, if_not_found_x, if_not_found_y = validate_if_not_found_coords(if_not_found_coords)\n\n    keypoints = []\n    for i in range(nb_keypoints):\n        hitidx_flat = np.argmax(distance_maps[..., i]) if inverted else np.argmin(distance_maps[..., i])\n        hitidx_ndim = np.unravel_index(hitidx_flat, (height, width))\n        keypoint = find_keypoint(hitidx_ndim, distance_maps[:, :, i], threshold, inverted)\n        if keypoint:\n            keypoints.append(keypoint)\n        elif not drop_if_not_found:\n            keypoints.append((if_not_found_x, if_not_found_y))\n\n    return keypoints\n</code></pre>"},{"location":"api_reference/augmentations/geometric/functional/#albumentations.augmentations.geometric.functional.generate_distorted_grid_polygons","title":"<code>def generate_distorted_grid_polygons    (dimensions, magnitude)    </code> [view source on GitHub]","text":"<p>Generate distorted grid polygons based on input dimensions and magnitude.</p> <p>This function creates a grid of polygons and applies random distortions to the internal vertices, while keeping the boundary vertices fixed. The distortion is applied consistently across shared vertices to avoid gaps or overlaps in the resulting grid.</p> <p>Parameters:</p> Name Type Description <code>dimensions</code> <code>np.ndarray</code> <p>A 3D array of shape (grid_height, grid_width, 4) where each element                      is [x_min, y_min, x_max, y_max] representing the dimensions of a grid cell.</p> <code>magnitude</code> <code>int</code> <p>Maximum pixel-wise displacement for distortion. The actual displacement              will be randomly chosen in the range [-magnitude, magnitude].</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>A 2D array of shape (total_cells, 8) where each row represents a distorted polygon             as [x1, y1, x2, y1, x2, y2, x1, y2]. The total_cells is equal to grid_height * grid_width.</p> <p>Note</p> <ul> <li>Only internal grid points are distorted; boundary points remain fixed.</li> <li>The function ensures consistent distortion across shared vertices of adjacent cells.</li> <li>The distortion is applied to the following points of each internal cell:<ul> <li>Bottom-right of the cell above and to the left</li> <li>Bottom-left of the cell above</li> <li>Top-right of the cell to the left</li> <li>Top-left of the current cell</li> </ul> </li> <li>Each square represents a cell, and the X marks indicate the coordinates where displacement occurs.     +--+--+--+--+     |  |  |  |  |     +--X--X--X--+     |  |  |  |  |     +--X--X--X--+     |  |  |  |  |     +--X--X--X--+     |  |  |  |  |     +--+--+--+--+</li> <li>For each X, the coordinates of the left, right, top, and bottom edges   in the four adjacent cells are displaced.</li> </ul> <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; dimensions = np.array([[[0, 0, 50, 50], [50, 0, 100, 50]],\n...                        [[0, 50, 50, 100], [50, 50, 100, 100]]])\n&gt;&gt;&gt; distorted = generate_distorted_grid_polygons(dimensions, magnitude=10)\n&gt;&gt;&gt; distorted.shape\n(4, 8)\n</code></pre> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>def generate_distorted_grid_polygons(\n    dimensions: np.ndarray,\n    magnitude: int,\n) -&gt; np.ndarray:\n    \"\"\"Generate distorted grid polygons based on input dimensions and magnitude.\n\n    This function creates a grid of polygons and applies random distortions to the internal vertices,\n    while keeping the boundary vertices fixed. The distortion is applied consistently across shared\n    vertices to avoid gaps or overlaps in the resulting grid.\n\n    Args:\n        dimensions (np.ndarray): A 3D array of shape (grid_height, grid_width, 4) where each element\n                                 is [x_min, y_min, x_max, y_max] representing the dimensions of a grid cell.\n        magnitude (int): Maximum pixel-wise displacement for distortion. The actual displacement\n                         will be randomly chosen in the range [-magnitude, magnitude].\n\n    Returns:\n        np.ndarray: A 2D array of shape (total_cells, 8) where each row represents a distorted polygon\n                    as [x1, y1, x2, y1, x2, y2, x1, y2]. The total_cells is equal to grid_height * grid_width.\n\n    Note:\n        - Only internal grid points are distorted; boundary points remain fixed.\n        - The function ensures consistent distortion across shared vertices of adjacent cells.\n        - The distortion is applied to the following points of each internal cell:\n            * Bottom-right of the cell above and to the left\n            * Bottom-left of the cell above\n            * Top-right of the cell to the left\n            * Top-left of the current cell\n        - Each square represents a cell, and the X marks indicate the coordinates where displacement occurs.\n            +--+--+--+--+\n            |  |  |  |  |\n            +--X--X--X--+\n            |  |  |  |  |\n            +--X--X--X--+\n            |  |  |  |  |\n            +--X--X--X--+\n            |  |  |  |  |\n            +--+--+--+--+\n        - For each X, the coordinates of the left, right, top, and bottom edges\n          in the four adjacent cells are displaced.\n\n    Example:\n        &gt;&gt;&gt; dimensions = np.array([[[0, 0, 50, 50], [50, 0, 100, 50]],\n        ...                        [[0, 50, 50, 100], [50, 50, 100, 100]]])\n        &gt;&gt;&gt; distorted = generate_distorted_grid_polygons(dimensions, magnitude=10)\n        &gt;&gt;&gt; distorted.shape\n        (4, 8)\n    \"\"\"\n    grid_height, grid_width = dimensions.shape[:2]\n    total_cells = grid_height * grid_width\n\n    # Initialize polygons\n    polygons = np.zeros((total_cells, 8), dtype=np.float32)\n    polygons[:, 0:2] = dimensions.reshape(-1, 4)[:, [0, 1]]  # x1, y1\n    polygons[:, 2:4] = dimensions.reshape(-1, 4)[:, [2, 1]]  # x2, y1\n    polygons[:, 4:6] = dimensions.reshape(-1, 4)[:, [2, 3]]  # x2, y2\n    polygons[:, 6:8] = dimensions.reshape(-1, 4)[:, [0, 3]]  # x1, y2\n\n    # Generate displacements for internal grid points only\n    internal_points_height, internal_points_width = grid_height - 1, grid_width - 1\n    displacements = random_utils.randint(\n        -magnitude,\n        magnitude + 1,\n        size=(internal_points_height, internal_points_width, 2),\n    ).astype(np.float32)\n\n    # Apply displacements to internal polygon vertices\n    for i in range(1, grid_height):\n        for j in range(1, grid_width):\n            dx, dy = displacements[i - 1, j - 1]\n\n            # Bottom-right of cell (i-1, j-1)\n            polygons[(i - 1) * grid_width + (j - 1), 4:6] += [dx, dy]\n\n            # Bottom-left of cell (i-1, j)\n            polygons[(i - 1) * grid_width + j, 6:8] += [dx, dy]\n\n            # Top-right of cell (i, j-1)\n            polygons[i * grid_width + (j - 1), 2:4] += [dx, dy]\n\n            # Top-left of cell (i, j)\n            polygons[i * grid_width + j, 0:2] += [dx, dy]\n\n    return polygons\n</code></pre>"},{"location":"api_reference/augmentations/geometric/functional/#albumentations.augmentations.geometric.functional.generate_reflected_bboxes","title":"<code>def generate_reflected_bboxes    (bboxes, grid_dims, image_shape, center_in_origin=False)    </code> [view source on GitHub]","text":"<p>Generate reflected bounding boxes for the entire reflection grid.</p> <p>Parameters:</p> Name Type Description <code>bboxes</code> <code>np.ndarray</code> <p>Original bounding boxes.</p> <code>grid_dims</code> <code>dict[str, tuple[int, int]]</code> <p>Grid dimensions and original position.</p> <code>image_shape</code> <code>tuple[int, int]</code> <p>Shape of the original image as (height, width).</p> <code>center_in_origin</code> <code>bool</code> <p>If True, center the grid at the origin. Default is False.</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Array of reflected and shifted bounding boxes for the entire grid.</p> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>def generate_reflected_bboxes(\n    bboxes: np.ndarray,\n    grid_dims: dict[str, tuple[int, int]],\n    image_shape: tuple[int, int],\n    center_in_origin: bool = False,\n) -&gt; np.ndarray:\n    \"\"\"Generate reflected bounding boxes for the entire reflection grid.\n\n    Args:\n        bboxes (np.ndarray): Original bounding boxes.\n        grid_dims (dict[str, tuple[int, int]]): Grid dimensions and original position.\n        image_shape (tuple[int, int]): Shape of the original image as (height, width).\n        center_in_origin (bool): If True, center the grid at the origin. Default is False.\n\n    Returns:\n        np.ndarray: Array of reflected and shifted bounding boxes for the entire grid.\n    \"\"\"\n    rows, cols = image_shape[:2]\n    grid_rows, grid_cols = grid_dims[\"grid_shape\"]\n    original_row, original_col = grid_dims[\"original_position\"]\n\n    # Prepare flipped versions of bboxes\n    bboxes_hflipped = flip_bboxes(bboxes, flip_horizontal=True, image_shape=image_shape)\n    bboxes_vflipped = flip_bboxes(bboxes, flip_vertical=True, image_shape=image_shape)\n    bboxes_hvflipped = flip_bboxes(bboxes, flip_horizontal=True, flip_vertical=True, image_shape=image_shape)\n\n    # Shift all versions to the original position\n    shift_vector = np.array([original_col * cols, original_row * rows, original_col * cols, original_row * rows])\n    bboxes = shift_bboxes(bboxes, shift_vector)\n    bboxes_hflipped = shift_bboxes(bboxes_hflipped, shift_vector)\n    bboxes_vflipped = shift_bboxes(bboxes_vflipped, shift_vector)\n    bboxes_hvflipped = shift_bboxes(bboxes_hvflipped, shift_vector)\n\n    new_bboxes = []\n\n    for grid_row in range(grid_rows):\n        for grid_col in range(grid_cols):\n            # Determine which version of bboxes to use based on grid position\n            if (grid_row - original_row) % 2 == 0 and (grid_col - original_col) % 2 == 0:\n                current_bboxes = bboxes\n            elif (grid_row - original_row) % 2 == 0:\n                current_bboxes = bboxes_hflipped\n            elif (grid_col - original_col) % 2 == 0:\n                current_bboxes = bboxes_vflipped\n            else:\n                current_bboxes = bboxes_hvflipped\n\n            # Shift to the current grid cell\n            cell_shift = np.array(\n                [\n                    (grid_col - original_col) * cols,\n                    (grid_row - original_row) * rows,\n                    (grid_col - original_col) * cols,\n                    (grid_row - original_row) * rows,\n                ],\n            )\n            shifted_bboxes = shift_bboxes(current_bboxes, cell_shift)\n\n            new_bboxes.append(shifted_bboxes)\n\n    result = np.vstack(new_bboxes)\n\n    return shift_bboxes(result, -shift_vector) if center_in_origin else result\n</code></pre>"},{"location":"api_reference/augmentations/geometric/functional/#albumentations.augmentations.geometric.functional.generate_reflected_keypoints","title":"<code>def generate_reflected_keypoints    (keypoints, grid_dims, image_shape, center_in_origin=False)    </code> [view source on GitHub]","text":"<p>Generate reflected keypoints for the entire reflection grid.</p> <p>This function creates a grid of keypoints by reflecting and shifting the original keypoints. It handles both centered and non-centered grids based on the <code>center_in_origin</code> parameter.</p> <p>Parameters:</p> Name Type Description <code>keypoints</code> <code>np.ndarray</code> <p>Original keypoints array of shape (N, 4+), where N is the number of keypoints,                     and each keypoint is represented by at least 4 values (x, y, angle, scale, ...).</p> <code>grid_dims</code> <code>dict[str, tuple[int, int]]</code> <p>A dictionary containing grid dimensions and original position. It should have the following keys: - \"grid_shape\": tuple[int, int] representing (grid_rows, grid_cols) - \"original_position\": tuple[int, int] representing (original_row, original_col)</p> <code>image_shape</code> <code>tuple[int, int]</code> <p>Shape of the original image as (height, width).</p> <code>center_in_origin</code> <code>bool</code> <p>If True, center the grid at the origin. Default is False.</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Array of reflected and shifted keypoints for the entire grid. The shape is             (N * grid_rows * grid_cols, 4+), where N is the number of original keypoints.</p> <p>Note</p> <ul> <li>The function handles keypoint flipping and shifting to create a grid of reflected keypoints.</li> <li>It preserves the angle and scale information of the keypoints during transformations.</li> <li>The resulting grid can be either centered at the origin or positioned based on the original grid.</li> </ul> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>def generate_reflected_keypoints(\n    keypoints: np.ndarray,\n    grid_dims: dict[str, tuple[int, int]],\n    image_shape: tuple[int, int],\n    center_in_origin: bool = False,\n) -&gt; np.ndarray:\n    \"\"\"Generate reflected keypoints for the entire reflection grid.\n\n    This function creates a grid of keypoints by reflecting and shifting the original keypoints.\n    It handles both centered and non-centered grids based on the `center_in_origin` parameter.\n\n    Args:\n        keypoints (np.ndarray): Original keypoints array of shape (N, 4+), where N is the number of keypoints,\n                                and each keypoint is represented by at least 4 values (x, y, angle, scale, ...).\n        grid_dims (dict[str, tuple[int, int]]): A dictionary containing grid dimensions and original position.\n            It should have the following keys:\n            - \"grid_shape\": tuple[int, int] representing (grid_rows, grid_cols)\n            - \"original_position\": tuple[int, int] representing (original_row, original_col)\n        image_shape (tuple[int, int]): Shape of the original image as (height, width).\n        center_in_origin (bool, optional): If True, center the grid at the origin. Default is False.\n\n    Returns:\n        np.ndarray: Array of reflected and shifted keypoints for the entire grid. The shape is\n                    (N * grid_rows * grid_cols, 4+), where N is the number of original keypoints.\n\n    Note:\n        - The function handles keypoint flipping and shifting to create a grid of reflected keypoints.\n        - It preserves the angle and scale information of the keypoints during transformations.\n        - The resulting grid can be either centered at the origin or positioned based on the original grid.\n    \"\"\"\n    grid_rows, grid_cols = grid_dims[\"grid_shape\"]\n    original_row, original_col = grid_dims[\"original_position\"]\n\n    # Prepare flipped versions of keypoints\n    keypoints_hflipped = flip_keypoints(keypoints, flip_horizontal=True, image_shape=image_shape)\n    keypoints_vflipped = flip_keypoints(keypoints, flip_vertical=True, image_shape=image_shape)\n    keypoints_hvflipped = flip_keypoints(keypoints, flip_horizontal=True, flip_vertical=True, image_shape=image_shape)\n\n    rows, cols = image_shape[:2]\n\n    # Shift all versions to the original position\n    shift_vector = np.array([original_col * cols, original_row * rows, 0, 0])  # Only shift x and y\n    keypoints = shift_keypoints(keypoints, shift_vector)\n    keypoints_hflipped = shift_keypoints(keypoints_hflipped, shift_vector)\n    keypoints_vflipped = shift_keypoints(keypoints_vflipped, shift_vector)\n    keypoints_hvflipped = shift_keypoints(keypoints_hvflipped, shift_vector)\n\n    new_keypoints = []\n\n    for grid_row in range(grid_rows):\n        for grid_col in range(grid_cols):\n            # Determine which version of keypoints to use based on grid position\n            if (grid_row - original_row) % 2 == 0 and (grid_col - original_col) % 2 == 0:\n                current_keypoints = keypoints\n            elif (grid_row - original_row) % 2 == 0:\n                current_keypoints = keypoints_hflipped\n            elif (grid_col - original_col) % 2 == 0:\n                current_keypoints = keypoints_vflipped\n            else:\n                current_keypoints = keypoints_hvflipped\n\n            # Shift to the current grid cell\n            cell_shift = np.array([(grid_col - original_col) * cols, (grid_row - original_row) * rows, 0, 0])\n            shifted_keypoints = shift_keypoints(current_keypoints, cell_shift)\n\n            new_keypoints.append(shifted_keypoints)\n\n    result = np.vstack(new_keypoints)\n\n    return shift_keypoints(result, -shift_vector) if center_in_origin else result\n</code></pre>"},{"location":"api_reference/augmentations/geometric/functional/#albumentations.augmentations.geometric.functional.get_pad_grid_dimensions","title":"<code>def get_pad_grid_dimensions    (pad_top, pad_bottom, pad_left, pad_right, image_shape)    </code> [view source on GitHub]","text":"<p>Calculate the dimensions of the grid needed for reflection padding and the position of the original image.</p> <p>Parameters:</p> Name Type Description <code>pad_top</code> <code>int</code> <p>Number of pixels to pad above the image.</p> <code>pad_bottom</code> <code>int</code> <p>Number of pixels to pad below the image.</p> <code>pad_left</code> <code>int</code> <p>Number of pixels to pad to the left of the image.</p> <code>pad_right</code> <code>int</code> <p>Number of pixels to pad to the right of the image.</p> <code>image_shape</code> <code>tuple[int, int]</code> <p>Shape of the original image as (height, width).</p> <p>Returns:</p> Type Description <code>dict[str, tuple[int, int]]</code> <p>A dictionary containing:     - 'grid_shape': A tuple (grid_rows, grid_cols) where:         - grid_rows (int): Number of times the image needs to be repeated vertically.         - grid_cols (int): Number of times the image needs to be repeated horizontally.     - 'original_position': A tuple (original_row, original_col) where:         - original_row (int): Row index of the original image in the grid.         - original_col (int): Column index of the original image in the grid.</p> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>def get_pad_grid_dimensions(\n    pad_top: int,\n    pad_bottom: int,\n    pad_left: int,\n    pad_right: int,\n    image_shape: tuple[int, int],\n) -&gt; dict[str, tuple[int, int]]:\n    \"\"\"Calculate the dimensions of the grid needed for reflection padding and the position of the original image.\n\n    Args:\n        pad_top (int): Number of pixels to pad above the image.\n        pad_bottom (int): Number of pixels to pad below the image.\n        pad_left (int): Number of pixels to pad to the left of the image.\n        pad_right (int): Number of pixels to pad to the right of the image.\n        image_shape (tuple[int, int]): Shape of the original image as (height, width).\n\n    Returns:\n        dict[str, tuple[int, int]]: A dictionary containing:\n            - 'grid_shape': A tuple (grid_rows, grid_cols) where:\n                - grid_rows (int): Number of times the image needs to be repeated vertically.\n                - grid_cols (int): Number of times the image needs to be repeated horizontally.\n            - 'original_position': A tuple (original_row, original_col) where:\n                - original_row (int): Row index of the original image in the grid.\n                - original_col (int): Column index of the original image in the grid.\n    \"\"\"\n    rows, cols = image_shape[:2]\n\n    grid_rows = 1 + math.ceil(pad_top / rows) + math.ceil(pad_bottom / rows)\n    grid_cols = 1 + math.ceil(pad_left / cols) + math.ceil(pad_right / cols)\n    original_row = math.ceil(pad_top / rows)\n    original_col = math.ceil(pad_left / cols)\n\n    return {\"grid_shape\": (grid_rows, grid_cols), \"original_position\": (original_row, original_col)}\n</code></pre>"},{"location":"api_reference/augmentations/geometric/functional/#albumentations.augmentations.geometric.functional.keypoint_d4","title":"<code>def keypoint_d4    (keypoint, group_member, image_shape, ** params)    </code> [view source on GitHub]","text":"<p>Applies a <code>D_4</code> symmetry group transformation to a keypoint.</p> <p>This function adjusts a keypoint's coordinates according to the specified <code>D_4</code> group transformation, which includes rotations and reflections suitable for image processing tasks. These transformations account for the dimensions of the image to ensure the keypoint remains within its boundaries.</p> <ul> <li>keypoint (KeypointInternalType): The keypoint to transform. T     his should be a structure or tuple specifying coordinates     like (x, y, [additional parameters]).</li> <li>group_member (D4Type): A string identifier for the <code>D_4</code> group transformation to apply.     Valid values are 'e', 'r90', 'r180', 'r270', 'v', 'hv', 'h', 't'.</li> <li>image_shape (tuple[int, int]): The shape of the image.</li> <li>params (Any): Not used</li> </ul> <ul> <li>KeypointInternalType: The transformed keypoint.</li> </ul> <ul> <li>ValueError: If an invalid group member is specified, indicating that the specified transformation does not exist.</li> </ul> <p>Examples:</p> <ul> <li>Rotating a keypoint by 90 degrees in a 100x100 image:   <code>keypoint_d4((50, 30), 'r90', 100, 100)</code>   This would move the keypoint from (50, 30) to (70, 50) assuming standard coordinate transformations.</li> </ul> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>def keypoint_d4(\n    keypoint: KeypointInternalType,\n    group_member: D4Type,\n    image_shape: tuple[int, int],\n    **params: Any,\n) -&gt; KeypointInternalType:\n    \"\"\"Applies a `D_4` symmetry group transformation to a keypoint.\n\n    This function adjusts a keypoint's coordinates according to the specified `D_4` group transformation,\n    which includes rotations and reflections suitable for image processing tasks. These transformations account\n    for the dimensions of the image to ensure the keypoint remains within its boundaries.\n\n    Parameters:\n    - keypoint (KeypointInternalType): The keypoint to transform. T\n        his should be a structure or tuple specifying coordinates\n        like (x, y, [additional parameters]).\n    - group_member (D4Type): A string identifier for the `D_4` group transformation to apply.\n        Valid values are 'e', 'r90', 'r180', 'r270', 'v', 'hv', 'h', 't'.\n    - image_shape (tuple[int, int]): The shape of the image.\n    - params (Any): Not used\n\n    Returns:\n    - KeypointInternalType: The transformed keypoint.\n\n    Raises:\n    - ValueError: If an invalid group member is specified, indicating that the specified transformation does not exist.\n\n    Examples:\n    - Rotating a keypoint by 90 degrees in a 100x100 image:\n      `keypoint_d4((50, 30), 'r90', 100, 100)`\n      This would move the keypoint from (50, 30) to (70, 50) assuming standard coordinate transformations.\n    \"\"\"\n    rows, cols = image_shape[:2]\n    transformations = {\n        \"e\": lambda x: x,  # Identity transformation\n        \"r90\": lambda x: keypoint_rot90(x, 1, image_shape),  # Rotate 90 degrees\n        \"r180\": lambda x: keypoint_rot90(x, 2, image_shape),  # Rotate 180 degrees\n        \"r270\": lambda x: keypoint_rot90(x, 3, image_shape),  # Rotate 270 degrees\n        \"v\": lambda x: keypoint_vflip(x, rows),  # Vertical flip\n        \"hvt\": lambda x: keypoint_transpose(keypoint_rot90(x, 2, image_shape)),  # Reflect over anti diagonal\n        \"h\": lambda x: keypoint_hflip(x, cols),  # Horizontal flip\n        \"t\": lambda x: keypoint_transpose(x),  # Transpose (reflect over main diagonal)\n    }\n    # Execute the appropriate transformation\n    if group_member in transformations:\n        return transformations[group_member](keypoint)\n\n    raise ValueError(f\"Invalid group member: {group_member}\")\n</code></pre>"},{"location":"api_reference/augmentations/geometric/functional/#albumentations.augmentations.geometric.functional.keypoint_flip","title":"<code>def keypoint_flip    (keypoint, d, image_shape)    </code> [view source on GitHub]","text":"<p>Flip a keypoint either vertically, horizontally or both depending on the value of <code>d</code>.</p> <p>Parameters:</p> Name Type Description <code>keypoint</code> <code>KeypointInternalType</code> <p>A keypoint <code>(x, y, angle, scale)</code>.</p> <code>d</code> <code>int</code> <p>Number of flip. Must be -1, 0 or 1: * 0 - vertical flip, * 1 - horizontal flip, * -1 - vertical and horizontal flip.</p> <code>image_shape</code> <code>tuple[int, int]</code> <p>A tuple of image shape <code>(height, width, channels)</code>.</p> <p>Returns:</p> Type Description <code>KeypointInternalType</code> <p>A keypoint <code>(x, y, angle, scale)</code>.</p> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>if value of <code>d</code> is not -1, 0 or 1.</p> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>@angle_2pi_range\ndef keypoint_flip(keypoint: KeypointInternalType, d: int, image_shape: tuple[int, int]) -&gt; KeypointInternalType:\n    \"\"\"Flip a keypoint either vertically, horizontally or both depending on the value of `d`.\n\n    Args:\n        keypoint: A keypoint `(x, y, angle, scale)`.\n        d: Number of flip. Must be -1, 0 or 1:\n            * 0 - vertical flip,\n            * 1 - horizontal flip,\n            * -1 - vertical and horizontal flip.\n        image_shape: A tuple of image shape `(height, width, channels)`.\n\n    Returns:\n        A keypoint `(x, y, angle, scale)`.\n\n    Raises:\n        ValueError: if value of `d` is not -1, 0 or 1.\n\n    \"\"\"\n    rows, cols = image_shape[:2]\n\n    if d == 0:\n        keypoint = keypoint_vflip(keypoint, rows)\n    elif d == 1:\n        keypoint = keypoint_hflip(keypoint, cols)\n    elif d == -1:\n        keypoint = keypoint_hflip(keypoint, cols)\n        keypoint = keypoint_vflip(keypoint, rows)\n    else:\n        raise ValueError(f\"Invalid d value {d}. Valid values are -1, 0 and 1\")\n    return keypoint\n</code></pre>"},{"location":"api_reference/augmentations/geometric/functional/#albumentations.augmentations.geometric.functional.keypoint_hflip","title":"<code>def keypoint_hflip    (keypoint, cols)    </code> [view source on GitHub]","text":"<p>Flip a keypoint horizontally around the y-axis.</p> <p>Parameters:</p> Name Type Description <code>keypoint</code> <code>KeypointInternalType</code> <p>A keypoint <code>(x, y, angle, scale)</code>.</p> <code>cols</code> <code>int</code> <p>Image width.</p> <p>Returns:</p> Type Description <code>KeypointInternalType</code> <p>A keypoint <code>(x, y, angle, scale)</code>.</p> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>@angle_2pi_range\ndef keypoint_hflip(keypoint: KeypointInternalType, cols: int) -&gt; KeypointInternalType:\n    \"\"\"Flip a keypoint horizontally around the y-axis.\n\n    Args:\n        keypoint: A keypoint `(x, y, angle, scale)`.\n        cols: Image width.\n\n    Returns:\n        A keypoint `(x, y, angle, scale)`.\n\n    \"\"\"\n    x, y, angle, scale = keypoint[:4]\n    angle = math.pi - angle\n    return (cols - 1) - x, y, angle, scale\n</code></pre>"},{"location":"api_reference/augmentations/geometric/functional/#albumentations.augmentations.geometric.functional.keypoint_rot90","title":"<code>def keypoint_rot90    (keypoint, factor, image_shape, ** params)    </code> [view source on GitHub]","text":"<p>Rotate a keypoint by 90 degrees counter-clockwise (CCW) a specified number of times.</p> <p>Parameters:</p> Name Type Description <code>keypoint</code> <code>KeypointInternalType</code> <p>A keypoint in the format <code>(x, y, angle, scale)</code>.</p> <code>factor</code> <code>int</code> <p>The number of 90 degree CCW rotations to apply. Must be in the range [0, 3].</p> <code>image_shape</code> <code>tuple[int, int]</code> <p>The shape of the image.</p> <code>**params</code> <code>Any</code> <p>Additional parameters.</p> <p>Returns:</p> Type Description <code>KeypointInternalType</code> <p>The rotated keypoint in the format <code>(x, y, angle, scale)</code>.</p> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If the factor is not in the set {0, 1, 2, 3}.</p> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>@angle_2pi_range\ndef keypoint_rot90(\n    keypoint: KeypointInternalType,\n    factor: int,\n    image_shape: tuple[int, int],\n    **params: Any,\n) -&gt; KeypointInternalType:\n    \"\"\"Rotate a keypoint by 90 degrees counter-clockwise (CCW) a specified number of times.\n\n    Args:\n        keypoint (KeypointInternalType): A keypoint in the format `(x, y, angle, scale)`.\n        factor (int): The number of 90 degree CCW rotations to apply. Must be in the range [0, 3].\n        image_shape (tuple[int, int]): The shape of the image.\n        **params: Additional parameters.\n\n    Returns:\n        KeypointInternalType: The rotated keypoint in the format `(x, y, angle, scale)`.\n\n    Raises:\n        ValueError: If the factor is not in the set {0, 1, 2, 3}.\n    \"\"\"\n    x, y, angle, scale = keypoint\n\n    if factor not in {0, 1, 2, 3}:\n        raise ValueError(\"Parameter factor must be in set {0, 1, 2, 3}\")\n\n    rows, cols = image_shape[:2]\n\n    if factor == 1:\n        x, y, angle = y, (cols - 1) - x, angle - math.pi / 2\n    elif factor == ROT90_180_FACTOR:\n        x, y, angle = (cols - 1) - x, (rows - 1) - y, angle - math.pi\n    elif factor == ROT90_270_FACTOR:\n        x, y, angle = (rows - 1) - y, x, angle + math.pi / 2\n\n    return x, y, angle, scale\n</code></pre>"},{"location":"api_reference/augmentations/geometric/functional/#albumentations.augmentations.geometric.functional.keypoint_rotate","title":"<code>def keypoint_rotate    (keypoint, angle, image_shape, ** params)    </code> [view source on GitHub]","text":"<p>Rotate a keypoint by a specified angle.</p> <p>Parameters:</p> Name Type Description <code>keypoint</code> <code>KeypointInternalType</code> <p>A keypoint in the format <code>(x, y, angle, scale)</code>.</p> <code>angle</code> <code>float</code> <p>The angle by which to rotate the keypoint, in degrees.</p> <code>image_shape</code> <code>tuple[int, int]</code> <p>The shape of the image the keypoint belongs to.</p> <code>**params</code> <code>Any</code> <p>Additional parameters.</p> <p>Returns:</p> Type Description <code>KeypointInternalType</code> <p>The rotated keypoint in the format <code>(x, y, angle, scale)</code>.</p> <p>Note</p> <p>The rotation is performed around the center of the image.</p> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>@angle_2pi_range\ndef keypoint_rotate(\n    keypoint: KeypointInternalType,\n    angle: float,\n    image_shape: tuple[int, int],\n    **params: Any,\n) -&gt; KeypointInternalType:\n    \"\"\"Rotate a keypoint by a specified angle.\n\n    Args:\n        keypoint (KeypointInternalType): A keypoint in the format `(x, y, angle, scale)`.\n        angle (float): The angle by which to rotate the keypoint, in degrees.\n        image_shape (tuple[int, int]): The shape of the image the keypoint belongs to.\n        **params: Additional parameters.\n\n    Returns:\n        KeypointInternalType: The rotated keypoint in the format `(x, y, angle, scale)`.\n\n    Note:\n        The rotation is performed around the center of the image.\n    \"\"\"\n    image_center = center(image_shape)\n    matrix = cv2.getRotationMatrix2D(image_center, angle, 1.0)\n    x, y, a, s = keypoint[:4]\n    x, y = cv2.transform(np.array([[[x, y]]]), matrix).squeeze()\n    return x, y, a + math.radians(angle), s\n</code></pre>"},{"location":"api_reference/augmentations/geometric/functional/#albumentations.augmentations.geometric.functional.keypoint_scale","title":"<code>def keypoint_scale    (keypoint, scale_x, scale_y)    </code> [view source on GitHub]","text":"<p>Scales a keypoint by scale_x and scale_y.</p> <p>Parameters:</p> Name Type Description <code>keypoint</code> <code>KeypointInternalType</code> <p>A keypoint <code>(x, y, angle, scale)</code>.</p> <code>scale_x</code> <code>float</code> <p>Scale coefficient x-axis.</p> <code>scale_y</code> <code>float</code> <p>Scale coefficient y-axis.</p> <p>Returns:</p> Type Description <code>KeypointInternalType</code> <p>A keypoint <code>(x, y, angle, scale)</code>.</p> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>def keypoint_scale(keypoint: KeypointInternalType, scale_x: float, scale_y: float) -&gt; KeypointInternalType:\n    \"\"\"Scales a keypoint by scale_x and scale_y.\n\n    Args:\n        keypoint: A keypoint `(x, y, angle, scale)`.\n        scale_x: Scale coefficient x-axis.\n        scale_y: Scale coefficient y-axis.\n\n    Returns:\n        A keypoint `(x, y, angle, scale)`.\n\n    \"\"\"\n    x, y, angle, scale = keypoint[:4]\n    return x * scale_x, y * scale_y, angle, scale * max(scale_x, scale_y)\n</code></pre>"},{"location":"api_reference/augmentations/geometric/functional/#albumentations.augmentations.geometric.functional.keypoint_transpose","title":"<code>def keypoint_transpose    (keypoint)    </code> [view source on GitHub]","text":"<p>Transposes a keypoint along a specified axis: main diagonal</p> <p>Parameters:</p> Name Type Description <code>keypoint</code> <code>KeypointInternalType</code> <p>A keypoint <code>(x, y, angle, scale)</code>.</p> <p>Returns:</p> Type Description <code>KeypointInternalType</code> <p>A transformed keypoint <code>(x, y, angle, scale)</code>.</p> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If axis is not 0 or 1.</p> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>@angle_2pi_range\ndef keypoint_transpose(keypoint: KeypointInternalType) -&gt; KeypointInternalType:\n    \"\"\"Transposes a keypoint along a specified axis: main diagonal\n\n    Args:\n        keypoint: A keypoint `(x, y, angle, scale)`.\n\n    Returns:\n        A transformed keypoint `(x, y, angle, scale)`.\n\n    Raises:\n        ValueError: If axis is not 0 or 1.\n\n    \"\"\"\n    x, y, angle, scale = keypoint[:4]\n\n    # Transpose over the main diagonal: swap x and y.\n    new_x, new_y = y, x\n    # Adjust angle to reflect the coordinate swap.\n    angle = np.pi / 2 - angle if angle &lt;= np.pi else 3 * np.pi / 2 - angle\n\n    return new_x, new_y, angle, scale\n</code></pre>"},{"location":"api_reference/augmentations/geometric/functional/#albumentations.augmentations.geometric.functional.keypoint_vflip","title":"<code>def keypoint_vflip    (keypoint, rows)    </code> [view source on GitHub]","text":"<p>Flip a keypoint vertically around the x-axis.</p> <p>Parameters:</p> Name Type Description <code>keypoint</code> <code>KeypointInternalType</code> <p>A keypoint <code>(x, y, angle, scale)</code>.</p> <code>rows</code> <code>int</code> <p>Image height.</p> <p>Returns:</p> Type Description <code>tuple</code> <p>A keypoint <code>(x, y, angle, scale)</code>.</p> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>@angle_2pi_range\ndef keypoint_vflip(keypoint: KeypointInternalType, rows: int) -&gt; KeypointInternalType:\n    \"\"\"Flip a keypoint vertically around the x-axis.\n\n    Args:\n        keypoint: A keypoint `(x, y, angle, scale)`.\n        rows: Image height.\n\n\n    Returns:\n        tuple: A keypoint `(x, y, angle, scale)`.\n\n    \"\"\"\n    x, y, angle, scale = keypoint[:4]\n    angle = -angle\n    return x, (rows - 1) - y, angle, scale\n</code></pre>"},{"location":"api_reference/augmentations/geometric/functional/#albumentations.augmentations.geometric.functional.keypoints_affine","title":"<code>def keypoints_affine    (keypoints, matrix, image_shape, scale, mode)    </code> [view source on GitHub]","text":"<p>Apply an affine transformation to keypoints.</p> <p>This function transforms keypoints using the given affine transformation matrix. It handles reflection padding if necessary, updates coordinates, angles, and scales.</p> <p>Parameters:</p> Name Type Description <code>keypoints</code> <code>np.ndarray</code> <p>Array of keypoints with shape (N, 4+) where N is the number of keypoints.                     Each keypoint is represented as [x, y, angle, scale, ...].</p> <code>matrix</code> <code>skimage.transform.ProjectiveTransform</code> <p>The affine transformation matrix.</p> <code>image_shape</code> <code>tuple[int, int]</code> <p>Shape of the image (height, width).</p> <code>scale</code> <code>dict[str, Any]</code> <p>Dictionary containing scale factors for x and y directions.                     Expected keys are 'x' and 'y'.</p> <code>mode</code> <code>int</code> <p>Border mode for handling keypoints near image edges.         Use cv2.BORDER_REFLECT_101, cv2.BORDER_REFLECT, etc.</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Transformed keypoints array with the same shape as input.</p> <p>Notes</p> <ul> <li>The function applies reflection padding if the mode is in REFLECT_BORDER_MODES.</li> <li>Coordinates (x, y) are transformed using the affine matrix.</li> <li>Angles are adjusted based on the rotation component of the affine transformation.</li> <li>Scales are multiplied by the maximum of x and y scale factors.</li> <li>The @angle_2pi_range decorator ensures angles remain in the [0, 2\u03c0] range.</li> </ul> <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; keypoints = np.array([[100, 100, 0, 1]])\n&gt;&gt;&gt; matrix = skimage.transform.ProjectiveTransform(...)\n&gt;&gt;&gt; scale = {'x': 1.5, 'y': 1.2}\n&gt;&gt;&gt; transformed_keypoints = keypoints_affine(keypoints, matrix, (480, 640), scale, cv2.BORDER_REFLECT_101)\n</code></pre> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>@angle_2pi_range\ndef keypoints_affine(\n    keypoints: np.ndarray,\n    matrix: skimage.transform.ProjectiveTransform,\n    image_shape: tuple[int, int],\n    scale: dict[str, Any],\n    mode: int,\n) -&gt; np.ndarray:\n    \"\"\"Apply an affine transformation to keypoints.\n\n    This function transforms keypoints using the given affine transformation matrix.\n    It handles reflection padding if necessary, updates coordinates, angles, and scales.\n\n    Args:\n        keypoints (np.ndarray): Array of keypoints with shape (N, 4+) where N is the number of keypoints.\n                                Each keypoint is represented as [x, y, angle, scale, ...].\n        matrix (skimage.transform.ProjectiveTransform): The affine transformation matrix.\n        image_shape (tuple[int, int]): Shape of the image (height, width).\n        scale (dict[str, Any]): Dictionary containing scale factors for x and y directions.\n                                Expected keys are 'x' and 'y'.\n        mode (int): Border mode for handling keypoints near image edges.\n                    Use cv2.BORDER_REFLECT_101, cv2.BORDER_REFLECT, etc.\n\n    Returns:\n        np.ndarray: Transformed keypoints array with the same shape as input.\n\n    Notes:\n        - The function applies reflection padding if the mode is in REFLECT_BORDER_MODES.\n        - Coordinates (x, y) are transformed using the affine matrix.\n        - Angles are adjusted based on the rotation component of the affine transformation.\n        - Scales are multiplied by the maximum of x and y scale factors.\n        - The @angle_2pi_range decorator ensures angles remain in the [0, 2\u03c0] range.\n\n    Example:\n        &gt;&gt;&gt; keypoints = np.array([[100, 100, 0, 1]])\n        &gt;&gt;&gt; matrix = skimage.transform.ProjectiveTransform(...)\n        &gt;&gt;&gt; scale = {'x': 1.5, 'y': 1.2}\n        &gt;&gt;&gt; transformed_keypoints = keypoints_affine(keypoints, matrix, (480, 640), scale, cv2.BORDER_REFLECT_101)\n    \"\"\"\n    keypoints = keypoints.copy().astype(np.float32)\n\n    if is_identity_matrix(matrix):\n        return keypoints\n\n    if mode in REFLECT_BORDER_MODES:\n        # Step 1: Compute affine transform padding\n        pad_left, pad_right, pad_top, pad_bottom = calculate_affine_transform_padding(matrix, image_shape)\n        grid_dimensions = get_pad_grid_dimensions(pad_top, pad_bottom, pad_left, pad_right, image_shape)\n        keypoints = generate_reflected_keypoints(keypoints, grid_dimensions, image_shape, center_in_origin=True)\n\n    # Extract x, y coordinates\n    xy = keypoints[:, :2]\n\n    # Transform x, y coordinates\n    xy_transformed = cv2.transform(xy.reshape(-1, 1, 2), matrix.params[:2]).squeeze()\n\n    # Calculate angle adjustment\n    angle_adjustment = rotation2d_matrix_to_euler_angles(matrix.params[:2], y_up=False)\n\n    # Update angles\n    keypoints[:, 2] = keypoints[:, 2] + angle_adjustment\n\n    # Update scales\n    max_scale = max(scale[\"x\"], scale[\"y\"])\n\n    keypoints[:, 3] *= max_scale\n\n    # Update x, y coordinates\n    keypoints[:, :2] = xy_transformed\n\n    return keypoints\n</code></pre>"},{"location":"api_reference/augmentations/geometric/functional/#albumentations.augmentations.geometric.functional.optical_distortion","title":"<code>def optical_distortion    (img, k, dx, dy, interpolation, border_mode, value=None)    </code> [view source on GitHub]","text":"<p>Barrel / pincushion distortion. Unconventional augment.</p> <p>Reference</p> <p>|  https://stackoverflow.com/questions/6199636/formulas-for-barrel-pincushion-distortion |  https://stackoverflow.com/questions/10364201/image-transformation-in-opencv |  https://stackoverflow.com/questions/2477774/correcting-fisheye-distortion-programmatically |  http://www.coldvision.io/2017/03/02/advanced-lane-finding-using-opencv/</p> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>@preserve_channel_dim\ndef optical_distortion(\n    img: np.ndarray,\n    k: int,\n    dx: int,\n    dy: int,\n    interpolation: int,\n    border_mode: int,\n    value: ColorType | None = None,\n) -&gt; np.ndarray:\n    \"\"\"Barrel / pincushion distortion. Unconventional augment.\n\n    Reference:\n        |  https://stackoverflow.com/questions/6199636/formulas-for-barrel-pincushion-distortion\n        |  https://stackoverflow.com/questions/10364201/image-transformation-in-opencv\n        |  https://stackoverflow.com/questions/2477774/correcting-fisheye-distortion-programmatically\n        |  http://www.coldvision.io/2017/03/02/advanced-lane-finding-using-opencv/\n    \"\"\"\n    height, width = img.shape[:2]\n\n    fx = width\n    fy = height\n\n    cx = width * 0.5 + dx\n    cy = height * 0.5 + dy\n\n    camera_matrix = np.array([[fx, 0, cx], [0, fy, cy], [0, 0, 1]], dtype=np.float32)\n\n    distortion = np.array([k, k, 0, 0, 0], dtype=np.float32)\n    map1, map2 = cv2.initUndistortRectifyMap(camera_matrix, distortion, None, None, (width, height), cv2.CV_32FC1)\n    return cv2.remap(img, map1, map2, interpolation=interpolation, borderMode=border_mode, borderValue=value)\n</code></pre>"},{"location":"api_reference/augmentations/geometric/functional/#albumentations.augmentations.geometric.functional.rotation2d_matrix_to_euler_angles","title":"<code>def rotation2d_matrix_to_euler_angles    (matrix, y_up)    </code> [view source on GitHub]","text":"<p>matrix (np.ndarray): Rotation matrix y_up (bool): is Y axis looks up or down</p> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>def rotation2d_matrix_to_euler_angles(matrix: np.ndarray, y_up: bool) -&gt; float:\n    \"\"\"Args:\n    matrix (np.ndarray): Rotation matrix\n    y_up (bool): is Y axis looks up or down\n\n    \"\"\"\n    if y_up:\n        return np.arctan2(matrix[1, 0], matrix[0, 0])\n    return np.arctan2(-matrix[1, 0], matrix[0, 0])\n</code></pre>"},{"location":"api_reference/augmentations/geometric/functional/#albumentations.augmentations.geometric.functional.shift_bboxes","title":"<code>def shift_bboxes    (bboxes, shift_vector)    </code> [view source on GitHub]","text":"<p>Shift bounding boxes by a given vector.</p> <p>Parameters:</p> Name Type Description <code>bboxes</code> <code>np.ndarray</code> <p>Array of bounding boxes with shape (n, m) where n is the number of bboxes                  and m &gt;= 4. The first 4 columns are [x_min, y_min, x_max, y_max].</p> <code>shift_vector</code> <code>np.ndarray</code> <p>Vector to shift the bounding boxes by, with shape (4,) for                        [shift_x, shift_y, shift_x, shift_y].</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Shifted bounding boxes with the same shape as input.</p> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>def shift_bboxes(bboxes: np.ndarray, shift_vector: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Shift bounding boxes by a given vector.\n\n    Args:\n        bboxes (np.ndarray): Array of bounding boxes with shape (n, m) where n is the number of bboxes\n                             and m &gt;= 4. The first 4 columns are [x_min, y_min, x_max, y_max].\n        shift_vector (np.ndarray): Vector to shift the bounding boxes by, with shape (4,) for\n                                   [shift_x, shift_y, shift_x, shift_y].\n\n    Returns:\n        np.ndarray: Shifted bounding boxes with the same shape as input.\n    \"\"\"\n    # Create a copy of the input array to avoid modifying it in-place\n    shifted_bboxes = bboxes.copy()\n\n    # Add the shift vector to the first 4 columns\n    shifted_bboxes[:, :4] += shift_vector\n\n    return shifted_bboxes\n</code></pre>"},{"location":"api_reference/augmentations/geometric/functional/#albumentations.augmentations.geometric.functional.to_distance_maps","title":"<code>def to_distance_maps    (keypoints, image_shape, inverted=False)    </code> [view source on GitHub]","text":"<p>Generate a <code>(H,W,N)</code> array of distance maps for <code>N</code> keypoints.</p> <p>The <code>n</code>-th distance map contains at every location <code>(y, x)</code> the euclidean distance to the <code>n</code>-th keypoint.</p> <p>This function can be used as a helper when augmenting keypoints with a method that only supports the augmentation of images.</p> <p>Parameters:</p> Name Type Description <code>keypoints</code> <code>Sequence[tuple[float, float]]</code> <p>keypoint coordinates</p> <code>image_shape</code> <code>tuple[int, int]</code> <p>tuple[int, int] shape of the image</p> <code>inverted</code> <code>bool</code> <p>If <code>True</code>, inverted distance maps are returned where each distance value d is replaced by <code>d/(d+1)</code>, i.e. the distance maps have values in the range <code>(0.0, 1.0]</code> with <code>1.0</code> denoting exactly the position of the respective keypoint.</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>(H, W, N) ndarray     A <code>float32</code> array containing <code>N</code> distance maps for <code>N</code>     keypoints. Each location <code>(y, x, n)</code> in the array denotes the     euclidean distance at <code>(y, x)</code> to the <code>n</code>-th keypoint.     If <code>inverted</code> is <code>True</code>, the distance <code>d</code> is replaced     by <code>d/(d+1)</code>. The height and width of the array match the     height and width in <code>KeypointsOnImage.shape</code>.</p> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>def to_distance_maps(\n    keypoints: Sequence[tuple[float, float]],\n    image_shape: tuple[int, int],\n    inverted: bool = False,\n) -&gt; np.ndarray:\n    \"\"\"Generate a ``(H,W,N)`` array of distance maps for ``N`` keypoints.\n\n    The ``n``-th distance map contains at every location ``(y, x)`` the\n    euclidean distance to the ``n``-th keypoint.\n\n    This function can be used as a helper when augmenting keypoints with a\n    method that only supports the augmentation of images.\n\n    Args:\n        keypoints: keypoint coordinates\n        image_shape: tuple[int, int] shape of the image\n        inverted (bool): If ``True``, inverted distance maps are returned where each\n            distance value d is replaced by ``d/(d+1)``, i.e. the distance\n            maps have values in the range ``(0.0, 1.0]`` with ``1.0`` denoting\n            exactly the position of the respective keypoint.\n\n    Returns:\n        (H, W, N) ndarray\n            A ``float32`` array containing ``N`` distance maps for ``N``\n            keypoints. Each location ``(y, x, n)`` in the array denotes the\n            euclidean distance at ``(y, x)`` to the ``n``-th keypoint.\n            If `inverted` is ``True``, the distance ``d`` is replaced\n            by ``d/(d+1)``. The height and width of the array match the\n            height and width in ``KeypointsOnImage.shape``.\n\n    \"\"\"\n    height, width = image_shape[:2]\n    distance_maps = np.zeros((height, width, len(keypoints)), dtype=np.float32)\n\n    yy = np.arange(0, height)\n    xx = np.arange(0, width)\n    grid_xx, grid_yy = np.meshgrid(xx, yy)\n\n    for i, (x, y) in enumerate(keypoints):\n        distance_maps[:, :, i] = (grid_xx - x) ** 2 + (grid_yy - y) ** 2\n\n    distance_maps = np.sqrt(distance_maps)\n    if inverted:\n        return 1 / (distance_maps + 1)\n    return distance_maps\n</code></pre>"},{"location":"api_reference/augmentations/geometric/functional/#albumentations.augmentations.geometric.functional.transpose","title":"<code>def transpose    (img)    </code> [view source on GitHub]","text":"<p>Transposes the first two dimensions of an array of any dimensionality. Retains the order of any additional dimensions.</p> <p>Parameters:</p> Name Type Description <code>img</code> <code>np.ndarray</code> <p>Input array.</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Transposed array.</p> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>def transpose(img: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Transposes the first two dimensions of an array of any dimensionality.\n    Retains the order of any additional dimensions.\n\n    Args:\n        img (np.ndarray): Input array.\n\n    Returns:\n        np.ndarray: Transposed array.\n    \"\"\"\n    # Generate the new axes order\n    new_axes = list(range(img.ndim))\n    new_axes[0], new_axes[1] = 1, 0  # Swap the first two dimensions\n\n    # Transpose the array using the new axes order\n    return img.transpose(new_axes)\n</code></pre>"},{"location":"api_reference/augmentations/geometric/functional/#albumentations.augmentations.geometric.functional.validate_bboxes","title":"<code>def validate_bboxes    (bboxes, image_shape)    </code> [view source on GitHub]","text":"<p>Validate bounding boxes and remove invalid ones.</p> <p>Parameters:</p> Name Type Description <code>bboxes</code> <code>np.ndarray</code> <p>Array of bounding boxes with shape (n, 4) where each row is [x_min, y_min, x_max, y_max].</p> <code>image_shape</code> <code>tuple[int, int]</code> <p>Shape of the image as (height, width).</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Array of valid bounding boxes, potentially with fewer boxes than the input.</p> <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; bboxes = np.array([[10, 20, 30, 40], [-10, -10, 5, 5], [100, 100, 120, 120]])\n&gt;&gt;&gt; valid_bboxes = validate_bboxes(bboxes, (100, 100))\n&gt;&gt;&gt; print(valid_bboxes)\n[[10 20 30 40]]\n</code></pre> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>def validate_bboxes(bboxes: np.ndarray, image_shape: Sequence[int]) -&gt; np.ndarray:\n    \"\"\"Validate bounding boxes and remove invalid ones.\n\n    Args:\n        bboxes (np.ndarray): Array of bounding boxes with shape (n, 4) where each row is [x_min, y_min, x_max, y_max].\n        image_shape (tuple[int, int]): Shape of the image as (height, width).\n\n    Returns:\n        np.ndarray: Array of valid bounding boxes, potentially with fewer boxes than the input.\n\n    Example:\n        &gt;&gt;&gt; bboxes = np.array([[10, 20, 30, 40], [-10, -10, 5, 5], [100, 100, 120, 120]])\n        &gt;&gt;&gt; valid_bboxes = validate_bboxes(bboxes, (100, 100))\n        &gt;&gt;&gt; print(valid_bboxes)\n        [[10 20 30 40]]\n    \"\"\"\n    rows, cols = image_shape[:2]\n\n    x_min, y_min, x_max, y_max = bboxes[:, 0], bboxes[:, 1], bboxes[:, 2], bboxes[:, 3]\n\n    valid_indices = (x_max &gt; 0) &amp; (y_max &gt; 0) &amp; (x_min &lt; cols) &amp; (y_min &lt; rows)\n\n    return bboxes[valid_indices]\n</code></pre>"},{"location":"api_reference/augmentations/geometric/functional/#albumentations.augmentations.geometric.functional.validate_if_not_found_coords","title":"<code>def validate_if_not_found_coords    (if_not_found_coords)    </code> [view source on GitHub]","text":"<p>Validate and process <code>if_not_found_coords</code> parameter.</p> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>def validate_if_not_found_coords(\n    if_not_found_coords: Sequence[int] | dict[str, Any] | None,\n) -&gt; tuple[bool, int, int]:\n    \"\"\"Validate and process `if_not_found_coords` parameter.\"\"\"\n    if if_not_found_coords is None:\n        return True, -1, -1\n    if isinstance(if_not_found_coords, (tuple, list)):\n        if len(if_not_found_coords) != PAIR:\n            msg = \"Expected tuple/list 'if_not_found_coords' to contain exactly two entries.\"\n            raise ValueError(msg)\n        return False, if_not_found_coords[0], if_not_found_coords[1]\n    if isinstance(if_not_found_coords, dict):\n        return False, if_not_found_coords[\"x\"], if_not_found_coords[\"y\"]\n\n    msg = \"Expected if_not_found_coords to be None, tuple, list, or dict.\"\n    raise ValueError(msg)\n</code></pre>"},{"location":"api_reference/augmentations/geometric/functional/#albumentations.augmentations.geometric.functional.validate_keypoints","title":"<code>def validate_keypoints    (keypoints, image_shape)    </code> [view source on GitHub]","text":"<p>Validate keypoints and remove those that fall outside the image boundaries.</p> <p>Parameters:</p> Name Type Description <code>keypoints</code> <code>np.ndarray</code> <p>Array of keypoints with shape (N, M) where N is the number of keypoints                     and M &gt;= 2. The first two columns represent x and y coordinates.</p> <code>image_shape</code> <code>tuple[int, int]</code> <p>Shape of the image as (height, width).</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Array of valid keypoints that fall within the image boundaries.</p> <p>Note</p> <p>This function only checks the x and y coordinates (first two columns) of the keypoints. Any additional columns (e.g., angle, scale) are preserved for valid keypoints.</p> Source code in <code>albumentations/augmentations/geometric/functional.py</code> Python<pre><code>def validate_keypoints(keypoints: np.ndarray, image_shape: tuple[int, int]) -&gt; np.ndarray:\n    \"\"\"Validate keypoints and remove those that fall outside the image boundaries.\n\n    Args:\n        keypoints (np.ndarray): Array of keypoints with shape (N, M) where N is the number of keypoints\n                                and M &gt;= 2. The first two columns represent x and y coordinates.\n        image_shape (tuple[int, int]): Shape of the image as (height, width).\n\n    Returns:\n        np.ndarray: Array of valid keypoints that fall within the image boundaries.\n\n    Note:\n        This function only checks the x and y coordinates (first two columns) of the keypoints.\n        Any additional columns (e.g., angle, scale) are preserved for valid keypoints.\n    \"\"\"\n    rows, cols = image_shape[:2]\n\n    x, y = keypoints[:, 0], keypoints[:, 1]\n\n    valid_indices = (x &gt;= 0) &amp; (x &lt; cols) &amp; (y &gt;= 0) &amp; (y &lt; rows)\n\n    return keypoints[valid_indices]\n</code></pre>"},{"location":"api_reference/augmentations/geometric/resize/","title":"Resizing transforms (augmentations.geometric.resize)","text":""},{"location":"api_reference/augmentations/geometric/resize/#albumentations.augmentations.geometric.resize.LongestMaxSize","title":"<code>class  LongestMaxSize</code> <code>     (max_size=1024, interpolation=1, always_apply=None, p=1)                 </code>  [view source on GitHub]","text":"<p>Rescale an image so that maximum side is equal to max_size, keeping the aspect ratio of the initial image.</p> <p>Parameters:</p> Name Type Description <code>max_size</code> <code>int, list of int</code> <p>maximum size of the image after the transformation. When using a list, max size will be randomly selected from the values in the list.</p> <code>interpolation</code> <code>OpenCV flag</code> <p>interpolation method. Default: cv2.INTER_LINEAR.</p> <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 1.</p> <p>Targets</p> <p>image, mask, bboxes, keypoints</p> <p>Image types:     uint8, float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/geometric/resize.py</code> Python<pre><code>class LongestMaxSize(DualTransform):\n    \"\"\"Rescale an image so that maximum side is equal to max_size, keeping the aspect ratio of the initial image.\n\n    Args:\n        max_size (int, list of int): maximum size of the image after the transformation. When using a list, max size\n            will be randomly selected from the values in the list.\n        interpolation (OpenCV flag): interpolation method. Default: cv2.INTER_LINEAR.\n        p (float): probability of applying the transform. Default: 1.\n\n    Targets:\n        image, mask, bboxes, keypoints\n\n    Image types:\n        uint8, float32\n\n    \"\"\"\n\n    _targets = (Targets.IMAGE, Targets.MASK, Targets.BBOXES, Targets.KEYPOINTS)\n\n    class InitSchema(MaxSizeInitSchema):\n        pass\n\n    def __init__(\n        self,\n        max_size: int | Sequence[int] = 1024,\n        interpolation: int = cv2.INTER_LINEAR,\n        always_apply: bool | None = None,\n        p: float = 1,\n    ):\n        super().__init__(p, always_apply)\n        self.interpolation = interpolation\n        self.max_size = max_size\n\n    def apply(\n        self,\n        img: np.ndarray,\n        max_size: int,\n        interpolation: int,\n        **params: Any,\n    ) -&gt; np.ndarray:\n        return fgeometric.longest_max_size(img, max_size=max_size, interpolation=interpolation)\n\n    def apply_to_bbox(self, bbox: BoxInternalType, **params: Any) -&gt; BoxInternalType:\n        # Bounding box coordinates are scale invariant\n        return bbox\n\n    def apply_to_keypoint(\n        self,\n        keypoint: KeypointInternalType,\n        max_size: int,\n        **params: Any,\n    ) -&gt; KeypointInternalType:\n        image_shape = params[\"shape\"][:2]\n\n        scale = max_size / max(image_shape)\n        return fgeometric.keypoint_scale(keypoint, scale, scale)\n\n    def get_params(self) -&gt; dict[str, int]:\n        return {\"max_size\": self.max_size if isinstance(self.max_size, int) else random.choice(self.max_size)}\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return \"max_size\", \"interpolation\"\n</code></pre>"},{"location":"api_reference/augmentations/geometric/resize/#albumentations.augmentations.geometric.resize.LongestMaxSize.apply","title":"<code>apply (self, img, max_size, interpolation, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/geometric/resize.py</code> Python<pre><code>def apply(\n    self,\n    img: np.ndarray,\n    max_size: int,\n    interpolation: int,\n    **params: Any,\n) -&gt; np.ndarray:\n    return fgeometric.longest_max_size(img, max_size=max_size, interpolation=interpolation)\n</code></pre>"},{"location":"api_reference/augmentations/geometric/resize/#albumentations.augmentations.geometric.resize.LongestMaxSize.get_params","title":"<code>get_params (self)</code>","text":"<p>Returns parameters independent of input.</p> Source code in <code>albumentations/augmentations/geometric/resize.py</code> Python<pre><code>def get_params(self) -&gt; dict[str, int]:\n    return {\"max_size\": self.max_size if isinstance(self.max_size, int) else random.choice(self.max_size)}\n</code></pre>"},{"location":"api_reference/augmentations/geometric/resize/#albumentations.augmentations.geometric.resize.LongestMaxSize.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/geometric/resize.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return \"max_size\", \"interpolation\"\n</code></pre>"},{"location":"api_reference/augmentations/geometric/resize/#albumentations.augmentations.geometric.resize.RandomScale","title":"<code>class  RandomScale</code> <code>     (scale_limit=0.1, interpolation=1, always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Randomly resize the input. Output image size is different from the input image size.</p> <p>Parameters:</p> Name Type Description <code>scale_limit</code> <code>float, float) or float</code> <p>scaling factor range. If scale_limit is a single float value, the range will be (-scale_limit, scale_limit). Note that the scale_limit will be biased by 1. If scale_limit is a tuple, like (low, high), sampling will be done from the range (1 + low, 1 + high). Default: (-0.1, 0.1).</p> <code>interpolation</code> <code>OpenCV flag</code> <p>flag that is used to specify the interpolation algorithm. Should be one of: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4. Default: cv2.INTER_LINEAR.</p> <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 0.5.</p> <p>Targets</p> <p>image, mask, bboxes, keypoints</p> <p>Image types:     uint8, float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/geometric/resize.py</code> Python<pre><code>class RandomScale(DualTransform):\n    \"\"\"Randomly resize the input. Output image size is different from the input image size.\n\n    Args:\n        scale_limit ((float, float) or float): scaling factor range. If scale_limit is a single float value, the\n            range will be (-scale_limit, scale_limit). Note that the scale_limit will be biased by 1.\n            If scale_limit is a tuple, like (low, high), sampling will be done from the range (1 + low, 1 + high).\n            Default: (-0.1, 0.1).\n        interpolation (OpenCV flag): flag that is used to specify the interpolation algorithm. Should be one of:\n            cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.\n            Default: cv2.INTER_LINEAR.\n        p (float): probability of applying the transform. Default: 0.5.\n\n    Targets:\n        image, mask, bboxes, keypoints\n\n    Image types:\n        uint8, float32\n\n    \"\"\"\n\n    _targets = (Targets.IMAGE, Targets.MASK, Targets.BBOXES, Targets.KEYPOINTS)\n\n    class InitSchema(BaseTransformInitSchema):\n        scale_limit: ScaleFloatType = Field(\n            default=0.1,\n            description=\"Scaling factor range. If a single float value =&gt; (1-scale_limit, 1 + scale_limit).\",\n        )\n        interpolation: InterpolationType = cv2.INTER_LINEAR\n\n        @field_validator(\"scale_limit\")\n        @classmethod\n        def check_scale_limit(cls, v: ScaleFloatType) -&gt; tuple[float, float]:\n            return to_tuple(v, bias=1.0)\n\n    def __init__(\n        self,\n        scale_limit: ScaleFloatType = 0.1,\n        interpolation: int = cv2.INTER_LINEAR,\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p, always_apply)\n        self.scale_limit = cast(Tuple[float, float], scale_limit)\n        self.interpolation = interpolation\n\n    def get_params(self) -&gt; dict[str, float]:\n        return {\"scale\": random.uniform(self.scale_limit[0], self.scale_limit[1])}\n\n    def apply(\n        self,\n        img: np.ndarray,\n        scale: float,\n        interpolation: int,\n        **params: Any,\n    ) -&gt; np.ndarray:\n        return fgeometric.scale(img, scale, interpolation)\n\n    def apply_to_bbox(self, bbox: BoxInternalType, **params: Any) -&gt; BoxInternalType:\n        # Bounding box coordinates are scale invariant\n        return bbox\n\n    def apply_to_keypoint(\n        self,\n        keypoint: KeypointInternalType,\n        scale: float,\n        **params: Any,\n    ) -&gt; KeypointInternalType:\n        return fgeometric.keypoint_scale(keypoint, scale, scale)\n\n    def get_transform_init_args(self) -&gt; dict[str, Any]:\n        return {\"interpolation\": self.interpolation, \"scale_limit\": to_tuple(self.scale_limit, bias=-1.0)}\n</code></pre>"},{"location":"api_reference/augmentations/geometric/resize/#albumentations.augmentations.geometric.resize.RandomScale.apply","title":"<code>apply (self, img, scale, interpolation, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/geometric/resize.py</code> Python<pre><code>def apply(\n    self,\n    img: np.ndarray,\n    scale: float,\n    interpolation: int,\n    **params: Any,\n) -&gt; np.ndarray:\n    return fgeometric.scale(img, scale, interpolation)\n</code></pre>"},{"location":"api_reference/augmentations/geometric/resize/#albumentations.augmentations.geometric.resize.RandomScale.get_params","title":"<code>get_params (self)</code>","text":"<p>Returns parameters independent of input.</p> Source code in <code>albumentations/augmentations/geometric/resize.py</code> Python<pre><code>def get_params(self) -&gt; dict[str, float]:\n    return {\"scale\": random.uniform(self.scale_limit[0], self.scale_limit[1])}\n</code></pre>"},{"location":"api_reference/augmentations/geometric/resize/#albumentations.augmentations.geometric.resize.Resize","title":"<code>class  Resize</code> <code>     (height, width, interpolation=1, always_apply=None, p=1)                 </code>  [view source on GitHub]","text":"<p>Resize the input to the given height and width.</p> <p>Parameters:</p> Name Type Description <code>height</code> <code>int</code> <p>desired height of the output.</p> <code>width</code> <code>int</code> <p>desired width of the output.</p> <code>interpolation</code> <code>OpenCV flag</code> <p>flag that is used to specify the interpolation algorithm. Should be one of: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4. Default: cv2.INTER_LINEAR.</p> <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 1.</p> <p>Targets</p> <p>image, mask, bboxes, keypoints</p> <p>Image types:     uint8, float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/geometric/resize.py</code> Python<pre><code>class Resize(DualTransform):\n    \"\"\"Resize the input to the given height and width.\n\n    Args:\n        height (int): desired height of the output.\n        width (int): desired width of the output.\n        interpolation (OpenCV flag): flag that is used to specify the interpolation algorithm. Should be one of:\n            cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.\n            Default: cv2.INTER_LINEAR.\n        p (float): probability of applying the transform. Default: 1.\n\n    Targets:\n        image, mask, bboxes, keypoints\n\n    Image types:\n        uint8, float32\n\n    \"\"\"\n\n    _targets = (Targets.IMAGE, Targets.MASK, Targets.KEYPOINTS, Targets.BBOXES)\n\n    class InitSchema(BaseTransformInitSchema):\n        height: int = Field(ge=1, description=\"Desired height of the output.\")\n        width: int = Field(ge=1, description=\"Desired width of the output.\")\n        interpolation: InterpolationType = cv2.INTER_LINEAR\n        p: ProbabilityType = 1\n\n    def __init__(\n        self,\n        height: int,\n        width: int,\n        interpolation: int = cv2.INTER_LINEAR,\n        always_apply: bool | None = None,\n        p: float = 1,\n    ):\n        super().__init__(p, always_apply)\n        self.height = height\n        self.width = width\n        self.interpolation = interpolation\n\n    def apply(self, img: np.ndarray, interpolation: int, **params: Any) -&gt; np.ndarray:\n        return fgeometric.resize(img, (self.height, self.width), interpolation=interpolation)\n\n    def apply_to_bbox(self, bbox: BoxInternalType, **params: Any) -&gt; BoxInternalType:\n        # Bounding box coordinates are scale invariant\n        return bbox\n\n    def apply_to_keypoint(self, keypoint: KeypointInternalType, **params: Any) -&gt; KeypointInternalType:\n        height, width = params[\"shape\"][:2]\n        scale_x = self.width / width\n        scale_y = self.height / height\n        return fgeometric.keypoint_scale(keypoint, scale_x, scale_y)\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return \"height\", \"width\", \"interpolation\"\n</code></pre>"},{"location":"api_reference/augmentations/geometric/resize/#albumentations.augmentations.geometric.resize.Resize.apply","title":"<code>apply (self, img, interpolation, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/geometric/resize.py</code> Python<pre><code>def apply(self, img: np.ndarray, interpolation: int, **params: Any) -&gt; np.ndarray:\n    return fgeometric.resize(img, (self.height, self.width), interpolation=interpolation)\n</code></pre>"},{"location":"api_reference/augmentations/geometric/resize/#albumentations.augmentations.geometric.resize.Resize.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/geometric/resize.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return \"height\", \"width\", \"interpolation\"\n</code></pre>"},{"location":"api_reference/augmentations/geometric/resize/#albumentations.augmentations.geometric.resize.SmallestMaxSize","title":"<code>class  SmallestMaxSize</code> <code>     (max_size=1024, interpolation=1, always_apply=None, p=1)                 </code>  [view source on GitHub]","text":"<p>Rescale an image so that minimum side is equal to max_size, keeping the aspect ratio of the initial image.</p> <p>Parameters:</p> Name Type Description <code>max_size</code> <code>int, list of int</code> <p>maximum size of smallest side of the image after the transformation. When using a list, max size will be randomly selected from the values in the list.</p> <code>interpolation</code> <code>OpenCV flag</code> <p>interpolation method. Default: cv2.INTER_LINEAR.</p> <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 1.</p> <p>Targets</p> <p>image, mask, bboxes, keypoints</p> <p>Image types:     uint8, float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/geometric/resize.py</code> Python<pre><code>class SmallestMaxSize(DualTransform):\n    \"\"\"Rescale an image so that minimum side is equal to max_size, keeping the aspect ratio of the initial image.\n\n    Args:\n        max_size (int, list of int): maximum size of smallest side of the image after the transformation. When using a\n            list, max size will be randomly selected from the values in the list.\n        interpolation (OpenCV flag): interpolation method. Default: cv2.INTER_LINEAR.\n        p (float): probability of applying the transform. Default: 1.\n\n    Targets:\n        image, mask, bboxes, keypoints\n\n    Image types:\n        uint8, float32\n\n    \"\"\"\n\n    _targets = (Targets.IMAGE, Targets.MASK, Targets.KEYPOINTS, Targets.BBOXES)\n\n    class InitSchema(MaxSizeInitSchema):\n        pass\n\n    def __init__(\n        self,\n        max_size: int | Sequence[int] = 1024,\n        interpolation: int = cv2.INTER_LINEAR,\n        always_apply: bool | None = None,\n        p: float = 1,\n    ):\n        super().__init__(p, always_apply)\n        self.interpolation = interpolation\n        self.max_size = max_size\n\n    def apply(\n        self,\n        img: np.ndarray,\n        max_size: int,\n        interpolation: int,\n        **params: Any,\n    ) -&gt; np.ndarray:\n        return fgeometric.smallest_max_size(img, max_size=max_size, interpolation=interpolation)\n\n    def apply_to_bbox(self, bbox: BoxInternalType, **params: Any) -&gt; BoxInternalType:\n        return bbox\n\n    def apply_to_keypoint(\n        self,\n        keypoint: KeypointInternalType,\n        max_size: int,\n        **params: Any,\n    ) -&gt; KeypointInternalType:\n        image_shape = params[\"shape\"][:2]\n        height, width = image_shape\n\n        scale = max_size / min(image_shape)\n        return fgeometric.keypoint_scale(keypoint, scale, scale)\n\n    def get_params(self) -&gt; dict[str, int]:\n        return {\"max_size\": self.max_size if isinstance(self.max_size, int) else random.choice(self.max_size)}\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return (\"max_size\", \"interpolation\")\n</code></pre>"},{"location":"api_reference/augmentations/geometric/resize/#albumentations.augmentations.geometric.resize.SmallestMaxSize.apply","title":"<code>apply (self, img, max_size, interpolation, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/geometric/resize.py</code> Python<pre><code>def apply(\n    self,\n    img: np.ndarray,\n    max_size: int,\n    interpolation: int,\n    **params: Any,\n) -&gt; np.ndarray:\n    return fgeometric.smallest_max_size(img, max_size=max_size, interpolation=interpolation)\n</code></pre>"},{"location":"api_reference/augmentations/geometric/resize/#albumentations.augmentations.geometric.resize.SmallestMaxSize.get_params","title":"<code>get_params (self)</code>","text":"<p>Returns parameters independent of input.</p> Source code in <code>albumentations/augmentations/geometric/resize.py</code> Python<pre><code>def get_params(self) -&gt; dict[str, int]:\n    return {\"max_size\": self.max_size if isinstance(self.max_size, int) else random.choice(self.max_size)}\n</code></pre>"},{"location":"api_reference/augmentations/geometric/resize/#albumentations.augmentations.geometric.resize.SmallestMaxSize.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/geometric/resize.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return (\"max_size\", \"interpolation\")\n</code></pre>"},{"location":"api_reference/augmentations/geometric/rotate/","title":"Rotation transforms (augmentations.geometric.functional)","text":""},{"location":"api_reference/augmentations/geometric/rotate/#albumentations.augmentations.geometric.rotate.RandomRotate90","title":"<code>class  RandomRotate90</code> <code> </code>  [view source on GitHub]","text":"<p>Randomly rotate the input by 90 degrees zero or more times.</p> <p>Parameters:</p> Name Type Description <code>p</code> <p>probability of applying the transform. Default: 0.5.</p> <p>Targets</p> <p>image, mask, bboxes, keypoints</p> <p>Image types:     uint8, float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/geometric/rotate.py</code> Python<pre><code>class RandomRotate90(DualTransform):\n    \"\"\"Randomly rotate the input by 90 degrees zero or more times.\n\n    Args:\n        p: probability of applying the transform. Default: 0.5.\n\n    Targets:\n        image, mask, bboxes, keypoints\n\n    Image types:\n        uint8, float32\n\n    \"\"\"\n\n    _targets = (Targets.IMAGE, Targets.MASK, Targets.BBOXES, Targets.KEYPOINTS)\n\n    def apply(self, img: np.ndarray, factor: int, **params: Any) -&gt; np.ndarray:\n        return fgeometric.rot90(img, factor)\n\n    def get_params(self) -&gt; dict[str, int]:\n        # Random int in the range [0, 3]\n        return {\"factor\": random.randint(0, 3)}\n\n    def apply_to_bbox(self, bbox: BoxInternalType, factor: int, **params: Any) -&gt; BoxInternalType:\n        return fgeometric.bbox_rot90(bbox, factor)\n\n    def apply_to_keypoint(self, keypoint: KeypointInternalType, factor: int, **params: Any) -&gt; BoxInternalType:\n        return fgeometric.keypoint_rot90(keypoint, factor, params[\"shape\"])\n\n    def get_transform_init_args_names(self) -&gt; tuple[()]:\n        return ()\n</code></pre>"},{"location":"api_reference/augmentations/geometric/rotate/#albumentations.augmentations.geometric.rotate.RandomRotate90.apply","title":"<code>apply (self, img, factor, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/geometric/rotate.py</code> Python<pre><code>def apply(self, img: np.ndarray, factor: int, **params: Any) -&gt; np.ndarray:\n    return fgeometric.rot90(img, factor)\n</code></pre>"},{"location":"api_reference/augmentations/geometric/rotate/#albumentations.augmentations.geometric.rotate.RandomRotate90.get_params","title":"<code>get_params (self)</code>","text":"<p>Returns parameters independent of input.</p> Source code in <code>albumentations/augmentations/geometric/rotate.py</code> Python<pre><code>def get_params(self) -&gt; dict[str, int]:\n    # Random int in the range [0, 3]\n    return {\"factor\": random.randint(0, 3)}\n</code></pre>"},{"location":"api_reference/augmentations/geometric/rotate/#albumentations.augmentations.geometric.rotate.RandomRotate90.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/geometric/rotate.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[()]:\n    return ()\n</code></pre>"},{"location":"api_reference/augmentations/geometric/rotate/#albumentations.augmentations.geometric.rotate.Rotate","title":"<code>class  Rotate</code> <code>     (limit=(-90, 90), interpolation=1, border_mode=4, value=None, mask_value=None, rotate_method='largest_box', crop_border=False, always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Rotate the input by an angle selected randomly from the uniform distribution.</p> <p>Parameters:</p> Name Type Description <code>limit</code> <code>ScaleFloatType</code> <p>range from which a random angle is picked. If limit is a single int an angle is picked from (-limit, limit). Default: (-90, 90)</p> <code>interpolation</code> <code>OpenCV flag</code> <p>flag that is used to specify the interpolation algorithm. Should be one of: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4. Default: cv2.INTER_LINEAR.</p> <code>border_mode</code> <code>OpenCV flag</code> <p>flag that is used to specify the pixel extrapolation method. Should be one of: cv2.BORDER_CONSTANT, cv2.BORDER_REPLICATE, cv2.BORDER_REFLECT, cv2.BORDER_WRAP, cv2.BORDER_REFLECT_101. Default: cv2.BORDER_REFLECT_101</p> <code>value</code> <code>int, float, list of ints, list of float</code> <p>padding value if border_mode is cv2.BORDER_CONSTANT.</p> <code>mask_value</code> <code>int, float,         list of ints,         list of float</code> <p>padding value if border_mode is cv2.BORDER_CONSTANT applied for masks.</p> <code>rotate_method</code> <code>str</code> <p>rotation method used for the bounding boxes. Should be one of \"largest_box\" or \"ellipse\". Default: \"largest_box\"</p> <code>crop_border</code> <code>bool</code> <p>If True would make a largest possible crop within rotated image</p> <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 0.5.</p> <p>Targets</p> <p>image, mask, bboxes, keypoints</p> <p>Image types:     uint8, float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/geometric/rotate.py</code> Python<pre><code>class Rotate(DualTransform):\n    \"\"\"Rotate the input by an angle selected randomly from the uniform distribution.\n\n    Args:\n        limit: range from which a random angle is picked. If limit is a single int\n            an angle is picked from (-limit, limit). Default: (-90, 90)\n        interpolation (OpenCV flag): flag that is used to specify the interpolation algorithm. Should be one of:\n            cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.\n            Default: cv2.INTER_LINEAR.\n        border_mode (OpenCV flag): flag that is used to specify the pixel extrapolation method. Should be one of:\n            cv2.BORDER_CONSTANT, cv2.BORDER_REPLICATE, cv2.BORDER_REFLECT, cv2.BORDER_WRAP, cv2.BORDER_REFLECT_101.\n            Default: cv2.BORDER_REFLECT_101\n        value (int, float, list of ints, list of float): padding value if border_mode is cv2.BORDER_CONSTANT.\n        mask_value (int, float,\n                    list of ints,\n                    list of float): padding value if border_mode is cv2.BORDER_CONSTANT applied for masks.\n        rotate_method (str): rotation method used for the bounding boxes. Should be one of \"largest_box\" or \"ellipse\".\n            Default: \"largest_box\"\n        crop_border (bool): If True would make a largest possible crop within rotated image\n        p (float): probability of applying the transform. Default: 0.5.\n\n    Targets:\n        image, mask, bboxes, keypoints\n\n    Image types:\n        uint8, float32\n\n    \"\"\"\n\n    _targets = (Targets.IMAGE, Targets.MASK, Targets.BBOXES, Targets.KEYPOINTS)\n\n    class InitSchema(RotateInitSchema):\n        rotate_method: Literal[\"largest_box\", \"ellipse\"] = \"largest_box\"\n        crop_border: bool = Field(\n            default=False,\n            description=\"If True, makes a largest possible crop within the rotated image.\",\n        )\n\n    def __init__(\n        self,\n        limit: ScaleFloatType = (-90, 90),\n        interpolation: int = cv2.INTER_LINEAR,\n        border_mode: int = cv2.BORDER_REFLECT_101,\n        value: ColorType | None = None,\n        mask_value: ColorType | None = None,\n        rotate_method: Literal[\"largest_box\", \"ellipse\"] = \"largest_box\",\n        crop_border: bool = False,\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p=p, always_apply=always_apply)\n        self.limit = cast(Tuple[float, float], limit)\n        self.interpolation = interpolation\n        self.border_mode = border_mode\n        self.value = value\n        self.mask_value = mask_value\n        self.rotate_method = rotate_method\n        self.crop_border = crop_border\n\n    def apply(\n        self,\n        img: np.ndarray,\n        angle: float,\n        interpolation: int,\n        x_min: int,\n        x_max: int,\n        y_min: int,\n        y_max: int,\n        **params: Any,\n    ) -&gt; np.ndarray:\n        img_out = fgeometric.rotate(img, angle, interpolation, self.border_mode, self.value)\n        if self.crop_border:\n            return fcrops.crop(img_out, x_min, y_min, x_max, y_max)\n        return img_out\n\n    def apply_to_mask(\n        self,\n        mask: np.ndarray,\n        angle: float,\n        x_min: int,\n        x_max: int,\n        y_min: int,\n        y_max: int,\n        **params: Any,\n    ) -&gt; np.ndarray:\n        img_out = fgeometric.rotate(mask, angle, cv2.INTER_NEAREST, self.border_mode, self.mask_value)\n        if self.crop_border:\n            return fcrops.crop(img_out, x_min, y_min, x_max, y_max)\n        return img_out\n\n    def apply_to_bbox(\n        self,\n        bbox: BoxInternalType,\n        angle: float,\n        x_min: int,\n        x_max: int,\n        y_min: int,\n        y_max: int,\n        **params: Any,\n    ) -&gt; np.ndarray:\n        image_shape = params[\"shape\"][:2]\n        bbox_out = fgeometric.bbox_rotate(bbox, angle, self.rotate_method, image_shape)\n        if self.crop_border:\n            return fcrops.crop_bbox_by_coords(bbox_out, (x_min, y_min, x_max, y_max), image_shape)\n        return bbox_out\n\n    def apply_to_keypoint(\n        self,\n        keypoint: KeypointInternalType,\n        angle: float,\n        x_min: int,\n        x_max: int,\n        y_min: int,\n        y_max: int,\n        **params: Any,\n    ) -&gt; KeypointInternalType:\n        keypoint_out = fgeometric.keypoint_rotate(keypoint, angle, params[\"shape\"][:2], **params)\n        if self.crop_border:\n            return fcrops.crop_keypoint_by_coords(keypoint_out, (x_min, y_min, x_max, y_max))\n        return keypoint_out\n\n    @staticmethod\n    def _rotated_rect_with_max_area(height: int, width: int, angle: float) -&gt; dict[str, int]:\n        \"\"\"Given a rectangle of size wxh that has been rotated by 'angle' (in\n        degrees), computes the width and height of the largest possible\n        axis-aligned rectangle (maximal area) within the rotated rectangle.\n\n        Reference:\n            https://stackoverflow.com/questions/16702966/rotate-image-and-crop-out-black-borders\n        \"\"\"\n        angle = math.radians(angle)\n        width_is_longer = width &gt;= height\n        side_long, side_short = (width, height) if width_is_longer else (height, width)\n\n        # since the solutions for angle, -angle and 180-angle are all the same,\n        # it is sufficient to look at the first quadrant and the absolute values of sin,cos:\n        sin_a, cos_a = abs(math.sin(angle)), abs(math.cos(angle))\n        if side_short &lt;= 2.0 * sin_a * cos_a * side_long or abs(sin_a - cos_a) &lt; SMALL_NUMBER:\n            # half constrained case: two crop corners touch the longer side,\n            # the other two corners are on the mid-line parallel to the longer line\n            x = 0.5 * side_short\n            wr, hr = (x / sin_a, x / cos_a) if width_is_longer else (x / cos_a, x / sin_a)\n        else:\n            # fully constrained case: crop touches all 4 sides\n            cos_2a = cos_a * cos_a - sin_a * sin_a\n            wr, hr = (width * cos_a - height * sin_a) / cos_2a, (height * cos_a - width * sin_a) / cos_2a\n\n        return {\n            \"x_min\": max(0, int(width / 2 - wr / 2)),\n            \"x_max\": min(width, int(width / 2 + wr / 2)),\n            \"y_min\": max(0, int(height / 2 - hr / 2)),\n            \"y_max\": min(height, int(height / 2 + hr / 2)),\n        }\n\n    def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n        out_params = {\"angle\": random.uniform(self.limit[0], self.limit[1])}\n        if self.crop_border:\n            height, width = params[\"shape\"][:2]\n            out_params.update(self._rotated_rect_with_max_area(height, width, out_params[\"angle\"]))\n        else:\n            out_params.update({\"x_min\": -1, \"x_max\": -1, \"y_min\": -1, \"y_max\": -1})\n\n        return out_params\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return \"limit\", \"interpolation\", \"border_mode\", \"value\", \"mask_value\", \"rotate_method\", \"crop_border\"\n</code></pre>"},{"location":"api_reference/augmentations/geometric/rotate/#albumentations.augmentations.geometric.rotate.Rotate.apply","title":"<code>apply (self, img, angle, interpolation, x_min, x_max, y_min, y_max, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/geometric/rotate.py</code> Python<pre><code>def apply(\n    self,\n    img: np.ndarray,\n    angle: float,\n    interpolation: int,\n    x_min: int,\n    x_max: int,\n    y_min: int,\n    y_max: int,\n    **params: Any,\n) -&gt; np.ndarray:\n    img_out = fgeometric.rotate(img, angle, interpolation, self.border_mode, self.value)\n    if self.crop_border:\n        return fcrops.crop(img_out, x_min, y_min, x_max, y_max)\n    return img_out\n</code></pre>"},{"location":"api_reference/augmentations/geometric/rotate/#albumentations.augmentations.geometric.rotate.Rotate.get_params_dependent_on_data","title":"<code>get_params_dependent_on_data (self, params, data)</code>","text":"<p>Returns parameters dependent on input.</p> Source code in <code>albumentations/augmentations/geometric/rotate.py</code> Python<pre><code>def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n    out_params = {\"angle\": random.uniform(self.limit[0], self.limit[1])}\n    if self.crop_border:\n        height, width = params[\"shape\"][:2]\n        out_params.update(self._rotated_rect_with_max_area(height, width, out_params[\"angle\"]))\n    else:\n        out_params.update({\"x_min\": -1, \"x_max\": -1, \"y_min\": -1, \"y_max\": -1})\n\n    return out_params\n</code></pre>"},{"location":"api_reference/augmentations/geometric/rotate/#albumentations.augmentations.geometric.rotate.Rotate.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/geometric/rotate.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return \"limit\", \"interpolation\", \"border_mode\", \"value\", \"mask_value\", \"rotate_method\", \"crop_border\"\n</code></pre>"},{"location":"api_reference/augmentations/geometric/rotate/#albumentations.augmentations.geometric.rotate.SafeRotate","title":"<code>class  SafeRotate</code> <code>     (limit=(-90, 90), interpolation=1, border_mode=4, value=None, mask_value=None, always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Rotate the input inside the input's frame by an angle selected randomly from the uniform distribution.</p> <p>The resulting image may have artifacts in it. After rotation, the image may have a different aspect ratio, and after resizing, it returns to its original shape with the original aspect ratio of the image. For these reason we may see some artifacts.</p> <p>Parameters:</p> Name Type Description <code>limit</code> <code>int, int) or int</code> <p>range from which a random angle is picked. If limit is a single int an angle is picked from (-limit, limit). Default: (-90, 90)</p> <code>interpolation</code> <code>OpenCV flag</code> <p>flag that is used to specify the interpolation algorithm. Should be one of: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4. Default: cv2.INTER_LINEAR.</p> <code>border_mode</code> <code>OpenCV flag</code> <p>flag that is used to specify the pixel extrapolation method. Should be one of: cv2.BORDER_CONSTANT, cv2.BORDER_REPLICATE, cv2.BORDER_REFLECT, cv2.BORDER_WRAP, cv2.BORDER_REFLECT_101. Default: cv2.BORDER_REFLECT_101</p> <code>value</code> <code>int, float, list of ints, list of float</code> <p>padding value if border_mode is cv2.BORDER_CONSTANT.</p> <code>mask_value</code> <code>int, float,         list of ints,         list of float</code> <p>padding value if border_mode is cv2.BORDER_CONSTANT applied for masks.</p> <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 0.5.</p> <p>Targets</p> <p>image, mask, bboxes, keypoints</p> <p>Image types:     uint8, float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/geometric/rotate.py</code> Python<pre><code>class SafeRotate(DualTransform):\n    \"\"\"Rotate the input inside the input's frame by an angle selected randomly from the uniform distribution.\n\n    The resulting image may have artifacts in it. After rotation, the image may have a different aspect ratio, and\n    after resizing, it returns to its original shape with the original aspect ratio of the image. For these reason we\n    may see some artifacts.\n\n    Args:\n        limit ((int, int) or int): range from which a random angle is picked. If limit is a single int\n            an angle is picked from (-limit, limit). Default: (-90, 90)\n        interpolation (OpenCV flag): flag that is used to specify the interpolation algorithm. Should be one of:\n            cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.\n            Default: cv2.INTER_LINEAR.\n        border_mode (OpenCV flag): flag that is used to specify the pixel extrapolation method. Should be one of:\n            cv2.BORDER_CONSTANT, cv2.BORDER_REPLICATE, cv2.BORDER_REFLECT, cv2.BORDER_WRAP, cv2.BORDER_REFLECT_101.\n            Default: cv2.BORDER_REFLECT_101\n        value (int, float, list of ints, list of float): padding value if border_mode is cv2.BORDER_CONSTANT.\n        mask_value (int, float,\n                    list of ints,\n                    list of float): padding value if border_mode is cv2.BORDER_CONSTANT applied for masks.\n        p (float): probability of applying the transform. Default: 0.5.\n\n    Targets:\n        image, mask, bboxes, keypoints\n\n    Image types:\n        uint8, float32\n\n    \"\"\"\n\n    _targets = (Targets.IMAGE, Targets.MASK, Targets.BBOXES, Targets.KEYPOINTS)\n\n    class InitSchema(RotateInitSchema):\n        pass\n\n    def __init__(\n        self,\n        limit: ScaleFloatType = (-90, 90),\n        interpolation: int = cv2.INTER_LINEAR,\n        border_mode: int = cv2.BORDER_REFLECT_101,\n        value: ColorType | None = None,\n        mask_value: ColorType | None = None,\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p=p, always_apply=always_apply)\n        self.limit = cast(Tuple[float, float], limit)\n        self.interpolation = interpolation\n        self.border_mode = border_mode\n        self.value = value\n        self.mask_value = mask_value\n\n    def apply(self, img: np.ndarray, matrix: np.ndarray, **params: Any) -&gt; np.ndarray:\n        return fgeometric.safe_rotate(img, matrix, self.interpolation, self.value, self.border_mode)\n\n    def apply_to_mask(self, mask: np.ndarray, matrix: np.ndarray, **params: Any) -&gt; np.ndarray:\n        return fgeometric.safe_rotate(mask, matrix, cv2.INTER_NEAREST, self.mask_value, self.border_mode)\n\n    def apply_to_bbox(self, bbox: BoxInternalType, **params: Any) -&gt; BoxInternalType:\n        return fgeometric.bbox_safe_rotate(bbox, params[\"matrix\"], params[\"shape\"])\n\n    def apply_to_keypoint(\n        self,\n        keypoint: KeypointInternalType,\n        angle: float,\n        scale_x: float,\n        scale_y: float,\n        **params: Any,\n    ) -&gt; KeypointInternalType:\n        return fgeometric.keypoint_safe_rotate(keypoint, params[\"matrix\"], angle, scale_x, scale_y, params[\"shape\"])\n\n    def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n        image_shape = params[\"shape\"]\n\n        angle = random.uniform(*self.limit)\n\n        # https://stackoverflow.com/questions/43892506/opencv-python-rotate-image-without-cropping-sides\n        image_center = center(image_shape)\n\n        # Rotation Matrix\n        rotation_mat = cv2.getRotationMatrix2D(image_center, angle, 1.0)\n\n        # rotation calculates the cos and sin, taking absolutes of those.\n        abs_cos = abs(rotation_mat[0, 0])\n        abs_sin = abs(rotation_mat[0, 1])\n\n        height, width = image_shape[:2]\n\n        # find the new width and height bounds\n        new_w = math.ceil(height * abs_sin + width * abs_cos)\n        new_h = math.ceil(height * abs_cos + width * abs_sin)\n\n        scale_x = width / new_w\n        scale_y = height / new_h\n\n        # Shift the image to create padding\n        rotation_mat[0, 2] += new_w / 2 - image_center[0]\n        rotation_mat[1, 2] += new_h / 2 - image_center[1]\n\n        # Rescale to original size\n        scale_mat = np.diag(np.ones(3))\n        scale_mat[0, 0] *= scale_x\n        scale_mat[1, 1] *= scale_y\n        _tmp = np.diag(np.ones(3))\n        _tmp[:2] = rotation_mat\n        _tmp = scale_mat @ _tmp\n        rotation_mat = _tmp[:2]\n\n        return {\"matrix\": rotation_mat, \"angle\": angle, \"scale_x\": scale_x, \"scale_y\": scale_y}\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return \"limit\", \"interpolation\", \"border_mode\", \"value\", \"mask_value\"\n</code></pre>"},{"location":"api_reference/augmentations/geometric/rotate/#albumentations.augmentations.geometric.rotate.SafeRotate.apply","title":"<code>apply (self, img, matrix, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/geometric/rotate.py</code> Python<pre><code>def apply(self, img: np.ndarray, matrix: np.ndarray, **params: Any) -&gt; np.ndarray:\n    return fgeometric.safe_rotate(img, matrix, self.interpolation, self.value, self.border_mode)\n</code></pre>"},{"location":"api_reference/augmentations/geometric/rotate/#albumentations.augmentations.geometric.rotate.SafeRotate.get_params_dependent_on_data","title":"<code>get_params_dependent_on_data (self, params, data)</code>","text":"<p>Returns parameters dependent on input.</p> Source code in <code>albumentations/augmentations/geometric/rotate.py</code> Python<pre><code>def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n    image_shape = params[\"shape\"]\n\n    angle = random.uniform(*self.limit)\n\n    # https://stackoverflow.com/questions/43892506/opencv-python-rotate-image-without-cropping-sides\n    image_center = center(image_shape)\n\n    # Rotation Matrix\n    rotation_mat = cv2.getRotationMatrix2D(image_center, angle, 1.0)\n\n    # rotation calculates the cos and sin, taking absolutes of those.\n    abs_cos = abs(rotation_mat[0, 0])\n    abs_sin = abs(rotation_mat[0, 1])\n\n    height, width = image_shape[:2]\n\n    # find the new width and height bounds\n    new_w = math.ceil(height * abs_sin + width * abs_cos)\n    new_h = math.ceil(height * abs_cos + width * abs_sin)\n\n    scale_x = width / new_w\n    scale_y = height / new_h\n\n    # Shift the image to create padding\n    rotation_mat[0, 2] += new_w / 2 - image_center[0]\n    rotation_mat[1, 2] += new_h / 2 - image_center[1]\n\n    # Rescale to original size\n    scale_mat = np.diag(np.ones(3))\n    scale_mat[0, 0] *= scale_x\n    scale_mat[1, 1] *= scale_y\n    _tmp = np.diag(np.ones(3))\n    _tmp[:2] = rotation_mat\n    _tmp = scale_mat @ _tmp\n    rotation_mat = _tmp[:2]\n\n    return {\"matrix\": rotation_mat, \"angle\": angle, \"scale_x\": scale_x, \"scale_y\": scale_y}\n</code></pre>"},{"location":"api_reference/augmentations/geometric/rotate/#albumentations.augmentations.geometric.rotate.SafeRotate.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/geometric/rotate.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return \"limit\", \"interpolation\", \"border_mode\", \"value\", \"mask_value\"\n</code></pre>"},{"location":"api_reference/augmentations/geometric/transforms/","title":"Geometric transforms (augmentations.geometric.transforms)","text":""},{"location":"api_reference/augmentations/geometric/transforms/#albumentations.augmentations.geometric.transforms.Affine","title":"<code>class  Affine</code> <code>     (scale=None, translate_percent=None, translate_px=None, rotate=None, shear=None, interpolation=1, mask_interpolation=0, cval=0, cval_mask=0, mode=0, fit_output=False, keep_ratio=False, rotate_method='largest_box', balanced_scale=False, always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Augmentation to apply affine transformations to images.</p> <p>Affine transformations involve:</p> <pre><code>- Translation (\"move\" image on the x-/y-axis)\n- Rotation\n- Scaling (\"zoom\" in/out)\n- Shear (move one side of the image, turning a square into a trapezoid)\n</code></pre> <p>All such transformations can create \"new\" pixels in the image without a defined content, e.g. if the image is translated to the left, pixels are created on the right. A method has to be defined to deal with these pixel values. The parameters <code>cval</code> and <code>mode</code> of this class deal with this.</p> <p>Some transformations involve interpolations between several pixels of the input image to generate output pixel values. The parameters <code>interpolation</code> and <code>mask_interpolation</code> deals with the method of interpolation used for this.</p> <p>Parameters:</p> Name Type Description <code>scale</code> <code>number, tuple of number or dict</code> <p>Scaling factor to use, where <code>1.0</code> denotes \"no change\" and <code>0.5</code> is zoomed out to <code>50</code> percent of the original size.     * If a single number, then that value will be used for all images.     * If a tuple <code>(a, b)</code>, then a value will be uniformly sampled per image from the interval <code>[a, b]</code>.       That the same range will be used for both x- and y-axis. To keep the aspect ratio, set       <code>keep_ratio=True</code>, then the same value will be used for both x- and y-axis.     * If a dictionary, then it is expected to have the keys <code>x</code> and/or <code>y</code>.       Each of these keys can have the same values as described above.       Using a dictionary allows to set different values for the two axis and sampling will then happen       independently per axis, resulting in samples that differ between the axes. Note that when       the <code>keep_ratio=True</code>, the x- and y-axis ranges should be the same.</p> <code>translate_percent</code> <code>None, number, tuple of number or dict</code> <p>Translation as a fraction of the image height/width (x-translation, y-translation), where <code>0</code> denotes \"no change\" and <code>0.5</code> denotes \"half of the axis size\".     * If <code>None</code> then equivalent to <code>0.0</code> unless <code>translate_px</code> has a value other than <code>None</code>.     * If a single number, then that value will be used for all images.     * If a tuple <code>(a, b)</code>, then a value will be uniformly sampled per image from the interval <code>[a, b]</code>.       That sampled fraction value will be used identically for both x- and y-axis.     * If a dictionary, then it is expected to have the keys <code>x</code> and/or <code>y</code>.       Each of these keys can have the same values as described above.       Using a dictionary allows to set different values for the two axis and sampling will then happen       independently per axis, resulting in samples that differ between the axes.</p> <code>translate_px</code> <code>None, int, tuple of int or dict</code> <p>Translation in pixels.     * If <code>None</code> then equivalent to <code>0</code> unless <code>translate_percent</code> has a value other than <code>None</code>.     * If a single int, then that value will be used for all images.     * If a tuple <code>(a, b)</code>, then a value will be uniformly sampled per image from       the discrete interval <code>[a..b]</code>. That number will be used identically for both x- and y-axis.     * If a dictionary, then it is expected to have the keys <code>x</code> and/or <code>y</code>.       Each of these keys can have the same values as described above.       Using a dictionary allows to set different values for the two axis and sampling will then happen       independently per axis, resulting in samples that differ between the axes.</p> <code>rotate</code> <code>number or tuple of number</code> <p>Rotation in degrees (NOT radians), i.e. expected value range is around <code>[-360, 360]</code>. Rotation happens around the center of the image, not the top left corner as in some other frameworks.     * If a number, then that value will be used for all images.     * If a tuple <code>(a, b)</code>, then a value will be uniformly sampled per image from the interval <code>[a, b]</code>       and used as the rotation value.</p> <code>shear</code> <code>number, tuple of number or dict</code> <p>Shear in degrees (NOT radians), i.e. expected value range is around <code>[-360, 360]</code>, with reasonable values being in the range of <code>[-45, 45]</code>.     * If a number, then that value will be used for all images as       the shear on the x-axis (no shear on the y-axis will be done).     * If a tuple <code>(a, b)</code>, then two value will be uniformly sampled per image       from the interval <code>[a, b]</code> and be used as the x- and y-shear value.     * If a dictionary, then it is expected to have the keys <code>x</code> and/or <code>y</code>.       Each of these keys can have the same values as described above.       Using a dictionary allows to set different values for the two axis and sampling will then happen       independently per axis, resulting in samples that differ between the axes.</p> <code>interpolation</code> <code>int</code> <p>OpenCV interpolation flag.</p> <code>mask_interpolation</code> <code>int</code> <p>OpenCV interpolation flag.</p> <code>cval</code> <code>number or sequence of number</code> <p>The constant value to use when filling in newly created pixels. (E.g. translating by 1px to the right will create a new 1px-wide column of pixels on the left of the image). The value is only used when <code>mode=constant</code>. The expected value range is <code>[0, 255]</code> for <code>uint8</code> images.</p> <code>cval_mask</code> <code>number or tuple of number</code> <p>Same as cval but only for masks.</p> <code>mode</code> <code>int</code> <p>OpenCV border flag.</p> <code>fit_output</code> <code>bool</code> <p>If True, the image plane size and position will be adjusted to tightly capture the whole image after affine transformation (<code>translate_percent</code> and <code>translate_px</code> are ignored). Otherwise (<code>False</code>),  parts of the transformed image may end up outside the image plane. Fitting the output shape can be useful to avoid corners of the image being outside the image plane after applying rotations. Default: False</p> <code>keep_ratio</code> <code>bool</code> <p>When True, the original aspect ratio will be kept when the random scale is applied. Default: False.</p> <code>rotate_method</code> <code>Literal[\"largest_box\", \"ellipse\"]</code> <p>rotation method used for the bounding boxes. Should be one of \"largest_box\" or \"ellipse\"[1]. Default: \"largest_box\"</p> <code>balanced_scale</code> <code>bool</code> <p>When True, scaling factors are chosen to be either entirely below or above 1, ensuring balanced scaling. Default: False.</p> <p>This is important because without it, scaling tends to lean towards upscaling. For example, if we want the image to zoom in and out by 2x, we may pick an interval [0.5, 2]. Since the interval [0.5, 1] is three times smaller than [1, 2], values above 1 are picked three times more often if sampled directly from [0.5, 2]. With <code>balanced_scale</code>, the  function ensures that half the time, the scaling factor is picked from below 1 (zooming out), and the other half from above 1 (zooming in). This makes the zooming in and out process more balanced.</p> <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 0.5.</p> <p>Targets</p> <p>image, mask, keypoints, bboxes</p> <p>Image types:     uint8, float32</p> <p>Reference</p> <p>[1] https://arxiv.org/abs/2109.13488</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>class Affine(DualTransform):\n    \"\"\"Augmentation to apply affine transformations to images.\n\n    Affine transformations involve:\n\n        - Translation (\"move\" image on the x-/y-axis)\n        - Rotation\n        - Scaling (\"zoom\" in/out)\n        - Shear (move one side of the image, turning a square into a trapezoid)\n\n    All such transformations can create \"new\" pixels in the image without a defined content, e.g.\n    if the image is translated to the left, pixels are created on the right.\n    A method has to be defined to deal with these pixel values.\n    The parameters `cval` and `mode` of this class deal with this.\n\n    Some transformations involve interpolations between several pixels\n    of the input image to generate output pixel values. The parameters `interpolation` and\n    `mask_interpolation` deals with the method of interpolation used for this.\n\n    Args:\n        scale (number, tuple of number or dict): Scaling factor to use, where ``1.0`` denotes \"no change\" and\n            ``0.5`` is zoomed out to ``50`` percent of the original size.\n                * If a single number, then that value will be used for all images.\n                * If a tuple ``(a, b)``, then a value will be uniformly sampled per image from the interval ``[a, b]``.\n                  That the same range will be used for both x- and y-axis. To keep the aspect ratio, set\n                  ``keep_ratio=True``, then the same value will be used for both x- and y-axis.\n                * If a dictionary, then it is expected to have the keys ``x`` and/or ``y``.\n                  Each of these keys can have the same values as described above.\n                  Using a dictionary allows to set different values for the two axis and sampling will then happen\n                  *independently* per axis, resulting in samples that differ between the axes. Note that when\n                  the ``keep_ratio=True``, the x- and y-axis ranges should be the same.\n        translate_percent (None, number, tuple of number or dict): Translation as a fraction of the image height/width\n            (x-translation, y-translation), where ``0`` denotes \"no change\"\n            and ``0.5`` denotes \"half of the axis size\".\n                * If ``None`` then equivalent to ``0.0`` unless `translate_px` has a value other than ``None``.\n                * If a single number, then that value will be used for all images.\n                * If a tuple ``(a, b)``, then a value will be uniformly sampled per image from the interval ``[a, b]``.\n                  That sampled fraction value will be used identically for both x- and y-axis.\n                * If a dictionary, then it is expected to have the keys ``x`` and/or ``y``.\n                  Each of these keys can have the same values as described above.\n                  Using a dictionary allows to set different values for the two axis and sampling will then happen\n                  *independently* per axis, resulting in samples that differ between the axes.\n        translate_px (None, int, tuple of int or dict): Translation in pixels.\n                * If ``None`` then equivalent to ``0`` unless `translate_percent` has a value other than ``None``.\n                * If a single int, then that value will be used for all images.\n                * If a tuple ``(a, b)``, then a value will be uniformly sampled per image from\n                  the discrete interval ``[a..b]``. That number will be used identically for both x- and y-axis.\n                * If a dictionary, then it is expected to have the keys ``x`` and/or ``y``.\n                  Each of these keys can have the same values as described above.\n                  Using a dictionary allows to set different values for the two axis and sampling will then happen\n                  *independently* per axis, resulting in samples that differ between the axes.\n        rotate (number or tuple of number): Rotation in degrees (**NOT** radians), i.e. expected value range is\n            around ``[-360, 360]``. Rotation happens around the *center* of the image,\n            not the top left corner as in some other frameworks.\n                * If a number, then that value will be used for all images.\n                * If a tuple ``(a, b)``, then a value will be uniformly sampled per image from the interval ``[a, b]``\n                  and used as the rotation value.\n        shear (number, tuple of number or dict): Shear in degrees (**NOT** radians), i.e. expected value range is\n            around ``[-360, 360]``, with reasonable values being in the range of ``[-45, 45]``.\n                * If a number, then that value will be used for all images as\n                  the shear on the x-axis (no shear on the y-axis will be done).\n                * If a tuple ``(a, b)``, then two value will be uniformly sampled per image\n                  from the interval ``[a, b]`` and be used as the x- and y-shear value.\n                * If a dictionary, then it is expected to have the keys ``x`` and/or ``y``.\n                  Each of these keys can have the same values as described above.\n                  Using a dictionary allows to set different values for the two axis and sampling will then happen\n                  *independently* per axis, resulting in samples that differ between the axes.\n        interpolation (int): OpenCV interpolation flag.\n        mask_interpolation (int): OpenCV interpolation flag.\n        cval (number or sequence of number): The constant value to use when filling in newly created pixels.\n            (E.g. translating by 1px to the right will create a new 1px-wide column of pixels\n            on the left of the image).\n            The value is only used when `mode=constant`. The expected value range is ``[0, 255]`` for ``uint8`` images.\n        cval_mask (number or tuple of number): Same as cval but only for masks.\n        mode (int): OpenCV border flag.\n        fit_output (bool): If True, the image plane size and position will be adjusted to tightly capture\n            the whole image after affine transformation (`translate_percent` and `translate_px` are ignored).\n            Otherwise (``False``),  parts of the transformed image may end up outside the image plane.\n            Fitting the output shape can be useful to avoid corners of the image being outside the image plane\n            after applying rotations. Default: False\n        keep_ratio (bool): When True, the original aspect ratio will be kept when the random scale is applied.\n            Default: False.\n        rotate_method (Literal[\"largest_box\", \"ellipse\"]): rotation method used for the bounding boxes.\n            Should be one of \"largest_box\" or \"ellipse\"[1]. Default: \"largest_box\"\n        balanced_scale (bool): When True, scaling factors are chosen to be either entirely below or above 1,\n            ensuring balanced scaling. Default: False.\n\n            This is important because without it, scaling tends to lean towards upscaling. For example, if we want\n            the image to zoom in and out by 2x, we may pick an interval [0.5, 2]. Since the interval [0.5, 1] is\n            three times smaller than [1, 2], values above 1 are picked three times more often if sampled directly\n            from [0.5, 2]. With `balanced_scale`, the  function ensures that half the time, the scaling\n            factor is picked from below 1 (zooming out), and the other half from above 1 (zooming in).\n            This makes the zooming in and out process more balanced.\n        p (float): probability of applying the transform. Default: 0.5.\n\n    Targets:\n        image, mask, keypoints, bboxes\n\n    Image types:\n        uint8, float32\n\n    Reference:\n        [1] https://arxiv.org/abs/2109.13488\n\n    \"\"\"\n\n    _targets = (Targets.IMAGE, Targets.MASK, Targets.BBOXES, Targets.KEYPOINTS)\n\n    class InitSchema(BaseTransformInitSchema):\n        scale: ScaleFloatType | dict[str, Any] | None = Field(\n            default=None,\n            description=\"Scaling factor or dictionary for independent axis scaling.\",\n        )\n        translate_percent: ScaleFloatType | dict[str, Any] | None = Field(\n            default=None,\n            description=\"Translation as a fraction of the image dimension.\",\n        )\n        translate_px: ScaleIntType | dict[str, Any] | None = Field(\n            default=None,\n            description=\"Translation in pixels.\",\n        )\n        rotate: ScaleFloatType | None = Field(default=None, description=\"Rotation angle in degrees.\")\n        shear: ScaleFloatType | dict[str, Any] | None = Field(\n            default=None,\n            description=\"Shear angle in degrees.\",\n        )\n        interpolation: InterpolationType = cv2.INTER_LINEAR\n        mask_interpolation: InterpolationType = cv2.INTER_NEAREST\n\n        cval: ColorType = Field(default=0, description=\"Value used for constant padding.\")\n        cval_mask: ColorType = Field(default=0, description=\"Value used for mask constant padding.\")\n        mode: BorderModeType = cv2.BORDER_CONSTANT\n        fit_output: Annotated[bool, Field(default=False, description=\"Adjust output to capture whole image.\")]\n        keep_ratio: Annotated[bool, Field(default=False, description=\"Maintain aspect ratio when scaling.\")]\n        rotate_method: Literal[\"largest_box\", \"ellipse\"] = \"largest_box\"\n        balanced_scale: Annotated[bool, Field(default=False, description=\"Use balanced scaling.\")]\n\n    def __init__(\n        self,\n        scale: ScaleFloatType | dict[str, Any] | None = None,\n        translate_percent: ScaleFloatType | dict[str, Any] | None = None,\n        translate_px: ScaleIntType | dict[str, Any] | None = None,\n        rotate: ScaleFloatType | None = None,\n        shear: ScaleFloatType | dict[str, Any] | None = None,\n        interpolation: int = cv2.INTER_LINEAR,\n        mask_interpolation: int = cv2.INTER_NEAREST,\n        cval: ColorType = 0,\n        cval_mask: ColorType = 0,\n        mode: int = cv2.BORDER_CONSTANT,\n        fit_output: bool = False,\n        keep_ratio: bool = False,\n        rotate_method: Literal[\"largest_box\", \"ellipse\"] = \"largest_box\",\n        balanced_scale: bool = False,\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p=p, always_apply=always_apply)\n\n        params = [scale, translate_percent, translate_px, rotate, shear]\n        if all(p is None for p in params):\n            scale = {\"x\": (0.9, 1.1), \"y\": (0.9, 1.1)}\n            translate_percent = {\"x\": (-0.1, 0.1), \"y\": (-0.1, 0.1)}\n            rotate = (-15, 15)\n            shear = {\"x\": (-10, 10), \"y\": (-10, 10)}\n        else:\n            scale = scale if scale is not None else 1.0\n            rotate = rotate if rotate is not None else 0.0\n            shear = shear if shear is not None else 0.0\n\n        self.interpolation = interpolation\n        self.mask_interpolation = mask_interpolation\n        self.cval = cval\n        self.cval_mask = cval_mask\n        self.mode = mode\n        self.scale = self._handle_dict_arg(scale, \"scale\")\n        self.translate_percent, self.translate_px = self._handle_translate_arg(translate_px, translate_percent)\n        self.rotate = to_tuple(rotate, rotate)\n        self.fit_output = fit_output\n        self.shear = self._handle_dict_arg(shear, \"shear\")\n        self.keep_ratio = keep_ratio\n        self.rotate_method = rotate_method\n        self.balanced_scale = balanced_scale\n\n        if self.keep_ratio and self.scale[\"x\"] != self.scale[\"y\"]:\n            raise ValueError(f\"When keep_ratio is True, the x and y scale range should be identical. got {self.scale}\")\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return (\n            \"interpolation\",\n            \"mask_interpolation\",\n            \"cval\",\n            \"mode\",\n            \"scale\",\n            \"translate_percent\",\n            \"translate_px\",\n            \"rotate\",\n            \"fit_output\",\n            \"shear\",\n            \"cval_mask\",\n            \"keep_ratio\",\n            \"rotate_method\",\n            \"balanced_scale\",\n        )\n\n    @staticmethod\n    def _handle_dict_arg(\n        val: float | tuple[float, float] | dict[str, Any],\n        name: str,\n        default: float = 1.0,\n    ) -&gt; dict[str, Any]:\n        if isinstance(val, dict):\n            if \"x\" not in val and \"y\" not in val:\n                raise ValueError(\n                    f'Expected {name} dictionary to contain at least key \"x\" or key \"y\". Found neither of them.',\n                )\n            x = val.get(\"x\", default)\n            y = val.get(\"y\", default)\n            return {\"x\": to_tuple(x, x), \"y\": to_tuple(y, y)}\n        return {\"x\": to_tuple(val, val), \"y\": to_tuple(val, val)}\n\n    @classmethod\n    def _handle_translate_arg(\n        cls,\n        translate_px: ScaleFloatType | dict[str, Any] | None,\n        translate_percent: ScaleFloatType | dict[str, Any] | None,\n    ) -&gt; Any:\n        if translate_percent is None and translate_px is None:\n            translate_px = 0\n\n        if translate_percent is not None and translate_px is not None:\n            msg = \"Expected either translate_percent or translate_px to be provided, but both were provided.\"\n            raise ValueError(msg)\n\n        if translate_percent is not None:\n            # translate by percent\n            return cls._handle_dict_arg(translate_percent, \"translate_percent\", default=0.0), translate_px\n\n        if translate_px is None:\n            msg = \"translate_px is None.\"\n            raise ValueError(msg)\n        # translate by pixels\n        return translate_percent, cls._handle_dict_arg(translate_px, \"translate_px\")\n\n    def apply(\n        self,\n        img: np.ndarray,\n        matrix: skimage.transform.ProjectiveTransform,\n        output_shape: SizeType,\n        **params: Any,\n    ) -&gt; np.ndarray:\n        return fgeometric.warp_affine(\n            img,\n            matrix,\n            interpolation=self.interpolation,\n            cval=self.cval,\n            mode=self.mode,\n            output_shape=output_shape,\n        )\n\n    def apply_to_mask(\n        self,\n        mask: np.ndarray,\n        matrix: skimage.transform.ProjectiveTransform,\n        output_shape: SizeType,\n        **params: Any,\n    ) -&gt; np.ndarray:\n        return fgeometric.warp_affine(\n            mask,\n            matrix,\n            interpolation=self.mask_interpolation,\n            cval=self.cval_mask,\n            mode=self.mode,\n            output_shape=output_shape,\n        )\n\n    def apply_to_bboxes(\n        self,\n        bboxes: Sequence[BoxType],\n        bbox_matrix: skimage.transform.AffineTransform,\n        output_shape: SizeType,\n        **params: Any,\n    ) -&gt; list[BoxType]:\n        bboxes_np = np.array(bboxes)\n        result = fgeometric.bboxes_affine(\n            bboxes_np,\n            bbox_matrix,\n            self.rotate_method,\n            params[\"shape\"][:2],\n            self.mode,\n            output_shape,\n        )\n        return result.tolist()\n\n    def apply_to_keypoints(\n        self,\n        keypoints: Sequence[KeypointType],\n        matrix: skimage.transform.AffineTransform,\n        scale: dict[str, Any],\n        **params: Any,\n    ) -&gt; list[KeypointType]:\n        # Convert keypoints to numpy array, including all attributes\n        keypoints_array = np.array([list(kp) for kp in keypoints], dtype=np.float32)\n\n        padded_keypoints = fgeometric.keypoints_affine(keypoints_array, matrix, params[\"shape\"][:2], scale, self.mode)\n\n        # Convert back to list of tuples\n        return [tuple(kp) for kp in padded_keypoints]\n\n    @staticmethod\n    def get_scale(\n        scale: dict[str, tuple[float, float]],\n        keep_ratio: bool,\n        balanced_scale: bool,\n    ) -&gt; fgeometric.ScaleDict:\n        result_scale = {}\n        if balanced_scale:\n            for key, value in scale.items():\n                lower_interval = (value[0], 1.0) if value[0] &lt; 1 else None\n                upper_interval = (1.0, value[1]) if value[1] &gt; 1 else None\n\n                if lower_interval is not None and upper_interval is not None:\n                    selected_interval = random.choice([lower_interval, upper_interval])\n                elif lower_interval is not None:\n                    selected_interval = lower_interval\n                elif upper_interval is not None:\n                    selected_interval = upper_interval\n                else:\n                    raise ValueError(f\"Both lower_interval and upper_interval are None for key: {key}\")\n\n                result_scale[key] = random.uniform(*selected_interval)\n        else:\n            result_scale = {key: random.uniform(*value) for key, value in scale.items()}\n\n        if keep_ratio:\n            result_scale[\"y\"] = result_scale[\"x\"]\n\n        return cast(fgeometric.ScaleDict, result_scale)\n\n    def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n        image_shape = params[\"shape\"][:2]\n\n        translate = self._get_translate_params(image_shape)\n        shear = self._get_shear_params()\n        scale = self.get_scale(self.scale, self.keep_ratio, self.balanced_scale)\n        rotate = -random.uniform(*self.rotate)\n\n        image_shift = center(image_shape)\n        bbox_shift = center_bbox(image_shape)\n\n        matrix = fgeometric.create_affine_transformation_matrix(translate, shear, scale, rotate, image_shift)\n        bbox_matrix = fgeometric.create_affine_transformation_matrix(translate, shear, scale, rotate, bbox_shift)\n\n        if self.fit_output:\n            matrix, output_shape = fgeometric.compute_affine_warp_output_shape(matrix, image_shape)\n            bbox_matrix, _ = fgeometric.compute_affine_warp_output_shape(bbox_matrix, image_shape)\n        else:\n            output_shape = image_shape\n\n        return {\n            \"rotate\": rotate,\n            \"scale\": scale,\n            \"matrix\": matrix,\n            \"bbox_matrix\": bbox_matrix,\n            \"output_shape\": output_shape,\n        }\n\n    def _get_translate_params(self, image_shape: tuple[int, int]) -&gt; fgeometric.TranslateDict:\n        height, width = image_shape[:2]\n        if self.translate_px is not None:\n            return cast(\n                fgeometric.TranslateDict,\n                {key: random.randint(*value) for key, value in self.translate_px.items()},\n            )\n        if self.translate_percent is not None:\n            translate = {key: random.uniform(*value) for key, value in self.translate_percent.items()}\n            return cast(fgeometric.TranslateDict, {\"x\": translate[\"x\"] * width, \"y\": translate[\"y\"] * height})\n        return cast(fgeometric.TranslateDict, {\"x\": 0, \"y\": 0})\n\n    def _get_shear_params(self) -&gt; fgeometric.ShearDict:\n        return cast(fgeometric.ShearDict, {key: -random.uniform(*value) for key, value in self.shear.items()})\n</code></pre>"},{"location":"api_reference/augmentations/geometric/transforms/#albumentations.augmentations.geometric.transforms.Affine.apply","title":"<code>apply (self, img, matrix, output_shape, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def apply(\n    self,\n    img: np.ndarray,\n    matrix: skimage.transform.ProjectiveTransform,\n    output_shape: SizeType,\n    **params: Any,\n) -&gt; np.ndarray:\n    return fgeometric.warp_affine(\n        img,\n        matrix,\n        interpolation=self.interpolation,\n        cval=self.cval,\n        mode=self.mode,\n        output_shape=output_shape,\n    )\n</code></pre>"},{"location":"api_reference/augmentations/geometric/transforms/#albumentations.augmentations.geometric.transforms.Affine.get_params_dependent_on_data","title":"<code>get_params_dependent_on_data (self, params, data)</code>","text":"<p>Returns parameters dependent on input.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n    image_shape = params[\"shape\"][:2]\n\n    translate = self._get_translate_params(image_shape)\n    shear = self._get_shear_params()\n    scale = self.get_scale(self.scale, self.keep_ratio, self.balanced_scale)\n    rotate = -random.uniform(*self.rotate)\n\n    image_shift = center(image_shape)\n    bbox_shift = center_bbox(image_shape)\n\n    matrix = fgeometric.create_affine_transformation_matrix(translate, shear, scale, rotate, image_shift)\n    bbox_matrix = fgeometric.create_affine_transformation_matrix(translate, shear, scale, rotate, bbox_shift)\n\n    if self.fit_output:\n        matrix, output_shape = fgeometric.compute_affine_warp_output_shape(matrix, image_shape)\n        bbox_matrix, _ = fgeometric.compute_affine_warp_output_shape(bbox_matrix, image_shape)\n    else:\n        output_shape = image_shape\n\n    return {\n        \"rotate\": rotate,\n        \"scale\": scale,\n        \"matrix\": matrix,\n        \"bbox_matrix\": bbox_matrix,\n        \"output_shape\": output_shape,\n    }\n</code></pre>"},{"location":"api_reference/augmentations/geometric/transforms/#albumentations.augmentations.geometric.transforms.Affine.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return (\n        \"interpolation\",\n        \"mask_interpolation\",\n        \"cval\",\n        \"mode\",\n        \"scale\",\n        \"translate_percent\",\n        \"translate_px\",\n        \"rotate\",\n        \"fit_output\",\n        \"shear\",\n        \"cval_mask\",\n        \"keep_ratio\",\n        \"rotate_method\",\n        \"balanced_scale\",\n    )\n</code></pre>"},{"location":"api_reference/augmentations/geometric/transforms/#albumentations.augmentations.geometric.transforms.D4","title":"<code>class  D4</code> <code>     (always_apply=None, p=1)                 </code>  [view source on GitHub]","text":"<p>Applies one of the eight possible D4 dihedral group transformations to a square-shaped input,     maintaining the square shape. These transformations correspond to the symmetries of a square,     including rotations and reflections.</p> <p>The D4 group transformations include: - 'e' (identity): No transformation is applied. - 'r90' (rotation by 90 degrees counterclockwise) - 'r180' (rotation by 180 degrees) - 'r270' (rotation by 270 degrees counterclockwise) - 'v' (reflection across the vertical midline) - 'hvt' (reflection across the anti-diagonal) - 'h' (reflection across the horizontal midline) - 't' (reflection across the main diagonal)</p> <p>Even if the probability (<code>p</code>) of applying the transform is set to 1, the identity transformation 'e' may still occur, which means the input will remain unchanged in one out of eight cases.</p> <p>Parameters:</p> Name Type Description <code>p</code> <code>float</code> <p>Probability of applying the transform. Default is 1, meaning the        transform is applied every time it is called.</p> <p>Targets</p> <p>image, mask, bboxes, keypoints</p> <p>Image types:     uint8, float32</p> <p>Note</p> <p>This transform is particularly useful when augmenting data that does not have a clear orientation: - Top view satellite or drone imagery - Medical images</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>class D4(DualTransform):\n    \"\"\"Applies one of the eight possible D4 dihedral group transformations to a square-shaped input,\n        maintaining the square shape. These transformations correspond to the symmetries of a square,\n        including rotations and reflections.\n\n    The D4 group transformations include:\n    - 'e' (identity): No transformation is applied.\n    - 'r90' (rotation by 90 degrees counterclockwise)\n    - 'r180' (rotation by 180 degrees)\n    - 'r270' (rotation by 270 degrees counterclockwise)\n    - 'v' (reflection across the vertical midline)\n    - 'hvt' (reflection across the anti-diagonal)\n    - 'h' (reflection across the horizontal midline)\n    - 't' (reflection across the main diagonal)\n\n    Even if the probability (`p`) of applying the transform is set to 1, the identity transformation\n    'e' may still occur, which means the input will remain unchanged in one out of eight cases.\n\n    Args:\n        p (float): Probability of applying the transform. Default is 1, meaning the\n                   transform is applied every time it is called.\n\n    Targets:\n        image, mask, bboxes, keypoints\n\n    Image types:\n        uint8, float32\n\n    Note:\n        This transform is particularly useful when augmenting data that does not have a clear orientation:\n        - Top view satellite or drone imagery\n        - Medical images\n\n    \"\"\"\n\n    _targets = (Targets.IMAGE, Targets.MASK, Targets.BBOXES, Targets.KEYPOINTS)\n\n    class InitSchema(BaseTransformInitSchema):\n        p: ProbabilityType = 1\n\n    def __init__(\n        self,\n        always_apply: bool | None = None,\n        p: float = 1,\n    ):\n        super().__init__(p=p, always_apply=always_apply)\n\n    def apply(self, img: np.ndarray, group_element: D4Type, **params: Any) -&gt; np.ndarray:\n        return fgeometric.d4(img, group_element)\n\n    def apply_to_bbox(self, bbox: BoxInternalType, group_element: D4Type, **params: Any) -&gt; BoxInternalType:\n        return fgeometric.bbox_d4(bbox, group_element)\n\n    def apply_to_keypoint(\n        self,\n        keypoint: KeypointInternalType,\n        group_element: D4Type,\n        **params: Any,\n    ) -&gt; KeypointInternalType:\n        return fgeometric.keypoint_d4(keypoint, group_element, params[\"shape\"])\n\n    def get_params(self) -&gt; dict[str, D4Type]:\n        return {\n            \"group_element\": random_utils.choice(d4_group_elements),\n        }\n\n    def get_transform_init_args_names(self) -&gt; tuple[()]:\n        return ()\n</code></pre>"},{"location":"api_reference/augmentations/geometric/transforms/#albumentations.augmentations.geometric.transforms.D4.apply","title":"<code>apply (self, img, group_element, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def apply(self, img: np.ndarray, group_element: D4Type, **params: Any) -&gt; np.ndarray:\n    return fgeometric.d4(img, group_element)\n</code></pre>"},{"location":"api_reference/augmentations/geometric/transforms/#albumentations.augmentations.geometric.transforms.D4.get_params","title":"<code>get_params (self)</code>","text":"<p>Returns parameters independent of input.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def get_params(self) -&gt; dict[str, D4Type]:\n    return {\n        \"group_element\": random_utils.choice(d4_group_elements),\n    }\n</code></pre>"},{"location":"api_reference/augmentations/geometric/transforms/#albumentations.augmentations.geometric.transforms.D4.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[()]:\n    return ()\n</code></pre>"},{"location":"api_reference/augmentations/geometric/transforms/#albumentations.augmentations.geometric.transforms.ElasticTransform","title":"<code>class  ElasticTransform</code> <code>     (alpha=3, sigma=50, alpha_affine=None, interpolation=1, border_mode=4, value=None, mask_value=None, always_apply=None, approximate=False, same_dxdy=False, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Apply elastic deformation to images, masks, and bounding boxes as described in [Simard2003]_.</p> <p>This transformation introduces random elastic distortions to images, which can be useful for data augmentation in training convolutional neural networks. The transformation can be applied in an approximate or precise manner, with an option to use the same displacement field for both x and y directions to speed up the process.</p> <p>Parameters:</p> Name Type Description <code>alpha</code> <code>float</code> <p>Scaling factor for the random displacement fields.</p> <code>sigma</code> <code>float</code> <p>Standard deviation for Gaussian filter applied to the displacement fields.</p> <code>interpolation</code> <code>int</code> <p>Interpolation method to be used. Should be one of: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4. Default is cv2.INTER_LINEAR.</p> <code>border_mode</code> <code>int</code> <p>Pixel extrapolation method. Should be one of: cv2.BORDER_CONSTANT, cv2.BORDER_REPLICATE, cv2.BORDER_REFLECT, cv2.BORDER_WRAP, cv2.BORDER_REFLECT_101. Default is cv2.BORDER_REFLECT_101.</p> <code>value</code> <code>int, float, list of int, list of float</code> <p>Padding value if border_mode is cv2.BORDER_CONSTANT.</p> <code>mask_value</code> <code>int, float, list of int, list of float</code> <p>Padding value if border_mode is cv2.BORDER_CONSTANT, applied to masks.</p> <code>approximate</code> <code>bool</code> <p>Whether to smooth displacement map with a fixed kernel size. Enabling this option gives ~2X speedup on large images. Default is False.</p> <code>same_dxdy</code> <code>bool</code> <p>Whether to use the same random displacement for x and y directions. Enabling this option gives ~2X speedup. Default is False.</p> <p>Targets</p> <p>image, mask, bboxes</p> <p>Image types:     uint8, float32</p> <p>Reference</p> <p>Simard, Steinkraus and Platt, \"Best Practices for Convolutional Neural Networks applied to Visual Document Analysis\", in Proc. of the International Conference on Document Analysis and Recognition, 2003. https://gist.github.com/ernestum/601cdf56d2b424757de5</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>class ElasticTransform(DualTransform):\n    \"\"\"Apply elastic deformation to images, masks, and bounding boxes as described in [Simard2003]_.\n\n    This transformation introduces random elastic distortions to images, which can be useful for data augmentation\n    in training convolutional neural networks. The transformation can be applied in an approximate or precise manner,\n    with an option to use the same displacement field for both x and y directions to speed up the process.\n\n    Args:\n        alpha (float): Scaling factor for the random displacement fields.\n        sigma (float): Standard deviation for Gaussian filter applied to the displacement fields.\n        interpolation (int): Interpolation method to be used. Should be one of:\n            cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.\n            Default is cv2.INTER_LINEAR.\n        border_mode (int): Pixel extrapolation method. Should be one of:\n            cv2.BORDER_CONSTANT, cv2.BORDER_REPLICATE, cv2.BORDER_REFLECT, cv2.BORDER_WRAP, cv2.BORDER_REFLECT_101.\n            Default is cv2.BORDER_REFLECT_101.\n        value (int, float, list of int, list of float, optional): Padding value if border_mode is cv2.BORDER_CONSTANT.\n        mask_value (int, float, list of int, list of float, optional): Padding value if border_mode is\n            cv2.BORDER_CONSTANT, applied to masks.\n        approximate (bool, optional): Whether to smooth displacement map with a fixed kernel size.\n            Enabling this option gives ~2X speedup on large images. Default is False.\n        same_dxdy (bool, optional): Whether to use the same random displacement for x and y directions.\n            Enabling this option gives ~2X speedup. Default is False.\n\n    Targets:\n        image, mask, bboxes\n\n    Image types:\n        uint8, float32\n\n    Reference:\n        Simard, Steinkraus and Platt, \"Best Practices for Convolutional Neural Networks applied to\n        Visual Document Analysis\", in Proc. of the International Conference on Document Analysis and Recognition, 2003.\n        https://gist.github.com/ernestum/601cdf56d2b424757de5\n    \"\"\"\n\n    _targets = (Targets.IMAGE, Targets.MASK, Targets.BBOXES)\n\n    class InitSchema(BaseTransformInitSchema):\n        alpha: Annotated[float, Field(description=\"Alpha parameter.\", ge=0)]\n        sigma: Annotated[float, Field(default=50, description=\"Sigma parameter for Gaussian filter.\", ge=1)]\n        alpha_affine: None = Field(\n            description=\"Alpha affine parameter.\",\n            deprecated=\"Use Affine transform to get affine effects\",\n        )\n        interpolation: InterpolationType = cv2.INTER_LINEAR\n        border_mode: BorderModeType = cv2.BORDER_REFLECT_101\n        value: int | float | list[int] | list[float] | None = Field(\n            default=None,\n            description=\"Padding value if border_mode is cv2.BORDER_CONSTANT.\",\n        )\n        mask_value: float | list[int] | list[float] | None = Field(\n            default=None,\n            description=\"Padding value if border_mode is cv2.BORDER_CONSTANT applied for masks.\",\n        )\n        approximate: Annotated[bool, Field(default=False, description=\"Approximate displacement map smoothing.\")]\n        same_dxdy: Annotated[bool, Field(default=False, description=\"Use same shift for x and y.\")]\n\n    def __init__(\n        self,\n        alpha: float = 3,\n        sigma: float = 50,\n        alpha_affine: None = None,\n        interpolation: int = cv2.INTER_LINEAR,\n        border_mode: int = cv2.BORDER_REFLECT_101,\n        value: ScalarType | list[ScalarType] | None = None,\n        mask_value: ScalarType | list[ScalarType] | None = None,\n        always_apply: bool | None = None,\n        approximate: bool = False,\n        same_dxdy: bool = False,\n        p: float = 0.5,\n    ):\n        super().__init__(p=p, always_apply=always_apply)\n        self.alpha = alpha\n        self.sigma = sigma\n        self.interpolation = interpolation\n        self.border_mode = border_mode\n        self.value = value\n        self.mask_value = mask_value\n        self.approximate = approximate\n        self.same_dxdy = same_dxdy\n\n    def apply(\n        self,\n        img: np.ndarray,\n        random_seed: int,\n        interpolation: int,\n        **params: Any,\n    ) -&gt; np.ndarray:\n        return fgeometric.elastic_transform(\n            img,\n            self.alpha,\n            self.sigma,\n            interpolation,\n            self.border_mode,\n            self.value,\n            np.random.RandomState(random_seed),\n            self.approximate,\n            self.same_dxdy,\n        )\n\n    def apply_to_mask(self, mask: np.ndarray, random_seed: int, **params: Any) -&gt; np.ndarray:\n        return fgeometric.elastic_transform(\n            mask,\n            self.alpha,\n            self.sigma,\n            cv2.INTER_NEAREST,\n            self.border_mode,\n            self.mask_value,\n            np.random.RandomState(random_seed),\n            self.approximate,\n            self.same_dxdy,\n        )\n\n    def apply_to_bbox(\n        self,\n        bbox: BoxInternalType,\n        random_seed: int,\n        **params: Any,\n    ) -&gt; BoxInternalType:\n        image_shape = params[\"shape\"][:2]\n\n        mask = np.zeros(image_shape, dtype=np.uint8)\n        bbox_denorm = fgeometric.denormalize_bbox(bbox, image_shape)\n        x_min, y_min, x_max, y_max = bbox_denorm[:4]\n        x_min, y_min, x_max, y_max = int(x_min), int(y_min), int(x_max), int(y_max)\n        mask[y_min:y_max, x_min:x_max] = 1\n        mask = fgeometric.elastic_transform(\n            mask,\n            self.alpha,\n            self.sigma,\n            cv2.INTER_NEAREST,\n            self.border_mode,\n            self.mask_value,\n            np.random.RandomState(random_seed),\n            self.approximate,\n        )\n        bbox_returned = bbox_from_mask(mask)\n        return cast(BoxInternalType, fgeometric.normalize_bbox(bbox_returned, image_shape))\n\n    def get_params(self) -&gt; dict[str, int]:\n        return {\"random_seed\": random_utils.get_random_seed()}\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return (\n            \"alpha\",\n            \"sigma\",\n            \"interpolation\",\n            \"border_mode\",\n            \"value\",\n            \"mask_value\",\n            \"approximate\",\n            \"same_dxdy\",\n        )\n\n    @property\n    def targets(self) -&gt; dict[str, Callable[..., Any]]:\n        return {\n            \"image\": self.apply,\n            \"mask\": self.apply_to_mask,\n            \"masks\": self.apply_to_masks,\n            \"bboxes\": self.apply_to_bboxes,\n        }\n</code></pre>"},{"location":"api_reference/augmentations/geometric/transforms/#albumentations.augmentations.geometric.transforms.ElasticTransform.apply","title":"<code>apply (self, img, random_seed, interpolation, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def apply(\n    self,\n    img: np.ndarray,\n    random_seed: int,\n    interpolation: int,\n    **params: Any,\n) -&gt; np.ndarray:\n    return fgeometric.elastic_transform(\n        img,\n        self.alpha,\n        self.sigma,\n        interpolation,\n        self.border_mode,\n        self.value,\n        np.random.RandomState(random_seed),\n        self.approximate,\n        self.same_dxdy,\n    )\n</code></pre>"},{"location":"api_reference/augmentations/geometric/transforms/#albumentations.augmentations.geometric.transforms.ElasticTransform.get_params","title":"<code>get_params (self)</code>","text":"<p>Returns parameters independent of input.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def get_params(self) -&gt; dict[str, int]:\n    return {\"random_seed\": random_utils.get_random_seed()}\n</code></pre>"},{"location":"api_reference/augmentations/geometric/transforms/#albumentations.augmentations.geometric.transforms.ElasticTransform.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return (\n        \"alpha\",\n        \"sigma\",\n        \"interpolation\",\n        \"border_mode\",\n        \"value\",\n        \"mask_value\",\n        \"approximate\",\n        \"same_dxdy\",\n    )\n</code></pre>"},{"location":"api_reference/augmentations/geometric/transforms/#albumentations.augmentations.geometric.transforms.Flip","title":"<code>class  Flip</code> <code>     (always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Deprecated. Consider using HorizontalFlip, VerticalFlip, RandomRotate90 or D4.</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>class Flip(DualTransform):\n    \"\"\"Deprecated. Consider using HorizontalFlip, VerticalFlip, RandomRotate90 or D4.\"\"\"\n\n    _targets = (Targets.IMAGE, Targets.MASK, Targets.BBOXES, Targets.KEYPOINTS)\n\n    def __init__(self, always_apply: bool | None = None, p: float = 0.5):\n        super().__init__(p=p, always_apply=always_apply)\n        warn(\n            \"Flip is deprecated. Consider using HorizontalFlip, VerticalFlip, RandomRotate90 or D4.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n\n    def apply(self, img: np.ndarray, d: int, **params: Any) -&gt; np.ndarray:\n        \"\"\"Args:\n        d (int): code that specifies how to flip the input. 0 for vertical flipping, 1 for horizontal flipping,\n                -1 for both vertical and horizontal flipping (which is also could be seen as rotating the input by\n                180 degrees).\n        \"\"\"\n        return fgeometric.random_flip(img, d)\n\n    def get_params(self) -&gt; dict[str, int]:\n        # Random int in the range [-1, 1]\n        return {\"d\": random.randint(-1, 1)}\n\n    def apply_to_bbox(self, bbox: BoxInternalType, **params: Any) -&gt; BoxInternalType:\n        return fgeometric.bbox_flip(bbox, params[\"d\"])\n\n    def apply_to_keypoint(self, keypoint: KeypointInternalType, **params: Any) -&gt; KeypointInternalType:\n        return fgeometric.keypoint_flip(keypoint, params[\"d\"], params[\"shape\"])\n\n    def get_transform_init_args_names(self) -&gt; tuple[()]:\n        return ()\n</code></pre>"},{"location":"api_reference/augmentations/geometric/transforms/#albumentations.augmentations.geometric.transforms.Flip.__init__","title":"<code>__init__ (self, always_apply=None, p=0.5)</code>  <code>special</code>","text":"<p>Initialize self.  See help(type(self)) for accurate signature.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def __init__(self, always_apply: bool | None = None, p: float = 0.5):\n    super().__init__(p=p, always_apply=always_apply)\n    warn(\n        \"Flip is deprecated. Consider using HorizontalFlip, VerticalFlip, RandomRotate90 or D4.\",\n        DeprecationWarning,\n        stacklevel=2,\n    )\n</code></pre>"},{"location":"api_reference/augmentations/geometric/transforms/#albumentations.augmentations.geometric.transforms.Flip.apply","title":"<code>apply (self, img, d, **params)</code>","text":"<p>d (int): code that specifies how to flip the input. 0 for vertical flipping, 1 for horizontal flipping,         -1 for both vertical and horizontal flipping (which is also could be seen as rotating the input by         180 degrees).</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def apply(self, img: np.ndarray, d: int, **params: Any) -&gt; np.ndarray:\n    \"\"\"Args:\n    d (int): code that specifies how to flip the input. 0 for vertical flipping, 1 for horizontal flipping,\n            -1 for both vertical and horizontal flipping (which is also could be seen as rotating the input by\n            180 degrees).\n    \"\"\"\n    return fgeometric.random_flip(img, d)\n</code></pre>"},{"location":"api_reference/augmentations/geometric/transforms/#albumentations.augmentations.geometric.transforms.Flip.get_params","title":"<code>get_params (self)</code>","text":"<p>Returns parameters independent of input.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def get_params(self) -&gt; dict[str, int]:\n    # Random int in the range [-1, 1]\n    return {\"d\": random.randint(-1, 1)}\n</code></pre>"},{"location":"api_reference/augmentations/geometric/transforms/#albumentations.augmentations.geometric.transforms.Flip.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[()]:\n    return ()\n</code></pre>"},{"location":"api_reference/augmentations/geometric/transforms/#albumentations.augmentations.geometric.transforms.GridDistortion","title":"<code>class  GridDistortion</code> <code>     (num_steps=5, distort_limit=(-0.3, 0.3), interpolation=1, border_mode=4, value=None, mask_value=None, normalized=False, always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Applies grid distortion augmentation to images, masks, and bounding boxes. This technique involves dividing the image into a grid of cells and randomly displacing the intersection points of the grid, resulting in localized distortions.</p> <p>Parameters:</p> Name Type Description <code>num_steps</code> <code>int</code> <p>Number of grid cells on each side (minimum 1).</p> <code>distort_limit</code> <code>float, (float, float</code> <p>Range of distortion limits. If a single float is provided, the range will be from (-distort_limit, distort_limit). Default: (-0.3, 0.3).</p> <code>interpolation</code> <code>OpenCV flag</code> <p>Interpolation algorithm used for image transformation. Options are: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4. Default: cv2.INTER_LINEAR.</p> <code>border_mode</code> <code>OpenCV flag</code> <p>Pixel extrapolation method used when pixels outside the image are required. Options are: cv2.BORDER_CONSTANT, cv2.BORDER_REPLICATE, cv2.BORDER_REFLECT, cv2.BORDER_WRAP, cv2.BORDER_REFLECT_101. Default: cv2.BORDER_REFLECT_101.</p> <code>value</code> <code>int, float, list of ints, list of floats</code> <p>Value used for padding when border_mode is cv2.BORDER_CONSTANT.</p> <code>mask_value</code> <code>int, float, list of ints, list of floats</code> <p>Padding value for masks when border_mode is cv2.BORDER_CONSTANT.</p> <code>normalized</code> <code>bool</code> <p>If True, ensures that distortion does not exceed image boundaries. Default: False. Reference: https://github.com/albumentations-team/albumentations/pull/722</p> <p>Targets</p> <p>image, mask, bboxes</p> <p>Image types:     uint8, float32</p> <p>Note</p> <p>This transform is helpful in medical imagery, Optical Character Recognition, and other tasks where local distance may not be preserved.</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>class GridDistortion(DualTransform):\n    \"\"\"Applies grid distortion augmentation to images, masks, and bounding boxes. This technique involves dividing\n    the image into a grid of cells and randomly displacing the intersection points of the grid,\n    resulting in localized distortions.\n\n    Args:\n        num_steps (int): Number of grid cells on each side (minimum 1).\n        distort_limit (float, (float, float)): Range of distortion limits. If a single float is provided,\n            the range will be from (-distort_limit, distort_limit). Default: (-0.3, 0.3).\n        interpolation (OpenCV flag): Interpolation algorithm used for image transformation. Options are:\n            cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.\n            Default: cv2.INTER_LINEAR.\n        border_mode (OpenCV flag): Pixel extrapolation method used when pixels outside the image are required.\n            Options are: cv2.BORDER_CONSTANT, cv2.BORDER_REPLICATE, cv2.BORDER_REFLECT, cv2.BORDER_WRAP,\n            cv2.BORDER_REFLECT_101.\n            Default: cv2.BORDER_REFLECT_101.\n        value (int, float, list of ints, list of floats, optional): Value used for padding when\n            border_mode is cv2.BORDER_CONSTANT.\n        mask_value (int, float, list of ints, list of floats, optional): Padding value for masks when\n            border_mode is cv2.BORDER_CONSTANT.\n        normalized (bool): If True, ensures that distortion does not exceed image boundaries. Default: False.\n            Reference: https://github.com/albumentations-team/albumentations/pull/722\n\n    Targets:\n        image, mask, bboxes\n\n    Image types:\n        uint8, float32\n\n    Note:\n        This transform is helpful in medical imagery, Optical Character Recognition, and other tasks where local\n        distance may not be preserved.\n    \"\"\"\n\n    _targets = (Targets.IMAGE, Targets.MASK, Targets.BBOXES)\n\n    class InitSchema(BaseTransformInitSchema):\n        num_steps: Annotated[int, Field(ge=1, description=\"Count of grid cells on each side.\")]\n        distort_limit: SymmetricRangeType = (-0.3, 0.3)\n        interpolation: InterpolationType = cv2.INTER_LINEAR\n        border_mode: BorderModeType = cv2.BORDER_REFLECT_101\n        value: ColorType | None = Field(\n            default=None,\n            description=\"Padding value if border_mode is cv2.BORDER_CONSTANT.\",\n        )\n        mask_value: ColorType | None = Field(\n            default=None,\n            description=\"Padding value for mask if border_mode is cv2.BORDER_CONSTANT.\",\n        )\n        normalized: bool = Field(\n            default=False,\n            description=\"If true, distortion will be normalized to not go outside the image.\",\n        )\n\n        @field_validator(\"distort_limit\")\n        @classmethod\n        def check_limits(cls, v: tuple[float, float], info: ValidationInfo) -&gt; tuple[float, float]:\n            bounds = -1, 1\n            result = to_tuple(v)\n            check_range(result, *bounds, info.field_name)\n            return result\n\n    def __init__(\n        self,\n        num_steps: int = 5,\n        distort_limit: ScaleFloatType = (-0.3, 0.3),\n        interpolation: int = cv2.INTER_LINEAR,\n        border_mode: int = cv2.BORDER_REFLECT_101,\n        value: ColorType | None = None,\n        mask_value: ColorType | None = None,\n        normalized: bool = False,\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p, always_apply)\n\n        self.num_steps = num_steps\n        self.distort_limit = cast(Tuple[float, float], distort_limit)\n        self.interpolation = interpolation\n        self.border_mode = border_mode\n        self.value = value\n        self.mask_value = mask_value\n        self.normalized = normalized\n\n    def apply(\n        self,\n        img: np.ndarray,\n        stepsx: tuple[()],\n        stepsy: tuple[()],\n        interpolation: int,\n        **params: Any,\n    ) -&gt; np.ndarray:\n        return fgeometric.grid_distortion(\n            img,\n            self.num_steps,\n            stepsx,\n            stepsy,\n            interpolation,\n            self.border_mode,\n            self.value,\n        )\n\n    def apply_to_mask(\n        self,\n        mask: np.ndarray,\n        stepsx: tuple[()],\n        stepsy: tuple[()],\n        **params: Any,\n    ) -&gt; np.ndarray:\n        return fgeometric.grid_distortion(\n            mask,\n            self.num_steps,\n            stepsx,\n            stepsy,\n            cv2.INTER_NEAREST,\n            self.border_mode,\n            self.mask_value,\n        )\n\n    def apply_to_bbox(\n        self,\n        bbox: BoxInternalType,\n        stepsx: tuple[()],\n        stepsy: tuple[()],\n        **params: Any,\n    ) -&gt; BoxInternalType:\n        image_shape = params[\"shape\"][:2]\n        mask = np.zeros(image_shape, dtype=np.uint8)\n        bbox_denorm = fgeometric.denormalize_bbox(bbox, image_shape)\n        x_min, y_min, x_max, y_max = bbox_denorm[:4]\n        x_min, y_min, x_max, y_max = int(x_min), int(y_min), int(x_max), int(y_max)\n        mask[y_min:y_max, x_min:x_max] = 1\n        mask = fgeometric.grid_distortion(\n            mask,\n            self.num_steps,\n            stepsx,\n            stepsy,\n            cv2.INTER_NEAREST,\n            self.border_mode,\n            self.mask_value,\n        )\n        bbox_returned = bbox_from_mask(mask)\n        return cast(BoxInternalType, fgeometric.normalize_bbox(bbox_returned, image_shape))\n\n    def _normalize(self, h: int, w: int, xsteps: list[float], ysteps: list[float]) -&gt; dict[str, Any]:\n        # compensate for smaller last steps in source image.\n        x_step = w // self.num_steps\n        last_x_step = min(w, ((self.num_steps + 1) * x_step)) - (self.num_steps * x_step)\n        xsteps[-1] *= last_x_step / x_step\n\n        y_step = h // self.num_steps\n        last_y_step = min(h, ((self.num_steps + 1) * y_step)) - (self.num_steps * y_step)\n        ysteps[-1] *= last_y_step / y_step\n\n        # now normalize such that distortion never leaves image bounds.\n        tx = w / math.floor(w / self.num_steps)\n        ty = h / math.floor(h / self.num_steps)\n        xsteps = np.array(xsteps) * (tx / np.sum(xsteps))\n        ysteps = np.array(ysteps) * (ty / np.sum(ysteps))\n\n        return {\"stepsx\": xsteps, \"stepsy\": ysteps}\n\n    def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n        height, width = params[\"shape\"][:2]\n\n        stepsx = [1 + random.uniform(self.distort_limit[0], self.distort_limit[1]) for _ in range(self.num_steps + 1)]\n        stepsy = [1 + random.uniform(self.distort_limit[0], self.distort_limit[1]) for _ in range(self.num_steps + 1)]\n\n        if self.normalized:\n            return self._normalize(height, width, stepsx, stepsy)\n\n        return {\"stepsx\": stepsx, \"stepsy\": stepsy}\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return \"num_steps\", \"distort_limit\", \"interpolation\", \"border_mode\", \"value\", \"mask_value\", \"normalized\"\n\n    @property\n    def targets(self) -&gt; dict[str, Callable[..., Any]]:\n        return {\n            \"image\": self.apply,\n            \"mask\": self.apply_to_mask,\n            \"masks\": self.apply_to_masks,\n            \"bboxes\": self.apply_to_bboxes,\n        }\n</code></pre>"},{"location":"api_reference/augmentations/geometric/transforms/#albumentations.augmentations.geometric.transforms.GridDistortion.apply","title":"<code>apply (self, img, stepsx, stepsy, interpolation, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def apply(\n    self,\n    img: np.ndarray,\n    stepsx: tuple[()],\n    stepsy: tuple[()],\n    interpolation: int,\n    **params: Any,\n) -&gt; np.ndarray:\n    return fgeometric.grid_distortion(\n        img,\n        self.num_steps,\n        stepsx,\n        stepsy,\n        interpolation,\n        self.border_mode,\n        self.value,\n    )\n</code></pre>"},{"location":"api_reference/augmentations/geometric/transforms/#albumentations.augmentations.geometric.transforms.GridDistortion.get_params_dependent_on_data","title":"<code>get_params_dependent_on_data (self, params, data)</code>","text":"<p>Returns parameters dependent on input.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n    height, width = params[\"shape\"][:2]\n\n    stepsx = [1 + random.uniform(self.distort_limit[0], self.distort_limit[1]) for _ in range(self.num_steps + 1)]\n    stepsy = [1 + random.uniform(self.distort_limit[0], self.distort_limit[1]) for _ in range(self.num_steps + 1)]\n\n    if self.normalized:\n        return self._normalize(height, width, stepsx, stepsy)\n\n    return {\"stepsx\": stepsx, \"stepsy\": stepsy}\n</code></pre>"},{"location":"api_reference/augmentations/geometric/transforms/#albumentations.augmentations.geometric.transforms.GridDistortion.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return \"num_steps\", \"distort_limit\", \"interpolation\", \"border_mode\", \"value\", \"mask_value\", \"normalized\"\n</code></pre>"},{"location":"api_reference/augmentations/geometric/transforms/#albumentations.augmentations.geometric.transforms.GridElasticDeform","title":"<code>class  GridElasticDeform</code> <code>     (num_grid_xy, magnitude, interpolation=1, mask_interpolation=0, p=1.0, always_apply=None)                 </code>  [view source on GitHub]","text":"<p>Grid-based Elastic deformation Albumentation implementation</p> <p>This class applies elastic transformations using a grid-based approach. The granularity and intensity of the distortions can be controlled using the dimensions of the overlaying distortion grid and the magnitude parameter. Larger grid sizes result in finer, less severe distortions.</p> <p>Parameters:</p> Name Type Description <code>num_grid_xy</code> <code>tuple[int, int]</code> <p>Number of grid cells along the width and height. Specified as (grid_width, grid_height). Each value must be greater than 1.</p> <code>magnitude</code> <code>int</code> <p>Maximum pixel-wise displacement for distortion. Must be greater than 0.</p> <code>interpolation</code> <code>int</code> <p>Interpolation method to be used for the image transformation. Default: cv2.INTER_LINEAR</p> <code>mask_interpolation</code> <code>int</code> <p>Interpolation method to be used for mask transformation. Default: cv2.INTER_NEAREST</p> <code>p</code> <code>float</code> <p>Probability of applying the transform. Default: 1.0.</p> <p>Targets</p> <p>image, mask</p> <p>Image types:     uint8, float32</p> <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; transform = GridElasticDeform(num_grid_xy=(4, 4), magnitude=10, p=1.0)\n&gt;&gt;&gt; result = transform(image=image, mask=mask)\n&gt;&gt;&gt; transformed_image, transformed_mask = result['image'], result['mask']\n</code></pre> <p>Note</p> <p>This transformation is particularly useful for data augmentation in medical imaging and other domains where elastic deformations can simulate realistic variations.</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>class GridElasticDeform(DualTransform):\n    \"\"\"Grid-based Elastic deformation Albumentation implementation\n\n    This class applies elastic transformations using a grid-based approach.\n    The granularity and intensity of the distortions can be controlled using\n    the dimensions of the overlaying distortion grid and the magnitude parameter.\n    Larger grid sizes result in finer, less severe distortions.\n\n    Args:\n        num_grid_xy (tuple[int, int]): Number of grid cells along the width and height.\n            Specified as (grid_width, grid_height). Each value must be greater than 1.\n        magnitude (int): Maximum pixel-wise displacement for distortion. Must be greater than 0.\n        interpolation (int): Interpolation method to be used for the image transformation.\n            Default: cv2.INTER_LINEAR\n        mask_interpolation (int): Interpolation method to be used for mask transformation.\n            Default: cv2.INTER_NEAREST\n        p (float): Probability of applying the transform. Default: 1.0.\n\n    Targets:\n        image, mask\n\n    Image types:\n        uint8, float32\n\n    Example:\n        &gt;&gt;&gt; transform = GridElasticDeform(num_grid_xy=(4, 4), magnitude=10, p=1.0)\n        &gt;&gt;&gt; result = transform(image=image, mask=mask)\n        &gt;&gt;&gt; transformed_image, transformed_mask = result['image'], result['mask']\n\n    Note:\n        This transformation is particularly useful for data augmentation in medical imaging\n        and other domains where elastic deformations can simulate realistic variations.\n    \"\"\"\n\n    _targets = (Targets.IMAGE, Targets.MASK)\n\n    class InitSchema(BaseTransformInitSchema):\n        num_grid_xy: Annotated[tuple[int, int], AfterValidator(check_1plus)]\n        p: ProbabilityType = 1.0\n        magnitude: int = Field(gt=0)\n        interpolation: InterpolationType = cv2.INTER_LINEAR\n        mask_interpolation: InterpolationType = cv2.INTER_NEAREST\n\n    def __init__(\n        self,\n        num_grid_xy: tuple[int, int],\n        magnitude: int,\n        interpolation: int = cv2.INTER_LINEAR,\n        mask_interpolation: int = cv2.INTER_NEAREST,\n        p: float = 1.0,\n        always_apply: bool | None = None,\n    ):\n        super().__init__(p=p, always_apply=always_apply)\n        self.num_grid_xy = num_grid_xy\n        self.magnitude = magnitude\n        self.interpolation = interpolation\n        self.mask_interpolation = mask_interpolation\n\n    @staticmethod\n    def generate_mesh(polygons: np.ndarray, dimensions: np.ndarray) -&gt; np.ndarray:\n        return np.hstack((dimensions.reshape(-1, 4), polygons))\n\n    def get_params_dependent_on_data(\n        self,\n        params: dict[str, Any],\n        data: dict[str, Any],\n    ) -&gt; dict[str, Any]:\n        image_shape = np.array(params[\"shape\"][:2])\n\n        dimensions = fgeometric.calculate_grid_dimensions(image_shape, self.num_grid_xy)\n\n        polygons = fgeometric.generate_distorted_grid_polygons(dimensions, self.magnitude)\n\n        generated_mesh = self.generate_mesh(polygons, dimensions)\n\n        return {\"generated_mesh\": generated_mesh}\n\n    def apply(self, img: np.ndarray, generated_mesh: np.ndarray, **params: Any) -&gt; np.ndarray:\n        return fgeometric.distort_image(img, generated_mesh, self.interpolation)\n\n    def apply_to_mask(self, mask: np.ndarray, generated_mesh: np.ndarray, **params: Any) -&gt; np.ndarray:\n        return fgeometric.distort_image(mask, generated_mesh, self.mask_interpolation)\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return \"num_grid_xy\", \"magnitude\", \"interpolation\", \"mask_interpolation\"\n</code></pre>"},{"location":"api_reference/augmentations/geometric/transforms/#albumentations.augmentations.geometric.transforms.GridElasticDeform.apply","title":"<code>apply (self, img, generated_mesh, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def apply(self, img: np.ndarray, generated_mesh: np.ndarray, **params: Any) -&gt; np.ndarray:\n    return fgeometric.distort_image(img, generated_mesh, self.interpolation)\n</code></pre>"},{"location":"api_reference/augmentations/geometric/transforms/#albumentations.augmentations.geometric.transforms.GridElasticDeform.get_params_dependent_on_data","title":"<code>get_params_dependent_on_data (self, params, data)</code>","text":"<p>Returns parameters dependent on input.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def get_params_dependent_on_data(\n    self,\n    params: dict[str, Any],\n    data: dict[str, Any],\n) -&gt; dict[str, Any]:\n    image_shape = np.array(params[\"shape\"][:2])\n\n    dimensions = fgeometric.calculate_grid_dimensions(image_shape, self.num_grid_xy)\n\n    polygons = fgeometric.generate_distorted_grid_polygons(dimensions, self.magnitude)\n\n    generated_mesh = self.generate_mesh(polygons, dimensions)\n\n    return {\"generated_mesh\": generated_mesh}\n</code></pre>"},{"location":"api_reference/augmentations/geometric/transforms/#albumentations.augmentations.geometric.transforms.GridElasticDeform.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return \"num_grid_xy\", \"magnitude\", \"interpolation\", \"mask_interpolation\"\n</code></pre>"},{"location":"api_reference/augmentations/geometric/transforms/#albumentations.augmentations.geometric.transforms.HorizontalFlip","title":"<code>class  HorizontalFlip</code> <code> </code>  [view source on GitHub]","text":"<p>Flip the input horizontally around the y-axis.</p> <p>Parameters:</p> Name Type Description <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 0.5.</p> <p>Targets</p> <p>image, mask, bboxes, keypoints</p> <p>Image types:     uint8, float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>class HorizontalFlip(DualTransform):\n    \"\"\"Flip the input horizontally around the y-axis.\n\n    Args:\n        p (float): probability of applying the transform. Default: 0.5.\n\n    Targets:\n        image, mask, bboxes, keypoints\n\n    Image types:\n        uint8, float32\n\n    \"\"\"\n\n    _targets = (Targets.IMAGE, Targets.MASK, Targets.BBOXES, Targets.KEYPOINTS)\n\n    def apply(self, img: np.ndarray, **params: Any) -&gt; np.ndarray:\n        if get_num_channels(img) &gt; 1 and img.dtype == np.uint8:\n            # Opencv is faster than numpy only in case of\n            # non-gray scale 8bits images\n            return fgeometric.hflip_cv2(img)\n\n        return fgeometric.hflip(img)\n\n    def apply_to_bbox(self, bbox: BoxInternalType, **params: Any) -&gt; BoxInternalType:\n        return fgeometric.bbox_hflip(bbox)\n\n    def apply_to_keypoint(self, keypoint: KeypointInternalType, **params: Any) -&gt; KeypointInternalType:\n        return fgeometric.keypoint_hflip(keypoint, params[\"cols\"])\n\n    def get_transform_init_args_names(self) -&gt; tuple[()]:\n        return ()\n</code></pre>"},{"location":"api_reference/augmentations/geometric/transforms/#albumentations.augmentations.geometric.transforms.HorizontalFlip.apply","title":"<code>apply (self, img, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def apply(self, img: np.ndarray, **params: Any) -&gt; np.ndarray:\n    if get_num_channels(img) &gt; 1 and img.dtype == np.uint8:\n        # Opencv is faster than numpy only in case of\n        # non-gray scale 8bits images\n        return fgeometric.hflip_cv2(img)\n\n    return fgeometric.hflip(img)\n</code></pre>"},{"location":"api_reference/augmentations/geometric/transforms/#albumentations.augmentations.geometric.transforms.HorizontalFlip.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[()]:\n    return ()\n</code></pre>"},{"location":"api_reference/augmentations/geometric/transforms/#albumentations.augmentations.geometric.transforms.OpticalDistortion","title":"<code>class  OpticalDistortion</code> <code>     (distort_limit=(-0.05, 0.05), shift_limit=(-0.05, 0.05), interpolation=1, border_mode=4, value=None, mask_value=None, always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Parameters:</p> Name Type Description <code>distort_limit</code> <code>float, (float, float</code> <p>If distort_limit is a single float, the range will be (-distort_limit, distort_limit). Default: (-0.05, 0.05).</p> <code>shift_limit</code> <code>float, (float, float</code> <p>If shift_limit is a single float, the range will be (-shift_limit, shift_limit). Default: (-0.05, 0.05).</p> <code>interpolation</code> <code>OpenCV flag</code> <p>flag that is used to specify the interpolation algorithm. Should be one of: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4. Default: cv2.INTER_LINEAR.</p> <code>border_mode</code> <code>OpenCV flag</code> <p>flag that is used to specify the pixel extrapolation method. Should be one of: cv2.BORDER_CONSTANT, cv2.BORDER_REPLICATE, cv2.BORDER_REFLECT, cv2.BORDER_WRAP, cv2.BORDER_REFLECT_101. Default: cv2.BORDER_REFLECT_101</p> <code>value</code> <code>int, float, list of ints, list of float</code> <p>padding value if border_mode is cv2.BORDER_CONSTANT.</p> <code>mask_value</code> <code>int, float,         list of ints,         list of float</code> <p>padding value if border_mode is cv2.BORDER_CONSTANT applied for masks.</p> <p>Targets</p> <p>image, mask, bboxes</p> <p>Image types:     uint8, float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>class OpticalDistortion(DualTransform):\n    \"\"\"Args:\n        distort_limit (float, (float, float)): If distort_limit is a single float, the range\n            will be (-distort_limit, distort_limit). Default: (-0.05, 0.05).\n        shift_limit (float, (float, float))): If shift_limit is a single float, the range\n            will be (-shift_limit, shift_limit). Default: (-0.05, 0.05).\n        interpolation (OpenCV flag): flag that is used to specify the interpolation algorithm. Should be one of:\n            cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.\n            Default: cv2.INTER_LINEAR.\n        border_mode (OpenCV flag): flag that is used to specify the pixel extrapolation method. Should be one of:\n            cv2.BORDER_CONSTANT, cv2.BORDER_REPLICATE, cv2.BORDER_REFLECT, cv2.BORDER_WRAP, cv2.BORDER_REFLECT_101.\n            Default: cv2.BORDER_REFLECT_101\n        value (int, float, list of ints, list of float): padding value if border_mode is cv2.BORDER_CONSTANT.\n        mask_value (int, float,\n                    list of ints,\n                    list of float): padding value if border_mode is cv2.BORDER_CONSTANT applied for masks.\n\n    Targets:\n        image, mask, bboxes\n\n    Image types:\n        uint8, float32\n\n    \"\"\"\n\n    _targets = (Targets.IMAGE, Targets.MASK, Targets.BBOXES)\n\n    class InitSchema(BaseTransformInitSchema):\n        distort_limit: SymmetricRangeType = (-0.05, 0.05)\n        shift_limit: SymmetricRangeType = (-0.05, 0.05)\n        interpolation: InterpolationType = cv2.INTER_LINEAR\n        border_mode: BorderModeType = cv2.BORDER_REFLECT_101\n        value: ColorType | None = Field(\n            default=None,\n            description=\"Padding value if border_mode is cv2.BORDER_CONSTANT.\",\n        )\n        mask_value: ColorType | None = Field(\n            default=None,\n            description=\"Padding value for mask if border_mode is cv2.BORDER_CONSTANT.\",\n        )\n\n    def __init__(\n        self,\n        distort_limit: ScaleFloatType = (-0.05, 0.05),\n        shift_limit: ScaleFloatType = (-0.05, 0.05),\n        interpolation: int = cv2.INTER_LINEAR,\n        border_mode: int = cv2.BORDER_REFLECT_101,\n        value: ColorType | None = None,\n        mask_value: ColorType | None = None,\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p, always_apply)\n        self.shift_limit = cast(Tuple[float, float], shift_limit)\n        self.distort_limit = cast(Tuple[float, float], distort_limit)\n        self.interpolation = interpolation\n        self.border_mode = border_mode\n        self.value = value\n        self.mask_value = mask_value\n\n    def apply(\n        self,\n        img: np.ndarray,\n        k: int,\n        dx: int,\n        dy: int,\n        interpolation: int,\n        **params: Any,\n    ) -&gt; np.ndarray:\n        return fgeometric.optical_distortion(img, k, dx, dy, interpolation, self.border_mode, self.value)\n\n    def apply_to_mask(self, mask: np.ndarray, k: int, dx: int, dy: int, **params: Any) -&gt; np.ndarray:\n        return fgeometric.optical_distortion(mask, k, dx, dy, cv2.INTER_NEAREST, self.border_mode, self.mask_value)\n\n    def apply_to_bbox(\n        self,\n        bbox: BoxInternalType,\n        k: int,\n        dx: int,\n        dy: int,\n        **params: Any,\n    ) -&gt; BoxInternalType:\n        image_shape = params[\"shape\"]\n        mask = np.zeros(image_shape[:2], dtype=np.uint8)\n        bbox_denorm = fgeometric.denormalize_bbox(bbox, image_shape)\n        x_min, y_min, x_max, y_max = bbox_denorm[:4]\n        x_min, y_min, x_max, y_max = int(x_min), int(y_min), int(x_max), int(y_max)\n        mask[y_min:y_max, x_min:x_max] = 1\n        mask = fgeometric.optical_distortion(mask, k, dx, dy, cv2.INTER_NEAREST, self.border_mode, self.mask_value)\n        bbox_returned = bbox_from_mask(mask)\n        return cast(BoxInternalType, fgeometric.normalize_bbox(bbox_returned, image_shape))\n\n    def get_params(self) -&gt; dict[str, Any]:\n        return {\n            \"k\": random.uniform(*self.distort_limit),\n            \"dx\": round(random.uniform(*self.shift_limit)),\n            \"dy\": round(random.uniform(*self.shift_limit)),\n        }\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return (\n            \"distort_limit\",\n            \"shift_limit\",\n            \"interpolation\",\n            \"border_mode\",\n            \"value\",\n            \"mask_value\",\n        )\n\n    @property\n    def targets(self) -&gt; dict[str, Callable[..., Any]]:\n        return {\n            \"image\": self.apply,\n            \"mask\": self.apply_to_mask,\n            \"masks\": self.apply_to_masks,\n            \"bboxes\": self.apply_to_bboxes,\n        }\n</code></pre>"},{"location":"api_reference/augmentations/geometric/transforms/#albumentations.augmentations.geometric.transforms.OpticalDistortion.apply","title":"<code>apply (self, img, k, dx, dy, interpolation, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def apply(\n    self,\n    img: np.ndarray,\n    k: int,\n    dx: int,\n    dy: int,\n    interpolation: int,\n    **params: Any,\n) -&gt; np.ndarray:\n    return fgeometric.optical_distortion(img, k, dx, dy, interpolation, self.border_mode, self.value)\n</code></pre>"},{"location":"api_reference/augmentations/geometric/transforms/#albumentations.augmentations.geometric.transforms.OpticalDistortion.get_params","title":"<code>get_params (self)</code>","text":"<p>Returns parameters independent of input.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def get_params(self) -&gt; dict[str, Any]:\n    return {\n        \"k\": random.uniform(*self.distort_limit),\n        \"dx\": round(random.uniform(*self.shift_limit)),\n        \"dy\": round(random.uniform(*self.shift_limit)),\n    }\n</code></pre>"},{"location":"api_reference/augmentations/geometric/transforms/#albumentations.augmentations.geometric.transforms.OpticalDistortion.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return (\n        \"distort_limit\",\n        \"shift_limit\",\n        \"interpolation\",\n        \"border_mode\",\n        \"value\",\n        \"mask_value\",\n    )\n</code></pre>"},{"location":"api_reference/augmentations/geometric/transforms/#albumentations.augmentations.geometric.transforms.PadIfNeeded","title":"<code>class  PadIfNeeded</code> <code>     (min_height=1024, min_width=1024, pad_height_divisor=None, pad_width_divisor=None, position=&lt;PositionType.CENTER: 'center'&gt;, border_mode=4, value=None, mask_value=None, always_apply=None, p=1.0)                 </code>  [view source on GitHub]","text":"<p>Pads the sides of an image if the image dimensions are less than the specified minimum dimensions. If the <code>pad_height_divisor</code> or <code>pad_width_divisor</code> is specified, the function additionally ensures that the image dimensions are divisible by these values.</p> <p>Parameters:</p> Name Type Description <code>min_height</code> <code>int</code> <p>Minimum desired height of the image. Ensures image height is at least this value.</p> <code>min_width</code> <code>int</code> <p>Minimum desired width of the image. Ensures image width is at least this value.</p> <code>pad_height_divisor</code> <code>int</code> <p>If set, pads the image height to make it divisible by this value.</p> <code>pad_width_divisor</code> <code>int</code> <p>If set, pads the image width to make it divisible by this value.</p> <code>position</code> <code>Union[str, PositionType]</code> <p>Position where the image is to be placed after padding. Can be one of 'center', 'top_left', 'top_right', 'bottom_left', 'bottom_right', or 'random'. Default is 'center'.</p> <code>border_mode</code> <code>int</code> <p>Specifies the border mode to use if padding is required. The default is <code>cv2.BORDER_REFLECT_101</code>.</p> <code>value</code> <code>Union[int, float, list[int], list[float]]</code> <p>Value to fill the border pixels if the border mode is <code>cv2.BORDER_CONSTANT</code>. Default is None.</p> <code>mask_value</code> <code>Union[int, float, list[int], list[float]]</code> <p>Similar to <code>value</code> but used for padding masks. Default is None.</p> <code>p</code> <code>float</code> <p>Probability of applying the transform. Default is 1.0.</p> <p>Targets</p> <p>image, mask, bboxes, keypoints</p> <p>Image types:     uint8, float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>class PadIfNeeded(DualTransform):\n    \"\"\"Pads the sides of an image if the image dimensions are less than the specified minimum dimensions.\n    If the `pad_height_divisor` or `pad_width_divisor` is specified, the function additionally ensures\n    that the image dimensions are divisible by these values.\n\n    Args:\n        min_height (int): Minimum desired height of the image. Ensures image height is at least this value.\n        min_width (int): Minimum desired width of the image. Ensures image width is at least this value.\n        pad_height_divisor (int, optional): If set, pads the image height to make it divisible by this value.\n        pad_width_divisor (int, optional): If set, pads the image width to make it divisible by this value.\n        position (Union[str, PositionType]): Position where the image is to be placed after padding.\n            Can be one of 'center', 'top_left', 'top_right', 'bottom_left', 'bottom_right', or 'random'.\n            Default is 'center'.\n        border_mode (int): Specifies the border mode to use if padding is required.\n            The default is `cv2.BORDER_REFLECT_101`.\n        value (Union[int, float, list[int], list[float]], optional): Value to fill the border pixels if\n            the border mode is `cv2.BORDER_CONSTANT`. Default is None.\n        mask_value (Union[int, float, list[int], list[float]], optional): Similar to `value` but used for padding masks.\n            Default is None.\n        p (float): Probability of applying the transform. Default is 1.0.\n\n    Targets:\n        image, mask, bboxes, keypoints\n\n    Image types:\n        uint8, float32\n\n    \"\"\"\n\n    class PositionType(Enum):\n        \"\"\"Enumerates the types of positions for placing an object within a container.\n\n        This Enum class is utilized to define specific anchor positions that an object can\n        assume relative to a container. It's particularly useful in image processing, UI layout,\n        and graphic design to specify the alignment and positioning of elements.\n\n        Attributes:\n            CENTER (str): Specifies that the object should be placed at the center.\n            TOP_LEFT (str): Specifies that the object should be placed at the top-left corner.\n            TOP_RIGHT (str): Specifies that the object should be placed at the top-right corner.\n            BOTTOM_LEFT (str): Specifies that the object should be placed at the bottom-left corner.\n            BOTTOM_RIGHT (str): Specifies that the object should be placed at the bottom-right corner.\n            RANDOM (str): Indicates that the object's position should be determined randomly.\n\n        \"\"\"\n\n        CENTER = \"center\"\n        TOP_LEFT = \"top_left\"\n        TOP_RIGHT = \"top_right\"\n        BOTTOM_LEFT = \"bottom_left\"\n        BOTTOM_RIGHT = \"bottom_right\"\n        RANDOM = \"random\"\n\n    _targets = (Targets.IMAGE, Targets.MASK, Targets.BBOXES, Targets.KEYPOINTS)\n\n    class InitSchema(BaseTransformInitSchema):\n        min_height: int | None = Field(default=None, ge=1, description=\"Minimal result image height.\")\n        min_width: int | None = Field(default=None, ge=1, description=\"Minimal result image width.\")\n        pad_height_divisor: int | None = Field(\n            default=None,\n            ge=1,\n            description=\"Ensures image height is divisible by this value.\",\n        )\n        pad_width_divisor: int | None = Field(\n            default=None,\n            ge=1,\n            description=\"Ensures image width is divisible by this value.\",\n        )\n        position: str = Field(default=\"center\", description=\"Position of the padded image.\")\n        border_mode: BorderModeType = cv2.BORDER_REFLECT_101\n        value: ColorType | None = Field(default=None, description=\"Value for border if BORDER_CONSTANT is used.\")\n        mask_value: ColorType | None = Field(\n            default=None,\n            description=\"Value for mask border if BORDER_CONSTANT is used.\",\n        )\n        p: ProbabilityType = 1.0\n\n        @model_validator(mode=\"after\")\n        def validate_divisibility(self) -&gt; Self:\n            if (self.min_height is None) == (self.pad_height_divisor is None):\n                msg = \"Only one of 'min_height' and 'pad_height_divisor' parameters must be set\"\n                raise ValueError(msg)\n            if (self.min_width is None) == (self.pad_width_divisor is None):\n                msg = \"Only one of 'min_width' and 'pad_width_divisor' parameters must be set\"\n                raise ValueError(msg)\n\n            if self.border_mode == cv2.BORDER_CONSTANT and self.value is None:\n                msg = \"If 'border_mode' is set to 'BORDER_CONSTANT', 'value' must be provided.\"\n                raise ValueError(msg)\n\n            return self\n\n    def __init__(\n        self,\n        min_height: int | None = 1024,\n        min_width: int | None = 1024,\n        pad_height_divisor: int | None = None,\n        pad_width_divisor: int | None = None,\n        position: PositionType | str = PositionType.CENTER,\n        border_mode: int = cv2.BORDER_REFLECT_101,\n        value: ColorType | None = None,\n        mask_value: ColorType | None = None,\n        always_apply: bool | None = None,\n        p: float = 1.0,\n    ):\n        super().__init__(p, always_apply)\n        self.min_height = min_height\n        self.min_width = min_width\n        self.pad_width_divisor = pad_width_divisor\n        self.pad_height_divisor = pad_height_divisor\n        self.position = PadIfNeeded.PositionType(position)\n        self.border_mode = border_mode\n        self.value = value\n        self.mask_value = mask_value\n\n    def update_params(self, params: dict[str, Any], **kwargs: Any) -&gt; dict[str, Any]:\n        params = super().update_params(params, **kwargs)\n        rows, cols = params[\"shape\"][:2]\n\n        if self.min_height is not None:\n            if rows &lt; self.min_height:\n                h_pad_top = int((self.min_height - rows) / 2.0)\n                h_pad_bottom = self.min_height - rows - h_pad_top\n            else:\n                h_pad_top = 0\n                h_pad_bottom = 0\n        else:\n            pad_remained = rows % self.pad_height_divisor\n            pad_rows = self.pad_height_divisor - pad_remained if pad_remained &gt; 0 else 0\n\n            h_pad_top = pad_rows // 2\n            h_pad_bottom = pad_rows - h_pad_top\n\n        if self.min_width is not None:\n            if cols &lt; self.min_width:\n                w_pad_left = int((self.min_width - cols) / 2.0)\n                w_pad_right = self.min_width - cols - w_pad_left\n            else:\n                w_pad_left = 0\n                w_pad_right = 0\n        else:\n            pad_remainder = cols % self.pad_width_divisor\n            pad_cols = self.pad_width_divisor - pad_remainder if pad_remainder &gt; 0 else 0\n\n            w_pad_left = pad_cols // 2\n            w_pad_right = pad_cols - w_pad_left\n\n        h_pad_top, h_pad_bottom, w_pad_left, w_pad_right = self.__update_position_params(\n            h_top=h_pad_top,\n            h_bottom=h_pad_bottom,\n            w_left=w_pad_left,\n            w_right=w_pad_right,\n        )\n\n        params.update(\n            {\n                \"pad_top\": h_pad_top,\n                \"pad_bottom\": h_pad_bottom,\n                \"pad_left\": w_pad_left,\n                \"pad_right\": w_pad_right,\n            },\n        )\n        return params\n\n    def apply(\n        self,\n        img: np.ndarray,\n        pad_top: int,\n        pad_bottom: int,\n        pad_left: int,\n        pad_right: int,\n        **params: Any,\n    ) -&gt; np.ndarray:\n        return fgeometric.pad_with_params(\n            img,\n            pad_top,\n            pad_bottom,\n            pad_left,\n            pad_right,\n            border_mode=self.border_mode,\n            value=self.value,\n        )\n\n    def apply_to_mask(\n        self,\n        mask: np.ndarray,\n        pad_top: int,\n        pad_bottom: int,\n        pad_left: int,\n        pad_right: int,\n        **params: Any,\n    ) -&gt; np.ndarray:\n        return fgeometric.pad_with_params(\n            mask,\n            pad_top,\n            pad_bottom,\n            pad_left,\n            pad_right,\n            border_mode=self.border_mode,\n            value=self.mask_value,\n        )\n\n    def apply_to_bboxes(\n        self,\n        bboxes: Sequence[BoxType],\n        pad_top: int,\n        pad_bottom: int,\n        pad_left: int,\n        pad_right: int,\n        **params: Any,\n    ) -&gt; list[BoxType]:\n        image_shape = params[\"shape\"][:2]\n\n        bboxes_np = np.array(bboxes)\n        bboxes_np = denormalize_bboxes(bboxes_np, params[\"shape\"])\n\n        result = fgeometric.pad_bboxes(\n            bboxes_np,\n            pad_top,\n            pad_bottom,\n            pad_left,\n            pad_right,\n            self.border_mode,\n            image_shape=image_shape,\n        )\n\n        rows, cols = params[\"shape\"][:2]\n\n        return list(normalize_bboxes(result, (rows + pad_top + pad_bottom, cols + pad_left + pad_right)))\n\n    def apply_to_keypoints(\n        self,\n        keypoints: Sequence[KeypointType],\n        pad_top: int,\n        pad_bottom: int,\n        pad_left: int,\n        pad_right: int,\n        **params: Any,\n    ) -&gt; Sequence[KeypointType]:\n        # Convert keypoints to numpy array, including all attributes\n        keypoints_array = np.array([list(kp) for kp in keypoints])\n\n        padded_keypoints = fgeometric.pad_keypoints(\n            keypoints_array,\n            pad_top,\n            pad_bottom,\n            pad_left,\n            pad_right,\n            self.border_mode,\n            image_shape=params[\"shape\"][:2],\n        )\n\n        # Convert back to list of tuples\n        return [tuple(kp) for kp in padded_keypoints]\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return (\n            \"min_height\",\n            \"min_width\",\n            \"pad_height_divisor\",\n            \"pad_width_divisor\",\n            \"position\",\n            \"border_mode\",\n            \"value\",\n            \"mask_value\",\n        )\n\n    def __update_position_params(\n        self,\n        h_top: int,\n        h_bottom: int,\n        w_left: int,\n        w_right: int,\n    ) -&gt; tuple[int, int, int, int]:\n        if self.position == PadIfNeeded.PositionType.TOP_LEFT:\n            h_bottom += h_top\n            w_right += w_left\n            h_top = 0\n            w_left = 0\n\n        elif self.position == PadIfNeeded.PositionType.TOP_RIGHT:\n            h_bottom += h_top\n            w_left += w_right\n            h_top = 0\n            w_right = 0\n\n        elif self.position == PadIfNeeded.PositionType.BOTTOM_LEFT:\n            h_top += h_bottom\n            w_right += w_left\n            h_bottom = 0\n            w_left = 0\n\n        elif self.position == PadIfNeeded.PositionType.BOTTOM_RIGHT:\n            h_top += h_bottom\n            w_left += w_right\n            h_bottom = 0\n            w_right = 0\n\n        elif self.position == PadIfNeeded.PositionType.RANDOM:\n            h_pad = h_top + h_bottom\n            w_pad = w_left + w_right\n            h_top = random.randint(0, h_pad)\n            h_bottom = h_pad - h_top\n            w_left = random.randint(0, w_pad)\n            w_right = w_pad - w_left\n\n        return h_top, h_bottom, w_left, w_right\n</code></pre>"},{"location":"api_reference/augmentations/geometric/transforms/#albumentations.augmentations.geometric.transforms.PadIfNeeded.PositionType","title":"<code>class  PositionType</code> <code> </code>","text":"<p>Enumerates the types of positions for placing an object within a container.</p> <p>This Enum class is utilized to define specific anchor positions that an object can assume relative to a container. It's particularly useful in image processing, UI layout, and graphic design to specify the alignment and positioning of elements.</p> <p>Attributes:</p> Name Type Description <code>CENTER</code> <code>str</code> <p>Specifies that the object should be placed at the center.</p> <code>TOP_LEFT</code> <code>str</code> <p>Specifies that the object should be placed at the top-left corner.</p> <code>TOP_RIGHT</code> <code>str</code> <p>Specifies that the object should be placed at the top-right corner.</p> <code>BOTTOM_LEFT</code> <code>str</code> <p>Specifies that the object should be placed at the bottom-left corner.</p> <code>BOTTOM_RIGHT</code> <code>str</code> <p>Specifies that the object should be placed at the bottom-right corner.</p> <code>RANDOM</code> <code>str</code> <p>Indicates that the object's position should be determined randomly.</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>class PositionType(Enum):\n    \"\"\"Enumerates the types of positions for placing an object within a container.\n\n    This Enum class is utilized to define specific anchor positions that an object can\n    assume relative to a container. It's particularly useful in image processing, UI layout,\n    and graphic design to specify the alignment and positioning of elements.\n\n    Attributes:\n        CENTER (str): Specifies that the object should be placed at the center.\n        TOP_LEFT (str): Specifies that the object should be placed at the top-left corner.\n        TOP_RIGHT (str): Specifies that the object should be placed at the top-right corner.\n        BOTTOM_LEFT (str): Specifies that the object should be placed at the bottom-left corner.\n        BOTTOM_RIGHT (str): Specifies that the object should be placed at the bottom-right corner.\n        RANDOM (str): Indicates that the object's position should be determined randomly.\n\n    \"\"\"\n\n    CENTER = \"center\"\n    TOP_LEFT = \"top_left\"\n    TOP_RIGHT = \"top_right\"\n    BOTTOM_LEFT = \"bottom_left\"\n    BOTTOM_RIGHT = \"bottom_right\"\n    RANDOM = \"random\"\n</code></pre>"},{"location":"api_reference/augmentations/geometric/transforms/#albumentations.augmentations.geometric.transforms.PadIfNeeded.apply","title":"<code>apply (self, img, pad_top, pad_bottom, pad_left, pad_right, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def apply(\n    self,\n    img: np.ndarray,\n    pad_top: int,\n    pad_bottom: int,\n    pad_left: int,\n    pad_right: int,\n    **params: Any,\n) -&gt; np.ndarray:\n    return fgeometric.pad_with_params(\n        img,\n        pad_top,\n        pad_bottom,\n        pad_left,\n        pad_right,\n        border_mode=self.border_mode,\n        value=self.value,\n    )\n</code></pre>"},{"location":"api_reference/augmentations/geometric/transforms/#albumentations.augmentations.geometric.transforms.PadIfNeeded.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return (\n        \"min_height\",\n        \"min_width\",\n        \"pad_height_divisor\",\n        \"pad_width_divisor\",\n        \"position\",\n        \"border_mode\",\n        \"value\",\n        \"mask_value\",\n    )\n</code></pre>"},{"location":"api_reference/augmentations/geometric/transforms/#albumentations.augmentations.geometric.transforms.PadIfNeeded.update_params","title":"<code>update_params (self, params, **kwargs)</code>","text":"<p>Update parameters with transform specific params. This method is deprecated, use: - <code>get_params</code> for transform specific params like interpolation and - <code>update_params_shape</code> for data like shape.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def update_params(self, params: dict[str, Any], **kwargs: Any) -&gt; dict[str, Any]:\n    params = super().update_params(params, **kwargs)\n    rows, cols = params[\"shape\"][:2]\n\n    if self.min_height is not None:\n        if rows &lt; self.min_height:\n            h_pad_top = int((self.min_height - rows) / 2.0)\n            h_pad_bottom = self.min_height - rows - h_pad_top\n        else:\n            h_pad_top = 0\n            h_pad_bottom = 0\n    else:\n        pad_remained = rows % self.pad_height_divisor\n        pad_rows = self.pad_height_divisor - pad_remained if pad_remained &gt; 0 else 0\n\n        h_pad_top = pad_rows // 2\n        h_pad_bottom = pad_rows - h_pad_top\n\n    if self.min_width is not None:\n        if cols &lt; self.min_width:\n            w_pad_left = int((self.min_width - cols) / 2.0)\n            w_pad_right = self.min_width - cols - w_pad_left\n        else:\n            w_pad_left = 0\n            w_pad_right = 0\n    else:\n        pad_remainder = cols % self.pad_width_divisor\n        pad_cols = self.pad_width_divisor - pad_remainder if pad_remainder &gt; 0 else 0\n\n        w_pad_left = pad_cols // 2\n        w_pad_right = pad_cols - w_pad_left\n\n    h_pad_top, h_pad_bottom, w_pad_left, w_pad_right = self.__update_position_params(\n        h_top=h_pad_top,\n        h_bottom=h_pad_bottom,\n        w_left=w_pad_left,\n        w_right=w_pad_right,\n    )\n\n    params.update(\n        {\n            \"pad_top\": h_pad_top,\n            \"pad_bottom\": h_pad_bottom,\n            \"pad_left\": w_pad_left,\n            \"pad_right\": w_pad_right,\n        },\n    )\n    return params\n</code></pre>"},{"location":"api_reference/augmentations/geometric/transforms/#albumentations.augmentations.geometric.transforms.Perspective","title":"<code>class  Perspective</code> <code>     (scale=(0.05, 0.1), keep_size=True, pad_mode=0, pad_val=0, mask_pad_val=0, fit_output=False, interpolation=1, always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Perform a random four point perspective transform of the input.</p> <p>Parameters:</p> Name Type Description <code>scale</code> <code>ScaleFloatType</code> <p>standard deviation of the normal distributions. These are used to sample the random distances of the subimage's corners from the full image's corners. If scale is a single float value, the range will be (0, scale). Default: (0.05, 0.1).</p> <code>keep_size</code> <code>bool</code> <p>Whether to resize image back to their original size after applying the perspective transform. If set to False, the resulting images may end up having different shapes and will always be a list, never an array. Default: True</p> <code>pad_mode</code> <code>OpenCV flag</code> <p>OpenCV border mode.</p> <code>pad_val</code> <code>int, float, list of int, list of float</code> <p>padding value if border_mode is cv2.BORDER_CONSTANT. Default: 0</p> <code>mask_pad_val</code> <code>int, float, list of int, list of float</code> <p>padding value for mask if border_mode is cv2.BORDER_CONSTANT. Default: 0</p> <code>fit_output</code> <code>bool</code> <p>If True, the image plane size and position will be adjusted to still capture the whole image after perspective transformation. (Followed by image resizing if keep_size is set to True.) Otherwise, parts of the transformed image may be outside of the image plane. This setting should not be set to True when using large scale values as it could lead to very large images. Default: False</p> <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 0.5.</p> <p>Targets</p> <p>image, mask, keypoints, bboxes</p> <p>Image types:     uint8, float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>class Perspective(DualTransform):\n    \"\"\"Perform a random four point perspective transform of the input.\n\n    Args:\n        scale: standard deviation of the normal distributions. These are used to sample\n            the random distances of the subimage's corners from the full image's corners.\n            If scale is a single float value, the range will be (0, scale). Default: (0.05, 0.1).\n        keep_size: Whether to resize image back to their original size after applying the perspective\n            transform. If set to False, the resulting images may end up having different shapes\n            and will always be a list, never an array. Default: True\n        pad_mode (OpenCV flag): OpenCV border mode.\n        pad_val (int, float, list of int, list of float): padding value if border_mode is cv2.BORDER_CONSTANT.\n            Default: 0\n        mask_pad_val (int, float, list of int, list of float): padding value for mask\n            if border_mode is cv2.BORDER_CONSTANT. Default: 0\n        fit_output (bool): If True, the image plane size and position will be adjusted to still capture\n            the whole image after perspective transformation. (Followed by image resizing if keep_size is set to True.)\n            Otherwise, parts of the transformed image may be outside of the image plane.\n            This setting should not be set to True when using large scale values as it could lead to very large images.\n            Default: False\n        p (float): probability of applying the transform. Default: 0.5.\n\n    Targets:\n        image, mask, keypoints, bboxes\n\n    Image types:\n        uint8, float32\n\n    \"\"\"\n\n    _targets = (Targets.IMAGE, Targets.MASK, Targets.KEYPOINTS, Targets.BBOXES)\n\n    class InitSchema(BaseTransformInitSchema):\n        scale: NonNegativeFloatRangeType = (0.05, 0.1)\n        keep_size: Annotated[bool, Field(default=True, description=\"Keep size after transform.\")]\n        pad_mode: BorderModeType = cv2.BORDER_CONSTANT\n        pad_val: ColorType | None = Field(\n            default=0,\n            description=\"Padding value if border_mode is cv2.BORDER_CONSTANT.\",\n        )\n        mask_pad_val: ColorType | None = Field(\n            default=0,\n            description=\"Mask padding value if border_mode is cv2.BORDER_CONSTANT.\",\n        )\n        fit_output: Annotated[bool, Field(default=False, description=\"Adjust image plane to capture whole image.\")]\n        interpolation: InterpolationType = cv2.INTER_LINEAR\n\n    def __init__(\n        self,\n        scale: ScaleFloatType = (0.05, 0.1),\n        keep_size: bool = True,\n        pad_mode: int = cv2.BORDER_CONSTANT,\n        pad_val: ColorType = 0,\n        mask_pad_val: ColorType = 0,\n        fit_output: bool = False,\n        interpolation: int = cv2.INTER_LINEAR,\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p, always_apply)\n        self.scale = cast(Tuple[float, float], scale)\n        self.keep_size = keep_size\n        self.pad_mode = pad_mode\n        self.pad_val = pad_val\n        self.mask_pad_val = mask_pad_val\n        self.fit_output = fit_output\n        self.interpolation = interpolation\n\n    def apply(\n        self,\n        img: np.ndarray,\n        matrix: np.ndarray,\n        max_height: int,\n        max_width: int,\n        **params: Any,\n    ) -&gt; np.ndarray:\n        return fgeometric.perspective(\n            img,\n            matrix,\n            max_width,\n            max_height,\n            self.pad_val,\n            self.pad_mode,\n            self.keep_size,\n            params[\"interpolation\"],\n        )\n\n    def apply_to_bbox(\n        self,\n        bbox: BoxInternalType,\n        matrix: np.ndarray,\n        max_height: int,\n        max_width: int,\n        **params: Any,\n    ) -&gt; BoxInternalType:\n        return fgeometric.perspective_bbox(\n            bbox,\n            params[\"shape\"],\n            matrix,\n            max_width,\n            max_height,\n            self.keep_size,\n        )\n\n    def apply_to_keypoint(\n        self,\n        keypoint: KeypointInternalType,\n        matrix: np.ndarray,\n        max_height: int,\n        max_width: int,\n        **params: Any,\n    ) -&gt; np.ndarray:\n        return fgeometric.perspective_keypoint(\n            keypoint,\n            params[\"shape\"],\n            matrix,\n            max_width,\n            max_height,\n            self.keep_size,\n        )\n\n    def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n        height, width = params[\"shape\"][:2]\n\n        scale = random.uniform(*self.scale)\n        points = random_utils.normal(0, scale, [4, 2])\n        points = np.mod(np.abs(points), 0.32)\n\n        # top left -- no changes needed, just use jitter\n        # top right\n        points[1, 0] = 1.0 - points[1, 0]  # w = 1.0 - jitter\n        # bottom right\n        points[2] = 1.0 - points[2]  # w = 1.0 - jitt\n        # bottom left\n        points[3, 1] = 1.0 - points[3, 1]  # h = 1.0 - jitter\n\n        points[:, 0] *= width\n        points[:, 1] *= height\n\n        # Obtain a consistent order of the points and unpack them individually.\n        # Warning: don't just do (tl, tr, br, bl) = _order_points(...)\n        # here, because the reordered points is used further below.\n        points = self._order_points(points)\n        tl, tr, br, bl = points\n\n        # compute the width of the new image, which will be the\n        # maximum distance between bottom-right and bottom-left\n        # x-coordiates or the top-right and top-left x-coordinates\n        min_width = None\n        max_width = None\n        while min_width is None or min_width &lt; TWO:\n            width_top = np.sqrt(((tr[0] - tl[0]) ** 2) + ((tr[1] - tl[1]) ** 2))\n            width_bottom = np.sqrt(((br[0] - bl[0]) ** 2) + ((br[1] - bl[1]) ** 2))\n            max_width = int(max(width_top, width_bottom))\n            min_width = int(min(width_top, width_bottom))\n            if min_width &lt; TWO:\n                step_size = (2 - min_width) / 2\n                tl[0] -= step_size\n                tr[0] += step_size\n                bl[0] -= step_size\n                br[0] += step_size\n\n        # compute the height of the new image, which will be the maximum distance between the top-right\n        # and bottom-right y-coordinates or the top-left and bottom-left y-coordinates\n        min_height = None\n        max_height = None\n        while min_height is None or min_height &lt; TWO:\n            height_right = np.sqrt(((tr[0] - br[0]) ** 2) + ((tr[1] - br[1]) ** 2))\n            height_left = np.sqrt(((tl[0] - bl[0]) ** 2) + ((tl[1] - bl[1]) ** 2))\n            max_height = int(max(height_right, height_left))\n            min_height = int(min(height_right, height_left))\n            if min_height &lt; TWO:\n                step_size = (2 - min_height) / 2\n                tl[1] -= step_size\n                tr[1] -= step_size\n                bl[1] += step_size\n                br[1] += step_size\n\n        # now that we have the dimensions of the new image, construct\n        # the set of destination points to obtain a \"birds eye view\",\n        # (i.e. top-down view) of the image, again specifying points\n        # in the top-left, top-right, bottom-right, and bottom-left order\n        # do not use width-1 or height-1 here, as for e.g. width=3, height=2\n        # the bottom right coordinate is at (3.0, 2.0) and not (2.0, 1.0)\n        dst = np.array([[0, 0], [max_width, 0], [max_width, max_height], [0, max_height]], dtype=np.float32)\n\n        # compute the perspective transform matrix and then apply it\n        m = cv2.getPerspectiveTransform(points, dst)\n\n        if self.fit_output:\n            m, max_width, max_height = self._expand_transform(m, (height, width))\n\n        return {\"matrix\": m, \"max_height\": max_height, \"max_width\": max_width, \"interpolation\": self.interpolation}\n\n    @classmethod\n    def _expand_transform(cls, matrix: np.ndarray, shape: SizeType) -&gt; tuple[np.ndarray, int, int]:\n        height, width = shape[:2]\n        # do not use width-1 or height-1 here, as for e.g. width=3, height=2, max_height\n        # the bottom right coordinate is at (3.0, 2.0) and not (2.0, 1.0)\n        rect = np.array([[0, 0], [width, 0], [width, height], [0, height]], dtype=np.float32)\n        dst = cv2.perspectiveTransform(np.array([rect]), matrix)[0]\n\n        # get min x, y over transformed 4 points\n        # then modify target points by subtracting these minima  =&gt; shift to (0, 0)\n        dst -= dst.min(axis=0, keepdims=True)\n        dst = np.around(dst, decimals=0)\n\n        matrix_expanded = cv2.getPerspectiveTransform(rect, dst)\n        max_width, max_height = dst.max(axis=0)\n        return matrix_expanded, int(max_width), int(max_height)\n\n    @staticmethod\n    def _order_points(pts: np.ndarray) -&gt; np.ndarray:\n        pts = np.array(sorted(pts, key=lambda x: x[0]))\n        left = pts[:2]  # points with smallest x coordinate - left points\n        right = pts[2:]  # points with greatest x coordinate - right points\n\n        if left[0][1] &lt; left[1][1]:\n            tl, bl = left\n        else:\n            bl, tl = left\n\n        if right[0][1] &lt; right[1][1]:\n            tr, br = right\n        else:\n            br, tr = right\n\n        return np.array([tl, tr, br, bl], dtype=np.float32)\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return \"scale\", \"keep_size\", \"pad_mode\", \"pad_val\", \"mask_pad_val\", \"fit_output\", \"interpolation\"\n</code></pre>"},{"location":"api_reference/augmentations/geometric/transforms/#albumentations.augmentations.geometric.transforms.Perspective.apply","title":"<code>apply (self, img, matrix, max_height, max_width, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def apply(\n    self,\n    img: np.ndarray,\n    matrix: np.ndarray,\n    max_height: int,\n    max_width: int,\n    **params: Any,\n) -&gt; np.ndarray:\n    return fgeometric.perspective(\n        img,\n        matrix,\n        max_width,\n        max_height,\n        self.pad_val,\n        self.pad_mode,\n        self.keep_size,\n        params[\"interpolation\"],\n    )\n</code></pre>"},{"location":"api_reference/augmentations/geometric/transforms/#albumentations.augmentations.geometric.transforms.Perspective.get_params_dependent_on_data","title":"<code>get_params_dependent_on_data (self, params, data)</code>","text":"<p>Returns parameters dependent on input.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n    height, width = params[\"shape\"][:2]\n\n    scale = random.uniform(*self.scale)\n    points = random_utils.normal(0, scale, [4, 2])\n    points = np.mod(np.abs(points), 0.32)\n\n    # top left -- no changes needed, just use jitter\n    # top right\n    points[1, 0] = 1.0 - points[1, 0]  # w = 1.0 - jitter\n    # bottom right\n    points[2] = 1.0 - points[2]  # w = 1.0 - jitt\n    # bottom left\n    points[3, 1] = 1.0 - points[3, 1]  # h = 1.0 - jitter\n\n    points[:, 0] *= width\n    points[:, 1] *= height\n\n    # Obtain a consistent order of the points and unpack them individually.\n    # Warning: don't just do (tl, tr, br, bl) = _order_points(...)\n    # here, because the reordered points is used further below.\n    points = self._order_points(points)\n    tl, tr, br, bl = points\n\n    # compute the width of the new image, which will be the\n    # maximum distance between bottom-right and bottom-left\n    # x-coordiates or the top-right and top-left x-coordinates\n    min_width = None\n    max_width = None\n    while min_width is None or min_width &lt; TWO:\n        width_top = np.sqrt(((tr[0] - tl[0]) ** 2) + ((tr[1] - tl[1]) ** 2))\n        width_bottom = np.sqrt(((br[0] - bl[0]) ** 2) + ((br[1] - bl[1]) ** 2))\n        max_width = int(max(width_top, width_bottom))\n        min_width = int(min(width_top, width_bottom))\n        if min_width &lt; TWO:\n            step_size = (2 - min_width) / 2\n            tl[0] -= step_size\n            tr[0] += step_size\n            bl[0] -= step_size\n            br[0] += step_size\n\n    # compute the height of the new image, which will be the maximum distance between the top-right\n    # and bottom-right y-coordinates or the top-left and bottom-left y-coordinates\n    min_height = None\n    max_height = None\n    while min_height is None or min_height &lt; TWO:\n        height_right = np.sqrt(((tr[0] - br[0]) ** 2) + ((tr[1] - br[1]) ** 2))\n        height_left = np.sqrt(((tl[0] - bl[0]) ** 2) + ((tl[1] - bl[1]) ** 2))\n        max_height = int(max(height_right, height_left))\n        min_height = int(min(height_right, height_left))\n        if min_height &lt; TWO:\n            step_size = (2 - min_height) / 2\n            tl[1] -= step_size\n            tr[1] -= step_size\n            bl[1] += step_size\n            br[1] += step_size\n\n    # now that we have the dimensions of the new image, construct\n    # the set of destination points to obtain a \"birds eye view\",\n    # (i.e. top-down view) of the image, again specifying points\n    # in the top-left, top-right, bottom-right, and bottom-left order\n    # do not use width-1 or height-1 here, as for e.g. width=3, height=2\n    # the bottom right coordinate is at (3.0, 2.0) and not (2.0, 1.0)\n    dst = np.array([[0, 0], [max_width, 0], [max_width, max_height], [0, max_height]], dtype=np.float32)\n\n    # compute the perspective transform matrix and then apply it\n    m = cv2.getPerspectiveTransform(points, dst)\n\n    if self.fit_output:\n        m, max_width, max_height = self._expand_transform(m, (height, width))\n\n    return {\"matrix\": m, \"max_height\": max_height, \"max_width\": max_width, \"interpolation\": self.interpolation}\n</code></pre>"},{"location":"api_reference/augmentations/geometric/transforms/#albumentations.augmentations.geometric.transforms.Perspective.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return \"scale\", \"keep_size\", \"pad_mode\", \"pad_val\", \"mask_pad_val\", \"fit_output\", \"interpolation\"\n</code></pre>"},{"location":"api_reference/augmentations/geometric/transforms/#albumentations.augmentations.geometric.transforms.PiecewiseAffine","title":"<code>class  PiecewiseAffine</code> <code>     (scale=(0.03, 0.05), nb_rows=4, nb_cols=4, interpolation=1, mask_interpolation=0, cval=0, cval_mask=0, mode='constant', absolute_scale=False, always_apply=None, keypoints_threshold=0.01, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Apply affine transformations that differ between local neighborhoods. This augmentation places a regular grid of points on an image and randomly moves the neighborhood of these point around via affine transformations. This leads to local distortions.</p> <p>This is mostly a wrapper around scikit-image's <code>PiecewiseAffine</code>. See also <code>Affine</code> for a similar technique.</p> <p>Note</p> <p>This augmenter is very slow. Try to use <code>ElasticTransformation</code> instead, which is at least 10x faster.</p> <p>Note</p> <p>For coordinate-based inputs (keypoints, bounding boxes, polygons, ...), this augmenter still has to perform an image-based augmentation, which will make it significantly slower and not fully correct for such inputs than other transforms.</p> <p>Parameters:</p> Name Type Description <code>scale</code> <code>float, tuple of float</code> <p>Each point on the regular grid is moved around via a normal distribution. This scale factor is equivalent to the normal distribution's sigma. Note that the jitter (how far each point is moved in which direction) is multiplied by the height/width of the image if <code>absolute_scale=False</code> (default), so this scale can be the same for different sized images. Recommended values are in the range <code>0.01</code> to <code>0.05</code> (weak to strong augmentations).     * If a single <code>float</code>, then that value will always be used as the scale.     * If a tuple <code>(a, b)</code> of <code>float</code> s, then a random value will       be uniformly sampled per image from the interval <code>[a, b]</code>.</p> <code>nb_rows</code> <code>int, tuple of int</code> <p>Number of rows of points that the regular grid should have. Must be at least <code>2</code>. For large images, you might want to pick a higher value than <code>4</code>. You might have to then adjust scale to lower values.     * If a single <code>int</code>, then that value will always be used as the number of rows.     * If a tuple <code>(a, b)</code>, then a value from the discrete interval       <code>[a..b]</code> will be uniformly sampled per image.</p> <code>nb_cols</code> <code>int, tuple of int</code> <p>Number of columns. Analogous to <code>nb_rows</code>.</p> <code>interpolation</code> <code>int</code> <p>The order of interpolation. The order has to be in the range 0-5:  - 0: Nearest-neighbor  - 1: Bi-linear (default)  - 2: Bi-quadratic  - 3: Bi-cubic  - 4: Bi-quartic  - 5: Bi-quintic</p> <code>mask_interpolation</code> <code>int</code> <p>same as interpolation but for mask.</p> <code>cval</code> <code>number</code> <p>The constant value to use when filling in newly created pixels.</p> <code>cval_mask</code> <code>number</code> <p>Same as cval but only for masks.</p> <code>mode</code> <code>str</code> <p>{'constant', 'edge', 'symmetric', 'reflect', 'wrap'}, optional Points outside the boundaries of the input are filled according to the given mode.  Modes match the behaviour of <code>numpy.pad</code>.</p> <code>absolute_scale</code> <code>bool</code> <p>Take <code>scale</code> as an absolute value rather than a relative value.</p> <code>keypoints_threshold</code> <code>float</code> <p>Used as threshold in conversion from distance maps to keypoints. The search for keypoints works by searching for the argmin (non-inverted) or argmax (inverted) in each channel. This parameters contains the maximum (non-inverted) or minimum (inverted) value to accept in order to view a hit as a keypoint. Use <code>None</code> to use no min/max. Default: 0.01</p> <p>Targets</p> <p>image, mask, keypoints, bboxes</p> <p>Image types:     uint8, float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>class PiecewiseAffine(DualTransform):\n    \"\"\"Apply affine transformations that differ between local neighborhoods.\n    This augmentation places a regular grid of points on an image and randomly moves the neighborhood of these point\n    around via affine transformations. This leads to local distortions.\n\n    This is mostly a wrapper around scikit-image's ``PiecewiseAffine``.\n    See also ``Affine`` for a similar technique.\n\n    Note:\n        This augmenter is very slow. Try to use ``ElasticTransformation`` instead, which is at least 10x faster.\n\n    Note:\n        For coordinate-based inputs (keypoints, bounding boxes, polygons, ...),\n        this augmenter still has to perform an image-based augmentation,\n        which will make it significantly slower and not fully correct for such inputs than other transforms.\n\n    Args:\n        scale (float, tuple of float): Each point on the regular grid is moved around via a normal distribution.\n            This scale factor is equivalent to the normal distribution's sigma.\n            Note that the jitter (how far each point is moved in which direction) is multiplied by the height/width of\n            the image if ``absolute_scale=False`` (default), so this scale can be the same for different sized images.\n            Recommended values are in the range ``0.01`` to ``0.05`` (weak to strong augmentations).\n                * If a single ``float``, then that value will always be used as the scale.\n                * If a tuple ``(a, b)`` of ``float`` s, then a random value will\n                  be uniformly sampled per image from the interval ``[a, b]``.\n        nb_rows (int, tuple of int): Number of rows of points that the regular grid should have.\n            Must be at least ``2``. For large images, you might want to pick a higher value than ``4``.\n            You might have to then adjust scale to lower values.\n                * If a single ``int``, then that value will always be used as the number of rows.\n                * If a tuple ``(a, b)``, then a value from the discrete interval\n                  ``[a..b]`` will be uniformly sampled per image.\n        nb_cols (int, tuple of int): Number of columns. Analogous to `nb_rows`.\n        interpolation (int): The order of interpolation. The order has to be in the range 0-5:\n             - 0: Nearest-neighbor\n             - 1: Bi-linear (default)\n             - 2: Bi-quadratic\n             - 3: Bi-cubic\n             - 4: Bi-quartic\n             - 5: Bi-quintic\n        mask_interpolation (int): same as interpolation but for mask.\n        cval (number): The constant value to use when filling in newly created pixels.\n        cval_mask (number): Same as cval but only for masks.\n        mode (str): {'constant', 'edge', 'symmetric', 'reflect', 'wrap'}, optional\n            Points outside the boundaries of the input are filled according\n            to the given mode.  Modes match the behaviour of `numpy.pad`.\n        absolute_scale (bool): Take `scale` as an absolute value rather than a relative value.\n        keypoints_threshold (float): Used as threshold in conversion from distance maps to keypoints.\n            The search for keypoints works by searching for the\n            argmin (non-inverted) or argmax (inverted) in each channel. This\n            parameters contains the maximum (non-inverted) or minimum (inverted) value to accept in order to view a hit\n            as a keypoint. Use ``None`` to use no min/max. Default: 0.01\n\n    Targets:\n        image, mask, keypoints, bboxes\n\n    Image types:\n        uint8, float32\n\n    \"\"\"\n\n    _targets = (Targets.IMAGE, Targets.MASK, Targets.BBOXES, Targets.KEYPOINTS)\n\n    class InitSchema(BaseTransformInitSchema):\n        scale: NonNegativeFloatRangeType = (0.03, 0.05)\n        nb_rows: ScaleIntType = Field(default=4, description=\"Number of rows in the regular grid.\")\n        nb_cols: ScaleIntType = Field(default=4, description=\"Number of columns in the regular grid.\")\n        interpolation: InterpolationType = cv2.INTER_LINEAR\n        mask_interpolation: InterpolationType = cv2.INTER_NEAREST\n        cval: int = Field(default=0, description=\"Constant value used for newly created pixels.\")\n        cval_mask: int = Field(default=0, description=\"Constant value used for newly created mask pixels.\")\n        mode: Literal[\"constant\", \"edge\", \"symmetric\", \"reflect\", \"wrap\"] = \"constant\"\n        absolute_scale: bool = Field(\n            default=False,\n            description=\"Whether scale is an absolute value rather than relative.\",\n        )\n        keypoints_threshold: float = Field(\n            default=0.01,\n            description=\"Threshold for conversion from distance maps to keypoints.\",\n        )\n\n        @field_validator(\"nb_rows\", \"nb_cols\")\n        @classmethod\n        def process_range(cls, value: ScaleFloatType, info: ValidationInfo) -&gt; tuple[float, float]:\n            bounds = 2, BIG_INTEGER\n            result = to_tuple(value, value)\n            check_range(result, *bounds, info.field_name)\n            return result\n\n    def __init__(\n        self,\n        scale: ScaleFloatType = (0.03, 0.05),\n        nb_rows: ScaleIntType = 4,\n        nb_cols: ScaleIntType = 4,\n        interpolation: int = cv2.INTER_LINEAR,\n        mask_interpolation: int = cv2.INTER_NEAREST,\n        cval: int = 0,\n        cval_mask: int = 0,\n        mode: Literal[\"constant\", \"edge\", \"symmetric\", \"reflect\", \"wrap\"] = \"constant\",\n        absolute_scale: bool = False,\n        always_apply: bool | None = None,\n        keypoints_threshold: float = 0.01,\n        p: float = 0.5,\n    ):\n        super().__init__(p, always_apply)\n\n        warn(\n            \"This augmenter is very slow. Try to use ``ElasticTransformation`` instead, which is at least 10x faster.\",\n            stacklevel=2,\n        )\n\n        self.scale = cast(Tuple[float, float], scale)\n        self.nb_rows = cast(Tuple[int, int], nb_rows)\n        self.nb_cols = cast(Tuple[int, int], nb_cols)\n        self.interpolation = interpolation\n        self.mask_interpolation = mask_interpolation\n        self.cval = cval\n        self.cval_mask = cval_mask\n        self.mode = mode\n        self.absolute_scale = absolute_scale\n        self.keypoints_threshold = keypoints_threshold\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return (\n            \"scale\",\n            \"nb_rows\",\n            \"nb_cols\",\n            \"interpolation\",\n            \"mask_interpolation\",\n            \"cval\",\n            \"cval_mask\",\n            \"mode\",\n            \"absolute_scale\",\n            \"keypoints_threshold\",\n        )\n\n    def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n        height, width = params[\"shape\"][:2]\n\n        nb_rows = np.clip(random.randint(*self.nb_rows), 2, None)\n        nb_cols = np.clip(random.randint(*self.nb_cols), 2, None)\n        nb_cells = nb_cols * nb_rows\n        scale = random.uniform(*self.scale)\n\n        jitter: np.ndarray = random_utils.normal(0, scale, (nb_cells, 2))\n        if not np.any(jitter &gt; 0):\n            for _ in range(10):  # See: https://github.com/albumentations-team/albumentations/issues/1442\n                jitter = random_utils.normal(0, scale, (nb_cells, 2))\n                if np.any(jitter &gt; 0):\n                    break\n            if not np.any(jitter &gt; 0):\n                return {\"matrix\": None}\n\n        y = np.linspace(0, height, nb_rows)\n        x = np.linspace(0, width, nb_cols)\n\n        # (H, W) and (H, W) for H=rows, W=cols\n        xx_src, yy_src = np.meshgrid(x, y)\n\n        # (1, HW, 2) =&gt; (HW, 2) for H=rows, W=cols\n        points_src = np.dstack([yy_src.flat, xx_src.flat])[0]\n\n        if self.absolute_scale:\n            jitter[:, 0] = jitter[:, 0] / height if height &gt; 0 else 0.0\n            jitter[:, 1] = jitter[:, 1] / width if width &gt; 0 else 0.0\n\n        jitter[:, 0] = jitter[:, 0] * height\n        jitter[:, 1] = jitter[:, 1] * width\n\n        points_dest = np.copy(points_src)\n        points_dest[:, 0] = points_dest[:, 0] + jitter[:, 0]\n        points_dest[:, 1] = points_dest[:, 1] + jitter[:, 1]\n\n        # Restrict all destination points to be inside the image plane.\n        # This is necessary, as otherwise keypoints could be augmented\n        # outside of the image plane and these would be replaced by\n        # (-1, -1), which would not conform with the behaviour of the other augmenters.\n        points_dest[:, 0] = np.clip(points_dest[:, 0], 0, height - 1)\n        points_dest[:, 1] = np.clip(points_dest[:, 1], 0, width - 1)\n\n        matrix = skimage.transform.PiecewiseAffineTransform()\n        matrix.estimate(points_src[:, ::-1], points_dest[:, ::-1])\n\n        return {\n            \"matrix\": matrix,\n        }\n\n    def apply(\n        self,\n        img: np.ndarray,\n        matrix: skimage.transform.PiecewiseAffineTransform,\n        **params: Any,\n    ) -&gt; np.ndarray:\n        return fgeometric.piecewise_affine(img, matrix, self.interpolation, self.mode, self.cval)\n\n    def apply_to_mask(\n        self,\n        mask: np.ndarray,\n        matrix: skimage.transform.PiecewiseAffineTransform,\n        **params: Any,\n    ) -&gt; np.ndarray:\n        return fgeometric.piecewise_affine(mask, matrix, self.mask_interpolation, self.mode, self.cval_mask)\n\n    def apply_to_bbox(\n        self,\n        bbox: BoxInternalType,\n        rows: int,\n        cols: int,\n        matrix: skimage.transform.PiecewiseAffineTransform,\n        **params: Any,\n    ) -&gt; BoxInternalType:\n        return fgeometric.bbox_piecewise_affine(bbox, matrix, params[\"shape\"], self.keypoints_threshold)\n\n    def apply_to_keypoint(\n        self,\n        keypoint: KeypointInternalType,\n        rows: int,\n        cols: int,\n        matrix: skimage.transform.PiecewiseAffineTransform,\n        **params: Any,\n    ) -&gt; KeypointInternalType:\n        return fgeometric.keypoint_piecewise_affine(keypoint, matrix, params[\"shape\"], self.keypoints_threshold)\n</code></pre>"},{"location":"api_reference/augmentations/geometric/transforms/#albumentations.augmentations.geometric.transforms.PiecewiseAffine.apply","title":"<code>apply (self, img, matrix, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def apply(\n    self,\n    img: np.ndarray,\n    matrix: skimage.transform.PiecewiseAffineTransform,\n    **params: Any,\n) -&gt; np.ndarray:\n    return fgeometric.piecewise_affine(img, matrix, self.interpolation, self.mode, self.cval)\n</code></pre>"},{"location":"api_reference/augmentations/geometric/transforms/#albumentations.augmentations.geometric.transforms.PiecewiseAffine.get_params_dependent_on_data","title":"<code>get_params_dependent_on_data (self, params, data)</code>","text":"<p>Returns parameters dependent on input.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n    height, width = params[\"shape\"][:2]\n\n    nb_rows = np.clip(random.randint(*self.nb_rows), 2, None)\n    nb_cols = np.clip(random.randint(*self.nb_cols), 2, None)\n    nb_cells = nb_cols * nb_rows\n    scale = random.uniform(*self.scale)\n\n    jitter: np.ndarray = random_utils.normal(0, scale, (nb_cells, 2))\n    if not np.any(jitter &gt; 0):\n        for _ in range(10):  # See: https://github.com/albumentations-team/albumentations/issues/1442\n            jitter = random_utils.normal(0, scale, (nb_cells, 2))\n            if np.any(jitter &gt; 0):\n                break\n        if not np.any(jitter &gt; 0):\n            return {\"matrix\": None}\n\n    y = np.linspace(0, height, nb_rows)\n    x = np.linspace(0, width, nb_cols)\n\n    # (H, W) and (H, W) for H=rows, W=cols\n    xx_src, yy_src = np.meshgrid(x, y)\n\n    # (1, HW, 2) =&gt; (HW, 2) for H=rows, W=cols\n    points_src = np.dstack([yy_src.flat, xx_src.flat])[0]\n\n    if self.absolute_scale:\n        jitter[:, 0] = jitter[:, 0] / height if height &gt; 0 else 0.0\n        jitter[:, 1] = jitter[:, 1] / width if width &gt; 0 else 0.0\n\n    jitter[:, 0] = jitter[:, 0] * height\n    jitter[:, 1] = jitter[:, 1] * width\n\n    points_dest = np.copy(points_src)\n    points_dest[:, 0] = points_dest[:, 0] + jitter[:, 0]\n    points_dest[:, 1] = points_dest[:, 1] + jitter[:, 1]\n\n    # Restrict all destination points to be inside the image plane.\n    # This is necessary, as otherwise keypoints could be augmented\n    # outside of the image plane and these would be replaced by\n    # (-1, -1), which would not conform with the behaviour of the other augmenters.\n    points_dest[:, 0] = np.clip(points_dest[:, 0], 0, height - 1)\n    points_dest[:, 1] = np.clip(points_dest[:, 1], 0, width - 1)\n\n    matrix = skimage.transform.PiecewiseAffineTransform()\n    matrix.estimate(points_src[:, ::-1], points_dest[:, ::-1])\n\n    return {\n        \"matrix\": matrix,\n    }\n</code></pre>"},{"location":"api_reference/augmentations/geometric/transforms/#albumentations.augmentations.geometric.transforms.PiecewiseAffine.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return (\n        \"scale\",\n        \"nb_rows\",\n        \"nb_cols\",\n        \"interpolation\",\n        \"mask_interpolation\",\n        \"cval\",\n        \"cval_mask\",\n        \"mode\",\n        \"absolute_scale\",\n        \"keypoints_threshold\",\n    )\n</code></pre>"},{"location":"api_reference/augmentations/geometric/transforms/#albumentations.augmentations.geometric.transforms.ShiftScaleRotate","title":"<code>class  ShiftScaleRotate</code> <code>     (shift_limit=(-0.0625, 0.0625), scale_limit=(-0.1, 0.1), rotate_limit=(-45, 45), interpolation=1, border_mode=4, value=0, mask_value=0, shift_limit_x=None, shift_limit_y=None, rotate_method='largest_box', always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Randomly apply affine transforms: translate, scale and rotate the input.</p> <p>Parameters:</p> Name Type Description <code>shift_limit</code> <code>float, float) or float</code> <p>shift factor range for both height and width. If shift_limit is a single float value, the range will be (-shift_limit, shift_limit). Absolute values for lower and upper bounds should lie in range [-1, 1]. Default: (-0.0625, 0.0625).</p> <code>scale_limit</code> <code>float, float) or float</code> <p>scaling factor range. If scale_limit is a single float value, the range will be (-scale_limit, scale_limit). Note that the scale_limit will be biased by 1. If scale_limit is a tuple, like (low, high), sampling will be done from the range (1 + low, 1 + high). Default: (-0.1, 0.1).</p> <code>rotate_limit</code> <code>int, int) or int</code> <p>rotation range. If rotate_limit is a single int value, the range will be (-rotate_limit, rotate_limit). Default: (-45, 45).</p> <code>interpolation</code> <code>OpenCV flag</code> <p>flag that is used to specify the interpolation algorithm. Should be one of: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4. Default: cv2.INTER_LINEAR.</p> <code>border_mode</code> <code>OpenCV flag</code> <p>flag that is used to specify the pixel extrapolation method. Should be one of: cv2.BORDER_CONSTANT, cv2.BORDER_REPLICATE, cv2.BORDER_REFLECT, cv2.BORDER_WRAP, cv2.BORDER_REFLECT_101. Default: cv2.BORDER_REFLECT_101</p> <code>value</code> <code>int, float, list of int, list of float</code> <p>padding value if border_mode is cv2.BORDER_CONSTANT.</p> <code>mask_value</code> <code>int, float,         list of int,         list of float</code> <p>padding value if border_mode is cv2.BORDER_CONSTANT applied for masks.</p> <code>shift_limit_x</code> <code>float, float) or float</code> <p>shift factor range for width. If it is set then this value instead of shift_limit will be used for shifting width.  If shift_limit_x is a single float value, the range will be (-shift_limit_x, shift_limit_x). Absolute values for lower and upper bounds should lie in the range [-1, 1]. Default: None.</p> <code>shift_limit_y</code> <code>float, float) or float</code> <p>shift factor range for height. If it is set then this value instead of shift_limit will be used for shifting height.  If shift_limit_y is a single float value, the range will be (-shift_limit_y, shift_limit_y). Absolute values for lower and upper bounds should lie in the range [-, 1]. Default: None.</p> <code>rotate_method</code> <code>str</code> <p>rotation method used for the bounding boxes. Should be one of \"largest_box\" or \"ellipse\". Default: \"largest_box\"</p> <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 0.5.</p> <p>Targets</p> <p>image, mask, keypoints, bboxes</p> <p>Image types:     uint8, float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>class ShiftScaleRotate(Affine):\n    \"\"\"Randomly apply affine transforms: translate, scale and rotate the input.\n\n    Args:\n        shift_limit ((float, float) or float): shift factor range for both height and width. If shift_limit\n            is a single float value, the range will be (-shift_limit, shift_limit). Absolute values for lower and\n            upper bounds should lie in range [-1, 1]. Default: (-0.0625, 0.0625).\n        scale_limit ((float, float) or float): scaling factor range. If scale_limit is a single float value, the\n            range will be (-scale_limit, scale_limit). Note that the scale_limit will be biased by 1.\n            If scale_limit is a tuple, like (low, high), sampling will be done from the range (1 + low, 1 + high).\n            Default: (-0.1, 0.1).\n        rotate_limit ((int, int) or int): rotation range. If rotate_limit is a single int value, the\n            range will be (-rotate_limit, rotate_limit). Default: (-45, 45).\n        interpolation (OpenCV flag): flag that is used to specify the interpolation algorithm. Should be one of:\n            cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.\n            Default: cv2.INTER_LINEAR.\n        border_mode (OpenCV flag): flag that is used to specify the pixel extrapolation method. Should be one of:\n            cv2.BORDER_CONSTANT, cv2.BORDER_REPLICATE, cv2.BORDER_REFLECT, cv2.BORDER_WRAP, cv2.BORDER_REFLECT_101.\n            Default: cv2.BORDER_REFLECT_101\n        value (int, float, list of int, list of float): padding value if border_mode is cv2.BORDER_CONSTANT.\n        mask_value (int, float,\n                    list of int,\n                    list of float): padding value if border_mode is cv2.BORDER_CONSTANT applied for masks.\n        shift_limit_x ((float, float) or float): shift factor range for width. If it is set then this value\n            instead of shift_limit will be used for shifting width.  If shift_limit_x is a single float value,\n            the range will be (-shift_limit_x, shift_limit_x). Absolute values for lower and upper bounds should lie in\n            the range [-1, 1]. Default: None.\n        shift_limit_y ((float, float) or float): shift factor range for height. If it is set then this value\n            instead of shift_limit will be used for shifting height.  If shift_limit_y is a single float value,\n            the range will be (-shift_limit_y, shift_limit_y). Absolute values for lower and upper bounds should lie\n            in the range [-, 1]. Default: None.\n        rotate_method (str): rotation method used for the bounding boxes. Should be one of \"largest_box\" or \"ellipse\".\n            Default: \"largest_box\"\n        p (float): probability of applying the transform. Default: 0.5.\n\n    Targets:\n        image, mask, keypoints, bboxes\n\n    Image types:\n        uint8, float32\n\n    \"\"\"\n\n    _targets = (Targets.IMAGE, Targets.MASK, Targets.KEYPOINTS, Targets.BBOXES)\n\n    class InitSchema(BaseTransformInitSchema):\n        shift_limit: SymmetricRangeType = (-0.0625, 0.0625)\n        scale_limit: SymmetricRangeType = (-0.1, 0.1)\n        rotate_limit: SymmetricRangeType = (-45, 45)\n        interpolation: InterpolationType = cv2.INTER_LINEAR\n        border_mode: BorderModeType = cv2.BORDER_REFLECT_101\n        value: ColorType = 0\n        mask_value: ColorType = 0\n        shift_limit_x: ScaleFloatType | None = Field(default=None)\n        shift_limit_y: ScaleFloatType | None = Field(default=None)\n        rotate_method: Literal[\"largest_box\", \"ellipse\"] = \"largest_box\"\n\n        @model_validator(mode=\"after\")\n        def check_shift_limit(self) -&gt; Self:\n            bounds = -1, 1\n            self.shift_limit_x = to_tuple(self.shift_limit_x if self.shift_limit_x is not None else self.shift_limit)\n            check_range(self.shift_limit_x, *bounds, \"shift_limit_x\")\n            self.shift_limit_y = to_tuple(self.shift_limit_y if self.shift_limit_y is not None else self.shift_limit)\n            check_range(self.shift_limit_y, *bounds, \"shift_limit_y\")\n            return self\n\n        @field_validator(\"scale_limit\")\n        @classmethod\n        def check_scale_limit(cls, value: ScaleFloatType, info: ValidationInfo) -&gt; ScaleFloatType:\n            bounds = 0, float(\"inf\")\n            result = to_tuple(value, bias=1.0)\n            check_range(result, *bounds, str(info.field_name))\n            return result\n\n    def __init__(\n        self,\n        shift_limit: ScaleFloatType = (-0.0625, 0.0625),\n        scale_limit: ScaleFloatType = (-0.1, 0.1),\n        rotate_limit: ScaleFloatType = (-45, 45),\n        interpolation: int = cv2.INTER_LINEAR,\n        border_mode: int = cv2.BORDER_REFLECT_101,\n        value: ColorType = 0,\n        mask_value: ColorType = 0,\n        shift_limit_x: ScaleFloatType | None = None,\n        shift_limit_y: ScaleFloatType | None = None,\n        rotate_method: Literal[\"largest_box\", \"ellipse\"] = \"largest_box\",\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(\n            scale=scale_limit,\n            translate_percent={\"x\": shift_limit_x, \"y\": shift_limit_y},\n            rotate=rotate_limit,\n            shear=(0, 0),\n            interpolation=interpolation,\n            mask_interpolation=cv2.INTER_NEAREST,\n            cval=value,\n            cval_mask=mask_value,\n            mode=border_mode,\n            fit_output=False,\n            keep_ratio=False,\n            rotate_method=rotate_method,\n            always_apply=always_apply,\n            p=p,\n        )\n        warn(\n            \"ShiftScaleRotate is deprecated. Please use Affine transform instead .\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        self.shift_limit_x = cast(Tuple[float, float], shift_limit_x)\n        self.shift_limit_y = cast(Tuple[float, float], shift_limit_y)\n        self.scale_limit = cast(Tuple[float, float], scale_limit)\n        self.rotate_limit = cast(Tuple[int, int], rotate_limit)\n        self.border_mode = border_mode\n        self.value = value\n        self.mask_value = mask_value\n\n    def get_transform_init_args(self) -&gt; dict[str, Any]:\n        return {\n            \"shift_limit_x\": self.shift_limit_x,\n            \"shift_limit_y\": self.shift_limit_y,\n            \"scale_limit\": to_tuple(self.scale_limit, bias=-1.0),\n            \"rotate_limit\": self.rotate_limit,\n            \"interpolation\": self.interpolation,\n            \"border_mode\": self.border_mode,\n            \"value\": self.value,\n            \"mask_value\": self.mask_value,\n            \"rotate_method\": self.rotate_method,\n        }\n</code></pre>"},{"location":"api_reference/augmentations/geometric/transforms/#albumentations.augmentations.geometric.transforms.Transpose","title":"<code>class  Transpose</code> <code> </code>  [view source on GitHub]","text":"<p>Transpose the input by swapping rows and columns.</p> <p>Parameters:</p> Name Type Description <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 0.5.</p> <p>Targets</p> <p>image, mask, bboxes, keypoints</p> <p>Image types:     uint8, float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>class Transpose(DualTransform):\n    \"\"\"Transpose the input by swapping rows and columns.\n\n    Args:\n        p (float): probability of applying the transform. Default: 0.5.\n\n    Targets:\n        image, mask, bboxes, keypoints\n\n    Image types:\n        uint8, float32\n\n    \"\"\"\n\n    _targets = (Targets.IMAGE, Targets.MASK, Targets.BBOXES, Targets.KEYPOINTS)\n\n    def apply(self, img: np.ndarray, **params: Any) -&gt; np.ndarray:\n        return fgeometric.transpose(img)\n\n    def apply_to_bbox(self, bbox: BoxInternalType, **params: Any) -&gt; BoxInternalType:\n        return fgeometric.bbox_transpose(bbox)\n\n    def apply_to_keypoint(self, keypoint: KeypointInternalType, **params: Any) -&gt; KeypointInternalType:\n        return fgeometric.keypoint_transpose(keypoint)\n\n    def get_transform_init_args_names(self) -&gt; tuple[()]:\n        return ()\n</code></pre>"},{"location":"api_reference/augmentations/geometric/transforms/#albumentations.augmentations.geometric.transforms.Transpose.apply","title":"<code>apply (self, img, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def apply(self, img: np.ndarray, **params: Any) -&gt; np.ndarray:\n    return fgeometric.transpose(img)\n</code></pre>"},{"location":"api_reference/augmentations/geometric/transforms/#albumentations.augmentations.geometric.transforms.Transpose.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[()]:\n    return ()\n</code></pre>"},{"location":"api_reference/augmentations/geometric/transforms/#albumentations.augmentations.geometric.transforms.VerticalFlip","title":"<code>class  VerticalFlip</code> <code> </code>  [view source on GitHub]","text":"<p>Flip the input vertically around the x-axis.</p> <p>Parameters:</p> Name Type Description <code>p</code> <code>float</code> <p>probability of applying the transform. Default: 0.5.</p> <p>Targets</p> <p>image, mask, bboxes, keypoints</p> <p>Image types:     uint8, float32</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>class VerticalFlip(DualTransform):\n    \"\"\"Flip the input vertically around the x-axis.\n\n    Args:\n        p (float): probability of applying the transform. Default: 0.5.\n\n    Targets:\n        image, mask, bboxes, keypoints\n\n    Image types:\n        uint8, float32\n\n    \"\"\"\n\n    _targets = (Targets.IMAGE, Targets.MASK, Targets.BBOXES, Targets.KEYPOINTS)\n\n    def apply(self, img: np.ndarray, **params: Any) -&gt; np.ndarray:\n        return fgeometric.vflip(img)\n\n    def apply_to_bbox(self, bbox: BoxInternalType, **params: Any) -&gt; BoxInternalType:\n        return fgeometric.bbox_vflip(bbox)\n\n    def apply_to_keypoint(self, keypoint: KeypointInternalType, **params: Any) -&gt; KeypointInternalType:\n        return fgeometric.keypoint_vflip(keypoint, params[\"rows\"])\n\n    def get_transform_init_args_names(self) -&gt; tuple[()]:\n        return ()\n</code></pre>"},{"location":"api_reference/augmentations/geometric/transforms/#albumentations.augmentations.geometric.transforms.VerticalFlip.apply","title":"<code>apply (self, img, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def apply(self, img: np.ndarray, **params: Any) -&gt; np.ndarray:\n    return fgeometric.vflip(img)\n</code></pre>"},{"location":"api_reference/augmentations/geometric/transforms/#albumentations.augmentations.geometric.transforms.VerticalFlip.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/geometric/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[()]:\n    return ()\n</code></pre>"},{"location":"api_reference/augmentations/mixing/","title":"Index","text":"<ul> <li>Mixing transforms (albumentations.augmentations.mixing.transforms)</li> </ul>"},{"location":"api_reference/augmentations/mixing/transforms/","title":"Mixing transforms (augmentations.mixing.transforms)","text":""},{"location":"api_reference/augmentations/mixing/transforms/#albumentations.augmentations.mixing.transforms.MixUp","title":"<code>class  MixUp</code> <code>     (reference_data=None, read_fn=&lt;function MixUp.&lt;lambda&gt; at 0x7f4aecf6f7e0&gt;, alpha=0.4, mix_coef_return_name='mix_coef', always_apply=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Performs MixUp data augmentation, blending images, masks, and class labels with reference data.</p> <p>MixUp augmentation linearly combines an input (image, mask, and class label) with another set from a predefined reference dataset. The mixing degree is controlled by a parameter \u03bb (lambda), sampled from a Beta distribution. This method is known for improving model generalization by promoting linear behavior between classes and smoothing decision boundaries.</p> <p>Reference</p> <ul> <li>Zhang, H., Cisse, M., Dauphin, Y.N., and Lopez-Paz, D. (2018). mixup: Beyond Empirical Risk Minimization. In International Conference on Learning Representations. https://arxiv.org/abs/1710.09412</li> </ul> <p>Parameters:</p> Name Type Description <code>reference_data</code> <code>Optional[Union[Generator[ReferenceImage, None, None], Sequence[Any]]]</code> <p>A sequence or generator of dictionaries containing the reference data for mixing If None or an empty sequence is provided, no operation is performed and a warning is issued.</p> <code>read_fn</code> <code>Callable[[ReferenceImage], dict[str, Any]]</code> <p>A function to process items from reference_data. It should accept items from reference_data and return a dictionary containing processed data:     - The returned dictionary must include an 'image' key with a numpy array value.     - It may also include 'mask', 'global_label' each associated with numpy array values. Defaults to a function that assumes input dictionary contains numpy arrays and directly returns it.</p> <code>mix_coef_return_name</code> <code>str</code> <p>Name used for the applied alpha coefficient in the returned dictionary. Defaults to \"mix_coef\".</p> <code>alpha</code> <code>float</code> <p>The alpha parameter for the Beta distribution, influencing the mix's balance. Must be \u2265 0. Higher values lead to more uniform mixing. Defaults to 0.4.</p> <code>p</code> <code>float</code> <p>The probability of applying the transformation. Defaults to 0.5.</p> <p>Targets</p> <p>image, mask, global_label</p> <p>Image types:     - uint8, float32</p> <p>Exceptions:</p> Type Description <code>- ValueError</code> <p>If the alpha parameter is negative.</p> <code>- NotImplementedError</code> <p>If the transform is applied to bounding boxes or keypoints.</p> <p>Notes</p> <ul> <li>If no reference data is provided, a warning is issued, and the transform acts as a no-op.</li> <li>Notes if images are in float32 format, they should be within [0, 1] range.</li> </ul> <p>Example Usage:     import albumentations as A     import numpy as np     from albumentations.core.types import ReferenceImage</p> <pre><code># Prepare reference data\n# Note: This code generates random reference data for demonstration purposes only.\n# In real-world applications, it's crucial to use meaningful and representative data.\n# The quality and relevance of your input data significantly impact the effectiveness\n# of the augmentation process. Ensure your data closely aligns with your specific\n# use case and application requirements.\nreference_data = [ReferenceImage(image=np.random.randint(0, 256, [100, 100, 3], dtype=np.uint8),\n                                 mask=np.random.randint(0, 4, (100, 100, 1), dtype=np.uint8),\n                                 global_label=np.random.choice([0, 1], size=3)) for i in range(10)]\n\n# In this example, the lambda function simply returns its input, which works well for\n# data already in the expected format. For more complex scenarios, where the data might not be in\n# the required format or additional processing is needed, a more sophisticated function can be implemented.\n# Below is a hypothetical example where the input data is a file path, # and the function reads the image\n# file, converts it to a specific format, and possibly performs other preprocessing steps.\n\n# Example of a more complex read_fn that reads an image from a file path, converts it to RGB, and resizes it.\n# def custom_read_fn(file_path):\n#     from PIL import Image\n#     image = Image.open(file_path).convert('RGB')\n#     image = image.resize((100, 100))  # Example resize, adjust as needed.\n#     return np.array(image)\n\n# aug = A.Compose([A.RandomRotate90(), A.MixUp(p=1, reference_data=reference_data, read_fn=lambda x: x)])\n\n# For simplicity, the original lambda function is used in this example.\n# Replace `lambda x: x` with `custom_read_fn`if you need to process the data more extensively.\n\n# Apply augmentations\nimage = np.empty([100, 100, 3], dtype=np.uint8)\nmask = np.empty([100, 100], dtype=np.uint8)\nglobal_label = np.array([0, 1, 0])\ndata = aug(image=image, global_label=global_label, mask=mask)\ntransformed_image = data[\"image\"]\ntransformed_mask = data[\"mask\"]\ntransformed_global_label = data[\"global_label\"]\n\n# Print applied mix coefficient\nprint(data[\"mix_coef\"])  # Output: e.g., 0.9991580344142427\n</code></pre> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/mixing/transforms.py</code> Python<pre><code>class MixUp(ReferenceBasedTransform):\n    \"\"\"Performs MixUp data augmentation, blending images, masks, and class labels with reference data.\n\n    MixUp augmentation linearly combines an input (image, mask, and class label) with another set from a predefined\n    reference dataset. The mixing degree is controlled by a parameter \u03bb (lambda), sampled from a Beta distribution.\n    This method is known for improving model generalization by promoting linear behavior between classes and\n    smoothing decision boundaries.\n\n    Reference:\n        - Zhang, H., Cisse, M., Dauphin, Y.N., and Lopez-Paz, D. (2018). mixup: Beyond Empirical Risk Minimization.\n        In International Conference on Learning Representations. https://arxiv.org/abs/1710.09412\n\n    Args:\n        reference_data (Optional[Union[Generator[ReferenceImage, None, None], Sequence[Any]]]):\n            A sequence or generator of dictionaries containing the reference data for mixing\n            If None or an empty sequence is provided, no operation is performed and a warning is issued.\n        read_fn (Callable[[ReferenceImage], dict[str, Any]]):\n            A function to process items from reference_data. It should accept items from reference_data\n            and return a dictionary containing processed data:\n                - The returned dictionary must include an 'image' key with a numpy array value.\n                - It may also include 'mask', 'global_label' each associated with numpy array values.\n            Defaults to a function that assumes input dictionary contains numpy arrays and directly returns it.\n        mix_coef_return_name (str): Name used for the applied alpha coefficient in the returned dictionary.\n            Defaults to \"mix_coef\".\n        alpha (float):\n            The alpha parameter for the Beta distribution, influencing the mix's balance. Must be \u2265 0.\n            Higher values lead to more uniform mixing. Defaults to 0.4.\n        p (float):\n            The probability of applying the transformation. Defaults to 0.5.\n\n    Targets:\n        image, mask, global_label\n\n    Image types:\n        - uint8, float32\n\n    Raises:\n        - ValueError: If the alpha parameter is negative.\n        - NotImplementedError: If the transform is applied to bounding boxes or keypoints.\n\n    Notes:\n        - If no reference data is provided, a warning is issued, and the transform acts as a no-op.\n        - Notes if images are in float32 format, they should be within [0, 1] range.\n\n    Example Usage:\n        import albumentations as A\n        import numpy as np\n        from albumentations.core.types import ReferenceImage\n\n        # Prepare reference data\n        # Note: This code generates random reference data for demonstration purposes only.\n        # In real-world applications, it's crucial to use meaningful and representative data.\n        # The quality and relevance of your input data significantly impact the effectiveness\n        # of the augmentation process. Ensure your data closely aligns with your specific\n        # use case and application requirements.\n        reference_data = [ReferenceImage(image=np.random.randint(0, 256, [100, 100, 3], dtype=np.uint8),\n                                         mask=np.random.randint(0, 4, (100, 100, 1), dtype=np.uint8),\n                                         global_label=np.random.choice([0, 1], size=3)) for i in range(10)]\n\n        # In this example, the lambda function simply returns its input, which works well for\n        # data already in the expected format. For more complex scenarios, where the data might not be in\n        # the required format or additional processing is needed, a more sophisticated function can be implemented.\n        # Below is a hypothetical example where the input data is a file path, # and the function reads the image\n        # file, converts it to a specific format, and possibly performs other preprocessing steps.\n\n        # Example of a more complex read_fn that reads an image from a file path, converts it to RGB, and resizes it.\n        # def custom_read_fn(file_path):\n        #     from PIL import Image\n        #     image = Image.open(file_path).convert('RGB')\n        #     image = image.resize((100, 100))  # Example resize, adjust as needed.\n        #     return np.array(image)\n\n        # aug = A.Compose([A.RandomRotate90(), A.MixUp(p=1, reference_data=reference_data, read_fn=lambda x: x)])\n\n        # For simplicity, the original lambda function is used in this example.\n        # Replace `lambda x: x` with `custom_read_fn`if you need to process the data more extensively.\n\n        # Apply augmentations\n        image = np.empty([100, 100, 3], dtype=np.uint8)\n        mask = np.empty([100, 100], dtype=np.uint8)\n        global_label = np.array([0, 1, 0])\n        data = aug(image=image, global_label=global_label, mask=mask)\n        transformed_image = data[\"image\"]\n        transformed_mask = data[\"mask\"]\n        transformed_global_label = data[\"global_label\"]\n\n        # Print applied mix coefficient\n        print(data[\"mix_coef\"])  # Output: e.g., 0.9991580344142427\n    \"\"\"\n\n    _targets = (Targets.IMAGE, Targets.MASK, Targets.GLOBAL_LABEL)\n\n    class InitSchema(BaseTransformInitSchema):\n        reference_data: Generator[Any, None, None] | Sequence[Any] | None = None\n        read_fn: Callable[[ReferenceImage], Any]\n        alpha: Annotated[float, Field(default=0.4, ge=0, le=1)]\n        mix_coef_return_name: str = \"mix_coef\"\n\n    def __init__(\n        self,\n        reference_data: Generator[Any, None, None] | Sequence[Any] | None = None,\n        read_fn: Callable[[ReferenceImage], Any] = lambda x: {\"image\": x, \"mask\": None, \"class_label\": None},\n        alpha: float = 0.4,\n        mix_coef_return_name: str = \"mix_coef\",\n        always_apply: bool | None = None,\n        p: float = 0.5,\n    ):\n        super().__init__(p=p, always_apply=always_apply)\n        self.mix_coef_return_name = mix_coef_return_name\n\n        self.read_fn = read_fn\n        self.alpha = alpha\n\n        if reference_data is None:\n            warn(\"No reference data provided for MixUp. This transform will act as a no-op.\", stacklevel=2)\n            # Create an empty generator\n            self.reference_data: list[Any] = []\n        elif (\n            isinstance(reference_data, types.GeneratorType)\n            or isinstance(reference_data, Iterable)\n            and not isinstance(reference_data, str)\n        ):\n            self.reference_data = reference_data  # type: ignore[assignment]\n        else:\n            msg = \"reference_data must be a list, tuple, generator, or None.\"\n            raise TypeError(msg)\n\n    def apply(self, img: np.ndarray, mix_data: ReferenceImage, mix_coef: float, **params: Any) -&gt; np.ndarray:\n        if not mix_data:\n            return img\n\n        mix_img = mix_data[\"image\"]\n\n        if img.shape != mix_img.shape and not is_grayscale_image(img):\n            msg = \"The shape of the reference image should be the same as the input image.\"\n            raise ValueError(msg)\n\n        return add_weighted(img, mix_coef, mix_img.reshape(img.shape), 1 - mix_coef) if mix_img is not None else img\n\n    def apply_to_mask(self, mask: np.ndarray, mix_data: ReferenceImage, mix_coef: float, **params: Any) -&gt; np.ndarray:\n        mix_mask = mix_data.get(\"mask\")\n        return (\n            add_weighted(mask, mix_coef, mix_mask.reshape(mask.shape), 1 - mix_coef) if mix_mask is not None else mask\n        )\n\n    def apply_to_global_label(\n        self,\n        label: np.ndarray,\n        mix_data: ReferenceImage,\n        mix_coef: float,\n        **params: Any,\n    ) -&gt; np.ndarray:\n        mix_label = mix_data.get(\"global_label\")\n        if mix_label is not None and label is not None:\n            return mix_coef * label + (1 - mix_coef) * mix_label\n        return label\n\n    def apply_to_bboxes(self, bboxes: Sequence[BoxType], mix_data: ReferenceImage, **params: Any) -&gt; Sequence[BoxType]:\n        msg = \"MixUp does not support bounding boxes yet, feel free to submit pull request to https://github.com/albumentations-team/albumentations/.\"\n        raise NotImplementedError(msg)\n\n    def apply_to_keypoints(\n        self,\n        keypoints: Sequence[KeypointType],\n        *args: Any,\n        **params: Any,\n    ) -&gt; Sequence[KeypointType]:\n        msg = \"MixUp does not support keypoints yet, feel free to submit pull request to https://github.com/albumentations-team/albumentations/.\"\n        raise NotImplementedError(msg)\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return \"reference_data\", \"alpha\"\n\n    def get_params(self) -&gt; dict[str, None | float | dict[str, Any]]:\n        mix_data = None\n        # Check if reference_data is not empty and is a sequence (list, tuple, np.array)\n        if isinstance(self.reference_data, Sequence) and not isinstance(self.reference_data, (str, bytes)):\n            if len(self.reference_data) &gt; 0:  # Additional check to ensure it's not empty\n                mix_idx = random.randint(0, len(self.reference_data) - 1)\n                mix_data = self.reference_data[mix_idx]\n        # Check if reference_data is an iterator or generator\n        elif isinstance(self.reference_data, Iterator):\n            try:\n                mix_data = next(self.reference_data)  # Attempt to get the next item\n            except StopIteration:\n                warn(\n                    \"Reference data iterator/generator has been exhausted. \"\n                    \"Further mixing augmentations will not be applied.\",\n                    RuntimeWarning,\n                    stacklevel=2,\n                )\n                return {\"mix_data\": {}, \"mix_coef\": 1}\n\n        # If mix_data is None or empty after the above checks, return default values\n        if mix_data is None:\n            return {\"mix_data\": {}, \"mix_coef\": 1}\n\n        # If mix_data is not None, calculate mix_coef and apply read_fn\n        mix_coef = beta(self.alpha, self.alpha)  # Assuming beta is defined elsewhere\n        return {\"mix_data\": self.read_fn(mix_data), \"mix_coef\": mix_coef}\n\n    def apply_with_params(self, params: dict[str, Any], *args: Any, **kwargs: Any) -&gt; dict[str, Any]:\n        res = super().apply_with_params(params, *args, **kwargs)\n        if self.mix_coef_return_name:\n            res[self.mix_coef_return_name] = params[\"mix_coef\"]\n            res[\"mix_data\"] = params[\"mix_data\"]\n        return res\n</code></pre>"},{"location":"api_reference/augmentations/mixing/transforms/#albumentations.augmentations.mixing.transforms.MixUp.apply","title":"<code>apply (self, img, mix_data, mix_coef, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/mixing/transforms.py</code> Python<pre><code>def apply(self, img: np.ndarray, mix_data: ReferenceImage, mix_coef: float, **params: Any) -&gt; np.ndarray:\n    if not mix_data:\n        return img\n\n    mix_img = mix_data[\"image\"]\n\n    if img.shape != mix_img.shape and not is_grayscale_image(img):\n        msg = \"The shape of the reference image should be the same as the input image.\"\n        raise ValueError(msg)\n\n    return add_weighted(img, mix_coef, mix_img.reshape(img.shape), 1 - mix_coef) if mix_img is not None else img\n</code></pre>"},{"location":"api_reference/augmentations/mixing/transforms/#albumentations.augmentations.mixing.transforms.MixUp.apply_with_params","title":"<code>apply_with_params (self, params, *args, **kwargs)</code>","text":"<p>Apply transforms with parameters.</p> Source code in <code>albumentations/augmentations/mixing/transforms.py</code> Python<pre><code>def apply_with_params(self, params: dict[str, Any], *args: Any, **kwargs: Any) -&gt; dict[str, Any]:\n    res = super().apply_with_params(params, *args, **kwargs)\n    if self.mix_coef_return_name:\n        res[self.mix_coef_return_name] = params[\"mix_coef\"]\n        res[\"mix_data\"] = params[\"mix_data\"]\n    return res\n</code></pre>"},{"location":"api_reference/augmentations/mixing/transforms/#albumentations.augmentations.mixing.transforms.MixUp.get_params","title":"<code>get_params (self)</code>","text":"<p>Returns parameters independent of input.</p> Source code in <code>albumentations/augmentations/mixing/transforms.py</code> Python<pre><code>def get_params(self) -&gt; dict[str, None | float | dict[str, Any]]:\n    mix_data = None\n    # Check if reference_data is not empty and is a sequence (list, tuple, np.array)\n    if isinstance(self.reference_data, Sequence) and not isinstance(self.reference_data, (str, bytes)):\n        if len(self.reference_data) &gt; 0:  # Additional check to ensure it's not empty\n            mix_idx = random.randint(0, len(self.reference_data) - 1)\n            mix_data = self.reference_data[mix_idx]\n    # Check if reference_data is an iterator or generator\n    elif isinstance(self.reference_data, Iterator):\n        try:\n            mix_data = next(self.reference_data)  # Attempt to get the next item\n        except StopIteration:\n            warn(\n                \"Reference data iterator/generator has been exhausted. \"\n                \"Further mixing augmentations will not be applied.\",\n                RuntimeWarning,\n                stacklevel=2,\n            )\n            return {\"mix_data\": {}, \"mix_coef\": 1}\n\n    # If mix_data is None or empty after the above checks, return default values\n    if mix_data is None:\n        return {\"mix_data\": {}, \"mix_coef\": 1}\n\n    # If mix_data is not None, calculate mix_coef and apply read_fn\n    mix_coef = beta(self.alpha, self.alpha)  # Assuming beta is defined elsewhere\n    return {\"mix_data\": self.read_fn(mix_data), \"mix_coef\": mix_coef}\n</code></pre>"},{"location":"api_reference/augmentations/mixing/transforms/#albumentations.augmentations.mixing.transforms.MixUp.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/mixing/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return \"reference_data\", \"alpha\"\n</code></pre>"},{"location":"api_reference/augmentations/mixing/transforms/#albumentations.augmentations.mixing.transforms.OverlayElements","title":"<code>class  OverlayElements</code> <code>     (metadata_key='overlay_metadata', p=0.5, always_apply=None)                 </code>  [view source on GitHub]","text":"<p>Apply overlay elements such as images and masks onto an input image. This transformation can be used to add various objects (e.g., stickers, logos) to images with optional masks and bounding boxes for better placement control.</p> <p>Parameters:</p> Name Type Description <code>metadata_key</code> <code>str</code> <p>Additional target key for metadata. Default <code>overlay_metadata</code>.</p> <code>p</code> <code>float</code> <p>Probability of applying the transformation. Default: 0.5.</p> <p>Possible Metadata Fields:     - image (np.ndarray): The overlay image to be applied. This is a required field.     - bbox (list[int]): The bounding box specifying the region where the overlay should be applied. It should                         contain four floats: [y_min, x_min, y_max, x_max]. If <code>label_id</code> is provided, it should                         be appended as the fifth element in the bbox. BBox should be in Albumentations format,                         that is the same as normalized Pascal VOC format                         [x_min / width, y_min / height, x_max / width, y_max / height]     - mask (np.ndarray): An optional mask that defines the non-rectangular region of the overlay image. If not                          provided, the entire overlay image is used.     - mask_id (int): An optional identifier for the mask. If provided, the regions specified by the mask will                      be labeled with this identifier in the output mask.</p> <p>Targets</p> <p>image, mask</p> <p>Image types:     uint8, float32</p> <p>Reference</p> <p>https://github.com/danaaubakirova/doc-augmentation</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/augmentations/mixing/transforms.py</code> Python<pre><code>class OverlayElements(ReferenceBasedTransform):\n    \"\"\"Apply overlay elements such as images and masks onto an input image. This transformation can be used to add\n    various objects (e.g., stickers, logos) to images with optional masks and bounding boxes for better placement\n    control.\n\n    Args:\n        metadata_key (str): Additional target key for metadata. Default `overlay_metadata`.\n        p (float): Probability of applying the transformation. Default: 0.5.\n\n    Possible Metadata Fields:\n        - image (np.ndarray): The overlay image to be applied. This is a required field.\n        - bbox (list[int]): The bounding box specifying the region where the overlay should be applied. It should\n                            contain four floats: [y_min, x_min, y_max, x_max]. If `label_id` is provided, it should\n                            be appended as the fifth element in the bbox. BBox should be in Albumentations format,\n                            that is the same as normalized Pascal VOC format\n                            [x_min / width, y_min / height, x_max / width, y_max / height]\n        - mask (np.ndarray): An optional mask that defines the non-rectangular region of the overlay image. If not\n                             provided, the entire overlay image is used.\n        - mask_id (int): An optional identifier for the mask. If provided, the regions specified by the mask will\n                         be labeled with this identifier in the output mask.\n\n    Targets:\n        image, mask\n\n    Image types:\n        uint8, float32\n\n    Reference:\n        https://github.com/danaaubakirova/doc-augmentation\n\n    \"\"\"\n\n    _targets = (Targets.IMAGE, Targets.MASK)\n\n    class InitSchema(BaseTransformInitSchema):\n        metadata_key: str\n\n    def __init__(\n        self,\n        metadata_key: str = \"overlay_metadata\",\n        p: float = 0.5,\n        always_apply: bool | None = None,\n    ):\n        super().__init__(p=p, always_apply=always_apply)\n        self.metadata_key = metadata_key\n\n    @property\n    def targets_as_params(self) -&gt; list[str]:\n        return [self.metadata_key]\n\n    @staticmethod\n    def preprocess_metadata(metadata: dict[str, Any], img_shape: SizeType) -&gt; dict[str, Any]:\n        overlay_image = metadata[\"image\"]\n        overlay_height, overlay_width = overlay_image.shape[:2]\n        image_height, image_width = img_shape[:2]\n\n        if \"bbox\" in metadata:\n            bbox = metadata[\"bbox\"]\n            check_bbox(bbox)\n            denormalized_bbox = denormalize_bbox(bbox[:4], img_shape[:2])\n\n            x_min, y_min, x_max, y_max = (int(x) for x in denormalized_bbox[:4])\n\n            if \"mask\" in metadata:\n                mask = metadata[\"mask\"]\n                mask = cv2.resize(mask, (x_max - x_min, y_max - y_min), interpolation=cv2.INTER_NEAREST)\n            else:\n                mask = np.ones((y_max - y_min, x_max - x_min), dtype=np.uint8)\n\n            overlay_image = cv2.resize(overlay_image, (x_max - x_min, y_max - y_min), interpolation=cv2.INTER_AREA)\n            offset = (y_min, x_min)\n\n            if len(bbox) == LENGTH_RAW_BBOX and \"bbox_id\" in metadata:\n                bbox = [x_min, y_min, x_max, y_max, metadata[\"bbox_id\"]]\n            else:\n                bbox = (x_min, y_min, x_max, y_max, *bbox[4:])\n        else:\n            if image_height &lt; overlay_height or image_width &lt; overlay_width:\n                overlay_image = cv2.resize(overlay_image, (image_width, image_height), interpolation=cv2.INTER_AREA)\n                overlay_height, overlay_width = overlay_image.shape[:2]\n\n            mask = metadata[\"mask\"] if \"mask\" in metadata else np.ones_like(overlay_image, dtype=np.uint8)\n\n            max_x_offset = image_width - overlay_width\n            max_y_offset = image_height - overlay_height\n\n            offset_x = random.randint(0, max_x_offset)\n            offset_y = random.randint(0, max_y_offset)\n\n            offset = (offset_y, offset_x)\n\n            bbox = [\n                offset_x,\n                offset_y,\n                offset_x + overlay_width,\n                offset_y + overlay_height,\n            ]\n\n            if \"bbox_id\" in metadata:\n                bbox = [*bbox, metadata[\"bbox_id\"]]\n\n        result = {\n            \"overlay_image\": overlay_image,\n            \"overlay_mask\": mask,\n            \"offset\": offset,\n            \"bbox\": bbox,\n        }\n\n        if \"mask_id\" in metadata:\n            result[\"mask_id\"] = metadata[\"mask_id\"]\n\n        return result\n\n    def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n        metadata = data[self.metadata_key]\n        img_shape = params[\"shape\"]\n\n        if isinstance(metadata, list):\n            overlay_data = [self.preprocess_metadata(md, img_shape) for md in metadata]\n        else:\n            overlay_data = [self.preprocess_metadata(metadata, img_shape)]\n\n        return {\n            \"overlay_data\": overlay_data,\n        }\n\n    def apply(\n        self,\n        img: np.ndarray,\n        overlay_data: list[dict[str, Any]],\n        **params: Any,\n    ) -&gt; np.ndarray:\n        for data in overlay_data:\n            overlay_image = data[\"overlay_image\"]\n            overlay_mask = data[\"overlay_mask\"]\n            offset = data[\"offset\"]\n            img = fmixing.copy_and_paste_blend(img, overlay_image, overlay_mask, offset=offset)\n        return img\n\n    def apply_to_mask(\n        self,\n        mask: np.ndarray,\n        overlay_data: list[dict[str, Any]],\n        **params: Any,\n    ) -&gt; np.ndarray:\n        for data in overlay_data:\n            if \"mask_id\" in data and data[\"mask_id\"] is not None:\n                overlay_mask = data[\"overlay_mask\"]\n                offset = data[\"offset\"]\n                mask_id = data[\"mask_id\"]\n\n                y_min, x_min = offset\n                y_max = y_min + overlay_mask.shape[0]\n                x_max = x_min + overlay_mask.shape[1]\n\n                mask_section = mask[y_min:y_max, x_min:x_max]\n                mask_section[overlay_mask &gt; 0] = mask_id\n\n        return mask\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return (\"metadata_key\",)\n</code></pre>"},{"location":"api_reference/augmentations/mixing/transforms/#albumentations.augmentations.mixing.transforms.OverlayElements.targets_as_params","title":"<code>targets_as_params: list[str]</code>  <code>property</code> <code>readonly</code>","text":"<p>Targets used to get params dependent on targets. This is used to check input has all required targets.</p>"},{"location":"api_reference/augmentations/mixing/transforms/#albumentations.augmentations.mixing.transforms.OverlayElements.apply","title":"<code>apply (self, img, overlay_data, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/augmentations/mixing/transforms.py</code> Python<pre><code>def apply(\n    self,\n    img: np.ndarray,\n    overlay_data: list[dict[str, Any]],\n    **params: Any,\n) -&gt; np.ndarray:\n    for data in overlay_data:\n        overlay_image = data[\"overlay_image\"]\n        overlay_mask = data[\"overlay_mask\"]\n        offset = data[\"offset\"]\n        img = fmixing.copy_and_paste_blend(img, overlay_image, overlay_mask, offset=offset)\n    return img\n</code></pre>"},{"location":"api_reference/augmentations/mixing/transforms/#albumentations.augmentations.mixing.transforms.OverlayElements.get_params_dependent_on_data","title":"<code>get_params_dependent_on_data (self, params, data)</code>","text":"<p>Returns parameters dependent on input.</p> Source code in <code>albumentations/augmentations/mixing/transforms.py</code> Python<pre><code>def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n    metadata = data[self.metadata_key]\n    img_shape = params[\"shape\"]\n\n    if isinstance(metadata, list):\n        overlay_data = [self.preprocess_metadata(md, img_shape) for md in metadata]\n    else:\n        overlay_data = [self.preprocess_metadata(metadata, img_shape)]\n\n    return {\n        \"overlay_data\": overlay_data,\n    }\n</code></pre>"},{"location":"api_reference/augmentations/mixing/transforms/#albumentations.augmentations.mixing.transforms.OverlayElements.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/augmentations/mixing/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return (\"metadata_key\",)\n</code></pre>"},{"location":"api_reference/core/","title":"Index","text":"<ul> <li>Composition API (albumentations.core.composition)</li> <li>Serialization API (albumentations.core.serialization)</li> <li>Transforms Interface (albumentations.core.transforms_interface)</li> <li>Helper functions for working with bounding boxes (albumentations.core.bbox_utils)</li> <li>Helper functions for working with keypoints (albumentations.core.keypoints_utils)</li> </ul>"},{"location":"api_reference/core/bbox_utils/","title":"Helper functions for working with bounding boxes (augmentations.core.bbox_utils)","text":""},{"location":"api_reference/core/bbox_utils/#albumentations.core.bbox_utils.BboxParams","title":"<code>class  BboxParams</code> <code>     (format, label_fields=None, min_area=0.0, min_visibility=0.0, min_width=0.0, min_height=0.0, check_each_transform=True, clip=False)                 </code>  [view source on GitHub]","text":"<p>Parameters of bounding boxes</p> <p>Parameters:</p> Name Type Description <code>format</code> <code>str</code> <p>format of bounding boxes. Should be <code>coco</code>, <code>pascal_voc</code>, <code>albumentations</code> or <code>yolo</code>.</p> <p>The <code>coco</code> format     <code>[x_min, y_min, width, height]</code>, e.g. [97, 12, 150, 200]. The <code>pascal_voc</code> format     <code>[x_min, y_min, x_max, y_max]</code>, e.g. [97, 12, 247, 212]. The <code>albumentations</code> format     is like <code>pascal_voc</code>, but normalized,     in other words: <code>[x_min, y_min, x_max, y_max]</code>, e.g. [0.2, 0.3, 0.4, 0.5]. The <code>yolo</code> format     <code>[x, y, width, height]</code>, e.g. [0.1, 0.2, 0.3, 0.4];     <code>x</code>, <code>y</code> - normalized bbox center; <code>width</code>, <code>height</code> - normalized bbox width and height.</p> <code>label_fields</code> <code>list</code> <p>List of fields joined with boxes, e.g., labels.</p> <code>min_area</code> <code>float</code> <p>Minimum area of a bounding box in pixels or normalized units. Bounding boxes with an area less than this value will be removed. Default: 0.0.</p> <code>min_visibility</code> <code>float</code> <p>Minimum fraction of area for a bounding box to remain in the list. Bounding boxes with a visible area less than this fraction will be removed. Default: 0.0.</p> <code>min_width</code> <code>float</code> <p>Minimum width of a bounding box in pixels or normalized units. Bounding boxes with a width less than this value will be removed. Default: 0.0.</p> <code>min_height</code> <code>float</code> <p>Minimum height of a bounding box in pixels or normalized units. Bounding boxes with a height less than this value will be removed. Default: 0.0.</p> <code>check_each_transform</code> <code>bool</code> <p>If True, bounding boxes will be checked after each dual transform. Default: True.</p> <code>clip</code> <code>bool</code> <p>If True, bounding boxes will be clipped to the image borders before applying any transform. Default: False.</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/core/bbox_utils.py</code> Python<pre><code>class BboxParams(Params):\n    \"\"\"Parameters of bounding boxes\n\n    Args:\n        format (str): format of bounding boxes. Should be `coco`, `pascal_voc`, `albumentations` or `yolo`.\n\n            The `coco` format\n                `[x_min, y_min, width, height]`, e.g. [97, 12, 150, 200].\n            The `pascal_voc` format\n                `[x_min, y_min, x_max, y_max]`, e.g. [97, 12, 247, 212].\n            The `albumentations` format\n                is like `pascal_voc`, but normalized,\n                in other words: `[x_min, y_min, x_max, y_max]`, e.g. [0.2, 0.3, 0.4, 0.5].\n            The `yolo` format\n                `[x, y, width, height]`, e.g. [0.1, 0.2, 0.3, 0.4];\n                `x`, `y` - normalized bbox center; `width`, `height` - normalized bbox width and height.\n\n        label_fields (list): List of fields joined with boxes, e.g., labels.\n        min_area (float): Minimum area of a bounding box in pixels or normalized units.\n            Bounding boxes with an area less than this value will be removed. Default: 0.0.\n        min_visibility (float): Minimum fraction of area for a bounding box to remain in the list.\n            Bounding boxes with a visible area less than this fraction will be removed. Default: 0.0.\n        min_width (float): Minimum width of a bounding box in pixels or normalized units.\n            Bounding boxes with a width less than this value will be removed. Default: 0.0.\n        min_height (float): Minimum height of a bounding box in pixels or normalized units.\n            Bounding boxes with a height less than this value will be removed. Default: 0.0.\n        check_each_transform (bool): If True, bounding boxes will be checked after each dual transform. Default: True.\n        clip (bool): If True, bounding boxes will be clipped to the image borders before applying any transform.\n            Default: False.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        format: str,  # noqa: A002\n        label_fields: Sequence[Any] | None = None,\n        min_area: float = 0.0,\n        min_visibility: float = 0.0,\n        min_width: float = 0.0,\n        min_height: float = 0.0,\n        check_each_transform: bool = True,\n        clip: bool = False,\n    ):\n        super().__init__(format, label_fields)\n        self.min_area = min_area\n        self.min_visibility = min_visibility\n        self.min_width = min_width\n        self.min_height = min_height\n        self.check_each_transform = check_each_transform\n        self.clip = clip\n\n    def to_dict_private(self) -&gt; dict[str, Any]:\n        data = super().to_dict_private()\n        data.update(\n            {\n                \"min_area\": self.min_area,\n                \"min_visibility\": self.min_visibility,\n                \"min_width\": self.min_width,\n                \"min_height\": self.min_height,\n                \"check_each_transform\": self.check_each_transform,\n                \"clip\": self.clip,\n            },\n        )\n        return data\n\n    @classmethod\n    def is_serializable(cls) -&gt; bool:\n        return True\n\n    @classmethod\n    def get_class_fullname(cls) -&gt; str:\n        return \"BboxParams\"\n</code></pre>"},{"location":"api_reference/core/bbox_utils/#albumentations.core.bbox_utils.calculate_bbox_area","title":"<code>def calculate_bbox_area    (bbox, image_shape)    </code> [view source on GitHub]","text":"<p>Calculate the area of a bounding box in (fractional) pixels.</p> <p>Parameters:</p> Name Type Description <code>bbox</code> <code>BoxType</code> <p>A bounding box <code>(x_min, y_min, x_max, y_max)</code>.</p> <code>image_shape</code> <code>Sequence[int]</code> <p>Image shape <code>(height, width)</code>.</p> <p>Returns:</p> Type Description <code>float</code> <p>Area in (fractional) pixels of the (denormalized) bounding box.</p> Source code in <code>albumentations/core/bbox_utils.py</code> Python<pre><code>def calculate_bbox_area(bbox: BoxType, image_shape: Sequence[int]) -&gt; float:\n    \"\"\"Calculate the area of a bounding box in (fractional) pixels.\n\n    Args:\n        bbox: A bounding box `(x_min, y_min, x_max, y_max)`.\n        image_shape: Image shape `(height, width)`.\n\n    Return:\n        Area in (fractional) pixels of the (denormalized) bounding box.\n\n    \"\"\"\n    bbox = denormalize_bbox(bbox, image_shape)\n    x_min, y_min, x_max, y_max = bbox[:4]\n    return (x_max - x_min) * (y_max - y_min)\n</code></pre>"},{"location":"api_reference/core/bbox_utils/#albumentations.core.bbox_utils.check_bbox","title":"<code>def check_bbox    (bbox)    </code> [view source on GitHub]","text":"<p>Check if bbox boundaries are in range 0, 1 and minimums are lesser then maximums</p> Source code in <code>albumentations/core/bbox_utils.py</code> Python<pre><code>def check_bbox(bbox: BoxType) -&gt; None:\n    \"\"\"Check if bbox boundaries are in range 0, 1 and minimums are lesser then maximums\"\"\"\n    for name, value in zip([\"x_min\", \"y_min\", \"x_max\", \"y_max\"], bbox[:4]):\n        if not 0 &lt;= value &lt;= 1 and not np.isclose(value, 0) and not np.isclose(value, 1):\n            raise ValueError(f\"Expected {name} for bbox {bbox} to be in the range [0.0, 1.0], got {value}.\")\n    x_min, y_min, x_max, y_max = bbox[:4]\n    if x_max &lt;= x_min:\n        raise ValueError(f\"x_max is less than or equal to x_min for bbox {bbox}.\")\n    if y_max &lt;= y_min:\n        raise ValueError(f\"y_max is less than or equal to y_min for bbox {bbox}.\")\n</code></pre>"},{"location":"api_reference/core/bbox_utils/#albumentations.core.bbox_utils.check_bboxes","title":"<code>def check_bboxes    (bboxes)    </code> [view source on GitHub]","text":"<p>Check if bboxes boundaries are in range 0, 1 and minimums are lesser then maximums</p> Source code in <code>albumentations/core/bbox_utils.py</code> Python<pre><code>def check_bboxes(bboxes: Sequence[BoxType]) -&gt; None:\n    \"\"\"Check if bboxes boundaries are in range 0, 1 and minimums are lesser then maximums\"\"\"\n    for bbox in bboxes:\n        check_bbox(bbox)\n</code></pre>"},{"location":"api_reference/core/bbox_utils/#albumentations.core.bbox_utils.clip_bbox","title":"<code>def clip_bbox    (bbox, image_shape)    </code> [view source on GitHub]","text":"<p>Clips the bounding box coordinates to ensure they fit within the boundaries of an image.</p> <p>The function first denormalizes the bounding box coordinates from relative to absolute (pixel) values. Each coordinate is then clipped to the respective dimension of the image to ensure that the bounding box does not exceed the image's boundaries. Finally, the bounding box is normalized back to relative values.</p> <p>Parameters:</p> Name Type Description <code>bbox</code> <code>BoxInternalType</code> <p>The bounding box in normalized format (relative to image dimensions).</p> <code>image_shape</code> <code>Sequence[int]</code> <p>Image shape <code>(height, width)</code>.</p> <p>Returns:</p> Type Description <code>BoxInternalType</code> <p>The clipped bounding box, normalized to the image dimensions.</p> Source code in <code>albumentations/core/bbox_utils.py</code> Python<pre><code>def clip_bbox(bbox: BoxType, image_shape: Sequence[int]) -&gt; BoxType:\n    \"\"\"Clips the bounding box coordinates to ensure they fit within the boundaries of an image.\n\n    The function first denormalizes the bounding box coordinates from relative to absolute (pixel) values.\n    Each coordinate is then clipped to the respective dimension of the image to ensure that the bounding box\n    does not exceed the image's boundaries. Finally, the bounding box is normalized back to relative values.\n\n    Parameters:\n        bbox (BoxInternalType): The bounding box in normalized format (relative to image dimensions).\n        image_shape (Sequence[int]): Image shape `(height, width)`.\n\n    Returns:\n        BoxInternalType: The clipped bounding box, normalized to the image dimensions.\n    \"\"\"\n    x_min, y_min, x_max, y_max = denormalize_bbox(bbox, image_shape)[:4]\n\n    ## Note:\n    # It could be tempting to use cols - 1 and rows - 1 as the upper bounds for the clipping\n\n    # But this would cause the bounding box to be clipped to the image dimensions - 1 which is not what we want.\n    # Bounding box lives not in the middle of pixels but between them.\n\n    # Example: for image with height 100, width 100, the pixel values are in the range [0, 99]\n    # but if we want bounding box to be 1 pixel width and height and lie on the boundary of the image\n    # it will be described as [99, 99, 100, 100] =&gt; clip by image_size - 1 will lead to [99, 99, 99, 99]\n    # which is incorrect\n\n    # It could be also tempting to clip `x_min`` to `cols - 1`` and `y_min` to `rows - 1`, but this also leads\n    # to another error. If image fully lies outside of the visible area and min_area is set to 0, then\n    # the bounding box will be clipped to the image size - 1 and will be 1 pixel in size and fully visible,\n    # but it should be completely removed.\n\n    rows, cols = image_shape[:2]\n\n    x_min = np.clip(x_min, 0, cols)\n    x_max = np.clip(x_max, 0, cols)\n    y_min = np.clip(y_min, 0, rows)\n    y_max = np.clip(y_max, 0, rows)\n    return cast(BoxType, normalize_bbox((x_min, y_min, x_max, y_max), image_shape) + tuple(bbox[4:]))\n</code></pre>"},{"location":"api_reference/core/bbox_utils/#albumentations.core.bbox_utils.convert_bbox_from_albumentations","title":"<code>def convert_bbox_from_albumentations    (bbox, target_format, image_shape, check_validity=False)    </code> [view source on GitHub]","text":"<p>Convert a bounding box from the format used by albumentations to a format, specified in <code>target_format</code>.</p> <p>Parameters:</p> Name Type Description <code>bbox</code> <code>BoxType</code> <p>An albumentations bounding box <code>(x_min, y_min, x_max, y_max)</code>.</p> <code>target_format</code> <code>str</code> <p>required format of the output bounding box. Should be 'coco', 'pascal_voc' or 'yolo'.</p> <code>image_shape</code> <code>Sequence[int]</code> <p>Image shape <code>(height, width)</code>.</p> <code>check_validity</code> <code>bool</code> <p>Check if all boxes are valid boxes.</p> <p>Returns:</p> Type Description <code>tuple</code> <p>A bounding box.</p> <p>Note</p> <p>The <code>coco</code> format of a bounding box looks like <code>[x_min, y_min, width, height]</code>, e.g. [97, 12, 150, 200]. The <code>pascal_voc</code> format of a bounding box looks like <code>[x_min, y_min, x_max, y_max]</code>, e.g. [97, 12, 247, 212]. The <code>yolo</code> format of a bounding box looks like <code>[x, y, width, height]</code>, e.g. [0.3, 0.1, 0.05, 0.07].</p> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>if <code>target_format</code> is not equal to <code>coco</code>, <code>pascal_voc</code> or <code>yolo</code>.</p> Source code in <code>albumentations/core/bbox_utils.py</code> Python<pre><code>def convert_bbox_from_albumentations(\n    bbox: BoxType,\n    target_format: str,\n    image_shape: Sequence[int],\n    check_validity: bool = False,\n) -&gt; BoxType:\n    \"\"\"Convert a bounding box from the format used by albumentations to a format, specified in `target_format`.\n\n    Args:\n        bbox: An albumentations bounding box `(x_min, y_min, x_max, y_max)`.\n        target_format: required format of the output bounding box. Should be 'coco', 'pascal_voc' or 'yolo'.\n        image_shape: Image shape `(height, width)`.\n        check_validity: Check if all boxes are valid boxes.\n\n    Returns:\n        tuple: A bounding box.\n\n    Note:\n        The `coco` format of a bounding box looks like `[x_min, y_min, width, height]`, e.g. [97, 12, 150, 200].\n        The `pascal_voc` format of a bounding box looks like `[x_min, y_min, x_max, y_max]`, e.g. [97, 12, 247, 212].\n        The `yolo` format of a bounding box looks like `[x, y, width, height]`, e.g. [0.3, 0.1, 0.05, 0.07].\n\n    Raises:\n        ValueError: if `target_format` is not equal to `coco`, `pascal_voc` or `yolo`.\n\n    \"\"\"\n    if target_format not in {\"coco\", \"pascal_voc\", \"yolo\"}:\n        raise ValueError(\n            f\"Unknown target_format {target_format}. Supported formats are: 'coco', 'pascal_voc' and 'yolo'\",\n        )\n    if check_validity:\n        check_bbox(bbox)\n\n    if target_format != \"yolo\":\n        bbox = denormalize_bbox(bbox, image_shape)\n    if target_format == \"coco\":\n        (x_min, y_min, x_max, y_max), tail = bbox[:4], tuple(bbox[4:])\n        width = x_max - x_min\n        height = y_max - y_min\n        bbox = cast(BoxType, (x_min, y_min, width, height, *tail))\n    elif target_format == \"yolo\":\n        (x_min, y_min, x_max, y_max), tail = bbox[:4], bbox[4:]\n        x = (x_min + x_max) / 2.0\n        y = (y_min + y_max) / 2.0\n        width = x_max - x_min\n        height = y_max - y_min\n        bbox = cast(BoxType, (x, y, width, height, *tail))\n    return bbox\n</code></pre>"},{"location":"api_reference/core/bbox_utils/#albumentations.core.bbox_utils.convert_bbox_to_albumentations","title":"<code>def convert_bbox_to_albumentations    (bbox, source_format, image_shape, check_validity=False)    </code> [view source on GitHub]","text":"<p>Convert a bounding box from a format specified in <code>source_format</code> to the format used by albumentations: normalized coordinates of top-left and bottom-right corners of the bounding box in a form of <code>(x_min, y_min, x_max, y_max)</code> e.g. <code>(0.15, 0.27, 0.67, 0.5)</code>.</p> <p>Parameters:</p> Name Type Description <code>bbox</code> <code>BoxType</code> <p>A bounding box tuple.</p> <code>source_format</code> <code>str</code> <p>format of the bounding box. Should be 'coco', 'pascal_voc', or 'yolo'.</p> <code>image_shape</code> <code>Sequence[int]</code> <p>Image shape <code>(height, width)</code>.</p> <code>check_validity</code> <code>bool</code> <p>Check if all boxes are valid boxes.</p> <p>Returns:</p> Type Description <code>tuple</code> <p>A bounding box <code>(x_min, y_min, x_max, y_max)</code>.</p> <p>Note</p> <p>The <code>coco</code> format of a bounding box looks like <code>(x_min, y_min, width, height)</code>, e.g. (97, 12, 150, 200). The <code>pascal_voc</code> format of a bounding box looks like <code>(x_min, y_min, x_max, y_max)</code>, e.g. (97, 12, 247, 212). The <code>yolo</code> format of a bounding box looks like <code>(x, y, width, height)</code>, e.g. (0.3, 0.1, 0.05, 0.07); where <code>x</code>, <code>y</code> coordinates of the center of the box, all values normalized to 1 by image height and width.</p> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>if <code>target_format</code> is not equal to <code>coco</code> or <code>pascal_voc</code>, or <code>yolo</code>.</p> <code>ValueError</code> <p>If in YOLO format all labels not in range (0, 1).</p> Source code in <code>albumentations/core/bbox_utils.py</code> Python<pre><code>def convert_bbox_to_albumentations(\n    bbox: BoxType,\n    source_format: str,\n    image_shape: Sequence[int],\n    check_validity: bool = False,\n) -&gt; BoxType:\n    \"\"\"Convert a bounding box from a format specified in `source_format` to the format used by albumentations:\n    normalized coordinates of top-left and bottom-right corners of the bounding box in a form of\n    `(x_min, y_min, x_max, y_max)` e.g. `(0.15, 0.27, 0.67, 0.5)`.\n\n    Args:\n        bbox: A bounding box tuple.\n        source_format: format of the bounding box. Should be 'coco', 'pascal_voc', or 'yolo'.\n        image_shape: Image shape `(height, width)`.\n        check_validity: Check if all boxes are valid boxes.\n\n\n    Returns:\n        tuple: A bounding box `(x_min, y_min, x_max, y_max)`.\n\n    Note:\n        The `coco` format of a bounding box looks like `(x_min, y_min, width, height)`, e.g. (97, 12, 150, 200).\n        The `pascal_voc` format of a bounding box looks like `(x_min, y_min, x_max, y_max)`, e.g. (97, 12, 247, 212).\n        The `yolo` format of a bounding box looks like `(x, y, width, height)`, e.g. (0.3, 0.1, 0.05, 0.07);\n        where `x`, `y` coordinates of the center of the box, all values normalized to 1 by image height and width.\n\n    Raises:\n        ValueError: if `target_format` is not equal to `coco` or `pascal_voc`, or `yolo`.\n        ValueError: If in YOLO format all labels not in range (0, 1).\n\n    \"\"\"\n    if source_format not in {\"coco\", \"pascal_voc\", \"yolo\"}:\n        raise ValueError(\n            f\"Unknown source_format {source_format}. Supported formats are: 'coco', 'pascal_voc' and 'yolo'\",\n        )\n\n    if source_format == \"coco\":\n        (x_min, y_min, width, height), tail = bbox[:4], bbox[4:]\n        x_max = x_min + width\n        y_max = y_min + height\n    elif source_format == \"yolo\":\n        # https://github.com/pjreddie/darknet/blob/f6d861736038da22c9eb0739dca84003c5a5e275/scripts/voc_label.py#L12\n        _bbox = np.array(bbox[:4])\n        if check_validity and np.any((_bbox &lt;= 0) | (_bbox &gt; 1)):\n            msg = \"In YOLO format all coordinates must be float and in range (0, 1]\"\n            raise ValueError(msg)\n\n        (x, y, width, height), tail = bbox[:4], bbox[4:]\n\n        w_half, h_half = width / 2, height / 2\n        x_min = x - w_half\n        y_min = y - h_half\n        x_max = x_min + width\n        y_max = y_min + height\n    else:\n        (x_min, y_min, x_max, y_max), tail = bbox[:4], bbox[4:]\n\n    bbox = (x_min, y_min, x_max, y_max, *tuple(tail))\n\n    if source_format != \"yolo\":\n        bbox = normalize_bbox(bbox, image_shape)\n    if check_validity:\n        check_bbox(bbox)\n    return bbox\n</code></pre>"},{"location":"api_reference/core/bbox_utils/#albumentations.core.bbox_utils.convert_bboxes_from_albumentations","title":"<code>def convert_bboxes_from_albumentations    (bboxes, target_format, image_shape, check_validity=False)    </code> [view source on GitHub]","text":"<p>Convert a list of bounding boxes from the format used by albumentations to a format, specified in <code>target_format</code>.</p> <p>Parameters:</p> Name Type Description <code>bboxes</code> <code>Sequence[BoxType]</code> <p>list of albumentations bounding box <code>(x_min, y_min, x_max, y_max)</code>.</p> <code>target_format</code> <code>str</code> <p>required format of the output bounding box. Should be 'coco', 'pascal_voc' or 'yolo'.</p> <code>image_shape</code> <code>Sequence[int]</code> <p>Image shape <code>(height, width)</code>.</p> <code>check_validity</code> <code>bool</code> <p>Check if all boxes are valid boxes.</p> <p>Returns:</p> Type Description <code>list[BoxType]</code> <p>list of bounding boxes.</p> Source code in <code>albumentations/core/bbox_utils.py</code> Python<pre><code>def convert_bboxes_from_albumentations(\n    bboxes: Sequence[BoxType],\n    target_format: str,\n    image_shape: Sequence[int],\n    check_validity: bool = False,\n) -&gt; list[BoxType]:\n    \"\"\"Convert a list of bounding boxes from the format used by albumentations to a format, specified\n    in `target_format`.\n\n    Args:\n        bboxes: list of albumentations bounding box `(x_min, y_min, x_max, y_max)`.\n        target_format: required format of the output bounding box. Should be 'coco', 'pascal_voc' or 'yolo'.\n        image_shape: Image shape `(height, width)`.\n        check_validity: Check if all boxes are valid boxes.\n\n    Returns:\n        list of bounding boxes.\n\n    \"\"\"\n    return [convert_bbox_from_albumentations(bbox, target_format, image_shape, check_validity) for bbox in bboxes]\n</code></pre>"},{"location":"api_reference/core/bbox_utils/#albumentations.core.bbox_utils.convert_bboxes_to_albumentations","title":"<code>def convert_bboxes_to_albumentations    (bboxes, source_format, image_shape, check_validity=False)    </code> [view source on GitHub]","text":"<p>Convert a list bounding boxes from a format specified in <code>source_format</code> to the format used by albumentations</p> Source code in <code>albumentations/core/bbox_utils.py</code> Python<pre><code>def convert_bboxes_to_albumentations(\n    bboxes: Sequence[BoxType],\n    source_format: str,\n    image_shape: Sequence[int],\n    check_validity: bool = False,\n) -&gt; list[BoxType]:\n    \"\"\"Convert a list bounding boxes from a format specified in `source_format` to the format used by albumentations\"\"\"\n    return [convert_bbox_to_albumentations(bbox, source_format, image_shape, check_validity) for bbox in bboxes]\n</code></pre>"},{"location":"api_reference/core/bbox_utils/#albumentations.core.bbox_utils.denormalize_bbox","title":"<code>def denormalize_bbox    (bbox, image_shape)    </code> [view source on GitHub]","text":"<p>Denormalize bounding box coordinates from relative to absolute pixel values.</p> <p>This function converts normalized bounding box coordinates (ranging from 0 to 1) to absolute pixel coordinates based on the given image shape.</p> <p>Parameters:</p> Name Type Description <code>bbox</code> <code>BoxType</code> <p>A bounding box in normalized coordinates (x_min, y_min, x_max, y_max, ...). Additional elements after the first four are preserved and returned unchanged.</p> <code>image_shape</code> <code>Sequence[int]</code> <p>The shape of the image (height, width, ...). Only the first two elements (height and width) are used.</p> <p>Returns:</p> Type Description <code>BoxType</code> <p>A bounding box with denormalized coordinates (x_min, y_min, x_max, y_max, ...).     The coordinates are in absolute pixel values.     Any additional elements from the input bbox are appended unchanged.</p> <p>Note</p> <ul> <li>Input bbox coordinates should be in the range [0, 1].</li> <li>The function assumes the first four elements of bbox are x_min, y_min, x_max, y_max.</li> <li>Any elements in bbox after the first four are returned as-is.</li> <li>The returned bbox type is cast to BoxType to maintain type consistency.</li> </ul> <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; denormalize_bbox((0.1, 0.2, 0.3, 0.4), (100, 200))\n(20.0, 20.0, 60.0, 40.0)\n&gt;&gt;&gt; denormalize_bbox((0.1, 0.2, 0.3, 0.4, 'label'), (100, 200))\n(20.0, 20.0, 60.0, 40.0, 'label')\n</code></pre> Source code in <code>albumentations/core/bbox_utils.py</code> Python<pre><code>def denormalize_bbox(bbox: BoxType, image_shape: Sequence[int]) -&gt; BoxType:\n    \"\"\"Denormalize bounding box coordinates from relative to absolute pixel values.\n\n    This function converts normalized bounding box coordinates (ranging from 0 to 1)\n    to absolute pixel coordinates based on the given image shape.\n\n    Args:\n        bbox (BoxType): A bounding box in normalized coordinates (x_min, y_min, x_max, y_max, ...).\n            Additional elements after the first four are preserved and returned unchanged.\n        image_shape (Sequence[int]): The shape of the image (height, width, ...).\n            Only the first two elements (height and width) are used.\n\n    Returns:\n        BoxType: A bounding box with denormalized coordinates (x_min, y_min, x_max, y_max, ...).\n            The coordinates are in absolute pixel values.\n            Any additional elements from the input bbox are appended unchanged.\n\n    Note:\n        - Input bbox coordinates should be in the range [0, 1].\n        - The function assumes the first four elements of bbox are x_min, y_min, x_max, y_max.\n        - Any elements in bbox after the first four are returned as-is.\n        - The returned bbox type is cast to BoxType to maintain type consistency.\n\n    Example:\n        &gt;&gt;&gt; denormalize_bbox((0.1, 0.2, 0.3, 0.4), (100, 200))\n        (20.0, 20.0, 60.0, 40.0)\n        &gt;&gt;&gt; denormalize_bbox((0.1, 0.2, 0.3, 0.4, 'label'), (100, 200))\n        (20.0, 20.0, 60.0, 40.0, 'label')\n    \"\"\"\n    rows, cols = image_shape[:2]\n\n    tail: tuple[Any, ...]\n    (x_min, y_min, x_max, y_max), tail = bbox[:4], tuple(bbox[4:])\n\n    x_min, x_max = x_min * cols, x_max * cols\n    y_min, y_max = y_min * rows, y_max * rows\n\n    return cast(BoxType, (x_min, y_min, x_max, y_max, *tail))\n</code></pre>"},{"location":"api_reference/core/bbox_utils/#albumentations.core.bbox_utils.denormalize_bboxes","title":"<code>def denormalize_bboxes    (bboxes, image_shape)    </code> [view source on GitHub]","text":"<p>Denormalize a list or array of bounding boxes.</p> <p>Parameters:</p> Name Type Description <code>bboxes</code> <code>Sequence[BoxType] | np.ndarray</code> <p>Normalized bounding boxes <code>[(x_min, y_min, x_max, y_max, ...)]</code>.</p> <code>image_shape</code> <code>Sequence[int]</code> <p>Image shape <code>(height, width)</code>.</p> <p>Returns:</p> Type Description <code>list[BoxType] | np.ndarray</code> <p>Denormalized bounding boxes <code>[(x_min, y_min, x_max, y_max, ...)]</code>.</p> Source code in <code>albumentations/core/bbox_utils.py</code> Python<pre><code>def denormalize_bboxes(\n    bboxes: Sequence[BoxType] | np.ndarray,\n    image_shape: Sequence[int],\n) -&gt; list[BoxType] | np.ndarray:\n    \"\"\"Denormalize a list or array of bounding boxes.\n\n    Args:\n        bboxes: Normalized bounding boxes `[(x_min, y_min, x_max, y_max, ...)]`.\n        image_shape: Image shape `(height, width)`.\n\n    Returns:\n        Denormalized bounding boxes `[(x_min, y_min, x_max, y_max, ...)]`.\n\n    \"\"\"\n    rows, cols = image_shape[:2]\n    if isinstance(bboxes, np.ndarray):\n        denormalized = bboxes.astype(float)\n        denormalized[:, [0, 2]] *= cols\n        denormalized[:, [1, 3]] *= rows\n        return denormalized\n\n    return [denormalize_bbox(bbox, image_shape) for bbox in bboxes]\n</code></pre>"},{"location":"api_reference/core/bbox_utils/#albumentations.core.bbox_utils.filter_bboxes","title":"<code>def filter_bboxes    (bboxes, image_shape, min_area=0.0, min_visibility=0.0, min_width=0.0, min_height=0.0)    </code> [view source on GitHub]","text":"<p>Remove bounding boxes that either lie outside of the visible area by more then min_visibility or whose area in pixels is under the threshold set by <code>min_area</code>. Also it crops boxes to final image size.</p> <p>Parameters:</p> Name Type Description <code>bboxes</code> <code>Sequence[BoxType]</code> <p>list of albumentations bounding box <code>(x_min, y_min, x_max, y_max)</code>.</p> <code>image_shape</code> <code>Sequence[int]</code> <p>Image shape <code>(height, width)</code>.</p> <code>min_area</code> <code>float</code> <p>Minimum area of a bounding box. All bounding boxes whose visible area in pixels. is less than this value will be removed. Default: 0.0.</p> <code>min_visibility</code> <code>float</code> <p>Minimum fraction of area for a bounding box to remain this box in list. Default: 0.0.</p> <code>min_width</code> <code>float</code> <p>Minimum width of a bounding box. All bounding boxes whose width is less than this value will be removed. Default: 0.0.</p> <code>min_height</code> <code>float</code> <p>Minimum height of a bounding box. All bounding boxes whose height is less than this value will be removed. Default: 0.0.</p> <p>Returns:</p> Type Description <code>list[BoxType]</code> <p>list of bounding boxes.</p> Source code in <code>albumentations/core/bbox_utils.py</code> Python<pre><code>def filter_bboxes(\n    bboxes: Sequence[BoxType],\n    image_shape: Sequence[int],\n    min_area: float = 0.0,\n    min_visibility: float = 0.0,\n    min_width: float = 0.0,\n    min_height: float = 0.0,\n) -&gt; list[BoxType]:\n    \"\"\"Remove bounding boxes that either lie outside of the visible area by more then min_visibility\n    or whose area in pixels is under the threshold set by `min_area`. Also it crops boxes to final image size.\n\n    Args:\n        bboxes: list of albumentations bounding box `(x_min, y_min, x_max, y_max)`.\n        image_shape: Image shape `(height, width)`.\n        min_area: Minimum area of a bounding box. All bounding boxes whose visible area in pixels.\n            is less than this value will be removed. Default: 0.0.\n        min_visibility: Minimum fraction of area for a bounding box to remain this box in list. Default: 0.0.\n        min_width: Minimum width of a bounding box. All bounding boxes whose width is\n            less than this value will be removed. Default: 0.0.\n        min_height: Minimum height of a bounding box. All bounding boxes whose height is\n            less than this value will be removed. Default: 0.0.\n\n    Returns:\n        list of bounding boxes.\n\n    \"\"\"\n    resulting_boxes: list[BoxType] = []\n    for i in range(len(bboxes)):\n        bbox = bboxes[i]\n        # Calculate areas of bounding box before and after clipping.\n        transformed_box_area = calculate_bbox_area(bbox, image_shape)\n        clipped_bbox = clip_bbox(bbox, image_shape)\n\n        bbox, tail = clipped_bbox[:4], clipped_bbox[4:]\n\n        clipped_box_area = calculate_bbox_area(bbox, image_shape)\n\n        # Calculate width and height of the clipped bounding box.\n        x_min, y_min, x_max, y_max = denormalize_bbox(bbox, image_shape)[:4]\n        clipped_width, clipped_height = x_max - x_min, y_max - y_min\n\n        if (\n            clipped_box_area != 0  # to ensure transformed_box_area!=0 and to handle min_area=0 or min_visibility=0\n            and clipped_box_area &gt;= min_area\n            and clipped_box_area / transformed_box_area &gt;= min_visibility\n            and clipped_width &gt;= min_width\n            and clipped_height &gt;= min_height\n        ):\n            resulting_boxes.append(cast(BoxType, bbox + tail))\n    return resulting_boxes\n</code></pre>"},{"location":"api_reference/core/bbox_utils/#albumentations.core.bbox_utils.filter_bboxes_by_visibility","title":"<code>def filter_bboxes_by_visibility    (original_shape, bboxes, transformed_shape, transformed_bboxes, threshold=0.0, min_area=0.0)    </code> [view source on GitHub]","text":"<p>Filter bounding boxes and return only those boxes whose visibility after transformation is above the threshold and minimal area of bounding box in pixels is more then min_area.</p> <p>Parameters:</p> Name Type Description <code>original_shape</code> <code>Sequence[int]</code> <p>Original image shape <code>(height, width, ...)</code>.</p> <code>bboxes</code> <code>Sequence[BoxType]</code> <p>Original bounding boxes <code>[(x_min, y_min, x_max, y_max)]</code>.</p> <code>transformed_shape</code> <code>Sequence[int]</code> <p>Transformed image shape <code>(height, width)</code>.</p> <code>transformed_bboxes</code> <code>Sequence[BoxType]</code> <p>Transformed bounding boxes <code>[(x_min, y_min, x_max, y_max)]</code>.</p> <code>threshold</code> <code>float</code> <p>visibility threshold. Should be a value in the range [0.0, 1.0].</p> <code>min_area</code> <code>float</code> <p>Minimal area threshold.</p> <p>Returns:</p> Type Description <code>list[BoxType]</code> <p>Filtered bounding boxes <code>[(x_min, y_min, x_max, y_max)]</code>.</p> Source code in <code>albumentations/core/bbox_utils.py</code> Python<pre><code>def filter_bboxes_by_visibility(\n    original_shape: Sequence[int],\n    bboxes: Sequence[BoxType],\n    transformed_shape: Sequence[int],\n    transformed_bboxes: Sequence[BoxType],\n    threshold: float = 0.0,\n    min_area: float = 0.0,\n) -&gt; list[BoxType]:\n    \"\"\"Filter bounding boxes and return only those boxes whose visibility after transformation is above\n    the threshold and minimal area of bounding box in pixels is more then min_area.\n\n    Args:\n        original_shape: Original image shape `(height, width, ...)`.\n        bboxes: Original bounding boxes `[(x_min, y_min, x_max, y_max)]`.\n        transformed_shape: Transformed image shape `(height, width)`.\n        transformed_bboxes: Transformed bounding boxes `[(x_min, y_min, x_max, y_max)]`.\n        threshold: visibility threshold. Should be a value in the range [0.0, 1.0].\n        min_area: Minimal area threshold.\n\n    Returns:\n        Filtered bounding boxes `[(x_min, y_min, x_max, y_max)]`.\n\n    \"\"\"\n    img_height, img_width = original_shape[:2]\n    transformed_img_height, transformed_img_width = transformed_shape[:2]\n\n    visible_bboxes = []\n    for bbox, transformed_bbox in zip(bboxes, transformed_bboxes):\n        if not all(0.0 &lt;= value &lt;= 1.0 for value in transformed_bbox[:4]):\n            continue\n        bbox_area = calculate_bbox_area(bbox, (img_height, img_width))\n        transformed_bbox_area = calculate_bbox_area(transformed_bbox, (transformed_img_height, transformed_img_width))\n        if transformed_bbox_area &lt; min_area:\n            continue\n        visibility = transformed_bbox_area / bbox_area\n        if visibility &gt;= threshold:\n            visible_bboxes.append(transformed_bbox)\n    return visible_bboxes\n</code></pre>"},{"location":"api_reference/core/bbox_utils/#albumentations.core.bbox_utils.normalize_bbox","title":"<code>def normalize_bbox    (bbox, image_shape)    </code> [view source on GitHub]","text":"<p>Normalize bounding box coordinates from absolute pixel values to relative values.</p> <p>This function converts absolute pixel coordinates of a bounding box to normalized coordinates (ranging from 0 to 1) based on the given image shape.</p> <p>Parameters:</p> Name Type Description <code>bbox</code> <code>BoxType</code> <p>A bounding box in absolute pixel coordinates (x_min, y_min, x_max, y_max, ...). Additional elements after the first four are preserved and returned unchanged.</p> <code>image_shape</code> <code>Sequence[int]</code> <p>The shape of the image (height, width, ...). Only the first two elements (height and width) are used.</p> <p>Returns:</p> Type Description <code>BoxType</code> <p>A bounding box with normalized coordinates (x_min, y_min, x_max, y_max, ...).     The coordinates are relative values in the range [0, 1].     Any additional elements from the input bbox are appended unchanged.</p> <p>Note</p> <ul> <li>Input bbox coordinates should be in pixel values, not exceeding image dimensions.</li> <li>The function assumes the first four elements of bbox are x_min, y_min, x_max, y_max.</li> <li>Any elements in bbox after the first four are returned as-is.</li> <li>The returned bbox type is cast to BoxType to maintain type consistency.</li> </ul> <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; normalize_bbox((20, 30, 60, 80), (100, 200))\n(0.1, 0.3, 0.3, 0.8)\n&gt;&gt;&gt; normalize_bbox((20, 30, 60, 80, 'label'), (100, 200))\n(0.1, 0.3, 0.3, 0.8, 'label')\n</code></pre> Source code in <code>albumentations/core/bbox_utils.py</code> Python<pre><code>def normalize_bbox(bbox: BoxType, image_shape: Sequence[int]) -&gt; BoxType:\n    \"\"\"Normalize bounding box coordinates from absolute pixel values to relative values.\n\n    This function converts absolute pixel coordinates of a bounding box to normalized coordinates\n    (ranging from 0 to 1) based on the given image shape.\n\n    Args:\n        bbox (BoxType): A bounding box in absolute pixel coordinates (x_min, y_min, x_max, y_max, ...).\n            Additional elements after the first four are preserved and returned unchanged.\n        image_shape (Sequence[int]): The shape of the image (height, width, ...).\n            Only the first two elements (height and width) are used.\n\n    Returns:\n        BoxType: A bounding box with normalized coordinates (x_min, y_min, x_max, y_max, ...).\n            The coordinates are relative values in the range [0, 1].\n            Any additional elements from the input bbox are appended unchanged.\n\n    Note:\n        - Input bbox coordinates should be in pixel values, not exceeding image dimensions.\n        - The function assumes the first four elements of bbox are x_min, y_min, x_max, y_max.\n        - Any elements in bbox after the first four are returned as-is.\n        - The returned bbox type is cast to BoxType to maintain type consistency.\n\n    Example:\n        &gt;&gt;&gt; normalize_bbox((20, 30, 60, 80), (100, 200))\n        (0.1, 0.3, 0.3, 0.8)\n        &gt;&gt;&gt; normalize_bbox((20, 30, 60, 80, 'label'), (100, 200))\n        (0.1, 0.3, 0.3, 0.8, 'label')\n    \"\"\"\n    rows, cols = image_shape[:2]\n\n    tail: tuple[Any, ...]\n    (x_min, y_min, x_max, y_max), tail = bbox[:4], tuple(bbox[4:])\n    x_min /= cols\n    x_max /= cols\n    y_min /= rows\n    y_max /= rows\n\n    return cast(BoxType, (x_min, y_min, x_max, y_max, *tail))\n</code></pre>"},{"location":"api_reference/core/bbox_utils/#albumentations.core.bbox_utils.normalize_bboxes","title":"<code>def normalize_bboxes    (bboxes, image_shape)    </code> [view source on GitHub]","text":"<p>Normalize a list or array of bounding boxes.</p> <p>Parameters:</p> Name Type Description <code>bboxes</code> <code>Sequence[BoxType] | np.ndarray</code> <p>Denormalized bounding boxes <code>[(x_min, y_min, x_max, y_max, ...)]</code>.</p> <code>image_shape</code> <code>Sequence[int]</code> <p>Image shape <code>(height, width)</code>.</p> <p>Returns:</p> Type Description <code>list[BoxType] | np.ndarray</code> <p>Normalized bounding boxes <code>[(x_min, y_min, x_max, y_max, ...)]</code>.</p> Source code in <code>albumentations/core/bbox_utils.py</code> Python<pre><code>def normalize_bboxes(bboxes: Sequence[BoxType] | np.ndarray, image_shape: Sequence[int]) -&gt; list[BoxType] | np.ndarray:\n    \"\"\"Normalize a list or array of bounding boxes.\n\n    Args:\n        bboxes: Denormalized bounding boxes `[(x_min, y_min, x_max, y_max, ...)]`.\n        image_shape: Image shape `(height, width)`.\n\n    Returns:\n        Normalized bounding boxes `[(x_min, y_min, x_max, y_max, ...)]`.\n\n    \"\"\"\n    rows, cols = image_shape[:2]\n    if isinstance(bboxes, np.ndarray):\n        normalized = bboxes.astype(float)\n        normalized[:, [0, 2]] /= cols\n        normalized[:, [1, 3]] /= rows\n        return normalized\n\n    return [normalize_bbox(bbox, image_shape) for bbox in bboxes]\n</code></pre>"},{"location":"api_reference/core/bbox_utils/#albumentations.core.bbox_utils.union_of_bboxes","title":"<code>def union_of_bboxes    (bboxes, erosion_rate)    </code> [view source on GitHub]","text":"<p>Calculate union of bounding boxes. Boxes could be in albumentations or Pascal Voc format.</p> <p>Parameters:</p> Name Type Description <code>bboxes</code> <code>list[tuple]</code> <p>List of bounding boxes</p> <code>erosion_rate</code> <code>float</code> <p>How much each bounding box can be shrunk, useful for erosive cropping. Set this in range [0, 1]. 0 will not be erosive at all, 1.0 can make any bbox lose its volume.</p> <p>Returns:</p> Type Description <code>Optional[tuple]</code> <p>A bounding box <code>(x_min, y_min, x_max, y_max)</code> or None if no bboxes are given or if                  the bounding boxes become invalid after erosion.</p> Source code in <code>albumentations/core/bbox_utils.py</code> Python<pre><code>def union_of_bboxes(bboxes: Sequence[BoxType], erosion_rate: float) -&gt; BoxInternalType | None:\n    \"\"\"Calculate union of bounding boxes. Boxes could be in albumentations or Pascal Voc format.\n\n    Args:\n        bboxes (list[tuple]): List of bounding boxes\n        erosion_rate (float): How much each bounding box can be shrunk, useful for erosive cropping.\n            Set this in range [0, 1]. 0 will not be erosive at all, 1.0 can make any bbox lose its volume.\n\n    Returns:\n        Optional[tuple]: A bounding box `(x_min, y_min, x_max, y_max)` or None if no bboxes are given or if\n                         the bounding boxes become invalid after erosion.\n    \"\"\"\n    if not bboxes:\n        return None\n\n    if len(bboxes) == 1:\n        if erosion_rate == 1:\n            return None\n        if erosion_rate == 0:\n            return bboxes[0][:4]\n\n    bboxes_np = np.array([bbox[:4] for bbox in bboxes])\n    x_min = bboxes_np[:, 0]\n    y_min = bboxes_np[:, 1]\n    x_max = bboxes_np[:, 2]\n    y_max = bboxes_np[:, 3]\n\n    bbox_width = x_max - x_min\n    bbox_height = y_max - y_min\n\n    # Adjust erosion rate to shrink bounding boxes accordingly\n    lim_x1 = x_min + erosion_rate * 0.5 * bbox_width\n    lim_y1 = y_min + erosion_rate * 0.5 * bbox_height\n    lim_x2 = x_max - erosion_rate * 0.5 * bbox_width\n    lim_y2 = y_max - erosion_rate * 0.5 * bbox_height\n\n    x1 = np.min(lim_x1)\n    y1 = np.min(lim_y1)\n    x2 = np.max(lim_x2)\n    y2 = np.max(lim_y2)\n\n    if x1 == x2 or y1 == y2:\n        return None\n\n    return x1, y1, x2, y2\n</code></pre>"},{"location":"api_reference/core/composition/","title":"Composition API (core.composition)","text":""},{"location":"api_reference/core/composition/#albumentations.core.composition.Compose","title":"<code>class  Compose</code> <code>     (transforms, bbox_params=None, keypoint_params=None, additional_targets=None, p=1.0, is_check_shapes=True, strict=True, return_params=False, save_key='applied_params')                 </code>  [view source on GitHub]","text":"<p>Compose transforms and handle all transformations regarding bounding boxes</p> <p>Parameters:</p> Name Type Description <code>transforms</code> <code>list</code> <p>list of transformations to compose.</p> <code>bbox_params</code> <code>BboxParams</code> <p>Parameters for bounding boxes transforms</p> <code>keypoint_params</code> <code>KeypointParams</code> <p>Parameters for keypoints transforms</p> <code>additional_targets</code> <code>dict</code> <p>Dict with keys - new target name, values - old target name. ex: {'image2': 'image'}</p> <code>p</code> <code>float</code> <p>probability of applying all list of transforms. Default: 1.0.</p> <code>is_check_shapes</code> <code>bool</code> <p>If True shapes consistency of images/mask/masks would be checked on each call. If you would like to disable this check - pass False (do it only if you are sure in your data consistency).</p> <code>strict</code> <code>bool</code> <p>If True, unknown keys will raise an error. If False, unknown keys will be ignored. Default: True.</p> <code>return_params</code> <code>bool</code> <p>if True returns params of each applied transform</p> <code>save_key</code> <code>str</code> <p>key to save applied params, default is 'applied_params'</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/core/composition.py</code> Python<pre><code>class Compose(BaseCompose, HubMixin):\n    \"\"\"Compose transforms and handle all transformations regarding bounding boxes\n\n    Args:\n        transforms (list): list of transformations to compose.\n        bbox_params (BboxParams): Parameters for bounding boxes transforms\n        keypoint_params (KeypointParams): Parameters for keypoints transforms\n        additional_targets (dict): Dict with keys - new target name, values - old target name. ex: {'image2': 'image'}\n        p (float): probability of applying all list of transforms. Default: 1.0.\n        is_check_shapes (bool): If True shapes consistency of images/mask/masks would be checked on each call. If you\n            would like to disable this check - pass False (do it only if you are sure in your data consistency).\n        strict (bool): If True, unknown keys will raise an error. If False, unknown keys will be ignored. Default: True.\n        return_params (bool): if True returns params of each applied transform\n        save_key (str): key to save applied params, default is 'applied_params'\n\n    \"\"\"\n\n    def __init__(\n        self,\n        transforms: TransformsSeqType,\n        bbox_params: dict[str, Any] | BboxParams | None = None,\n        keypoint_params: dict[str, Any] | KeypointParams | None = None,\n        additional_targets: dict[str, str] | None = None,\n        p: float = 1.0,\n        is_check_shapes: bool = True,\n        strict: bool = True,\n        return_params: bool = False,\n        save_key: str = \"applied_params\",\n    ):\n        super().__init__(transforms, p)\n\n        if bbox_params:\n            if isinstance(bbox_params, dict):\n                b_params = BboxParams(**bbox_params)\n            elif isinstance(bbox_params, BboxParams):\n                b_params = bbox_params\n            else:\n                msg = \"unknown format of bbox_params, please use `dict` or `BboxParams`\"\n                raise ValueError(msg)\n            self.processors[\"bboxes\"] = BboxProcessor(b_params)\n\n        if keypoint_params:\n            if isinstance(keypoint_params, dict):\n                k_params = KeypointParams(**keypoint_params)\n            elif isinstance(keypoint_params, KeypointParams):\n                k_params = keypoint_params\n            else:\n                msg = \"unknown format of keypoint_params, please use `dict` or `KeypointParams`\"\n                raise ValueError(msg)\n            self.processors[\"keypoints\"] = KeypointsProcessor(k_params)\n\n        for proc in self.processors.values():\n            proc.ensure_transforms_valid(self.transforms)\n\n        self.add_targets(additional_targets)\n        if not self.transforms:  # if no transforms -&gt; do nothing, all keys will be available\n            self._available_keys.update(AVAILABLE_KEYS)\n\n        self.is_check_args = True\n        self.strict = strict\n\n        self.is_check_shapes = is_check_shapes\n        self.check_each_transform = tuple(  # processors that checks after each transform\n            proc for proc in self.processors.values() if getattr(proc.params, \"check_each_transform\", False)\n        )\n        self._set_check_args_for_transforms(self.transforms)\n\n        self.return_params = return_params\n        if return_params:\n            self.save_key = save_key\n            self._available_keys.add(save_key)\n            self._transforms_dict = get_transforms_dict(self.transforms)\n            self.set_deterministic(True, save_key=save_key)\n\n    def _set_check_args_for_transforms(self, transforms: TransformsSeqType) -&gt; None:\n        for transform in transforms:\n            if isinstance(transform, BaseCompose):\n                self._set_check_args_for_transforms(transform.transforms)\n                transform.check_each_transform = self.check_each_transform\n                transform.processors = self.processors\n            if isinstance(transform, Compose):\n                transform.disable_check_args_private()\n\n    def disable_check_args_private(self) -&gt; None:\n        self.is_check_args = False\n        self.strict = False\n        self.main_compose = False\n\n    def __call__(self, *args: Any, force_apply: bool = False, **data: Any) -&gt; dict[str, Any]:\n        if args:\n            msg = \"You have to pass data to augmentations as named arguments, for example: aug(image=image)\"\n            raise KeyError(msg)\n\n        if not isinstance(force_apply, (bool, int)):\n            msg = \"force_apply must have bool or int type\"\n            raise TypeError(msg)\n\n        if self.return_params and self.main_compose:\n            data[self.save_key] = OrderedDict()\n\n        need_to_run = force_apply or random.random() &lt; self.p\n        if not need_to_run:\n            return data\n\n        self.preprocess(data)\n\n        for t in self.transforms:\n            data = t(**data)\n            data = self.check_data_post_transform(data)\n\n        return self.postprocess(data)\n\n    def run_with_params(self, *, params: dict[int, dict[str, Any]], **data: Any) -&gt; dict[str, Any]:\n        \"\"\"Run transforms with given parameters. Available only for Compose with `return_params=True`.\"\"\"\n        if self._transforms_dict is None:\n            raise RuntimeError(\"`run_with_params` is not available for Compose with `return_params=False`.\")\n\n        self.preprocess(data)\n\n        for tr_id, param in params.items():\n            tr = self._transforms_dict[tr_id]\n            data = tr.apply_with_params(param, **data)\n            data = self.check_data_post_transform(data)\n\n        return self.postprocess(data)\n\n    def preprocess(self, data: Any) -&gt; None:\n        if self.strict:\n            for data_name in data:\n                if data_name not in self._available_keys and data_name not in MASK_KEYS and data_name not in IMAGE_KEYS:\n                    msg = f\"Key {data_name} is not in available keys.\"\n                    raise ValueError(msg)\n        if self.is_check_args:\n            self._check_args(**data)\n        if self.main_compose:\n            for p in self.processors.values():\n                p.ensure_data_valid(data)\n            for p in self.processors.values():\n                p.preprocess(data)\n\n    def postprocess(self, data: dict[str, Any]) -&gt; dict[str, Any]:\n        if self.main_compose:\n            for p in self.processors.values():\n                p.postprocess(data)\n        return data\n\n    def to_dict_private(self) -&gt; dict[str, Any]:\n        dictionary = super().to_dict_private()\n        bbox_processor = self.processors.get(\"bboxes\")\n        keypoints_processor = self.processors.get(\"keypoints\")\n        dictionary.update(\n            {\n                \"bbox_params\": bbox_processor.params.to_dict_private() if bbox_processor else None,\n                \"keypoint_params\": (keypoints_processor.params.to_dict_private() if keypoints_processor else None),\n                \"additional_targets\": self.additional_targets,\n                \"is_check_shapes\": self.is_check_shapes,\n            },\n        )\n        return dictionary\n\n    def get_dict_with_id(self) -&gt; dict[str, Any]:\n        dictionary = super().get_dict_with_id()\n        bbox_processor = self.processors.get(\"bboxes\")\n        keypoints_processor = self.processors.get(\"keypoints\")\n        dictionary.update(\n            {\n                \"bbox_params\": bbox_processor.params.to_dict_private() if bbox_processor else None,\n                \"keypoint_params\": (keypoints_processor.params.to_dict_private() if keypoints_processor else None),\n                \"additional_targets\": self.additional_targets,\n                \"params\": None,\n                \"is_check_shapes\": self.is_check_shapes,\n            },\n        )\n        return dictionary\n\n    def _check_args(self, **kwargs: Any) -&gt; None:\n        shapes = []\n\n        for data_name, data in kwargs.items():\n            internal_data_name = self._additional_targets.get(data_name, data_name)\n            if internal_data_name in CHECKED_SINGLE:\n                if not isinstance(data, np.ndarray):\n                    raise TypeError(f\"{data_name} must be numpy array type\")\n                shapes.append(data.shape[:2])\n            if internal_data_name in CHECKED_MULTI and data is not None and len(data):\n                if not isinstance(data, Sequence) or not isinstance(data[0], np.ndarray):\n                    raise TypeError(f\"{data_name} must be list of numpy arrays\")\n                shapes.append(data[0].shape[:2])\n            if internal_data_name in CHECK_BBOX_PARAM and self.processors.get(\"bboxes\") is None:\n                msg = \"bbox_params must be specified for bbox transformations\"\n                raise ValueError(msg)\n\n            if internal_data_name in CHECK_KEYPOINTS_PARAM and self.processors.get(\"keypoints\") is None:\n                msg = \"keypoints_params must be specified for keypoint transformations\"\n                raise ValueError(msg)\n\n        if self.is_check_shapes and shapes and shapes.count(shapes[0]) != len(shapes):\n            msg = (\n                \"Height and Width of image, mask or masks should be equal. You can disable shapes check \"\n                \"by setting a parameter is_check_shapes=False of Compose class (do it only if you are sure \"\n                \"about your data consistency).\"\n            )\n            raise ValueError(msg)\n</code></pre>"},{"location":"api_reference/core/composition/#albumentations.core.composition.Compose.run_with_params","title":"<code>run_with_params (self, *, params, **data)</code>","text":"<p>Run transforms with given parameters. Available only for Compose with <code>return_params=True</code>.</p> Source code in <code>albumentations/core/composition.py</code> Python<pre><code>def run_with_params(self, *, params: dict[int, dict[str, Any]], **data: Any) -&gt; dict[str, Any]:\n    \"\"\"Run transforms with given parameters. Available only for Compose with `return_params=True`.\"\"\"\n    if self._transforms_dict is None:\n        raise RuntimeError(\"`run_with_params` is not available for Compose with `return_params=False`.\")\n\n    self.preprocess(data)\n\n    for tr_id, param in params.items():\n        tr = self._transforms_dict[tr_id]\n        data = tr.apply_with_params(param, **data)\n        data = self.check_data_post_transform(data)\n\n    return self.postprocess(data)\n</code></pre>"},{"location":"api_reference/core/composition/#albumentations.core.composition.OneOf","title":"<code>class  OneOf</code> <code>     (transforms, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Select one of transforms to apply. Selected transform will be called with <code>force_apply=True</code>. Transforms probabilities will be normalized to one 1, so in this case transforms probabilities works as weights.</p> <p>Parameters:</p> Name Type Description <code>transforms</code> <code>list</code> <p>list of transformations to compose.</p> <code>p</code> <code>float</code> <p>probability of applying selected transform. Default: 0.5.</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/core/composition.py</code> Python<pre><code>class OneOf(BaseCompose):\n    \"\"\"Select one of transforms to apply. Selected transform will be called with `force_apply=True`.\n    Transforms probabilities will be normalized to one 1, so in this case transforms probabilities works as weights.\n\n    Args:\n        transforms (list): list of transformations to compose.\n        p (float): probability of applying selected transform. Default: 0.5.\n\n    \"\"\"\n\n    def __init__(self, transforms: TransformsSeqType, p: float = 0.5):\n        super().__init__(transforms, p)\n        transforms_ps = [t.p for t in self.transforms]\n        s = sum(transforms_ps)\n        self.transforms_ps = [t / s for t in transforms_ps]\n\n    def __call__(self, *args: Any, force_apply: bool = False, **data: Any) -&gt; dict[str, Any]:\n        if self.replay_mode:\n            for t in self.transforms:\n                data = t(**data)\n            return data\n\n        if self.transforms_ps and (force_apply or random.random() &lt; self.p):\n            idx: int = random_utils.choice(len(self.transforms), p=self.transforms_ps)\n            t = self.transforms[idx]\n            data = t(force_apply=True, **data)\n        return data\n</code></pre>"},{"location":"api_reference/core/composition/#albumentations.core.composition.OneOrOther","title":"<code>class  OneOrOther</code> <code>     (first=None, second=None, transforms=None, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Select one or another transform to apply. Selected transform will be called with <code>force_apply=True</code>.</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/core/composition.py</code> Python<pre><code>class OneOrOther(BaseCompose):\n    \"\"\"Select one or another transform to apply. Selected transform will be called with `force_apply=True`.\"\"\"\n\n    def __init__(\n        self,\n        first: TransformType | None = None,\n        second: TransformType | None = None,\n        transforms: TransformsSeqType | None = None,\n        p: float = 0.5,\n    ):\n        if transforms is None:\n            if first is None or second is None:\n                msg = \"You must set both first and second or set transforms argument.\"\n                raise ValueError(msg)\n            transforms = [first, second]\n        super().__init__(transforms, p)\n        if len(self.transforms) != NUM_ONEOF_TRANSFORMS:\n            warnings.warn(\"Length of transforms is not equal to 2.\", stacklevel=2)\n\n    def __call__(self, *args: Any, force_apply: bool = False, **data: Any) -&gt; dict[str, Any]:\n        if self.replay_mode:\n            for t in self.transforms:\n                data = t(**data)\n            return data\n\n        if random.random() &lt; self.p:\n            return self.transforms[0](force_apply=True, **data)\n\n        return self.transforms[-1](force_apply=True, **data)\n</code></pre>"},{"location":"api_reference/core/composition/#albumentations.core.composition.SelectiveChannelTransform","title":"<code>class  SelectiveChannelTransform</code> <code>     (transforms, channels=(0, 1, 2), p=1.0)                 </code>  [view source on GitHub]","text":"<p>A transformation class to apply specified transforms to selected channels of an image.</p> <p>This class extends BaseCompose to allow selective application of transformations to specified image channels. It extracts the selected channels, applies the transformations, and then reinserts the transformed channels back into their original positions in the image.</p> <p>Parameters:</p> Name Type Description <code>transforms</code> <code>TransformsSeqType</code> <p>A sequence of transformations (from Albumentations) to be applied to the specified channels.</p> <code>channels</code> <code>Sequence[int]</code> <p>A sequence of integers specifying the indices of the channels to which the transforms should be applied.</p> <code>p</code> <code>float</code> <p>Probability that the transform will be applied; the default is 1.0 (always apply).</p> <p>Methods</p> <p>call(args, *kwargs):     Applies the transforms to the image according to the specified channels.     The input data should include 'image' key with the image array.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>The transformed data dictionary, which includes the transformed 'image' key.</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/core/composition.py</code> Python<pre><code>class SelectiveChannelTransform(BaseCompose):\n    \"\"\"A transformation class to apply specified transforms to selected channels of an image.\n\n    This class extends BaseCompose to allow selective application of transformations to\n    specified image channels. It extracts the selected channels, applies the transformations,\n    and then reinserts the transformed channels back into their original positions in the image.\n\n    Parameters:\n        transforms (TransformsSeqType):\n            A sequence of transformations (from Albumentations) to be applied to the specified channels.\n        channels (Sequence[int]):\n            A sequence of integers specifying the indices of the channels to which the transforms should be applied.\n        p (float):\n            Probability that the transform will be applied; the default is 1.0 (always apply).\n\n    Methods:\n        __call__(*args, **kwargs):\n            Applies the transforms to the image according to the specified channels.\n            The input data should include 'image' key with the image array.\n\n    Returns:\n        dict[str, Any]: The transformed data dictionary, which includes the transformed 'image' key.\n    \"\"\"\n\n    def __init__(\n        self,\n        transforms: TransformsSeqType,\n        channels: Sequence[int] = (0, 1, 2),\n        p: float = 1.0,\n    ) -&gt; None:\n        super().__init__(transforms, p)\n        self.channels = channels\n\n    def __call__(self, *args: Any, force_apply: bool = False, **data: Any) -&gt; dict[str, Any]:\n        if force_apply or random.random() &lt; self.p:\n            image = data[\"image\"]\n\n            selected_channels = image[:, :, self.channels]\n            sub_image = np.ascontiguousarray(selected_channels)\n\n            for t in self.transforms:\n                sub_image = t(image=sub_image)[\"image\"]\n\n            transformed_channels = cv2.split(sub_image)\n            output_img = image.copy()\n\n            for idx, channel in zip(self.channels, transformed_channels):\n                output_img[:, :, idx] = channel\n\n            data[\"image\"] = np.ascontiguousarray(output_img)\n\n        return data\n</code></pre>"},{"location":"api_reference/core/composition/#albumentations.core.composition.Sequential","title":"<code>class  Sequential</code> <code>     (transforms, p=0.5)                 </code>  [view source on GitHub]","text":"<p>Sequentially applies all transforms to targets.</p> <p>Note</p> <p>This transform is not intended to be a replacement for <code>Compose</code>. Instead, it should be used inside <code>Compose</code> the same way <code>OneOf</code> or <code>OneOrOther</code> are used. For instance, you can combine <code>OneOf</code> with <code>Sequential</code> to create an augmentation pipeline that contains multiple sequences of augmentations and applies one randomly chose sequence to input data (see the <code>Example</code> section for an example definition of such pipeline).</p> <p>Examples:</p> Python<pre><code>&gt;&gt;&gt; import albumentations as A\n&gt;&gt;&gt; transform = A.Compose([\n&gt;&gt;&gt;    A.OneOf([\n&gt;&gt;&gt;        A.Sequential([\n&gt;&gt;&gt;            A.HorizontalFlip(p=0.5),\n&gt;&gt;&gt;            A.ShiftScaleRotate(p=0.5),\n&gt;&gt;&gt;        ]),\n&gt;&gt;&gt;        A.Sequential([\n&gt;&gt;&gt;            A.VerticalFlip(p=0.5),\n&gt;&gt;&gt;            A.RandomBrightnessContrast(p=0.5),\n&gt;&gt;&gt;        ]),\n&gt;&gt;&gt;    ], p=1)\n&gt;&gt;&gt; ])\n</code></pre> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/core/composition.py</code> Python<pre><code>class Sequential(BaseCompose):\n    \"\"\"Sequentially applies all transforms to targets.\n\n    Note:\n        This transform is not intended to be a replacement for `Compose`. Instead, it should be used inside `Compose`\n        the same way `OneOf` or `OneOrOther` are used. For instance, you can combine `OneOf` with `Sequential` to\n        create an augmentation pipeline that contains multiple sequences of augmentations and applies one randomly\n        chose sequence to input data (see the `Example` section for an example definition of such pipeline).\n\n    Example:\n        &gt;&gt;&gt; import albumentations as A\n        &gt;&gt;&gt; transform = A.Compose([\n        &gt;&gt;&gt;    A.OneOf([\n        &gt;&gt;&gt;        A.Sequential([\n        &gt;&gt;&gt;            A.HorizontalFlip(p=0.5),\n        &gt;&gt;&gt;            A.ShiftScaleRotate(p=0.5),\n        &gt;&gt;&gt;        ]),\n        &gt;&gt;&gt;        A.Sequential([\n        &gt;&gt;&gt;            A.VerticalFlip(p=0.5),\n        &gt;&gt;&gt;            A.RandomBrightnessContrast(p=0.5),\n        &gt;&gt;&gt;        ]),\n        &gt;&gt;&gt;    ], p=1)\n        &gt;&gt;&gt; ])\n\n    \"\"\"\n\n    def __init__(self, transforms: TransformsSeqType, p: float = 0.5):\n        super().__init__(transforms, p)\n\n    def __call__(self, *args: Any, force_apply: bool = False, **data: Any) -&gt; dict[str, Any]:\n        if self.replay_mode or force_apply or random.random() &lt; self.p:\n            for t in self.transforms:\n                data = t(**data)\n                data = self.check_data_post_transform(data)\n        return data\n</code></pre>"},{"location":"api_reference/core/composition/#albumentations.core.composition.SomeOf","title":"<code>class  SomeOf</code> <code>     (transforms, n, replace=True, p=1)                 </code>  [view source on GitHub]","text":"<p>Select N transforms to apply. Selected transforms will be called with <code>force_apply=True</code>. Transforms probabilities will be normalized to one 1, so in this case transforms probabilities works as weights.</p> <p>Parameters:</p> Name Type Description <code>transforms</code> <code>list</code> <p>list of transformations to compose.</p> <code>n</code> <code>int</code> <p>number of transforms to apply.</p> <code>replace</code> <code>bool</code> <p>Whether the sampled transforms are with or without replacement. Default: True.</p> <code>p</code> <code>float</code> <p>probability of applying selected transform. Default: 1.</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/core/composition.py</code> Python<pre><code>class SomeOf(BaseCompose):\n    \"\"\"Select N transforms to apply. Selected transforms will be called with `force_apply=True`.\n    Transforms probabilities will be normalized to one 1, so in this case transforms probabilities works as weights.\n\n    Args:\n        transforms (list): list of transformations to compose.\n        n (int): number of transforms to apply.\n        replace (bool): Whether the sampled transforms are with or without replacement. Default: True.\n        p (float): probability of applying selected transform. Default: 1.\n\n    \"\"\"\n\n    def __init__(self, transforms: TransformsSeqType, n: int, replace: bool = True, p: float = 1):\n        super().__init__(transforms, p)\n        self.n = n\n        self.replace = replace\n        transforms_ps = [t.p for t in self.transforms]\n        s = sum(transforms_ps)\n        self.transforms_ps = [t / s for t in transforms_ps]\n\n    def __call__(self, *arg: Any, force_apply: bool = False, **data: Any) -&gt; dict[str, Any]:\n        if self.replay_mode:\n            for t in self.transforms:\n                data = t(**data)\n                data = self.check_data_post_transform(data)\n            return data\n\n        if self.transforms_ps and (force_apply or random.random() &lt; self.p):\n            idx = random_utils.choice(len(self.transforms), size=self.n, replace=self.replace, p=self.transforms_ps)\n            for i in idx:\n                t = self.transforms[i]\n                data = t(force_apply=True, **data)\n                data = self.check_data_post_transform(data)\n        return data\n\n    def to_dict_private(self) -&gt; dict[str, Any]:\n        dictionary = super().to_dict_private()\n        dictionary.update({\"n\": self.n, \"replace\": self.replace})\n        return dictionary\n</code></pre>"},{"location":"api_reference/core/keypoints_utils/","title":"Helper functions for working with keypoints (augmentations.core.keypoints_utils)","text":""},{"location":"api_reference/core/keypoints_utils/#albumentations.core.keypoints_utils.KeypointParams","title":"<code>class  KeypointParams</code> <code>     (format, label_fields=None, remove_invisible=True, angle_in_degrees=True, check_each_transform=True)                 </code>  [view source on GitHub]","text":"<p>Parameters of keypoints</p> <p>Parameters:</p> Name Type Description <code>format</code> <code>str</code> <p>format of keypoints. Should be 'xy', 'yx', 'xya', 'xys', 'xyas', 'xysa'.</p> <p>x - X coordinate,</p> <p>y - Y coordinate</p> <p>s - Keypoint scale</p> <p>a - Keypoint orientation in radians or degrees (depending on KeypointParams.angle_in_degrees)</p> <code>label_fields</code> <code>list</code> <p>list of fields that are joined with keypoints, e.g labels. Should be same type as keypoints.</p> <code>remove_invisible</code> <code>bool</code> <p>to remove invisible points after transform or not</p> <code>angle_in_degrees</code> <code>bool</code> <p>angle in degrees or radians in 'xya', 'xyas', 'xysa' keypoints</p> <code>check_each_transform</code> <code>bool</code> <p>if <code>True</code>, then keypoints will be checked after each dual transform. Default: <code>True</code></p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/core/keypoints_utils.py</code> Python<pre><code>class KeypointParams(Params):\n    \"\"\"Parameters of keypoints\n\n    Args:\n        format (str): format of keypoints. Should be 'xy', 'yx', 'xya', 'xys', 'xyas', 'xysa'.\n\n            x - X coordinate,\n\n            y - Y coordinate\n\n            s - Keypoint scale\n\n            a - Keypoint orientation in radians or degrees (depending on KeypointParams.angle_in_degrees)\n        label_fields (list): list of fields that are joined with keypoints, e.g labels.\n            Should be same type as keypoints.\n        remove_invisible (bool): to remove invisible points after transform or not\n        angle_in_degrees (bool): angle in degrees or radians in 'xya', 'xyas', 'xysa' keypoints\n        check_each_transform (bool): if `True`, then keypoints will be checked after each dual transform.\n            Default: `True`\n\n    \"\"\"\n\n    def __init__(\n        self,\n        format: str,  # noqa: A002\n        label_fields: Sequence[str] | None = None,\n        remove_invisible: bool = True,\n        angle_in_degrees: bool = True,\n        check_each_transform: bool = True,\n    ):\n        super().__init__(format, label_fields)\n        self.remove_invisible = remove_invisible\n        self.angle_in_degrees = angle_in_degrees\n        self.check_each_transform = check_each_transform\n\n    def to_dict_private(self) -&gt; dict[str, Any]:\n        data = super().to_dict_private()\n        data.update(\n            {\n                \"remove_invisible\": self.remove_invisible,\n                \"angle_in_degrees\": self.angle_in_degrees,\n                \"check_each_transform\": self.check_each_transform,\n            },\n        )\n        return data\n\n    @classmethod\n    def is_serializable(cls) -&gt; bool:\n        return True\n\n    @classmethod\n    def get_class_fullname(cls) -&gt; str:\n        return \"KeypointParams\"\n</code></pre>"},{"location":"api_reference/core/keypoints_utils/#albumentations.core.keypoints_utils.check_keypoint","title":"<code>def check_keypoint    (kp, image_shape)    </code> [view source on GitHub]","text":"<p>Check if keypoint coordinates are less than image shapes</p> Source code in <code>albumentations/core/keypoints_utils.py</code> Python<pre><code>def check_keypoint(kp: KeypointType, image_shape: Sequence[int]) -&gt; None:\n    \"\"\"Check if keypoint coordinates are less than image shapes\"\"\"\n    rows, cols = image_shape[:2]\n    for name, value, size in zip([\"x\", \"y\"], kp[:2], [cols, rows]):\n        if not 0 &lt;= value &lt; size:\n            raise ValueError(f\"Expected {name} for keypoint {kp} to be in the range [0.0, {size}], got {value}.\")\n\n    angle = kp[2]\n    if not (0 &lt;= angle &lt; 2 * math.pi):\n        raise ValueError(f\"Keypoint angle must be in range [0, 2 * PI). Got: {angle}\")\n</code></pre>"},{"location":"api_reference/core/keypoints_utils/#albumentations.core.keypoints_utils.check_keypoints","title":"<code>def check_keypoints    (keypoints, image_shape)    </code> [view source on GitHub]","text":"<p>Check if keypoints boundaries are less than image shapes</p> Source code in <code>albumentations/core/keypoints_utils.py</code> Python<pre><code>def check_keypoints(keypoints: Sequence[KeypointType], image_shape: Sequence[int]) -&gt; None:\n    \"\"\"Check if keypoints boundaries are less than image shapes\"\"\"\n    for kp in keypoints:\n        check_keypoint(kp, image_shape)\n</code></pre>"},{"location":"api_reference/core/serialization/","title":"Serialization API (core.serialization)","text":""},{"location":"api_reference/core/serialization/#albumentations.core.serialization.Serializable","title":"<code>class  Serializable</code> <code> </code>  [view source on GitHub]","text":"<p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/core/serialization.py</code> Python<pre><code>class Serializable(metaclass=SerializableMeta):\n    @classmethod\n    @abstractmethod\n    def is_serializable(cls) -&gt; bool:\n        raise NotImplementedError\n\n    @classmethod\n    @abstractmethod\n    def get_class_fullname(cls) -&gt; str:\n        raise NotImplementedError\n\n    @abstractmethod\n    def to_dict_private(self) -&gt; dict[str, Any]:\n        raise NotImplementedError\n\n    def to_dict(self, on_not_implemented_error: str = \"raise\") -&gt; dict[str, Any]:\n        \"\"\"Take a transform pipeline and convert it to a serializable representation that uses only standard\n        python data types: dictionaries, lists, strings, integers, and floats.\n\n        Args:\n            self: A transform that should be serialized. If the transform doesn't implement the `to_dict`\n                method and `on_not_implemented_error` equals to 'raise' then `NotImplementedError` is raised.\n                If `on_not_implemented_error` equals to 'warn' then `NotImplementedError` will be ignored\n                but no transform parameters will be serialized.\n            on_not_implemented_error (str): `raise` or `warn`.\n\n        \"\"\"\n        if on_not_implemented_error not in {\"raise\", \"warn\"}:\n            msg = f\"Unknown on_not_implemented_error value: {on_not_implemented_error}. Supported values are: 'raise' \"\n            \"and 'warn'\"\n            raise ValueError(msg)\n        try:\n            transform_dict = self.to_dict_private()\n        except NotImplementedError:\n            if on_not_implemented_error == \"raise\":\n                raise\n\n            transform_dict = {}\n            warnings.warn(\n                f\"Got NotImplementedError while trying to serialize {self}. Object arguments are not preserved. \"\n                f\"Implement either '{self.__class__.__name__}.get_transform_init_args_names' \"\n                f\"or '{self.__class__.__name__}.get_transform_init_args' \"\n                \"method to make the transform serializable\",\n                stacklevel=2,\n            )\n        return {\"__version__\": __version__, \"transform\": transform_dict}\n</code></pre>"},{"location":"api_reference/core/serialization/#albumentations.core.serialization.Serializable.to_dict","title":"<code>to_dict (self, on_not_implemented_error='raise')</code>","text":"<p>Take a transform pipeline and convert it to a serializable representation that uses only standard python data types: dictionaries, lists, strings, integers, and floats.</p> <p>Parameters:</p> Name Type Description <code>self</code> <p>A transform that should be serialized. If the transform doesn't implement the <code>to_dict</code> method and <code>on_not_implemented_error</code> equals to 'raise' then <code>NotImplementedError</code> is raised. If <code>on_not_implemented_error</code> equals to 'warn' then <code>NotImplementedError</code> will be ignored but no transform parameters will be serialized.</p> <code>on_not_implemented_error</code> <code>str</code> <p><code>raise</code> or <code>warn</code>.</p> Source code in <code>albumentations/core/serialization.py</code> Python<pre><code>def to_dict(self, on_not_implemented_error: str = \"raise\") -&gt; dict[str, Any]:\n    \"\"\"Take a transform pipeline and convert it to a serializable representation that uses only standard\n    python data types: dictionaries, lists, strings, integers, and floats.\n\n    Args:\n        self: A transform that should be serialized. If the transform doesn't implement the `to_dict`\n            method and `on_not_implemented_error` equals to 'raise' then `NotImplementedError` is raised.\n            If `on_not_implemented_error` equals to 'warn' then `NotImplementedError` will be ignored\n            but no transform parameters will be serialized.\n        on_not_implemented_error (str): `raise` or `warn`.\n\n    \"\"\"\n    if on_not_implemented_error not in {\"raise\", \"warn\"}:\n        msg = f\"Unknown on_not_implemented_error value: {on_not_implemented_error}. Supported values are: 'raise' \"\n        \"and 'warn'\"\n        raise ValueError(msg)\n    try:\n        transform_dict = self.to_dict_private()\n    except NotImplementedError:\n        if on_not_implemented_error == \"raise\":\n            raise\n\n        transform_dict = {}\n        warnings.warn(\n            f\"Got NotImplementedError while trying to serialize {self}. Object arguments are not preserved. \"\n            f\"Implement either '{self.__class__.__name__}.get_transform_init_args_names' \"\n            f\"or '{self.__class__.__name__}.get_transform_init_args' \"\n            \"method to make the transform serializable\",\n            stacklevel=2,\n        )\n    return {\"__version__\": __version__, \"transform\": transform_dict}\n</code></pre>"},{"location":"api_reference/core/serialization/#albumentations.core.serialization.SerializableMeta","title":"<code>class  SerializableMeta</code> <code> </code>  [view source on GitHub]","text":"<p>A metaclass that is used to register classes in <code>SERIALIZABLE_REGISTRY</code> or <code>NON_SERIALIZABLE_REGISTRY</code> so they can be found later while deserializing transformation pipeline using classes full names.</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/core/serialization.py</code> Python<pre><code>class SerializableMeta(ABCMeta):\n    \"\"\"A metaclass that is used to register classes in `SERIALIZABLE_REGISTRY` or `NON_SERIALIZABLE_REGISTRY`\n    so they can be found later while deserializing transformation pipeline using classes full names.\n    \"\"\"\n\n    def __new__(cls, name: str, bases: tuple[type, ...], *args: Any, **kwargs: Any) -&gt; SerializableMeta:\n        cls_obj = super().__new__(cls, name, bases, *args, **kwargs)\n        if name != \"Serializable\" and ABC not in bases:\n            if cls_obj.is_serializable():\n                SERIALIZABLE_REGISTRY[cls_obj.get_class_fullname()] = cls_obj\n            else:\n                NON_SERIALIZABLE_REGISTRY[cls_obj.get_class_fullname()] = cls_obj\n        return cls_obj\n\n    @classmethod\n    def is_serializable(cls) -&gt; bool:\n        return False\n\n    @classmethod\n    def get_class_fullname(cls) -&gt; str:\n        return get_shortest_class_fullname(cls)\n\n    @classmethod\n    def _to_dict(cls) -&gt; dict[str, Any]:\n        return {}\n</code></pre>"},{"location":"api_reference/core/serialization/#albumentations.core.serialization.SerializableMeta.__new__","title":"<code>__new__ (cls, name, bases, *args, **kwargs)</code>  <code>special</code> <code>staticmethod</code>","text":"<p>Create and return a new object.  See help(type) for accurate signature.</p> Source code in <code>albumentations/core/serialization.py</code> Python<pre><code>def __new__(cls, name: str, bases: tuple[type, ...], *args: Any, **kwargs: Any) -&gt; SerializableMeta:\n    cls_obj = super().__new__(cls, name, bases, *args, **kwargs)\n    if name != \"Serializable\" and ABC not in bases:\n        if cls_obj.is_serializable():\n            SERIALIZABLE_REGISTRY[cls_obj.get_class_fullname()] = cls_obj\n        else:\n            NON_SERIALIZABLE_REGISTRY[cls_obj.get_class_fullname()] = cls_obj\n    return cls_obj\n</code></pre>"},{"location":"api_reference/core/serialization/#albumentations.core.serialization.from_dict","title":"<code>def from_dict    (transform_dict, nonserializable=None)    </code> [view source on GitHub]","text":"<p>transform_dict: A dictionary with serialized transform pipeline. nonserializable (dict): A dictionary that contains non-serializable transforms.     This dictionary is required when you are restoring a pipeline that contains non-serializable transforms.     Keys in that dictionary should be named same as <code>name</code> arguments in respective transforms from     a serialized pipeline.</p> Source code in <code>albumentations/core/serialization.py</code> Python<pre><code>def from_dict(\n    transform_dict: dict[str, Any],\n    nonserializable: dict[str, Any] | None = None,\n) -&gt; Serializable | None:\n    \"\"\"Args:\n    transform_dict: A dictionary with serialized transform pipeline.\n    nonserializable (dict): A dictionary that contains non-serializable transforms.\n        This dictionary is required when you are restoring a pipeline that contains non-serializable transforms.\n        Keys in that dictionary should be named same as `name` arguments in respective transforms from\n        a serialized pipeline.\n\n    \"\"\"\n    register_additional_transforms()\n    transform = transform_dict[\"transform\"]\n    lmbd = instantiate_nonserializable(transform, nonserializable)\n    if lmbd:\n        return lmbd\n    name = transform[\"__class_fullname__\"]\n    args = {k: v for k, v in transform.items() if k != \"__class_fullname__\"}\n    cls = SERIALIZABLE_REGISTRY[shorten_class_name(name)]\n    if \"transforms\" in args:\n        args[\"transforms\"] = [from_dict({\"transform\": t}, nonserializable=nonserializable) for t in args[\"transforms\"]]\n    return cls(**args)\n</code></pre>"},{"location":"api_reference/core/serialization/#albumentations.core.serialization.get_shortest_class_fullname","title":"<code>def get_shortest_class_fullname    (cls)    </code> [view source on GitHub]","text":"<p>The function <code>get_shortest_class_fullname</code> takes a class object as input and returns its shortened full name.</p> <p>:param cls: The parameter <code>cls</code> is of type <code>Type[BasicCompose]</code>, which means it expects a class that is a subclass of <code>BasicCompose</code> :type cls: Type[BasicCompose] :return: a string, which is the shortened version of the full class name.</p> Source code in <code>albumentations/core/serialization.py</code> Python<pre><code>def get_shortest_class_fullname(cls: type[Any]) -&gt; str:\n    \"\"\"The function `get_shortest_class_fullname` takes a class object as input and returns its shortened\n    full name.\n\n    :param cls: The parameter `cls` is of type `Type[BasicCompose]`, which means it expects a class that\n    is a subclass of `BasicCompose`\n    :type cls: Type[BasicCompose]\n    :return: a string, which is the shortened version of the full class name.\n    \"\"\"\n    class_fullname = f\"{cls.__module__}.{cls.__name__}\"\n    return shorten_class_name(class_fullname)\n</code></pre>"},{"location":"api_reference/core/serialization/#albumentations.core.serialization.load","title":"<code>def load    (filepath_or_buffer, data_format='json', nonserializable=None)    </code> [view source on GitHub]","text":"<p>Load a serialized pipeline from a file or file-like object and construct a transform pipeline.</p> <p>Parameters:</p> Name Type Description <code>filepath_or_buffer</code> <code>Union[str, Path, TextIO]</code> <p>The file path or file-like object to read the serialized data from. If a string is provided, it is interpreted as a path to a file. If a file-like object is provided, the serialized data will be read from it directly.</p> <code>data_format</code> <code>str</code> <p>The format of the serialized data. Valid options are 'json' and 'yaml'. Defaults to 'json'.</p> <code>nonserializable</code> <code>Optional[dict[str, Any]]</code> <p>A dictionary that contains non-serializable transforms. This dictionary is required when restoring a pipeline that contains non-serializable transforms. Keys in the dictionary should be named the same as the <code>name</code> arguments in respective transforms from the serialized pipeline. Defaults to None.</p> <p>Returns:</p> Type Description <code>object</code> <p>The deserialized transform pipeline.</p> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If <code>data_format</code> is 'yaml' but PyYAML is not installed.</p> Source code in <code>albumentations/core/serialization.py</code> Python<pre><code>def load(\n    filepath_or_buffer: str | Path | TextIO,\n    data_format: str = \"json\",\n    nonserializable: dict[str, Any] | None = None,\n) -&gt; object:\n    \"\"\"Load a serialized pipeline from a file or file-like object and construct a transform pipeline.\n\n    Args:\n        filepath_or_buffer (Union[str, Path, TextIO]): The file path or file-like object to read the serialized\n            data from.\n            If a string is provided, it is interpreted as a path to a file. If a file-like object is provided,\n            the serialized data will be read from it directly.\n        data_format (str): The format of the serialized data. Valid options are 'json' and 'yaml'.\n            Defaults to 'json'.\n        nonserializable (Optional[dict[str, Any]]): A dictionary that contains non-serializable transforms.\n            This dictionary is required when restoring a pipeline that contains non-serializable transforms.\n            Keys in the dictionary should be named the same as the `name` arguments in respective transforms\n            from the serialized pipeline. Defaults to None.\n\n    Returns:\n        object: The deserialized transform pipeline.\n\n    Raises:\n        ValueError: If `data_format` is 'yaml' but PyYAML is not installed.\n\n    \"\"\"\n    check_data_format(data_format)\n\n    if isinstance(filepath_or_buffer, (str, Path)):  # Assume it's a filepath\n        with open(filepath_or_buffer) as f:\n            if data_format == \"json\":\n                transform_dict = json.load(f)\n            else:\n                if not yaml_available:\n                    msg = \"You need to install PyYAML to load a pipeline in yaml format\"\n                    raise ValueError(msg)\n                transform_dict = yaml.safe_load(f)\n    elif data_format == \"json\":\n        transform_dict = json.load(filepath_or_buffer)\n    else:\n        if not yaml_available:\n            msg = \"You need to install PyYAML to load a pipeline in yaml format\"\n            raise ValueError(msg)\n        transform_dict = yaml.safe_load(filepath_or_buffer)\n\n    return from_dict(transform_dict, nonserializable=nonserializable)\n</code></pre>"},{"location":"api_reference/core/serialization/#albumentations.core.serialization.register_additional_transforms","title":"<code>def register_additional_transforms    ()    </code> [view source on GitHub]","text":"<p>Register transforms that are not imported directly into the <code>albumentations</code> module by checking the availability of optional dependencies.</p> Source code in <code>albumentations/core/serialization.py</code> Python<pre><code>def register_additional_transforms() -&gt; None:\n    \"\"\"Register transforms that are not imported directly into the `albumentations` module by checking\n    the availability of optional dependencies.\n    \"\"\"\n    if importlib.util.find_spec(\"torch\") is not None:\n        try:\n            # Import `albumentations.pytorch` only if `torch` is installed.\n            import albumentations.pytorch\n\n            # Use a dummy operation to acknowledge the use of the imported module and avoid linting errors.\n            _ = albumentations.pytorch.ToTensorV2\n        except ImportError:\n            pass\n</code></pre>"},{"location":"api_reference/core/serialization/#albumentations.core.serialization.save","title":"<code>def save    (transform, filepath_or_buffer, data_format='json', on_not_implemented_error='raise')    </code> [view source on GitHub]","text":"<p>Serialize a transform pipeline and save it to either a file specified by a path or a file-like object in either JSON or YAML format.</p> <p>Parameters:</p> Name Type Description <code>transform</code> <code>Serializable</code> <p>The transform pipeline to serialize.</p> <code>filepath_or_buffer</code> <code>Union[str, Path, TextIO]</code> <p>The file path or file-like object to write the serialized data to. If a string is provided, it is interpreted as a path to a file. If a file-like object is provided, the serialized data will be written to it directly.</p> <code>data_format</code> <code>str</code> <p>The format to serialize the data in. Valid options are 'json' and 'yaml'. Defaults to 'json'.</p> <code>on_not_implemented_error</code> <code>str</code> <p>Determines the behavior if a transform does not implement the <code>to_dict</code> method. If set to 'raise', a <code>NotImplementedError</code> is raised. If set to 'warn', the exception is ignored, and no transform arguments are saved. Defaults to 'raise'.</p> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If <code>data_format</code> is 'yaml' but PyYAML is not installed.</p> Source code in <code>albumentations/core/serialization.py</code> Python<pre><code>def save(\n    transform: Serializable,\n    filepath_or_buffer: str | Path | TextIO,\n    data_format: str = \"json\",\n    on_not_implemented_error: str = \"raise\",\n) -&gt; None:\n    \"\"\"Serialize a transform pipeline and save it to either a file specified by a path or a file-like object\n    in either JSON or YAML format.\n\n    Args:\n        transform (Serializable): The transform pipeline to serialize.\n        filepath_or_buffer (Union[str, Path, TextIO]): The file path or file-like object to write the serialized\n            data to.\n            If a string is provided, it is interpreted as a path to a file. If a file-like object is provided,\n            the serialized data will be written to it directly.\n        data_format (str): The format to serialize the data in. Valid options are 'json' and 'yaml'.\n            Defaults to 'json'.\n        on_not_implemented_error (str): Determines the behavior if a transform does not implement the `to_dict` method.\n            If set to 'raise', a `NotImplementedError` is raised. If set to 'warn', the exception is ignored, and\n            no transform arguments are saved. Defaults to 'raise'.\n\n    Raises:\n        ValueError: If `data_format` is 'yaml' but PyYAML is not installed.\n\n    \"\"\"\n    check_data_format(data_format)\n    transform_dict = transform.to_dict(on_not_implemented_error=on_not_implemented_error)\n    transform_dict = serialize_enum(transform_dict)\n\n    # Determine whether to write to a file or a file-like object\n    if isinstance(filepath_or_buffer, (str, Path)):  # It's a filepath\n        with open(filepath_or_buffer, \"w\") as f:\n            if data_format == \"yaml\":\n                if not yaml_available:\n                    msg = \"You need to install PyYAML to save a pipeline in YAML format\"\n                    raise ValueError(msg)\n                yaml.safe_dump(transform_dict, f, default_flow_style=False)\n            elif data_format == \"json\":\n                json.dump(transform_dict, f)\n    elif data_format == \"yaml\":\n        if not yaml_available:\n            msg = \"You need to install PyYAML to save a pipeline in YAML format\"\n            raise ValueError(msg)\n        yaml.safe_dump(transform_dict, filepath_or_buffer, default_flow_style=False)\n    elif data_format == \"json\":\n        json.dump(transform_dict, filepath_or_buffer, indent=2)\n</code></pre>"},{"location":"api_reference/core/serialization/#albumentations.core.serialization.serialize_enum","title":"<code>def serialize_enum    (obj)    </code> [view source on GitHub]","text":"<p>Recursively search for Enum objects and convert them to their value. Also handle any Mapping or Sequence types.</p> Source code in <code>albumentations/core/serialization.py</code> Python<pre><code>def serialize_enum(obj: Any) -&gt; Any:\n    \"\"\"Recursively search for Enum objects and convert them to their value.\n    Also handle any Mapping or Sequence types.\n    \"\"\"\n    if isinstance(obj, Mapping):\n        return {k: serialize_enum(v) for k, v in obj.items()}\n    if isinstance(obj, Sequence) and not isinstance(obj, str):  # exclude strings since they're also sequences\n        return [serialize_enum(v) for v in obj]\n    return obj.value if isinstance(obj, Enum) else obj\n</code></pre>"},{"location":"api_reference/core/serialization/#albumentations.core.serialization.to_dict","title":"<code>def to_dict    (transform, on_not_implemented_error='raise')    </code> [view source on GitHub]","text":"<p>Take a transform pipeline and convert it to a serializable representation that uses only standard python data types: dictionaries, lists, strings, integers, and floats.</p> <p>Parameters:</p> Name Type Description <code>transform</code> <code>Serializable</code> <p>A transform that should be serialized. If the transform doesn't implement the <code>to_dict</code> method and <code>on_not_implemented_error</code> equals to 'raise' then <code>NotImplementedError</code> is raised. If <code>on_not_implemented_error</code> equals to 'warn' then <code>NotImplementedError</code> will be ignored but no transform parameters will be serialized.</p> <code>on_not_implemented_error</code> <code>str</code> <p><code>raise</code> or <code>warn</code>.</p> Source code in <code>albumentations/core/serialization.py</code> Python<pre><code>def to_dict(transform: Serializable, on_not_implemented_error: str = \"raise\") -&gt; dict[str, Any]:\n    \"\"\"Take a transform pipeline and convert it to a serializable representation that uses only standard\n    python data types: dictionaries, lists, strings, integers, and floats.\n\n    Args:\n        transform: A transform that should be serialized. If the transform doesn't implement the `to_dict`\n            method and `on_not_implemented_error` equals to 'raise' then `NotImplementedError` is raised.\n            If `on_not_implemented_error` equals to 'warn' then `NotImplementedError` will be ignored\n            but no transform parameters will be serialized.\n        on_not_implemented_error (str): `raise` or `warn`.\n\n    \"\"\"\n    return transform.to_dict(on_not_implemented_error)\n</code></pre>"},{"location":"api_reference/core/transforms_interface/","title":"Transforms Interface (core.transforms_interface)","text":""},{"location":"api_reference/core/transforms_interface/#albumentations.core.transforms_interface.BasicTransform","title":"<code>class  BasicTransform</code> <code>     (p=0.5, always_apply=None)                 </code>  [view source on GitHub]","text":"<p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/core/transforms_interface.py</code> Python<pre><code>class BasicTransform(Serializable, metaclass=CombinedMeta):\n    _targets: tuple[Targets, ...] | Targets  # targets that this transform can work on\n    _available_keys: set[str]  # targets that this transform, as string, lower-cased\n    _key2func: dict[\n        str,\n        Callable[..., Any],\n    ]  # mapping for targets (plus additional targets) and methods for which they depend\n    call_backup = None\n    interpolation: int\n    fill_value: ColorType\n    mask_fill_value: ColorType | None\n    # replay mode params\n    deterministic: bool = False\n    save_key = \"replay\"\n    replay_mode = False\n    applied_in_replay = False\n\n    class InitSchema(BaseTransformInitSchema):\n        pass\n\n    def __init__(self, p: float = 0.5, always_apply: bool | None = None):\n        self.p = p\n        if always_apply is not None:\n            if always_apply:\n                warn(\n                    \"always_apply is deprecated. Use `p=1` if you want to always apply the transform.\"\n                    \" self.p will be set to 1.\",\n                    DeprecationWarning,\n                    stacklevel=2,\n                )\n                self.p = 1.0\n            else:\n                warn(\n                    \"always_apply is deprecated.\",\n                    DeprecationWarning,\n                    stacklevel=2,\n                )\n        self._additional_targets: dict[str, str] = {}\n        # replay mode params\n        self.params: dict[Any, Any] = {}\n        self._key2func = {}\n        self._set_keys()\n\n    def __call__(self, *args: Any, force_apply: bool = False, **kwargs: Any) -&gt; Any:\n        if args:\n            msg = \"You have to pass data to augmentations as named arguments, for example: aug(image=image)\"\n            raise KeyError(msg)\n        if self.replay_mode:\n            if self.applied_in_replay:\n                return self.apply_with_params(self.params, **kwargs)\n\n            return kwargs\n\n        if force_apply or (random.random() &lt; self.p):\n            params = self.get_params()\n            params = self.update_params_shape(params=params, data=kwargs)\n\n            if self.targets_as_params:  # check if all required targets are in kwargs.\n                missing_keys = set(self.targets_as_params).difference(kwargs.keys())\n                if missing_keys and not (missing_keys == {\"image\"} and \"images\" in kwargs):\n                    msg = f\"{self.__class__.__name__} requires {self.targets_as_params} missing keys: {missing_keys}\"\n                    raise ValueError(msg)\n\n            params_dependent_on_data = self.get_params_dependent_on_data(params=params, data=kwargs)\n            params.update(params_dependent_on_data)\n\n            if self.targets_as_params:  # this block will be removed after removing `get_params_dependent_on_targets`\n                targets_as_params = {k: kwargs.get(k, None) for k in self.targets_as_params}\n                if missing_keys:  # here we expecting case when missing_keys == {\"image\"} and \"images\" in kwargs\n                    targets_as_params[\"image\"] = kwargs[\"images\"][0]\n                params_dependent_on_targets = self.get_params_dependent_on_targets(targets_as_params)\n                params.update(params_dependent_on_targets)\n            if self.deterministic:\n                kwargs[self.save_key][id(self)] = deepcopy(params)\n            return self.apply_with_params(params, **kwargs)\n\n        return kwargs\n\n    def apply_with_params(self, params: dict[str, Any], *args: Any, **kwargs: Any) -&gt; dict[str, Any]:\n        \"\"\"Apply transforms with parameters.\"\"\"\n        params = self.update_params(params, **kwargs)  # remove after move parameters like interpolation\n        res = {}\n        for key, arg in kwargs.items():\n            if key in self._key2func and arg is not None:\n                target_function = self._key2func[key]\n                if isinstance(arg, np.ndarray):\n                    result = target_function(np.require(arg, requirements=[\"C_CONTIGUOUS\"]), **params)\n                    if isinstance(result, np.ndarray):\n                        res[key] = np.require(result, requirements=[\"C_CONTIGUOUS\"])\n                    else:\n                        res[key] = result\n                else:\n                    res[key] = target_function(arg, **params)\n            else:\n                res[key] = arg\n        return res\n\n    def set_deterministic(self, flag: bool, save_key: str = \"replay\") -&gt; BasicTransform:\n        \"\"\"Set transform to be deterministic.\"\"\"\n        if save_key == \"params\":\n            msg = \"params save_key is reserved\"\n            raise KeyError(msg)\n\n        self.deterministic = flag\n        if self.deterministic and self.targets_as_params:\n            warn(\n                self.get_class_fullname() + \" could work incorrectly in ReplayMode for other input data\"\n                \" because its' params depend on targets.\",\n                stacklevel=2,\n            )\n        self.save_key = save_key\n        return self\n\n    def __repr__(self) -&gt; str:\n        state = self.get_base_init_args()\n        state.update(self.get_transform_init_args())\n        return f\"{self.__class__.__name__}({format_args(state)})\"\n\n    def apply(self, img: np.ndarray, *args: Any, **params: Any) -&gt; np.ndarray:\n        \"\"\"Apply transform on image.\"\"\"\n        raise NotImplementedError\n\n    def apply_to_images(self, images: np.ndarray, **params: Any) -&gt; list[np.ndarray]:\n        \"\"\"Apply transform on images.\"\"\"\n        return [self.apply(image, **params) for image in images]\n\n    def get_params(self) -&gt; dict[str, Any]:\n        \"\"\"Returns parameters independent of input.\"\"\"\n        return {}\n\n    def update_params_shape(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n        \"\"\"Updates parameters with input image shape.\"\"\"\n        # here we expects `image` or `images` in kwargs. it's checked at Compose._check_args\n        shape = data[\"image\"].shape if \"image\" in data else data[\"images\"][0].shape\n        params[\"shape\"] = shape\n        params.update({\"cols\": shape[1], \"rows\": shape[0]})\n        return params\n\n    def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n        \"\"\"Returns parameters dependent on input.\"\"\"\n        return params\n\n    @property\n    def targets(self) -&gt; dict[str, Callable[..., Any]]:\n        # mapping for targets and methods for which they depend\n        # for example:\n        # &gt;&gt;  {\"image\": self.apply}\n        # &gt;&gt;  {\"masks\": self.apply_to_masks}\n        raise NotImplementedError\n\n    def _set_keys(self) -&gt; None:\n        \"\"\"Set _available_keys.\"\"\"\n        if not hasattr(self, \"_targets\"):\n            self._available_keys = set()\n        else:\n            self._available_keys = {\n                target.value.lower()\n                for target in (self._targets if isinstance(self._targets, tuple) else [self._targets])\n            }\n        self._available_keys.update(self.targets.keys())\n        self._key2func = {key: self.targets[key] for key in self._available_keys if key in self.targets}\n\n    @property\n    def available_keys(self) -&gt; set[str]:\n        \"\"\"Returns set of available keys.\"\"\"\n        return self._available_keys\n\n    def update_params(self, params: dict[str, Any], **kwargs: Any) -&gt; dict[str, Any]:\n        \"\"\"Update parameters with transform specific params.\n        This method is deprecated, use:\n        - `get_params` for transform specific params like interpolation and\n        - `update_params_shape` for data like shape.\n        \"\"\"\n        if hasattr(self, \"interpolation\"):\n            params[\"interpolation\"] = self.interpolation\n        if hasattr(self, \"fill_value\"):\n            params[\"fill_value\"] = self.fill_value\n        if hasattr(self, \"mask_fill_value\"):\n            params[\"mask_fill_value\"] = self.mask_fill_value\n\n        # here we expects `image` or `images` in kwargs. it's checked at Compose._check_args\n        shape = kwargs[\"image\"].shape if \"image\" in kwargs else kwargs[\"images\"][0].shape\n        params[\"shape\"] = shape\n        params.update({\"cols\": shape[1], \"rows\": shape[0]})\n        return params\n\n    def add_targets(self, additional_targets: dict[str, str]) -&gt; None:\n        \"\"\"Add targets to transform them the same way as one of existing targets.\n        ex: {'target_image': 'image'}\n        ex: {'obj1_mask': 'mask', 'obj2_mask': 'mask'}\n        by the way you must have at least one object with key 'image'\n\n        Args:\n            additional_targets (dict): keys - new target name, values - old target name. ex: {'image2': 'image'}\n\n        \"\"\"\n        for k, v in additional_targets.items():\n            if k in self._additional_targets and v != self._additional_targets[k]:\n                raise ValueError(\n                    f\"Trying to overwrite existed additional targets. \"\n                    f\"Key={k} Exists={self._additional_targets[k]} New value: {v}\",\n                )\n            if v in self._available_keys:\n                self._additional_targets[k] = v\n                self._key2func[k] = self.targets[v]\n                self._available_keys.add(k)\n\n    @property\n    def targets_as_params(self) -&gt; list[str]:\n        \"\"\"Targets used to get params dependent on targets.\n        This is used to check input has all required targets.\n        \"\"\"\n        return []\n\n    def get_params_dependent_on_targets(self, params: dict[str, Any]) -&gt; dict[str, Any]:\n        \"\"\"This method is deprecated.\n        Use `get_params_dependent_on_data` instead.\n        Returns parameters dependent on targets.\n        Dependent target is defined in `self.targets_as_params`\n        \"\"\"\n        return {}\n\n    @classmethod\n    def get_class_fullname(cls) -&gt; str:\n        return get_shortest_class_fullname(cls)\n\n    @classmethod\n    def is_serializable(cls) -&gt; bool:\n        return True\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        \"\"\"Returns names of arguments that are used in __init__ method of the transform.\"\"\"\n        msg = (\n            f\"Class {self.get_class_fullname()} is not serializable because the `get_transform_init_args_names` \"\n            \"method is not implemented\"\n        )\n        raise NotImplementedError(msg)\n\n    def get_base_init_args(self) -&gt; dict[str, Any]:\n        \"\"\"Returns base init args - p\"\"\"\n        return {\"p\": self.p}\n\n    def get_transform_init_args(self) -&gt; dict[str, Any]:\n        return {k: getattr(self, k) for k in self.get_transform_init_args_names()}\n\n    def to_dict_private(self) -&gt; dict[str, Any]:\n        state = {\"__class_fullname__\": self.get_class_fullname()}\n        state.update(self.get_base_init_args())\n        state.update(self.get_transform_init_args())\n\n        return state\n\n    def get_dict_with_id(self) -&gt; dict[str, Any]:\n        d = self.to_dict_private()\n        d[\"id\"] = id(self)\n        return d\n</code></pre>"},{"location":"api_reference/core/transforms_interface/#albumentations.core.transforms_interface.BasicTransform.available_keys","title":"<code>available_keys: set[str]</code>  <code>property</code> <code>readonly</code>","text":"<p>Returns set of available keys.</p>"},{"location":"api_reference/core/transforms_interface/#albumentations.core.transforms_interface.BasicTransform.targets_as_params","title":"<code>targets_as_params: list[str]</code>  <code>property</code> <code>readonly</code>","text":"<p>Targets used to get params dependent on targets. This is used to check input has all required targets.</p>"},{"location":"api_reference/core/transforms_interface/#albumentations.core.transforms_interface.BasicTransform.add_targets","title":"<code>add_targets (self, additional_targets)</code>","text":"<p>Add targets to transform them the same way as one of existing targets. ex: {'target_image': 'image'} ex: {'obj1_mask': 'mask', 'obj2_mask': 'mask'} by the way you must have at least one object with key 'image'</p> <p>Parameters:</p> Name Type Description <code>additional_targets</code> <code>dict</code> <p>keys - new target name, values - old target name. ex: {'image2': 'image'}</p> Source code in <code>albumentations/core/transforms_interface.py</code> Python<pre><code>def add_targets(self, additional_targets: dict[str, str]) -&gt; None:\n    \"\"\"Add targets to transform them the same way as one of existing targets.\n    ex: {'target_image': 'image'}\n    ex: {'obj1_mask': 'mask', 'obj2_mask': 'mask'}\n    by the way you must have at least one object with key 'image'\n\n    Args:\n        additional_targets (dict): keys - new target name, values - old target name. ex: {'image2': 'image'}\n\n    \"\"\"\n    for k, v in additional_targets.items():\n        if k in self._additional_targets and v != self._additional_targets[k]:\n            raise ValueError(\n                f\"Trying to overwrite existed additional targets. \"\n                f\"Key={k} Exists={self._additional_targets[k]} New value: {v}\",\n            )\n        if v in self._available_keys:\n            self._additional_targets[k] = v\n            self._key2func[k] = self.targets[v]\n            self._available_keys.add(k)\n</code></pre>"},{"location":"api_reference/core/transforms_interface/#albumentations.core.transforms_interface.BasicTransform.apply","title":"<code>apply (self, img, *args, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/core/transforms_interface.py</code> Python<pre><code>def apply(self, img: np.ndarray, *args: Any, **params: Any) -&gt; np.ndarray:\n    \"\"\"Apply transform on image.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api_reference/core/transforms_interface/#albumentations.core.transforms_interface.BasicTransform.apply_to_images","title":"<code>apply_to_images (self, images, **params)</code>","text":"<p>Apply transform on images.</p> Source code in <code>albumentations/core/transforms_interface.py</code> Python<pre><code>def apply_to_images(self, images: np.ndarray, **params: Any) -&gt; list[np.ndarray]:\n    \"\"\"Apply transform on images.\"\"\"\n    return [self.apply(image, **params) for image in images]\n</code></pre>"},{"location":"api_reference/core/transforms_interface/#albumentations.core.transforms_interface.BasicTransform.apply_with_params","title":"<code>apply_with_params (self, params, *args, **kwargs)</code>","text":"<p>Apply transforms with parameters.</p> Source code in <code>albumentations/core/transforms_interface.py</code> Python<pre><code>def apply_with_params(self, params: dict[str, Any], *args: Any, **kwargs: Any) -&gt; dict[str, Any]:\n    \"\"\"Apply transforms with parameters.\"\"\"\n    params = self.update_params(params, **kwargs)  # remove after move parameters like interpolation\n    res = {}\n    for key, arg in kwargs.items():\n        if key in self._key2func and arg is not None:\n            target_function = self._key2func[key]\n            if isinstance(arg, np.ndarray):\n                result = target_function(np.require(arg, requirements=[\"C_CONTIGUOUS\"]), **params)\n                if isinstance(result, np.ndarray):\n                    res[key] = np.require(result, requirements=[\"C_CONTIGUOUS\"])\n                else:\n                    res[key] = result\n            else:\n                res[key] = target_function(arg, **params)\n        else:\n            res[key] = arg\n    return res\n</code></pre>"},{"location":"api_reference/core/transforms_interface/#albumentations.core.transforms_interface.BasicTransform.get_base_init_args","title":"<code>get_base_init_args (self)</code>","text":"<p>Returns base init args - p</p> Source code in <code>albumentations/core/transforms_interface.py</code> Python<pre><code>def get_base_init_args(self) -&gt; dict[str, Any]:\n    \"\"\"Returns base init args - p\"\"\"\n    return {\"p\": self.p}\n</code></pre>"},{"location":"api_reference/core/transforms_interface/#albumentations.core.transforms_interface.BasicTransform.get_params","title":"<code>get_params (self)</code>","text":"<p>Returns parameters independent of input.</p> Source code in <code>albumentations/core/transforms_interface.py</code> Python<pre><code>def get_params(self) -&gt; dict[str, Any]:\n    \"\"\"Returns parameters independent of input.\"\"\"\n    return {}\n</code></pre>"},{"location":"api_reference/core/transforms_interface/#albumentations.core.transforms_interface.BasicTransform.get_params_dependent_on_data","title":"<code>get_params_dependent_on_data (self, params, data)</code>","text":"<p>Returns parameters dependent on input.</p> Source code in <code>albumentations/core/transforms_interface.py</code> Python<pre><code>def get_params_dependent_on_data(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n    \"\"\"Returns parameters dependent on input.\"\"\"\n    return params\n</code></pre>"},{"location":"api_reference/core/transforms_interface/#albumentations.core.transforms_interface.BasicTransform.get_params_dependent_on_targets","title":"<code>get_params_dependent_on_targets (self, params)</code>","text":"<p>This method is deprecated. Use <code>get_params_dependent_on_data</code> instead. Returns parameters dependent on targets. Dependent target is defined in <code>self.targets_as_params</code></p> Source code in <code>albumentations/core/transforms_interface.py</code> Python<pre><code>def get_params_dependent_on_targets(self, params: dict[str, Any]) -&gt; dict[str, Any]:\n    \"\"\"This method is deprecated.\n    Use `get_params_dependent_on_data` instead.\n    Returns parameters dependent on targets.\n    Dependent target is defined in `self.targets_as_params`\n    \"\"\"\n    return {}\n</code></pre>"},{"location":"api_reference/core/transforms_interface/#albumentations.core.transforms_interface.BasicTransform.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/core/transforms_interface.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    \"\"\"Returns names of arguments that are used in __init__ method of the transform.\"\"\"\n    msg = (\n        f\"Class {self.get_class_fullname()} is not serializable because the `get_transform_init_args_names` \"\n        \"method is not implemented\"\n    )\n    raise NotImplementedError(msg)\n</code></pre>"},{"location":"api_reference/core/transforms_interface/#albumentations.core.transforms_interface.BasicTransform.set_deterministic","title":"<code>set_deterministic (self, flag, save_key='replay')</code>","text":"<p>Set transform to be deterministic.</p> Source code in <code>albumentations/core/transforms_interface.py</code> Python<pre><code>def set_deterministic(self, flag: bool, save_key: str = \"replay\") -&gt; BasicTransform:\n    \"\"\"Set transform to be deterministic.\"\"\"\n    if save_key == \"params\":\n        msg = \"params save_key is reserved\"\n        raise KeyError(msg)\n\n    self.deterministic = flag\n    if self.deterministic and self.targets_as_params:\n        warn(\n            self.get_class_fullname() + \" could work incorrectly in ReplayMode for other input data\"\n            \" because its' params depend on targets.\",\n            stacklevel=2,\n        )\n    self.save_key = save_key\n    return self\n</code></pre>"},{"location":"api_reference/core/transforms_interface/#albumentations.core.transforms_interface.BasicTransform.update_params","title":"<code>update_params (self, params, **kwargs)</code>","text":"<p>Update parameters with transform specific params. This method is deprecated, use: - <code>get_params</code> for transform specific params like interpolation and - <code>update_params_shape</code> for data like shape.</p> Source code in <code>albumentations/core/transforms_interface.py</code> Python<pre><code>def update_params(self, params: dict[str, Any], **kwargs: Any) -&gt; dict[str, Any]:\n    \"\"\"Update parameters with transform specific params.\n    This method is deprecated, use:\n    - `get_params` for transform specific params like interpolation and\n    - `update_params_shape` for data like shape.\n    \"\"\"\n    if hasattr(self, \"interpolation\"):\n        params[\"interpolation\"] = self.interpolation\n    if hasattr(self, \"fill_value\"):\n        params[\"fill_value\"] = self.fill_value\n    if hasattr(self, \"mask_fill_value\"):\n        params[\"mask_fill_value\"] = self.mask_fill_value\n\n    # here we expects `image` or `images` in kwargs. it's checked at Compose._check_args\n    shape = kwargs[\"image\"].shape if \"image\" in kwargs else kwargs[\"images\"][0].shape\n    params[\"shape\"] = shape\n    params.update({\"cols\": shape[1], \"rows\": shape[0]})\n    return params\n</code></pre>"},{"location":"api_reference/core/transforms_interface/#albumentations.core.transforms_interface.BasicTransform.update_params_shape","title":"<code>update_params_shape (self, params, data)</code>","text":"<p>Updates parameters with input image shape.</p> Source code in <code>albumentations/core/transforms_interface.py</code> Python<pre><code>def update_params_shape(self, params: dict[str, Any], data: dict[str, Any]) -&gt; dict[str, Any]:\n    \"\"\"Updates parameters with input image shape.\"\"\"\n    # here we expects `image` or `images` in kwargs. it's checked at Compose._check_args\n    shape = data[\"image\"].shape if \"image\" in data else data[\"images\"][0].shape\n    params[\"shape\"] = shape\n    params.update({\"cols\": shape[1], \"rows\": shape[0]})\n    return params\n</code></pre>"},{"location":"api_reference/core/transforms_interface/#albumentations.core.transforms_interface.DualTransform","title":"<code>class  DualTransform</code> <code> </code>  [view source on GitHub]","text":"<p>A base class for transformations that should be applied both to an image and its corresponding properties such as masks, bounding boxes, and keypoints. This class ensures that when a transform is applied to an image, all associated entities are transformed accordingly to maintain consistency between the image and its annotations.</p> <p>Properties</p> <p>targets (dict[str, Callable[..., Any]]): Defines the types of targets (e.g., image, mask, bboxes, keypoints)     that the transform should be applied to and maps them to the corresponding methods.</p> <p>Methods</p> <p>apply_to_bbox(bbox: BoxInternalType, args: Any, *params: Any) -&gt; BoxInternalType:     Applies the transform to a single bounding box. Should be implemented in the subclass.</p> <p>apply_to_keypoint(keypoint: KeypointInternalType, args: Any, *params: Any) -&gt; KeypointInternalType:     Applies the transform to a single keypoint. Should be implemented in the subclass.</p> <p>apply_to_bboxes(bboxes: Sequence[BoxType], args: Any, *params: Any) -&gt; Sequence[BoxType]:     Applies the transform to a list of bounding boxes. Delegates to <code>apply_to_bbox</code> for each bounding box.</p> <p>apply_to_keypoints(keypoints: Sequence[KeypointType], args: Any, *params: Any) -&gt; Sequence[KeypointType]:     Applies the transform to a list of keypoints. Delegates to <code>apply_to_keypoint</code> for each keypoint.</p> <p>apply_to_mask(mask: np.ndarray, args: Any, *params: Any) -&gt; np.ndarray:     Applies the transform specifically to a single mask.</p> <p>apply_to_masks(masks: Sequence[np.ndarray], **params: Any) -&gt; list[np.ndarray]:     Applies the transform to a list of masks. Delegates to <code>apply_to_mask</code> for each mask.</p> <p>Note</p> <p>This class is intended to be subclassed and should not be used directly. Subclasses are expected to implement the specific logic for each type of target (e.g., image, mask, bboxes, keypoints) in the corresponding <code>apply_to_*</code> methods.</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/core/transforms_interface.py</code> Python<pre><code>class DualTransform(BasicTransform):\n    \"\"\"A base class for transformations that should be applied both to an image and its corresponding properties\n    such as masks, bounding boxes, and keypoints. This class ensures that when a transform is applied to an image,\n    all associated entities are transformed accordingly to maintain consistency between the image and its annotations.\n\n    Properties:\n        targets (dict[str, Callable[..., Any]]): Defines the types of targets (e.g., image, mask, bboxes, keypoints)\n            that the transform should be applied to and maps them to the corresponding methods.\n\n    Methods:\n        apply_to_bbox(bbox: BoxInternalType, *args: Any, **params: Any) -&gt; BoxInternalType:\n            Applies the transform to a single bounding box. Should be implemented in the subclass.\n\n        apply_to_keypoint(keypoint: KeypointInternalType, *args: Any, **params: Any) -&gt; KeypointInternalType:\n            Applies the transform to a single keypoint. Should be implemented in the subclass.\n\n        apply_to_bboxes(bboxes: Sequence[BoxType], *args: Any, **params: Any) -&gt; Sequence[BoxType]:\n            Applies the transform to a list of bounding boxes. Delegates to `apply_to_bbox` for each bounding box.\n\n        apply_to_keypoints(keypoints: Sequence[KeypointType], *args: Any, **params: Any) -&gt; Sequence[KeypointType]:\n            Applies the transform to a list of keypoints. Delegates to `apply_to_keypoint` for each keypoint.\n\n        apply_to_mask(mask: np.ndarray, *args: Any, **params: Any) -&gt; np.ndarray:\n            Applies the transform specifically to a single mask.\n\n        apply_to_masks(masks: Sequence[np.ndarray], **params: Any) -&gt; list[np.ndarray]:\n            Applies the transform to a list of masks. Delegates to `apply_to_mask` for each mask.\n\n    Note:\n        This class is intended to be subclassed and should not be used directly. Subclasses are expected to\n        implement the specific logic for each type of target (e.g., image, mask, bboxes, keypoints) in the\n        corresponding `apply_to_*` methods.\n\n    \"\"\"\n\n    @property\n    def targets(self) -&gt; dict[str, Callable[..., Any]]:\n        return {\n            \"image\": self.apply,\n            \"images\": self.apply_to_images,\n            \"mask\": self.apply_to_mask,\n            \"masks\": self.apply_to_masks,\n            \"bboxes\": self.apply_to_bboxes,\n            \"keypoints\": self.apply_to_keypoints,\n        }\n\n    def apply_to_bbox(self, bbox: BoxInternalType, *args: Any, **params: Any) -&gt; BoxInternalType:\n        msg = f\"Method apply_to_bbox is not implemented in class {self.__class__.__name__}\"\n        raise NotImplementedError(msg)\n\n    def apply_to_keypoint(self, keypoint: KeypointInternalType, *args: Any, **params: Any) -&gt; KeypointInternalType:\n        msg = f\"Method apply_to_keypoint is not implemented in class {self.__class__.__name__}\"\n        raise NotImplementedError(msg)\n\n    def apply_to_global_label(self, label: np.ndarray, *args: Any, **params: Any) -&gt; np.ndarray:\n        msg = f\"Method apply_to_global_label is not implemented in class {self.__class__.__name__}\"\n        raise NotImplementedError(msg)\n\n    def apply_to_bboxes(self, bboxes: Sequence[BoxType], *args: Any, **params: Any) -&gt; Sequence[BoxType]:\n        return [\n            self.apply_to_bbox(cast(BoxInternalType, tuple(cast(BoxInternalType, bbox[:4]))), **params)\n            + tuple(bbox[4:])\n            for bbox in bboxes\n        ]\n\n    def apply_to_keypoints(\n        self,\n        keypoints: Sequence[KeypointType],\n        *args: Any,\n        **params: Any,\n    ) -&gt; Sequence[KeypointType]:\n        return [\n            self.apply_to_keypoint(cast(KeypointInternalType, tuple(keypoint[:4])), **params) + tuple(keypoint[4:])\n            for keypoint in keypoints\n        ]\n\n    def apply_to_mask(self, mask: np.ndarray, *args: Any, **params: Any) -&gt; np.ndarray:\n        return self.apply(mask, **{k: cv2.INTER_NEAREST if k == \"interpolation\" else v for k, v in params.items()})\n\n    def apply_to_masks(self, masks: Sequence[np.ndarray], **params: Any) -&gt; list[np.ndarray]:\n        return [self.apply_to_mask(mask, **params) for mask in masks]\n\n    def apply_to_global_labels(self, labels: Sequence[np.ndarray], **params: Any) -&gt; list[np.ndarray]:\n        return [self.apply_to_global_label(label, **params) for label in labels]\n</code></pre>"},{"location":"api_reference/core/transforms_interface/#albumentations.core.transforms_interface.ImageOnlyTransform","title":"<code>class  ImageOnlyTransform</code> <code> </code>  [view source on GitHub]","text":"<p>Transform applied to image only.</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/core/transforms_interface.py</code> Python<pre><code>class ImageOnlyTransform(BasicTransform):\n    \"\"\"Transform applied to image only.\"\"\"\n\n    _targets = Targets.IMAGE\n\n    @property\n    def targets(self) -&gt; dict[str, Callable[..., Any]]:\n        return {\"image\": self.apply, \"images\": self.apply_to_images}\n</code></pre>"},{"location":"api_reference/core/transforms_interface/#albumentations.core.transforms_interface.NoOp","title":"<code>class  NoOp</code> <code> </code>  [view source on GitHub]","text":"<p>Identity transform (does nothing).</p> <p>Targets</p> <p>image, mask, bboxes, keypoints, global_label</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/core/transforms_interface.py</code> Python<pre><code>class NoOp(DualTransform):\n    \"\"\"Identity transform (does nothing).\n\n    Targets:\n        image, mask, bboxes, keypoints, global_label\n    \"\"\"\n\n    _targets = (Targets.IMAGE, Targets.MASK, Targets.BBOXES, Targets.KEYPOINTS, Targets.GLOBAL_LABEL)\n\n    def apply_to_keypoint(self, keypoint: KeypointInternalType, **params: Any) -&gt; KeypointInternalType:\n        return keypoint\n\n    def apply_to_bbox(self, bbox: BoxInternalType, **params: Any) -&gt; BoxInternalType:\n        return bbox\n\n    def apply(self, img: np.ndarray, **params: Any) -&gt; np.ndarray:\n        return img\n\n    def apply_to_mask(self, mask: np.ndarray, **params: Any) -&gt; np.ndarray:\n        return mask\n\n    def apply_to_global_label(self, label: np.ndarray, **params: Any) -&gt; np.ndarray:\n        return label\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return ()\n</code></pre>"},{"location":"api_reference/core/transforms_interface/#albumentations.core.transforms_interface.NoOp.apply","title":"<code>apply (self, img, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/core/transforms_interface.py</code> Python<pre><code>def apply(self, img: np.ndarray, **params: Any) -&gt; np.ndarray:\n    return img\n</code></pre>"},{"location":"api_reference/core/transforms_interface/#albumentations.core.transforms_interface.NoOp.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/core/transforms_interface.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return ()\n</code></pre>"},{"location":"api_reference/pytorch/","title":"Index","text":"<ul> <li>Transforms (albumentations.pytorch.transforms)</li> </ul>"},{"location":"api_reference/pytorch/transforms/","title":"Transforms (pytorch.transforms)","text":""},{"location":"api_reference/pytorch/transforms/#albumentations.pytorch.transforms.ToTensorV2","title":"<code>class  ToTensorV2</code> <code>     (transpose_mask=False, p=1.0, always_apply=None)                 </code>  [view source on GitHub]","text":"<p>Converts images/masks to PyTorch Tensors, inheriting from BasicTransform. Supports images in numpy <code>HWC</code> format and converts them to PyTorch <code>CHW</code> format. If the image is in <code>HW</code> format, it will be converted to PyTorch <code>HW</code>.</p> <p>Attributes:</p> Name Type Description <code>transpose_mask</code> <code>bool</code> <p>If True, transposes 3D input mask dimensions from <code>[height, width, num_channels]</code> to <code>[num_channels, height, width]</code>.</p> <code>always_apply</code> <code>bool</code> <p>Deprecated. Default: None.</p> <code>p</code> <code>float</code> <p>Probability of applying the transform. Default: 1.0.</p> <p>Interactive Tool Available!</p> <p> Explore this transform visually and adjust parameters interactively using this tool: </p> <p> Open Tool </p> Source code in <code>albumentations/pytorch/transforms.py</code> Python<pre><code>class ToTensorV2(BasicTransform):\n    \"\"\"Converts images/masks to PyTorch Tensors, inheriting from BasicTransform. Supports images in numpy `HWC` format\n    and converts them to PyTorch `CHW` format. If the image is in `HW` format, it will be converted to PyTorch `HW`.\n\n    Attributes:\n        transpose_mask (bool): If True, transposes 3D input mask dimensions from `[height, width, num_channels]` to\n            `[num_channels, height, width]`.\n        always_apply (bool): Deprecated. Default: None.\n        p (float): Probability of applying the transform. Default: 1.0.\n\n    \"\"\"\n\n    _targets = (Targets.IMAGE, Targets.MASK)\n\n    def __init__(self, transpose_mask: bool = False, p: float = 1.0, always_apply: bool | None = None):\n        super().__init__(p=p, always_apply=always_apply)\n        self.transpose_mask = transpose_mask\n\n    @property\n    def targets(self) -&gt; dict[str, Any]:\n        return {\"image\": self.apply, \"mask\": self.apply_to_mask, \"masks\": self.apply_to_masks}\n\n    def apply(self, img: np.ndarray, **params: Any) -&gt; torch.Tensor:\n        if len(img.shape) not in [2, 3]:\n            msg = \"Albumentations only supports images in HW or HWC format\"\n            raise ValueError(msg)\n\n        if len(img.shape) == MONO_CHANNEL_DIMENSIONS:\n            img = np.expand_dims(img, 2)\n\n        return torch.from_numpy(img.transpose(2, 0, 1))\n\n    def apply_to_mask(self, mask: np.ndarray, **params: Any) -&gt; torch.Tensor:\n        if self.transpose_mask and mask.ndim == NUM_MULTI_CHANNEL_DIMENSIONS:\n            mask = mask.transpose(2, 0, 1)\n        return torch.from_numpy(mask)\n\n    def apply_to_masks(self, masks: list[np.ndarray], **params: Any) -&gt; list[torch.Tensor]:\n        return [self.apply_to_mask(mask, **params) for mask in masks]\n\n    def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n        return (\"transpose_mask\",)\n</code></pre>"},{"location":"api_reference/pytorch/transforms/#albumentations.pytorch.transforms.ToTensorV2.__init__","title":"<code>__init__ (self, transpose_mask=False, p=1.0, always_apply=None)</code>  <code>special</code>","text":"<p>Initialize self.  See help(type(self)) for accurate signature.</p> Source code in <code>albumentations/pytorch/transforms.py</code> Python<pre><code>def __init__(self, transpose_mask: bool = False, p: float = 1.0, always_apply: bool | None = None):\n    super().__init__(p=p, always_apply=always_apply)\n    self.transpose_mask = transpose_mask\n</code></pre>"},{"location":"api_reference/pytorch/transforms/#albumentations.pytorch.transforms.ToTensorV2.apply","title":"<code>apply (self, img, **params)</code>","text":"<p>Apply transform on image.</p> Source code in <code>albumentations/pytorch/transforms.py</code> Python<pre><code>def apply(self, img: np.ndarray, **params: Any) -&gt; torch.Tensor:\n    if len(img.shape) not in [2, 3]:\n        msg = \"Albumentations only supports images in HW or HWC format\"\n        raise ValueError(msg)\n\n    if len(img.shape) == MONO_CHANNEL_DIMENSIONS:\n        img = np.expand_dims(img, 2)\n\n    return torch.from_numpy(img.transpose(2, 0, 1))\n</code></pre>"},{"location":"api_reference/pytorch/transforms/#albumentations.pytorch.transforms.ToTensorV2.get_transform_init_args_names","title":"<code>get_transform_init_args_names (self)</code>","text":"<p>Returns names of arguments that are used in init method of the transform.</p> Source code in <code>albumentations/pytorch/transforms.py</code> Python<pre><code>def get_transform_init_args_names(self) -&gt; tuple[str, ...]:\n    return (\"transpose_mask\",)\n</code></pre>"},{"location":"autoalbument/","title":"AutoAlbument Overview","text":"<p>AutoAlbument is an AutoML tool that learns image augmentation policies from data using the Faster AutoAugment algorithm. It relieves the user from manually selecting augmentations and tuning their parameters. AutoAlbument provides a complete ready-to-use configuration for an augmentation pipeline.</p> <p>AutoAlbument supports image classification and semantic segmentation tasks. The library requires Python 3.6 or higher.</p> <p>The source code and issue tracker are available at https://github.com/albumentations-team/autoalbument</p> <p>Table of contents:</p> <ul> <li>AutoAlbument introduction and core concepts</li> <li>Installation</li> <li>Benchmarks and a comparison with baseline augmentation strategies</li> <li>How to use AutoAlbument</li> <li>How to use an AutoAlbument Docker image</li> <li>How to use a custom classification or semantic segmentation model</li> <li>Metrics and their meaning</li> <li>Tuning parameters</li> <li>Examples</li> <li>Search algorithms</li> <li>FAQ</li> </ul>"},{"location":"autoalbument/benchmarks/","title":"Benchmarks and a comparison with baseline augmentation strategies","text":"<p>Here is a comparison between a baseline augmentation strategy and an augmentation policy discovered by AutoAlbument for different classification and semantic segmentation tasks. You can read more about these benchmarks in the autoalbument-benchmarks repository.</p>"},{"location":"autoalbument/benchmarks/#classification","title":"Classification","text":"Dataset Baseline Top-1 Accuracy AutoAlbument Top-1 Accuracy CIFAR10 91.79 96.02 SVHN 98.31 98.48 ImageNet 73.27 75.17"},{"location":"autoalbument/benchmarks/#semantic-segmentation","title":"Semantic segmentation","text":"Dataset Baseline mIOU AutoAlbument mIOU Pascal VOC 73.34 75.55 Cityscapes 79.47 79.92"},{"location":"autoalbument/custom_model/","title":"How to use a custom classification or semantic segmentation model","text":"<p>By default AutoAlbument uses <code>pytorch-image-models</code> for classification and <code>segmentation_models.pytorch</code> for semantic segmentation. You can use any model from these packages by providing an appropriate model name.</p> <p>However, you can also use a custom model with AutoAlbument. To do so, you need to define a Discriminator model. This Discriminator model should have two outputs.</p> <ul> <li> <p>The first output should provide a prediction for a classification or semantic segmentation task. For classification, it should output a tensor with a shape <code>[batch_size, num_classes]</code> with logits. For semantic segmentation, it should output a tensor with the shape <code>[batch_size, num_classes, height, width]</code> with logits.</p> </li> <li> <p>The second (auxiliary) output should return a tensor with the shape <code>[batch_size]</code> that contains logits for Discriminator's predictions (whether Discriminator thinks that an image wasn't or was augmented).</p> </li> </ul> <p>To create such a model, you need to subclass the <code>autoalbument.faster_autoaugment.models.BaseDiscriminator</code> class and implement the <code>forward</code> method. This method should take a batch of images, that is, a tensor with the shape <code>[batch_size, num_channels, height, width]</code>. It should return a tuple that contains tensors from the two outputs described above.</p> <p>As an example, take a look at how default classification and semantic segmentation models are defined in AutoAlbument - https://github.com/albumentations-team/autoalbument/blob/master/autoalbument/faster_autoaugment/models.py or explore an example of a custom model for the CIFAR10 dataset.</p> <p>Next, you need to specify this custom model in <code>config.yaml</code>, an  AutoAlbument config file. AutoAlbument uses the <code>instantiate</code> function from Hydra to instantiate an object. You need to set the <code>_target_</code> config variable in the <code>classification_model</code> or <code>semantic_segmentation_model</code> section, depending on the task. In this config variable, you need to provide a path to a class with the model. This path should be located inside PYTHONPATH, so Hydra could correctly use it. The simplest way is to define your model in a file such as <code>model.py</code> and place this file in the same directory with <code>dataset.py</code> and <code>search.yaml</code> because this directory is automatically added to PYTHONPATH. Next, you could define <code>_target_</code> such as <code>_target_: model.MyClassificationModel</code>.</p> <p>Take a look at the CIFAR10 example config that uses a custom model defined in model.py as a starting point for defining a custom model.</p>"},{"location":"autoalbument/docker/","title":"How to use an AutoAlbument Docker image","text":"<p>You can run AutoAlbument from a Docker image. The <code>ghcr.io/albumentations-team/autoalbument:latest</code> Docker image contains the latest release version of AutoAlbument.</p> <p>You can also use an image that contains a specific version of AutoAlbument. In that case, you need to use the AutoAlbument version as a tag for a Docker image, e.g., the <code>ghcr.io/albumentations-team/autoalbument:0.3.0</code> image contains AutoAlbument 0.3.0.</p> <p>The latest AutoAlbument image is based on the <code>pytorch/pytorch:1.7.0-cuda11.0-cudnn8-runtime</code> image.</p> <p>When you run a Docker container with AutoAlbument, you need to mount a config directory (a directory containing <code>dataset.py</code> and <code>search.yaml</code> files) and other required directories, such as a directory that contains training data.</p> <p>Here is an example command that runs a Docker container that will search for CIFAR10 augmentation policies.</p> <p><code>docker run -it --rm --gpus all --ipc=host -v ~/projects/autoalbument/examples/cifar10:/config -v ~/data:/home/autoalbument/data -u $(id -u ${USER}):$(id -g ${USER}) ghcr.io/albumentations-team/autoalbument:latest</code></p> <p>Let's take a look at the arguments:</p> <ul> <li><code>--it</code>. Tell Docker that you run an interactive process. Read more in the Docker documentation.</li> <li><code>--rm</code>. Automatically clean up a container when it exits. Read more in the Docker documentation.</li> <li><code>--gpus all</code>. Specify GPUs to use. Read more in the Docker documentation.</li> <li><code>--ipc=host</code>. Increase shared memory size for PyTorch DataLoader. Read more in the PyTorch documentation.</li> <li><code>-v ~/projects/autoalbument/examples/cifar10:/config</code>. Mounts the <code>~/projects/autoalbument/examples/cifar10</code> directory from the host to the <code>/config</code> directory into the container. This example assumes that you have the AutoAlbument repository in the <code>~/projects/autoalbument/</code> directory. Generally speaking, you need to mount a directory containing <code>dataset.py</code> and <code>search.yaml</code> into the <code>/config</code> directory in a container.</li> <li><code>-v ~/data:/home/autoalbument/data</code>. Mounts the directory <code>~/data</code> that contains the CIFAR10 dataset into the <code>/home/autoalbument/data</code> directory. You can mount a host directory with a dataset into any container directory, but you need to specify config parameters accordingly. In this example, we mount the directory into <code>/home/autoalbument/data</code> because we set this directory (<code>~/data/cifar10</code>) in the config as a root directory for the dataset. Note that Docker doesn't support tilde expansion for the HOME directory, so we explicitly name HOME directory as <code>/home/autoalbument</code> because <code>autoalbument</code> is a default user inside the container.</li> <li><code>-u $(id -u ${USER}):$(id -g ${USER})</code>. We use that command to tell Docker to use the host's user ID to run code inside a container. We need this command because AutoAlbument will produce artifacts in the config directory (such as augmentation configs and logs). We need that the host user owns those files (and not <code>root</code>, for example) so you can access them afterward.</li> <li><code>ghcr.io/albumentations-team/autoalbument:latest</code> is the Docker image's name. <code>latest</code> is a tag for the latest stable release. Alternatively, you can use a tag that specifies an AutoAlbument version, e.g., <code>ghcr.io/albumentations-team/autoalbument:0.3.0</code>.</li> </ul>"},{"location":"autoalbument/faq/","title":"FAQ","text":""},{"location":"autoalbument/faq/#search-takes-a-lot-of-time-how-can-i-speed-it-up","title":"Search takes a lot of time. How can I speed it up?","text":"<p>Instead of a full training dataset, you can use a reduced version to search for augmentation policies. For example, the authors of Faster AutoAugment used 6000 images from the 120 selected classes to find augmentation policies for ImageNet (while the full dataset for ILSVRC contains 1.2 million images and 1000 classes).</p>"},{"location":"autoalbument/how_to_use/","title":"How to use AutoAlbument","text":"<ol> <li>You need to create a configuration file with AutoAlbument parameters and a Python file that implements a custom PyTorch Dataset for your data. Next, you need to pass those files to AutoAlbument.</li> <li>AutoAlbument will use Generative Adversarial Network to discover augmentation policies and then create a file containing those policies.</li> <li>Finally, you can use Albumentations to load augmentation policies from the file and utilize them in your computer vision pipeline.</li> </ol>"},{"location":"autoalbument/how_to_use/#step-1-create-a-configuration-file-and-a-custom-pytorch-dataset-for-your-data","title":"Step 1. Create a configuration file and a custom PyTorch Dataset for your data","text":""},{"location":"autoalbument/how_to_use/#a-create-a-directory-with-configuration-files","title":"a. Create a directory with configuration files","text":"<p>Run <code>autoalbument-create --config-dir &lt;/path/to/directory&gt; --task &lt;deep_learning_task&gt; --num-classes &lt;num_classes&gt;</code>, e.g. <code>autoalbument-create --config-dir ~/experiments/autoalbument-search-cifar10 --task classification --num-classes 10</code>.  - A value for the <code>--config-dir</code> option should contain a path to the directory. AutoAlbument will create this directory and put two files into it: <code>dataset.py</code> and <code>search.yaml</code> (more on them later).   - A value for the <code>--task</code> option should contain the name of a deep learning task. Supported values are <code>classification</code> and <code>semantic_segmentation</code>.  - A value for the <code>--num-classes</code> option should contain the number of distinct classes in the classification or segmentation dataset.</p> <p>By default, AutoAlbument creates a <code>search.yaml</code> file that contains only most important configuration parameters. To explore all available parameters you can create a config file that contains them all by providing the <code>--generate-full-config</code> argument, e.g. <code>autoalbument-create --config-dir ~/experiments/autoalbument-search-cifar10 --task classification --num-classes 10 --generate-full-config</code></p>"},{"location":"autoalbument/how_to_use/#b-add-implementation-for-__len__-and-__getitem__-methods-in-datasetpy","title":"b. Add implementation for <code>__len__</code> and <code>__getitem__</code> methods in <code>dataset.py</code>","text":"<p>The <code>dataset.py</code> file created at step 1 by <code>autoalbument-create</code> contains stubs for implementing a PyTorch dataset (you can read more about creating custom PyTorch datasets here). You need to add implementation for <code>__len__</code> and <code>__getitem__</code> methods (and optionally add the initialization logic if required).</p> <p>A dataset for a classification task should return an image and a class label. A dataset for a segmentation task should return an image and an associated mask.</p>"},{"location":"autoalbument/how_to_use/#c-optional-adjust-search-parameters-in-searchyaml","title":"c. [Optional] Adjust search parameters in <code>search.yaml</code>","text":"<p>You may want to change the parameters that AutoAlbument will use to search for augmentation policies. To do this, you need to edit the <code>search.yaml</code> file created by <code>autoalbument-create</code> at step 1. Each configuration parameter contains a comment that describes the meaning of the setting. Please refer to the  \"Tuning the search parameters\" section that includes a description of the most critical parameters.</p> <p><code>search.yaml</code> is a Hydra config file. You can use all Hydra features inside it.</p>"},{"location":"autoalbument/how_to_use/#step-2-use-autoalbument-to-search-for-augmentation-policies","title":"Step 2. Use AutoAlbument to search for augmentation policies.","text":"<p>To search for augmentation policies, run <code>autoalbument-search --config-dir &lt;/path/to/directory&gt;</code>, e.g. <code>autoalbument-search --config-dir ~/experiments/autoalbument-search-cifar10</code>. The value of <code>--config-dir</code> should be the same value that was passed to <code>autoalbument-create</code> at step 1.</p> <p><code>autoalbument-search</code> will create a directory with output files (by default the path of the directory will be <code>&lt;config_dir&gt;/outputs/&lt;current_date&gt;/&lt;current_time&gt;</code>, but you can customize it in search.yaml).  The <code>policy</code> subdirectory will contain JSON files with policies found at each search phase's epoch.</p> <p><code>autoalbument-search</code> is a command wrapped with the <code>@hydra.main</code> decorator from Hydra. You can use all Hydra features when calling this command.</p> <p>AutoAlbument uses PyTorch to search for augmentation policies. You can speed up the search by using a CUDA-capable GPU.</p>"},{"location":"autoalbument/how_to_use/#step-3-use-albumentations-to-load-augmentation-policies-and-utilize-them-in-your-training-pipeline","title":"Step 3. Use Albumentations to load augmentation policies and utilize them in your training pipeline.","text":"<p>AutoAlbument produces a JSON file that contains a configuration for an augmentation pipeline. You can load that JSON file with Albumentations:</p> Text Only<pre><code>import albumentations as A\ntransform = A.load(\"/path/to/policy.json\")\n</code></pre> <p>Then you can use the created augmentation pipeline to augment the input data.</p> <p>For example, to augment an image for a classification task:</p> Text Only<pre><code>transformed = transform(image=image)\ntransformed_image = transformed[\"image\"]\n</code></pre> <p>To augment an image and a mask for a semantic segmentation task: Text Only<pre><code>transformed = transform(image=image, mask=mask)\ntransformed_image = transformed[\"image\"]\ntransformed_mask = transformed[\"mask\"]\n</code></pre></p>"},{"location":"autoalbument/how_to_use/#additional-resources","title":"Additional resources","text":"<ul> <li> <p>You can read more about the most important configuration parameters for AutoAlbument in Tuning the search parameters.</p> </li> <li> <p>To see examples of configuration files and custom PyTorch Datasets, please refer to Examples</p> </li> <li> <p>You can read more about using Albumentations for augmentation in those articles Image augmentation for classification, Mask augmentation for segmentation.</p> </li> <li> <p>Refer to this section of the documentation to get examples of how to use Albumentations with PyTorch and TensorFlow 2.</p> </li> </ul>"},{"location":"autoalbument/installation/","title":"Installation","text":"<p>AutoAlbument requires Python 3.6 or higher.</p>"},{"location":"autoalbument/installation/#pypi","title":"PyPI","text":"<p>To install the latest stable version from PyPI:</p> <p><code>pip install -U autoalbument</code></p>"},{"location":"autoalbument/installation/#github","title":"GitHub","text":"<p>To install the latest version from GitHub:</p> <p><code>pip install -U git+https://github.com/albumentations-team/autoalbument</code></p>"},{"location":"autoalbument/introduction/","title":"AutoAlbument introduction and core concepts","text":""},{"location":"autoalbument/introduction/#what-is-autoalbument","title":"What is AutoAlbument","text":"<p>AutoAlbument is a tool that automatically searches for the best augmentation policies for your data.</p> <p>Under the hood, it uses the Faster AutoAugment algorithm. Briefly speaking, the idea is to use a GAN-like architecture in which Generator applies augmentation to some input images, and Discriminator must determine whether an image was or wasn't augmented. This process helps to find augmentation policies that will produce images similar to the original images.</p>"},{"location":"autoalbument/introduction/#how-to-use-autoalbument","title":"How to use AutoAlbument","text":"<p>To use AutoAlbument, you need to define two things: a PyTorch Dataset for your data and configuration parameters for AutoAlbument. You can read the detailed instruction in the How to use AutoAlbument article.</p> <p>Internally AutoAlbument uses PyTorch Lightning for training a GAN and Hydra for handling configuration parameters.</p> <p>Here are a few things about AutoAlbument and Hydra.</p>"},{"location":"autoalbument/introduction/#hydra","title":"Hydra","text":"<p>The main internal configuration file is located at autoalbument/cli/conf/config.yaml</p> <p>Here is its content:</p> Text Only<pre><code>defaults:\n - _version\n - task\n - policy_model: default\n - classification_model: default\n - semantic_segmentation_model: default\n - data: default\n - searcher: default\n - trainer: default\n - optim: default\n - callbacks: default\n - logger: default\n - hydra: default\n - seed\n - search\n</code></pre> <p>Basically, it includes a bunch of config files with default values. Those config files are split into sets of closely related parameters such as model parameters or optimizer parameters. All default config files are located in their respective directories inside autoalbument/cli/conf</p> <p>The main config file also includes the <code>search.yaml</code> file, which you will use for overriding default parameters for your specific dataset and task (you can read more about creating the <code>search.yaml</code> file with <code>autoalbument-create</code> in How to use AutoAlbument)</p> <p>To allow great flexibility, AutoAlbument relies heavily on the <code>instantiate</code> function from Hydra. This function allows to define a path to a Python class in a YAML config (using the <code>_target_</code> parameter) along with arguments to that class, and Hydra will create an instance of this class with the provided arguments.</p> <p>As a practice example, if a config contains a definition like this:</p> Text Only<pre><code>_target_: autoalbument.faster_autoaugment.models.ClassificationModel\nnum_classes: 10\narchitecture: resnet18\npretrained: False\n</code></pre> <p>AutoAlbument will translate it approximately to the following call:</p> Text Only<pre><code>from autoalbument.faster_autoaugment.models import ClassificationModel\n\nmodel = ClassificationModel(num_classes=10, architecture='resnet18', pretrained=False)\n</code></pre> <p>By relying on this feature, AutoAlbument allows customizing its behavior without changing the library's internal code.</p>"},{"location":"autoalbument/introduction/#pytorch-lightning","title":"PyTorch Lightning","text":"<p>AutoAlbument relies on PyTorch Lightning to train a GAN. In AutoAlbument configs, you can configure PyTorch Lightning by passing the appropriate arguments to Trainer through the <code>trainer</code> config or defining a list of Callbacks through the <code>callbacks</code> config.</p>"},{"location":"autoalbument/metrics/","title":"Metrics and their meaning","text":"<p>During the search phase, AutoAlbument outputs four metrics: <code>loss</code>, <code>d_loss</code>, <code>a_loss</code>, and <code>Average Parameter Change</code> (at the end of an epoch).</p>"},{"location":"autoalbument/metrics/#a_loss","title":"a_loss","text":"<p><code>a_loss</code> is a loss for the policy network (or Generator in terms of GAN), which applies augmentations to input images.</p>"},{"location":"autoalbument/metrics/#d_loss","title":"d_loss","text":"<p><code>d_loss</code> is a loss for the Discriminator, the network that tries to guess whether the input image is an augmented or non-augmented one.</p>"},{"location":"autoalbument/metrics/#loss","title":"loss","text":"<p><code>loss</code> is a task-specific loss (<code>CrossEntropyLoss</code> for classification, <code>BCEWithLogitsLoss</code> for semantic segmentation) that acts as a regularizer and prevents the policy network from applying such augmentations that will make an object with class A looks like an object with class B.</p>"},{"location":"autoalbument/metrics/#average-parameter-change","title":"Average Parameter Change","text":"<p><code>Average Parameter Change</code> is a difference between magnitudes of augmentation parameters multiplied by their probabilities at the end of an epoch and the same parameters at the beginning of the epoch. The metric is calculated using the following formula:</p> <p></p> <ul> <li><code>m'</code>  and <code>m</code> are magnitude values for the i-th augmentation at the end and the beginning of the epoch, respectively.</li> <li><code>p'</code>  and <code>p</code> are probability values for the i-th augmentation at the end and the beginning of the epoch, respectively.</li> </ul> <p>The intuition behind this metric is that at the beginning, augmentation parameters are initialized at random, so they are now optimal and prone to heavy change at each epoch. After some time, these parameters should begin to converge, and they should change less at each epoch.</p>"},{"location":"autoalbument/metrics/#examples-for-metric-values","title":"Examples for metric values","text":"<p>Below are TensorBoard logs for AutoAlbument on different datasets. The search was performed using AutoAlbument configs from the examples directory.</p> <ul> <li>CIFAR10</li> <li>SVHN</li> <li>ImageNet</li> <li>Pascal VOC</li> <li>Cityscapes</li> </ul> <p>As you see, in all these charts, <code>loss</code> is slightly decreasing at each epoch, and <code>a_loss</code> or <code>d_loss</code> could either decrease or increase. <code>Average Parameter Change</code> is usually large at first epochs, but then it starts to decrease. As a rule of thumb, to decide whether you should stop AutoAlbument search and use the resulting policy, you should check that <code>Average Parameter Change</code> is stopped decreasing and started to oscillate, wait for a few more epochs, and use the found policy from that epoch.</p> <p>In autoalbument-benchmaks, we use AutoAlbument policies produced by the last epoch on these charts.</p>"},{"location":"autoalbument/search_algorithms/","title":"Search algorithms","text":"<p>AutoAlbument uses the following algorithms to search for augmentation policies.</p>"},{"location":"autoalbument/search_algorithms/#faster-autoaugment","title":"Faster AutoAugment","text":"<p>\"Faster AutoAugment: Learning Augmentation Strategies using Backpropagation\" by Ryuichiro Hataya, Jan Zdenek, Kazuki Yoshizoe, and Hideki Nakayama. Paper | Original implementation</p>"},{"location":"autoalbument/tuning_parameters/","title":"Tuning the search parameters","text":"<p>The <code>search.yaml</code> file contains parameters for the search of augmentation policies. Here is an example <code>search.yaml</code> for image classification on the CIFAR-10 dataset, and here is an example <code>search.yaml</code> for semantic segmentation on the Pascal VOC dataset.</p>"},{"location":"autoalbument/tuning_parameters/#task-specific-model","title":"Task-specific model","text":"<p>A task-specific model is a model that classifies images for a classification task or outputs masks for a semantic segmentation task. Settings for a task-specific model are defined by either <code>classification_model</code> or <code>semantic_segmentation_model</code> depending on a selected task. Ideally, you should select the same model (the same architecture and the same pretrained weights) that you will use in an actual task. AutoAlbument uses models from PyTorch Image Models and Segmentation models packages for classification and semantic segmentation respectively.</p>"},{"location":"autoalbument/tuning_parameters/#base-pytorch-parameters","title":"Base PyTorch parameters.","text":"<p>You may want to adjust the following parameters for a PyTorch pipeline:</p> <ul> <li><code>data.dataloader</code> parameters such as batch_size and <code>num_workers</code></li> <li>Number of epochs to search for best augmentation policies in <code>optim.epochs</code>.</li> <li>Learning rate for optimizers in <code>optim.main.lr</code> and <code>optim.policy.lr</code>.</li> </ul>"},{"location":"autoalbument/tuning_parameters/#parameters-for-the-augmentations-search","title":"Parameters for the augmentations search.","text":"<p>Those parameters are defined in <code>policy_model</code>. You may want to tune the following ones:</p> <ul> <li> <p><code>num_sub_policies</code> - number of distinct augmentation sub-policies. A random sub-policy is selected in each iteration, and that sub-policy is applied to input data. The larger number of sub-policies will produce a more diverse set of augmentations. On the other side, the more sub-policies you have, the more time and data you need to tune those sub-policies correctly.</p> </li> <li> <p><code>num_chunks</code> controls the balance between speed and diversity of augmentations in a search phase. Each batch is split-up into <code>num_chunks</code> chunks, and then a random sub-policy is applied to each chunk separately. The larger the value of <code>num_chunks</code> helps to learn augmentation policies better but simultaneously increases the searching time. Authors of FasterAutoAugment used such values for <code>num_chunks</code> that each chunk consisted of 8 to 16 images.</p> </li> <li> <p><code>operation_count</code> - the number of augmentation operations that will be applied to each input data instance. For example, <code>operation_count: 1</code> means that only one operation will be applied to an input image/mask, and <code>operation_count: 4</code> means that four sequential operations will be applied to each input image/mask. The larger number of operations produces a more diverse set of augmentations but simultaneously increases the searching time.</p> </li> </ul>"},{"location":"autoalbument/tuning_parameters/#preprocessing-transforms","title":"Preprocessing transforms","text":"<p>If images have different sizes or you want to train a model on image patches, you could define preprocessing transforms (such as Resizing, Cropping, and Padding) in <code>data.preprocessing</code>. Those transforms will always be applied to all input data. Found augmentation policies will also contain those preprocessing transforms.</p> <p>Note that it is crucial for Policy Model (a model that searches for augmentation parameters) to receive images of the same size that will be used during the training of an actual model. For some augmentations, parameters depend on input data's height and width (for example, hole sizes for the Cutout augmentation).</p>"},{"location":"autoalbument/examples/cifar10/","title":"Image classification on the CIFAR10 dataset","text":"<p>The following files are also available on GitHub - https://github.com/albumentations-team/autoalbument/tree/master/examples/cifar10</p>"},{"location":"autoalbument/examples/cifar10/#datasetpy","title":"dataset.py","text":"Python<pre><code>import cv2\nimport torchvision\n\ncv2.setNumThreads(0)\ncv2.ocl.setUseOpenCL(False)\n\n\nclass Cifar10SearchDataset(torchvision.datasets.CIFAR10):\n    def __init__(self, root=\"~/data/cifar10\", train=True, download=True, transform=None):\n        super().__init__(root=root, train=train, download=download, transform=transform)\n\n    def __getitem__(self, index):\n        image, label = self.data[index], self.targets[index]\n\n        if self.transform is not None:\n            transformed = self.transform(image=image)\n            image = transformed[\"image\"]\n\n        return image, label\n</code></pre>"},{"location":"autoalbument/examples/cifar10/#searchyaml","title":"search.yaml","text":"YAML<pre><code># @package _global_\n\n_version: 2  # An internal value that indicates a version of the config schema. This value is used by\n# `autoalbument-search` and `autoalbument-migrate` to upgrade the config to the latest version if necessary.\n# Please do not change it manually.\n\n\ntask: classification # Deep learning task. Should either be `classification` or `semantic_segmentation`.\n\n\npolicy_model:\n  # Settings for Policy Model that searches augmentation policies.\n\n  task_factor: 0.1\n  # Multiplier for classification loss of a model. Faster AutoAugment uses classification loss to prevent augmentations\n  # from transforming images of a particular class to another class. The authors of Faster AutoAugment use 0.1 as\n  # default value.\n\n  gp_factor: 10\n  # Multiplier for the gradient penalty for WGAN-GP training. 10 is the default value that was proposed in\n  # `Improved Training of Wasserstein GANs`.\n\n  temperature: 0.05\n  # Temperature for Relaxed Bernoulli distribution. The probability of applying a certain augmentation is sampled from\n  # Relaxed Bernoulli distribution (because Bernoulli distribution is not differentiable). With lower values of\n  # `temperature` Relaxed Bernoulli distribution behaves like Bernoulli distribution. In the paper, the authors\n  # of Faster AutoAugment used 0.05 as a default value for `temperature`.\n\n  num_sub_policies: 100\n  # Number of augmentation sub-policies. When an image passes through an augmentation pipeline, Faster AutoAugment\n  # randomly chooses one sub-policy and uses augmentations from that sub-policy to transform an input image. A larger\n  # number of sub-policies leads to a more diverse set of augmentations and better performance of a model trained on\n  # augmented images. However, an increase in the number of sub-policies leads to the exponential growth of a search\n  # space of augmentations, so you need more training data for Policy Model to find good augmentation policies.\n\n  num_chunks: 8\n  # Number of chunks in a batch. Faster AutoAugment splits each batch of images into `num_chunks` chunks. Then it\n  # applies the same sub-policy with the same parameters to each image in a chunk. This parameter controls the tradeoff\n  # between the speed of augmentation search and diversity of augmentations. Larger `num_chunks` values will lead to\n  # faster searching but less diverse set of augmentations. Note that this parameter is used only in the searching\n  # phase. When you train a model with found sub-policies, Albumentations will apply a distinct set of transformations\n  # to each image separately.\n\n  operation_count: 4\n  # Number of consecutive augmentations in each sub-policy. Faster AutoAugment will sequentially apply `operation_count`\n  # augmentations from a sub-policy to an image. Larger values of `operation_count` lead to better performance of\n  # a model trained on augmented images. Simultaneously, larger values of `operation_count` affect the speed of search\n  # and increase the searching time.\n\n\nclassification_model:\n  # Settings for Classification Model that is used for two purposes:\n  # 1. As a model that performs classification of input images.\n  # 2. As a Discriminator for Policy Model.\n\n  _target_: model.Cifar10ClassificationModel\n  # A custom classification model is used. This model is defined inside the `model.py` file which is located\n  # in the same directory with `search.yaml` and `dataset.py`.\n  #  # As an alternative, you could use a built-in AutoAlbument model using the following config:\n  #  #  _target_: autoalbument.faster_autoaugment.models.ClassificationModel\n  #\n  #  # Number of classes in the dataset. The dataset implementation should return an integer in the range\n  #  # [0, num_classes - 1] as a class label of an image.\n  #  num_classes: 10\n  #\n  #  # The architecture of Classification Model. AutoAlbument uses models from\n  #  # https://github.com/rwightman/pytorch-image-models/. Please refer to its documentation to get a list of available\n  #  # models - https://rwightman.github.io/pytorch-image-models/#list-models-with-pretrained-weights.\n  #  architecture: resnet18\n  #\n  #  # Boolean flag that indicates whether the selected model architecture should load pretrained weights or use randomly\n  #  # initialized weights.\n  #  pretrained: False\n\n\ndata:\n  dataset:\n    _target_: dataset.Cifar10SearchDataset\n    root: ~/data/cifar10\n    train: true\n    download: true\n  # Class for the PyTorch Dataset and arguments to it. AutoAlbument will create an object of this class using\n  # the `instantiate` method from Hydra - https://hydra.cc/docs/next/patterns/instantiate_objects/overview/.\n  #\n  # Note that the target class value in the `_target_` argument should be located inside PYTHONPATH so Hydra could\n  # find it. The directory with the config file is automatically added to PYTHONPATH, so the default value\n  # `dataset.SearchDataset` points to the class `SearchDataset` from the `dataset.py` file. This `dataset.py` file is\n  # located along with the `search.yaml` file in the same directory provided by `--config-dir`.\n  #\n  # As an alternative, you could provide a path to a Python file with the dataset using the `dataset_file` parameter\n  # instead of the `dataset` parameter. The Python file should contain the implementation of a PyTorch dataset for\n  # augmentation search. The dataset class should have named `SearchDataset`. The value in `dataset_file` could either\n  # be a relative or an absolute path ; in the case of a relative path, the path should be relative to this config\n  # file's location.\n  #\n  # - Example of a relative path:\n  # dataset_file: dataset.py\n  #\n  # - Example of an absolute path:\n  # dataset_file: /projects/pytorch/dataset.py\n  #\n\n  input_dtype: uint8\n  # The data type of input images. Two values are supported:\n  # - uint8. In that case, all input images should be NumPy arrays with the np.uint8 data type and values in the range\n  #   [0, 255].\n  # - float32. In that case, all input images should be NumPy arrays with the np.float32 data type and values in the\n  #   range [0.0, 1.0].\n\n  preprocessing: null\n  # A list of preprocessing augmentations that will be applied to each image before applying augmentations from\n  # a policy. A preprocessing augmentation should be defined as `key`: `value`, where `key` is the name of augmentation\n  # from Albumentations, and `value` is a dictionary with augmentation parameters. The found policy will also apply\n  # those preprocessing augmentations before applying the main augmentations.\n  #\n  # Here is an example of an augmentation pipeline that first pads an image to the size 512x512 pixels, then resizes\n  # the resulting image to the size 256x256 pixels and finally crops a random patch with the size 224x224 pixels.\n  #\n  #  preprocessing:\n  #    - PadIfNeeded:\n  #        min_height: 512\n  #        min_width: 512\n  #    - Resize:\n  #        height: 256\n  #        width: 256\n  #    - RandomCrop:\n  #        height: 224\n  #        width: 224\n  #\n\n  normalization:\n    mean: [0.4914, 0.4822, 0.4465]\n    std: [0.247, 0.243, 0.261]\n  # Normalization values for images. For each image, the search pipeline will subtract `mean` and divide by `std`.\n  # Normalization is applied after transforms defined in `preprocessing`. Note that regardless of `input_dtype`,\n  # the normalization function will always receive a `float32` input with values in the range [0.0, 1.0], so you should\n  # define `mean` and `std` values accordingly.\n\n  dataloader:\n    _target_: torch.utils.data.DataLoader\n    batch_size: 128\n    shuffle: true\n    num_workers: 8\n    pin_memory: true\n    drop_last: true\n  # Parameters for the PyTorch DataLoader. Please refer to the PyTorch documentation for the description of parameters -\n  # https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader.\n\n\noptim:\n  main:\n    _target_: torch.optim.Adam\n    lr: 1e-3\n    betas: [0, 0.999]\n  # Optimizer configuration for the main (either Classification or Semantic Segmentation) Model\n\n  policy:\n    _target_: torch.optim.Adam\n    lr: 1e-3\n    betas: [0, 0.999]\n  # Optimizer configuration for Policy Model\n\n\nseed: 42 # Random seed. If the value is not null, it will be passed to `seed_everything` -\n# https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.utilities.seed.html?highlight=seed_everything\n\n\nhydra:\n  run:\n    dir: ${config_dir:}/outputs/${now:%Y-%m-%d}/${now:%H-%M-%S}\n    # Path to the directory that will contain all outputs produced by the search algorithm. `${config_dir:}` contains\n    # path to the directory with the `search.yaml` config file. Please refer to the Hydra documentation for more\n    # information - https://hydra.cc/docs/configure_hydra/workdir.\n\n\ntrainer:\n  # Configuration for PyTorch Lightning Trainer. You can read more about Trainer and its arguments at\n  # https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html.\n\n  max_epochs: 40\n  # Number of epochs to search for augmentation parameters.\n  # More detailed description - https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html#max-epochs\n\n  benchmark: true\n  # If true enables cudnn.benchmark.\n  # More detailed description - https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html#benchmark\n\n  gpus: 1\n  # Number of GPUs to train on. Set to `0` or None` to use CPU for training.\n  # More detailed description - https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html#gpus\n</code></pre>"},{"location":"autoalbument/examples/cifar10/#modelpy","title":"model.py","text":"Python<pre><code>\"\"\"WideResNet code from https://github.com/xternalz/WideResNet-pytorch\"\"\"\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom autoalbument.faster_autoaugment.models import BaseDiscriminator\n\n\nclass BasicBlock(nn.Module):\n    def __init__(self, in_planes, out_planes, stride):\n        super(BasicBlock, self).__init__()\n        self.bn1 = nn.BatchNorm2d(in_planes)\n        self.relu1 = nn.ReLU(inplace=True)\n        self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_planes)\n        self.relu2 = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(out_planes, out_planes, kernel_size=3, stride=1, padding=1, bias=False)\n        self.equal_in_out = in_planes == out_planes\n        self.conv_shortcut = (\n            (not self.equal_in_out)\n            and nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, padding=0, bias=False)\n            or None\n        )\n\n    def forward(self, x):\n        if not self.equal_in_out:\n            x = self.relu1(self.bn1(x))\n        else:\n            out = self.relu1(self.bn1(x))\n        out = self.relu2(self.bn2(self.conv1(out if self.equal_in_out else x)))\n        out = self.conv2(out)\n        return torch.add(x if self.equal_in_out else self.conv_shortcut(x), out)\n\n\nclass NetworkBlock(nn.Module):\n    def __init__(self, nb_layers, in_planes, out_planes, block, stride):\n        super(NetworkBlock, self).__init__()\n        self.layer = self._make_layer(block, in_planes, out_planes, nb_layers, stride)\n\n    def _make_layer(self, block, in_planes, out_planes, nb_layers, stride):\n        layers = []\n        for i in range(int(nb_layers)):\n            layers.append(block(i == 0 and in_planes or out_planes, out_planes, i == 0 and stride or 1))\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.layer(x)\n\n\nclass WideResNet(nn.Module):\n    def __init__(self, depth, num_classes, widen_factor=1):\n        super(WideResNet, self).__init__()\n        n_channels = [16, 16 * widen_factor, 32 * widen_factor, 64 * widen_factor]\n        assert (depth - 4) % 6 == 0\n        n = (depth - 4) / 6\n        block = BasicBlock\n        # 1st conv before any network block\n        self.conv1 = nn.Conv2d(3, n_channels[0], kernel_size=3, stride=1, padding=1, bias=False)\n        # 1st block\n        self.block1 = NetworkBlock(n, n_channels[0], n_channels[1], block, 1)\n        # 2nd block\n        self.block2 = NetworkBlock(n, n_channels[1], n_channels[2], block, 2)\n        # 3rd block\n        self.block3 = NetworkBlock(n, n_channels[2], n_channels[3], block, 2)\n        # global average pooling and classifier\n        self.bn1 = nn.BatchNorm2d(n_channels[3])\n        self.relu = nn.ReLU(inplace=True)\n        self.fc = nn.Linear(n_channels[3], num_classes)\n        self.n_channels = n_channels[3]\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.Linear):\n                m.bias.data.zero_()\n\n    def forward_features(self, x):\n        x = self.conv1(x)\n        x = self.block1(x)\n        x = self.block2(x)\n        x = self.block3(x)\n        x = self.relu(self.bn1(x))\n        x = F.avg_pool2d(x, 8, 1, 0)\n        x = x.view(-1, self.n_channels)\n        return x\n\n    def forward_classifier(self, x):\n        return self.fc(x)\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = self.forward_classifier(x)\n        return x\n\n\ndef wide_resnet_28x10(num_classes):\n    return WideResNet(depth=28, widen_factor=10, num_classes=num_classes)\n\n\nclass Cifar10ClassificationModel(BaseDiscriminator):\n    def __init__(self, *args, **kwargs):\n        super().__init__()\n        self.base_model = wide_resnet_28x10(num_classes=10)\n        num_features = self.base_model.fc.in_features\n        self.discriminator = nn.Sequential(\n            nn.Linear(num_features, num_features), nn.ReLU(), nn.Linear(num_features, 1)\n        )\n\n    def forward(self, input):\n        x = self.base_model.forward_features(input)\n        return self.base_model.forward_classifier(x), self.discriminator(x).view(-1)\n</code></pre>"},{"location":"autoalbument/examples/cityscapes/","title":"Semantic segmentation on Cityscapes dataset","text":"<p>The following files are also available on GitHub - https://github.com/albumentations-team/autoalbument/tree/master/examples/cityscapes</p>"},{"location":"autoalbument/examples/cityscapes/#datasetpy","title":"dataset.py","text":"Python<pre><code>import cv2\nimport numpy as np\nimport torchvision\n\ncv2.setNumThreads(0)\ncv2.ocl.setUseOpenCL(False)\n\n\nclass CityscapesSearchDataset(torchvision.datasets.Cityscapes):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs, target_type=\"semantic\")\n        self.semantic_target_type_index = [i for i, t in enumerate(self.target_type) if t == \"semantic\"][0]\n        self.colormap = self._generate_colormap()\n\n    def _generate_colormap(self):\n        colormap = {}\n        for class_ in self.classes:\n            if class_.train_id in (-1, 255):\n                continue\n            colormap[class_.train_id] = class_.id\n        return colormap\n\n    def _convert_to_segmentation_mask(self, mask):\n        height, width = mask.shape[:2]\n        segmentation_mask = np.zeros((height, width, len(self.colormap)), dtype=np.float32)\n        for label_index, label in self.colormap.items():\n            segmentation_mask[:, :, label_index] = (mask == label).astype(float)\n        return segmentation_mask\n\n    def __getitem__(self, index):\n        image = cv2.imread(self.images[index])\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        mask = cv2.imread(self.targets[index][self.semantic_target_type_index], cv2.IMREAD_UNCHANGED)\n\n        mask = self._convert_to_segmentation_mask(mask)\n\n        if self.transform is not None:\n            transformed = self.transform(image=image, mask=mask)\n            image = transformed[\"image\"]\n            mask = transformed[\"mask\"]\n\n        return image, mask\n</code></pre>"},{"location":"autoalbument/examples/cityscapes/#searchyaml","title":"search.yaml","text":"YAML<pre><code># @package _global_\n\n_version: 2  # An internal value that indicates a version of the config schema. This value is used by\n# `autoalbument-search` and `autoalbument-migrate` to upgrade the config to the latest version if necessary.\n# Please do not change it manually.\n\ntask: semantic_segmentation # Deep learning task. Should either be `classification` or `semantic_segmentation`.\n\npolicy_model:\n  # Settings for Policy Model that searches augmentation policies.\n\n  task_factor: 0.1\n  # Multiplier for segmentation loss of a model. Faster AutoAugment uses segmentation loss to prevent augmentations\n  # from transforming images of a particular class to another class.\n\n  gp_factor: 10\n  # Multiplier for the gradient penalty for WGAN-GP training. 10 is the default value that was proposed in\n  # `Improved Training of Wasserstein GANs`.\n\n  temperature: 0.05\n  # Temperature for Relaxed Bernoulli distribution. The probability of applying a certain augmentation is sampled from\n  # Relaxed Bernoulli distribution (because Bernoulli distribution is not differentiable). With lower values of\n  # `temperature` Relaxed Bernoulli distribution behaves like Bernoulli distribution. In the paper, the authors\n  # of Faster AutoAugment used 0.05 as a default value for `temperature`.\n\n  num_sub_policies: 25\n  # Number of augmentation sub-policies. When an image passes through an augmentation pipeline, Faster AutoAugment\n  # randomly chooses one sub-policy and uses augmentations from that sub-policy to transform an input image. A larger\n  # number of sub-policies leads to a more diverse set of augmentations and better performance of a model trained on\n  # augmented images. However, an increase in the number of sub-policies leads to the exponential growth of a search\n  # space of augmentations, so you need more training data for Policy Model to find good augmentation policies.\n\n  num_chunks: 4\n  # Number of chunks in a batch. Faster AutoAugment splits each batch of images into `num_chunks` chunks. Then it\n  # applies the same sub-policy with the same parameters to each image in a chunk. This parameter controls the tradeoff\n  # between the speed of augmentation search and diversity of augmentations. Larger `num_chunks` values will lead to\n  # faster searching but less diverse set of augmentations. Note that this parameter is used only in the searching\n  # phase. When you train a model with found sub-policies, Albumentations will apply a distinct set of transformations\n  # to each image separately.\n\n  operation_count: 4\n  # Number of consecutive augmentations in each sub-policy. Faster AutoAugment will sequentially apply `operation_count`\n  # augmentations from a sub-policy to an image. Larger values of `operation_count` lead to better performance of\n  # a model trained on augmented images. Simultaneously, larger values of `operation_count` affect the speed of search\n  # and increase the searching time.\n\nsemantic_segmentation_model:\n  # Settings for Semantic Segmentation Model that is used for two purposes:\n  # 1. As a model that performs semantic segmentation of input images.\n  # 2. As a Discriminator for Policy Model.\n\n  _target_: autoalbument.faster_autoaugment.models.SemanticSegmentationModel\n  # By default, AutoAlbument uses an instance of `autoalbument.faster_autoaugment.models.SemanticSegmentationModel` as\n  # a semantic segmentation model.\n  # This model takes four parameters: `num_classes`, `architecture`, `encoder_architecture` and `pretrained`.\n\n  num_classes: 19\n  # The number of classes in the dataset. The dataset implementation should return a mask as a NumPy array with\n  # the shape [height, width, num_classes]. In a case of binary segmentation you can set `num_classes` to 1.\n\n  architecture: DeepLabV3Plus\n  # The architecture of Semantic Segmentation Model. AutoAlbument uses models from\n  # https://github.com/qubvel/segmentation_models.pytorch. Please refer to its documentation to get a list of available\n  # models - https://github.com/qubvel/segmentation_models.pytorch#models-.\n\n  encoder_architecture: resnet50\n  # The architecture of encoder in Semantic Segmentation Model. Please refer to Segmentation Models' documentation to\n  # get a list of available encoders - https://github.com/qubvel/segmentation_models.pytorch#encoders-\n\n  pretrained: true\n  # Either boolean flag or string with that indicates whether the selected encoder architecture should load pretrained\n  # weights or use randomly initialized weights.\n  # - In the case of boolean flag `true` means using pretrained weights from ImageNet and `false` means using randomly\n  #   initialized weights.\n  # - In the case of string the value should specify the name of the weights. For the list of available weights please\n  #   refer to https://github.com/qubvel/segmentation_models.pytorch#encoders-\n\ndata:\n  dataset:\n    _target_: dataset.CityscapesSearchDataset\n    root: ~/data/cityscapes/data\n    split: train\n  # Class for the PyTorch Dataset and arguments to it. AutoAlbument will create an object of this class using\n  # the `instantiate` method from Hydra - https://hydra.cc/docs/next/patterns/instantiate_objects/overview/.\n  #\n  # Note that the target class value in the `_target_` argument should be located inside PYTHONPATH so Hydra could\n  # find it. The directory with the config file is automatically added to PYTHONPATH, so the default value\n  # `dataset.SearchDataset` points to the class `SearchDataset` from the `dataset.py` file. This `dataset.py` file is\n  # located along with the `search.yaml` file in the same directory provided by `--config-dir`.\n  #\n  # As an alternative, you could provide a path to a Python file with the dataset using the `dataset_file` parameter\n  # instead of the `dataset` parameter. The Python file should contain the implementation of a PyTorch dataset for\n  # augmentation search. The dataset class should have named `SearchDataset`. The value in `dataset_file` could either\n  # be a relative or an absolute path ; in the case of a relative path, the path should be relative to this config\n  # file's location.\n  #\n  # - Example of a relative path:\n  # dataset_file: dataset.py\n  #\n  # - Example of an absolute path:\n  # dataset_file: /projects/pytorch/dataset.py\n  #\n\n  input_dtype: uint8\n  # The data type of input images. Two values are supported:\n  # - uint8. In that case, all input images should be NumPy arrays with the np.uint8 data type and values in the range\n  #   [0, 255].\n  # - float32. In that case, all input images should be NumPy arrays with the np.float32 data type and values in the\n  #   range [0.0, 1.0].\n\n  preprocessing:\n  - LongestMaxSize:\n      max_size: 256\n  - PadIfNeeded:\n      min_height: 256\n      min_width: 256\n      border_mode: 0\n      value: [0, 0, 0]\n  # A list of preprocessing augmentations that will be applied to each image before applying augmentations from\n  # a policy. A preprocessing augmentation should be defined as `key`: `value`, where `key` is the name of augmentation\n  # from Albumentations, and `value` is a dictionary with augmentation parameters. The found policy will also apply\n  # those preprocessing augmentations before applying the main augmentations.\n  #\n  # Here is an example of an augmentation pipeline that first pads an image to the size 512x512 pixels, then resizes\n  # the resulting image to the size 256x256 pixels and finally crops a random patch with the size 224x224 pixels.\n  #\n  #  preprocessing:\n  #    - PadIfNeeded:\n  #        min_height: 512\n  #        min_width: 512\n  #    - Resize:\n  #        height: 256\n  #        width: 256\n  #    - RandomCrop:\n  #        height: 224\n  #        width: 224\n  #\n\n  normalization:\n    mean: [0.485, 0.456, 0.406]\n    std: [0.229, 0.224, 0.225]\n  # Normalization values for images. For each image, the search pipeline will subtract `mean` and divide by `std`.\n  # Normalization is applied after transforms defined in `preprocessing`. Note that regardless of `input_dtype`,\n  # the normalization function will always receive a `float32` input with values in the range [0.0, 1.0], so you should\n  # define `mean` and `std` values accordingly. ImageNet normalization is used by default.\n\n  dataloader:\n    _target_: torch.utils.data.DataLoader\n    batch_size: 32\n    shuffle: true\n    num_workers: 8\n    pin_memory: true\n    drop_last: true\n  # Parameters for the PyTorch DataLoader. Please refer to the PyTorch documentation for the description of parameters -\n  # https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader.\n\noptim:\n  main:\n    _target_: torch.optim.Adam\n    lr: 1e-3\n    betas: [0, 0.999]\n  # Optimizer configuration for the main (either Classification or Semantic Segmentation) Model\n\n  policy:\n    _target_: torch.optim.Adam\n    lr: 1e-3\n    betas: [0, 0.999]\n  # Optimizer configuration for Policy Model\n\nseed: 42 # Random seed. If the value is not null, it will be passed to `seed_everything` -\n# https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.utilities.seed.html?highlight=seed_everything\n\nhydra:\n  run:\n    dir: ${config_dir:}/outputs/${now:%Y-%m-%d}/${now:%H-%M-%S}\n    # Path to the directory that will contain all outputs produced by the search algorithm. `${config_dir:}` contains\n    # path to the directory with the `search.yaml` config file. Please refer to the Hydra documentation for more\n    # information - https://hydra.cc/docs/configure_hydra/workdir.\n\ntrainer:\n  # Configuration for PyTorch Lightning Trainer. You can read more about Trainer and its arguments at\n  # https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html.\n  max_epochs: 50\n  # Number of epochs to search for augmentation parameters.\n  # More detailed description - https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html#max-epochs\n\n  benchmark: true\n  # If true enables cudnn.benchmark.\n  # More detailed description - https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html#benchmark\n\n  gpus: 1\n  # Number of GPUs to train on. Set to `0` or None` to use CPU for training.\n  # More detailed description - https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html#gpus\n</code></pre>"},{"location":"autoalbument/examples/imagenet/","title":"Image classification on the ImageNet dataset","text":"<p>The following files are also available on GitHub - https://github.com/albumentations-team/autoalbument/tree/master/examples/imagenet</p>"},{"location":"autoalbument/examples/imagenet/#datasetpy","title":"dataset.py","text":"Python<pre><code>import cv2\nimport torchvision\n\ncv2.setNumThreads(0)\ncv2.ocl.setUseOpenCL(False)\n\n\nclass ImageNetSearchDataset(torchvision.datasets.ImageNet):\n    def __getitem__(self, index):\n        path, label = self.samples[index]\n        image = cv2.imread(path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        if self.transform is not None:\n            transformed = self.transform(image=image)\n            image = transformed[\"image\"]\n\n        return image, label\n</code></pre>"},{"location":"autoalbument/examples/imagenet/#searchyaml","title":"search.yaml","text":"YAML<pre><code># @package _global_\n\n_version: 2  # An internal value that indicates a version of the config schema. This value is used by\n# `autoalbument-search` and `autoalbument-migrate` to upgrade the config to the latest version if necessary.\n# Please do not change it manually.\n\n\ntask: classification # Deep learning task. Should either be `classification` or `semantic_segmentation`.\n\n\npolicy_model:\n  # Settings for Policy Model that searches augmentation policies.\n\n  task_factor: 0.1\n  # Multiplier for classification loss of a model. Faster AutoAugment uses classification loss to prevent augmentations\n  # from transforming images of a particular class to another class. The authors of Faster AutoAugment use 0.1 as\n  # default value.\n\n  gp_factor: 10\n  # Multiplier for the gradient penalty for WGAN-GP training. 10 is the default value that was proposed in\n  # `Improved Training of Wasserstein GANs`.\n\n  temperature: 0.05\n  # Temperature for Relaxed Bernoulli distribution. The probability of applying a certain augmentation is sampled from\n  # Relaxed Bernoulli distribution (because Bernoulli distribution is not differentiable). With lower values of\n  # `temperature` Relaxed Bernoulli distribution behaves like Bernoulli distribution. In the paper, the authors\n  # of Faster AutoAugment used 0.05 as a default value for `temperature`.\n\n  num_sub_policies: 100\n  # Number of augmentation sub-policies. When an image passes through an augmentation pipeline, Faster AutoAugment\n  # randomly chooses one sub-policy and uses augmentations from that sub-policy to transform an input image. A larger\n  # number of sub-policies leads to a more diverse set of augmentations and better performance of a model trained on\n  # augmented images. However, an increase in the number of sub-policies leads to the exponential growth of a search\n  # space of augmentations, so you need more training data for Policy Model to find good augmentation policies.\n\n  num_chunks: 6\n  # Number of chunks in a batch. Faster AutoAugment splits each batch of images into `num_chunks` chunks. Then it\n  # applies the same sub-policy with the same parameters to each image in a chunk. This parameter controls the tradeoff\n  # between the speed of augmentation search and diversity of augmentations. Larger `num_chunks` values will lead to\n  # faster searching but less diverse set of augmentations. Note that this parameter is used only in the searching\n  # phase. When you train a model with found sub-policies, Albumentations will apply a distinct set of transformations\n  # to each image separately.\n\n  operation_count: 4\n  # Number of consecutive augmentations in each sub-policy. Faster AutoAugment will sequentially apply `operation_count`\n  # augmentations from a sub-policy to an image. Larger values of `operation_count` lead to better performance of\n  # a model trained on augmented images. Simultaneously, larger values of `operation_count` affect the speed of search\n  # and increase the searching time.\n\nclassification_model:\n  # Settings for Classification Model that is used for two purposes:\n  # 1. As a model that performs classification of input images.\n  # 2. As a Discriminator for Policy Model.\n\n  num_classes: 1000\n  # Number of classes in the dataset. The dataset implementation should return an integer in the range\n  # [0, num_classes - 1] as a class label of an image.\n\n  architecture: resnet50\n  # The architecture of Classification Model. AutoAlbument uses models from\n  # https://github.com/rwightman/pytorch-image-models/. Please refer to its documentation to get a list of available\n  # models - https://rwightman.github.io/pytorch-image-models/#list-models-with-pretrained-weights.\n\n  pretrained: false\n  # Boolean flag that indicates whether the selected model architecture should load pretrained weights or use randomly\n  # initialized weights.\n\ndata:\n  dataset:\n    _target_: dataset.ImageNetSearchDataset\n    root: ~/data/imagenet\n    split: train\n  # Class for the PyTorch Dataset and arguments to it. AutoAlbument will create an object of this class using\n  # the `instantiate` method from Hydra - https://hydra.cc/docs/next/patterns/instantiate_objects/overview/.\n  #\n  # Note that the target class value in the `_target_` argument should be located inside PYTHONPATH so Hydra could\n  # find it. The directory with the config file is automatically added to PYTHONPATH, so the default value\n  # `dataset.SearchDataset` points to the class `SearchDataset` from the `dataset.py` file. This `dataset.py` file is\n  # located along with the `search.yaml` file in the same directory provided by `--config-dir`.\n  #\n  # As an alternative, you could provide a path to a Python file with the dataset using the `dataset_file` parameter\n  # instead of the `dataset` parameter. The Python file should contain the implementation of a PyTorch dataset for\n  # augmentation search. The dataset class should have named `SearchDataset`. The value in `dataset_file` could either\n  # be a relative or an absolute path ; in the case of a relative path, the path should be relative to this config\n  # file's location.\n  #\n  # - Example of a relative path:\n  # dataset_file: dataset.py\n  #\n  # - Example of an absolute path:\n  # dataset_file: /projects/pytorch/dataset.py\n  #\n\n  input_dtype: uint8\n  # The data type of input images. Two values are supported:\n  # - uint8. In that case, all input images should be NumPy arrays with the np.uint8 data type and values in the range\n  #   [0, 255].\n  # - float32. In that case, all input images should be NumPy arrays with the np.float32 data type and values in the\n  #   range [0.0, 1.0].\n\n  preprocessing:\n  - Resize:\n      height: 256\n      width: 256\n  - RandomCrop:\n      height: 224\n      width: 224\n  # A list of preprocessing augmentations that will be applied to each image before applying augmentations from\n  # a policy. A preprocessing augmentation should be defined as `key`: `value`, where `key` is the name of augmentation\n  # from Albumentations, and `value` is a dictionary with augmentation parameters. The found policy will also apply\n  # those preprocessing augmentations before applying the main augmentations.\n  #\n  # Here is an example of an augmentation pipeline that first pads an image to the size 512x512 pixels, then resizes\n  # the resulting image to the size 256x256 pixels and finally crops a random patch with the size 224x224 pixels.\n  #\n  #  preprocessing:\n  #    - PadIfNeeded:\n  #        min_height: 512\n  #        min_width: 512\n  #    - Resize:\n  #        height: 256\n  #        width: 256\n  #    - RandomCrop:\n  #        height: 224\n  #        width: 224\n  #\n\n  normalization:\n    mean: [0.485, 0.456, 0.406]\n    std: [0.229, 0.224, 0.225]\n  # Normalization values for images. For each image, the search pipeline will subtract `mean` and divide by `std`.\n  # Normalization is applied after transforms defined in `preprocessing`. Note that regardless of `input_dtype`,\n  # the normalization function will always receive a `float32` input with values in the range [0.0, 1.0], so you should\n  # define `mean` and `std` values accordingly.\n\n  dataloader:\n    _target_: torch.utils.data.DataLoader\n    batch_size: 96\n    shuffle: true\n    num_workers: 8\n    pin_memory: true\n    drop_last: true\n  # Parameters for the PyTorch DataLoader. Please refer to the PyTorch documentation for the description of parameters -\n  # https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader.\n\noptim:\n  main:\n    _target_: torch.optim.Adam\n    lr: 1e-3\n    betas: [0, 0.999]\n  # Optimizer configuration for the main (either Classification or Semantic Segmentation) Model\n\n\n  policy:\n    _target_: torch.optim.Adam\n    lr: 1e-3\n    betas: [0, 0.999]\n  # Optimizer configuration for Policy Model\n\n\nseed: 42 # Random seed. If the value is not null, it will be passed to `seed_everything` -\n# https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.utilities.seed.html?highlight=seed_everything\n\nhydra:\n  run:\n    dir: ${config_dir:}/outputs/${now:%Y-%m-%d}/${now:%H-%M-%S}\n    # Path to the directory that will contain all outputs produced by the search algorithm. `${config_dir:}` contains\n    # path to the directory with the `search.yaml` config file. Please refer to the Hydra documentation for more\n    # information - https://hydra.cc/docs/configure_hydra/workdir.\n\ntrainer:\n  # Configuration for PyTorch Lightning Trainer. You can read more about Trainer and its arguments at\n  # https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html.\n  max_epochs: 1\n  # Number of epochs to search for augmentation parameters.\n  # More detailed description - https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html#max-epochs\n\n  benchmark: true\n  # If true enables cudnn.benchmark.\n  # More detailed description - https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html#benchmark\n\n  gpus: 1\n  # Number of GPUs to train on. Set to `0` or None` to use CPU for training.\n  # More detailed description - https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html#gpus\n</code></pre>"},{"location":"autoalbument/examples/list/","title":"List of examples","text":"<ul> <li>Image classification on the CIFAR10 dataset.</li> <li>Image classification on the SVHN dataset.</li> <li>Image classification on the ImageNet dataset.</li> <li>Semantic segmentation on the Pascal VOC dataset.</li> <li>Semantic segmentation on the Cityscapes dataset.</li> </ul> <p>To run the search with an example config:</p> Bash<pre><code>autoalbument-search --config-dir &lt;/path/to/directory_with_dataset.py_and_search.yaml&gt;\n</code></pre>"},{"location":"autoalbument/examples/pascal_voc/","title":"Semantic segmentation on the Pascal VOC dataset","text":"<p>The following files are also available on GitHub - https://github.com/albumentations-team/autoalbument/tree/master/examples/pascal_voc</p>"},{"location":"autoalbument/examples/pascal_voc/#datasetpy","title":"dataset.py","text":"Python<pre><code>import cv2\nimport numpy as np\nfrom torchvision.datasets import VOCSegmentation\n\ncv2.setNumThreads(0)\ncv2.ocl.setUseOpenCL(False)\n\n\nVOC_CLASSES = [\n    \"background\",\n    \"aeroplane\",\n    \"bicycle\",\n    \"bird\",\n    \"boat\",\n    \"bottle\",\n    \"bus\",\n    \"car\",\n    \"cat\",\n    \"chair\",\n    \"cow\",\n    \"diningtable\",\n    \"dog\",\n    \"horse\",\n    \"motorbike\",\n    \"person\",\n    \"potted plant\",\n    \"sheep\",\n    \"sofa\",\n    \"train\",\n    \"tv/monitor\",\n]\n\n\nVOC_COLORMAP = [\n    [0, 0, 0],\n    [128, 0, 0],\n    [0, 128, 0],\n    [128, 128, 0],\n    [0, 0, 128],\n    [128, 0, 128],\n    [0, 128, 128],\n    [128, 128, 128],\n    [64, 0, 0],\n    [192, 0, 0],\n    [64, 128, 0],\n    [192, 128, 0],\n    [64, 0, 128],\n    [192, 0, 128],\n    [64, 128, 128],\n    [192, 128, 128],\n    [0, 64, 0],\n    [128, 64, 0],\n    [0, 192, 0],\n    [128, 192, 0],\n    [0, 64, 128],\n]\n\n\nclass PascalVOCSearchDataset(VOCSegmentation):\n    def __init__(self, root=\"~/data/pascal_voc\", image_set=\"train\", download=True, transform=None):\n        super().__init__(root=root, image_set=image_set, download=download, transform=transform)\n\n    @staticmethod\n    def _convert_to_segmentation_mask(mask):\n        # This function converts a mask from the Pascal VOC format to the format required by AutoAlbument.\n        #\n        # Pascal VOC uses an RGB image to encode the segmentation mask for that image. RGB values of a pixel\n        # encode the pixel's class.\n        #\n        # AutoAlbument requires a segmentation mask to be a NumPy array with the shape [height, width, num_classes].\n        # Each channel in this mask should encode values for a single class. Pixel in a mask channel should have\n        # a value of 1.0 if the pixel of the image belongs to this class and 0.0 otherwise.\n        height, width = mask.shape[:2]\n        segmentation_mask = np.zeros((height, width, len(VOC_COLORMAP)), dtype=np.float32)\n        for label_index, label in enumerate(VOC_COLORMAP):\n            segmentation_mask[:, :, label_index] = np.all(mask == label, axis=-1).astype(float)\n        return segmentation_mask\n\n    def __getitem__(self, index):\n        image = cv2.imread(self.images[index])\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        mask = cv2.imread(self.masks[index])\n        mask = cv2.cvtColor(mask, cv2.COLOR_BGR2RGB)\n        mask = self._convert_to_segmentation_mask(mask)\n        if self.transform is not None:\n            transformed = self.transform(image=image, mask=mask)\n            image = transformed[\"image\"]\n            mask = transformed[\"mask\"]\n        return image, mask\n</code></pre>"},{"location":"autoalbument/examples/pascal_voc/#searchyaml","title":"search.yaml","text":"YAML<pre><code># @package _global_\n\n_version: 2  # An internal value that indicates a version of the config schema. This value is used by\n# `autoalbument-search` and `autoalbument-migrate` to upgrade the config to the latest version if necessary.\n# Please do not change it manually.\n\n\ntask: semantic_segmentation # Deep learning task. Should either be `classification` or `semantic_segmentation`.\n\n\npolicy_model:\n  # Settings for Policy Model that searches augmentation policies.\n\n  task_factor: 0.1\n  # Multiplier for segmentation loss of a model. Faster AutoAugment uses segmentation loss to prevent augmentations\n  # from transforming images of a particular class to another class.\n\n  gp_factor: 10\n  # Multiplier for the gradient penalty for WGAN-GP training. 10 is the default value that was proposed in\n  # `Improved Training of Wasserstein GANs`.\n\n  temperature: 0.05\n  # Temperature for Relaxed Bernoulli distribution. The probability of applying a certain augmentation is sampled from\n  # Relaxed Bernoulli distribution (because Bernoulli distribution is not differentiable). With lower values of\n  # `temperature` Relaxed Bernoulli distribution behaves like Bernoulli distribution. In the paper, the authors\n  # of Faster AutoAugment used 0.05 as a default value for `temperature`.\n\n  num_sub_policies: 25\n  # Number of augmentation sub-policies. When an image passes through an augmentation pipeline, Faster AutoAugment\n  # randomly chooses one sub-policy and uses augmentations from that sub-policy to transform an input image. A larger\n  # number of sub-policies leads to a more diverse set of augmentations and better performance of a model trained on\n  # augmented images. However, an increase in the number of sub-policies leads to the exponential growth of a search\n  # space of augmentations, so you need more training data for Policy Model to find good augmentation policies.\n\n  num_chunks: 4\n  # Number of chunks in a batch. Faster AutoAugment splits each batch of images into `num_chunks` chunks. Then it\n  # applies the same sub-policy with the same parameters to each image in a chunk. This parameter controls the tradeoff\n  # between the speed of augmentation search and diversity of augmentations. Larger `num_chunks` values will lead to\n  # faster searching but less diverse set of augmentations. Note that this parameter is used only in the searching\n  # phase. When you train a model with found sub-policies, Albumentations will apply a distinct set of transformations\n  # to each image separately.\n\n  operation_count: 4\n  # Number of consecutive augmentations in each sub-policy. Faster AutoAugment will sequentially apply `operation_count`\n  # augmentations from a sub-policy to an image. Larger values of `operation_count` lead to better performance of\n  # a model trained on augmented images. Simultaneously, larger values of `operation_count` affect the speed of search\n  # and increase the searching time.\n\nsemantic_segmentation_model:\n  # Settings for Semantic Segmentation Model that is used for two purposes:\n  # 1. As a model that performs semantic segmentation of input images.\n  # 2. As a Discriminator for Policy Model.\n\n  _target_: autoalbument.faster_autoaugment.models.SemanticSegmentationModel\n  # By default, AutoAlbument uses an instance of `autoalbument.faster_autoaugment.models.SemanticSegmentationModel` as\n  # a semantic segmentation model.\n  # This model takes four parameters: `num_classes`, `architecture`, `encoder_architecture` and `pretrained`.\n\n  num_classes: 21\n  # The number of classes in the dataset. The dataset implementation should return a mask as a NumPy array with\n  # the shape [height, width, num_classes]. In a case of binary segmentation you can set `num_classes` to 1.\n\n  architecture: DeepLabV3Plus\n  # The architecture of Semantic Segmentation Model. AutoAlbument uses models from\n  # https://github.com/qubvel/segmentation_models.pytorch. Please refer to its documentation to get a list of available\n  # models - https://github.com/qubvel/segmentation_models.pytorch#models-.\n\n  encoder_architecture: resnet50\n  # The architecture of encoder in Semantic Segmentation Model. Please refer to Segmentation Models' documentation to\n  # get a list of available encoders - https://github.com/qubvel/segmentation_models.pytorch#encoders-\n\n  pretrained: true\n  # Either boolean flag or string with that indicates whether the selected encoder architecture should load pretrained\n  # weights or use randomly initialized weights.\n  # - In the case of boolean flag `true` means using pretrained weights from ImageNet and `false` means using randomly\n  #   initialized weights.\n  # - In the case of string the value should specify the name of the weights. For the list of available weights please\n  #   refer to https://github.com/qubvel/segmentation_models.pytorch#encoders-\n\ndata:\n  dataset:\n    _target_: dataset.PascalVOCSearchDataset\n    root: ~/data/pascal_voc\n    image_set: train\n    download: false\n  # Class for the PyTorch Dataset and arguments to it. AutoAlbument will create an object of this class using\n  # the `instantiate` method from Hydra - https://hydra.cc/docs/next/patterns/instantiate_objects/overview/.\n  #\n  # Note that the target class value in the `_target_` argument should be located inside PYTHONPATH so Hydra could\n  # find it. The directory with the config file is automatically added to PYTHONPATH, so the default value\n  # `dataset.SearchDataset` points to the class `SearchDataset` from the `dataset.py` file. This `dataset.py` file is\n  # located along with the `search.yaml` file in the same directory provided by `--config-dir`.\n  #\n  # As an alternative, you could provide a path to a Python file with the dataset using the `dataset_file` parameter\n  # instead of the `dataset` parameter. The Python file should contain the implementation of a PyTorch dataset for\n  # augmentation search. The dataset class should have named `SearchDataset`. The value in `dataset_file` could either\n  # be a relative or an absolute path ; in the case of a relative path, the path should be relative to this config\n  # file's location.\n  #\n  # - Example of a relative path:\n  # dataset_file: dataset.py\n  #\n  # - Example of an absolute path:\n  # dataset_file: /projects/pytorch/dataset.py\n  #\n\n  input_dtype: uint8\n  # The data type of input images. Two values are supported:\n  # - uint8. In that case, all input images should be NumPy arrays with the np.uint8 data type and values in the range\n  #   [0, 255].\n  # - float32. In that case, all input images should be NumPy arrays with the np.float32 data type and values in the\n  #   range [0.0, 1.0].\n\n  preprocessing:\n  - LongestMaxSize:\n      max_size: 256\n  - PadIfNeeded:\n      min_height: 256\n      min_width: 256\n      border_mode: 0\n      value: [0, 0, 0]\n  # A list of preprocessing augmentations that will be applied to each image before applying augmentations from\n  # a policy. A preprocessing augmentation should be defined as `key`: `value`, where `key` is the name of augmentation\n  # from Albumentations, and `value` is a dictionary with augmentation parameters. The found policy will also apply\n  # those preprocessing augmentations before applying the main augmentations.\n  #\n  # Here is an example of an augmentation pipeline that first pads an image to the size 512x512 pixels, then resizes\n  # the resulting image to the size 256x256 pixels and finally crops a random patch with the size 224x224 pixels.\n  #\n  #  preprocessing:\n  #    - PadIfNeeded:\n  #        min_height: 512\n  #        min_width: 512\n  #    - Resize:\n  #        height: 256\n  #        width: 256\n  #    - RandomCrop:\n  #        height: 224\n  #        width: 224\n  #\n\n  normalization:\n    mean: [0.485, 0.456, 0.406]\n    std: [0.229, 0.224, 0.225]\n  # Normalization values for images. For each image, the search pipeline will subtract `mean` and divide by `std`.\n  # Normalization is applied after transforms defined in `preprocessing`. Note that regardless of `input_dtype`,\n  # the normalization function will always receive a `float32` input with values in the range [0.0, 1.0], so you should\n  # define `mean` and `std` values accordingly. ImageNet normalization is used by default.\n\n  dataloader:\n    _target_: torch.utils.data.DataLoader\n    batch_size: 32\n    shuffle: true\n    num_workers: 8\n    pin_memory: true\n    drop_last: true\n  # Parameters for the PyTorch DataLoader. Please refer to the PyTorch documentation for the description of parameters -\n  # https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader.\n\n\noptim:\n  main:\n    _target_: torch.optim.Adam\n    lr: 1e-3\n    betas: [0, 0.999]\n  # Optimizer configuration for the main (either Classification or Semantic Segmentation) Model\n\n  policy:\n    _target_: torch.optim.Adam\n    lr: 1e-3\n    betas: [0, 0.999]\n  # Optimizer configuration for Policy Model\n\n\nseed: 42 # Random seed. If the value is not null, it will be passed to `seed_everything` -\n# https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.utilities.seed.html?highlight=seed_everything\n\n\nhydra:\n  run:\n    dir: ${config_dir:}/outputs/${now:%Y-%m-%d}/${now:%H-%M-%S}\n    # Path to the directory that will contain all outputs produced by the search algorithm. `${config_dir:}` contains\n    # path to the directory with the `search.yaml` config file. Please refer to the Hydra documentation for more\n    # information - https://hydra.cc/docs/configure_hydra/workdir.\n\n\ntrainer:\n  # Configuration for PyTorch Lightning Trainer. You can read more about Trainer and its arguments at\n  # https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html.\n  max_epochs: 50\n  # Number of epochs to search for augmentation parameters.\n  # More detailed description - https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html#max-epochs\n\n  benchmark: true\n  # If true enables cudnn.benchmark.\n  # More detailed description - https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html#benchmark\n\n  gpus: 1\n  # Number of GPUs to train on. Set to `0` or None` to use CPU for training.\n  # More detailed description - https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html#gpus\n</code></pre>"},{"location":"autoalbument/examples/svhn/","title":"Image classification on the SVHN dataset","text":"<p>The following files are also available on GitHub - https://github.com/albumentations-team/autoalbument/tree/master/examples/svhn</p>"},{"location":"autoalbument/examples/svhn/#datasetpy","title":"dataset.py","text":"Python<pre><code>import cv2\nimport numpy as np\nimport torch\nimport torchvision\n\ncv2.setNumThreads(0)\ncv2.ocl.setUseOpenCL(False)\n\n\nclass SVHNSearchDataset(torchvision.datasets.SVHN):\n    def __getitem__(self, index):\n        image, label = self.data[index], int(self.labels[index])\n        image = np.transpose(image, (1, 2, 0))\n\n        if self.transform is not None:\n            transformed = self.transform(image=image)\n            image = transformed[\"image\"]\n\n        return image, label\n\n\nclass ConcatSVHNSearchDataset(torch.utils.data.ConcatDataset):\n    def __init__(self, root, download, transform=None):\n        datasets = [\n            SVHNSearchDataset(root=root, split=\"train\", download=download, transform=transform),\n            SVHNSearchDataset(root=root, split=\"extra\", download=download, transform=transform),\n        ]\n        super().__init__(datasets)\n</code></pre>"},{"location":"autoalbument/examples/svhn/#searchyaml","title":"search.yaml","text":"YAML<pre><code># @package _global_\n\n_version: 2  # An internal value that indicates a version of the config schema. This value is used by\n# `autoalbument-search` and `autoalbument-migrate` to upgrade the config to the latest version if necessary.\n# Please do not change it manually.\n\ntask: classification # Deep learning task. Should either be `classification` or `semantic_segmentation`.\n\n# Settings for Policy Model that searches augmentation policies.\npolicy_model:\n\n  # Multiplier for classification loss of a model. Faster AutoAugment uses classification loss to prevent augmentations\n  # from transforming images of a particular class to another class. The authors of Faster AutoAugment use 0.1 as\n  # default value.\n  task_factor: 0.1\n\n  # Multiplier for the gradient penalty for WGAN-GP training. 10 is the default value that was proposed in\n  # `Improved Training of Wasserstein GANs`.\n  gp_factor: 10\n\n  # Temperature for Relaxed Bernoulli distribution. The probability of applying a certain augmentation is sampled from\n  # Relaxed Bernoulli distribution (because Bernoulli distribution is not differentiable). With lower values of\n  # `temperature` Relaxed Bernoulli distribution behaves like Bernoulli distribution. In the paper, the authors\n  # of Faster AutoAugment used 0.05 as a default value for `temperature`.\n  temperature: 0.05\n\n  # Number of augmentation sub-policies. When an image passes through an augmentation pipeline, Faster AutoAugment\n  # randomly chooses one sub-policy and uses augmentations from that sub-policy to transform an input image. A larger\n  # number of sub-policies leads to a more diverse set of augmentations and better performance of a model trained on\n  # augmented images. However, an increase in the number of sub-policies leads to the exponential growth of a search\n  # space of augmentations, so you need more training data for Policy Model to find good augmentation policies.\n  num_sub_policies: 100\n\n  # Number of chunks in a batch. Faster AutoAugment splits each batch of images into `num_chunks` chunks. Then it\n  # applies the same sub-policy with the same parameters to each image in a chunk. This parameter controls the tradeoff\n  # between the speed of augmentation search and diversity of augmentations. Larger `num_chunks` values will lead to\n  # faster searching but less diverse set of augmentations. Note that this parameter is used only in the searching\n  # phase. When you train a model with found sub-policies, Albumentations will apply a distinct set of transformations\n  # to each image separately.\n  num_chunks: 8\n\n  # Number of consecutive augmentations in each sub-policy. Faster AutoAugment will sequentially apply `operation_count`\n  # augmentations from a sub-policy to an image. Larger values of `operation_count` lead to better performance of\n  # a model trained on augmented images. Simultaneously, larger values of `operation_count` affect the speed of search\n  # and increase the searching time.\n  operation_count: 4\n\n# Settings for Classification Model that is used for two purposes:\n# 1. As a model that performs classification of input images.\n# 2. As a Discriminator for Policy Model.\nclassification_model:\n  # A custom classification model is used. This model is defined inside the `model.py` file which is located\n  # in the same directory with `search.yaml` and `dataset.py`.\n  _target_: model.SVHNClassificationModel\n\n  #  # As an alternative, you could use a built-in AutoAldbument model using the following config:\n  #  #  _target_: autoalbument.faster_autoaugment.models.ClassificationModel\n  #\n  #  # Number of classes in the dataset. The dataset implementation should return an integer in the range\n  #  # [0, num_classes - 1] as a class label of an image.\n  #  num_classes: 10\n  #\n  #  # The architecture of Classification Model. AutoAlbument uses models from\n  #  # https://github.com/rwightman/pytorch-image-models/. Please refer to its documentation to get a list of available\n  #  # models - https://rwightman.github.io/pytorch-image-models/#list-models-with-pretrained-weights.\n  #  architecture: resnet18\n  #\n  #  # Boolean flag that indicates whether the selected model architecture should load pretrained weights or use randomly\n  #  # initialized weights.\n  #  pretrained: False\n\ndata:\n  # Class for the PyTorch Dataset and arguments to it. AutoAlbument will create an object of this class using\n  # the `instantiate` method from Hydra - https://hydra.cc/docs/next/patterns/instantiate_objects/overview/.\n  #\n  # Note that the target class value in the `_target_` argument should be located inside PYTHONPATH so Hydra could\n  # find it. The directory with the config file is automatically added to PYTHONPATH, so the default value\n  # `dataset.SearchDataset` points to the class `SearchDataset` from the `dataset.py` file. This `dataset.py` file is\n  # located along with the `search.yaml` file in the same directory provided by `--config-dir`.\n  #\n  # As an alternative, you could provide a path to a Python file with the dataset using the `dataset_file` parameter\n  # instead of the `dataset` parameter. The Python file should contain the implementation of a PyTorch dataset for\n  # augmentation search. The dataset class should have named `SearchDataset`. The value in `dataset_file` could either\n  # be a relative or an absolute path ; in the case of a relative path, the path should be relative to this config\n  # file's location.\n  #\n  # - Example of a relative path:\n  # dataset_file: dataset.py\n  #\n  # - Example of an absolute path:\n  # dataset_file: /projects/pytorch/dataset.py\n  #\n  dataset:\n    _target_: dataset.ConcatSVHNSearchDataset\n    root: ~/data/svhn\n    download: true\n\n  # The data type of input images. Two values are supported:\n  # - uint8. In that case, all input images should be NumPy arrays with the np.uint8 data type and values in the range\n  #   [0, 255].\n  # - float32. In that case, all input images should be NumPy arrays with the np.float32 data type and values in the\n  #   range [0.0, 1.0].\n  input_dtype: uint8\n\n  # A list of preprocessing augmentations that will be applied to each image before applying augmentations from\n  # a policy. A preprocessing augmentation should be defined as `key`: `value`, where `key` is the name of augmentation\n  # from Albumentations, and `value` is a dictionary with augmentation parameters. The found policy will also apply\n  # those preprocessing augmentations before applying the main augmentations.\n  #\n  # Here is an example of an augmentation pipeline that first pads an image to the size 512x512 pixels, then resizes\n  # the resulting image to the size 256x256 pixels and finally crops a random patch with the size 224x224 pixels.\n  #\n  #  preprocessing:\n  #    - PadIfNeeded:\n  #        min_height: 512\n  #        min_width: 512\n  #    - Resize:\n  #        height: 256\n  #        width: 256\n  #    - RandomCrop:\n  #        height: 224\n  #        width: 224\n  #\n  preprocessing: null\n\n  # Normalization values for images. For each image, the search pipeline will subtract `mean` and divide by `std`.\n  # Normalization is applied after transforms defined in `preprocessing`. Note that regardless of `input_dtype`,\n  # the normalization function will always receive a `float32` input with values in the range [0.0, 1.0], so you should\n  # define `mean` and `std` values accordingly.\n  normalization:\n    mean: [0.4376821, 0.4437697, 0.47280442]\n    std: [0.19803012, 0.20101562, 0.19703614]\n\n  # Parameters for the PyTorch DataLoader. Please refer to the PyTorch documentation for the description of parameters -\n  # https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader.\n  dataloader:\n    _target_: torch.utils.data.DataLoader\n    batch_size: 128\n    shuffle: true\n    num_workers: 8\n    pin_memory: true\n    drop_last: true\n\noptim:\n  # Number of epochs to search parameters of augmentations.\n  main:\n    _target_: torch.optim.Adam\n    lr: 1e-3\n    betas: [0, 0.999]\n\n  # Optimizer configuration for Policy Model\n  policy:\n    _target_: torch.optim.Adam\n    lr: 1e-3\n    betas: [0, 0.999]\n\nseed: 42 # Random seed. If the value is not null, it will be passed to `seed_everything` -\n# https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.utilities.seed.html?highlight=seed_everything\n\n\nhydra:\n  run:\n    # Path to the directory that will contain all outputs produced by the search algorithm. `${config_dir:}` contains\n    # path to the directory with the `search.yaml` config file. Please refer to the Hydra documentation for more\n    # information - https://hydra.cc/docs/configure_hydra/workdir.\n    dir: ${config_dir:}/outputs/${now:%Y-%m-%d}/${now:%H-%M-%S}\n\ntrainer:\n  # Configuration for PyTorch Lightning Trainer. You can read more about Trainer and its arguments at\n  # https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html.\n  max_epochs: 4\n  # Number of epochs to search for augmentation parameters.\n  # More detailed description - https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html#max-epochs\n\n  benchmark: true\n  # If true enables cudnn.benchmark.\n  # More detailed description - https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html#benchmark\n\n  gpus: 1\n  # Number of GPUs to train on. Set to `0` or None` to use CPU for training.\n  # More detailed description - https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html#gpus\n</code></pre>"},{"location":"autoalbument/examples/svhn/#modelpy","title":"model.py","text":"Python<pre><code>\"\"\"WideResNet code from https://github.com/xternalz/WideResNet-pytorch\"\"\"\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom autoalbument.faster_autoaugment.models import BaseDiscriminator\n\n\nclass BasicBlock(nn.Module):\n    def __init__(self, in_planes, out_planes, stride):\n        super(BasicBlock, self).__init__()\n        self.bn1 = nn.BatchNorm2d(in_planes)\n        self.relu1 = nn.ReLU(inplace=True)\n        self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_planes)\n        self.relu2 = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(out_planes, out_planes, kernel_size=3, stride=1, padding=1, bias=False)\n        self.equal_in_out = in_planes == out_planes\n        self.conv_shortcut = (\n            (not self.equal_in_out)\n            and nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, padding=0, bias=False)\n            or None\n        )\n\n    def forward(self, x):\n        if not self.equal_in_out:\n            x = self.relu1(self.bn1(x))\n        else:\n            out = self.relu1(self.bn1(x))\n        out = self.relu2(self.bn2(self.conv1(out if self.equal_in_out else x)))\n        out = self.conv2(out)\n        return torch.add(x if self.equal_in_out else self.conv_shortcut(x), out)\n\n\nclass NetworkBlock(nn.Module):\n    def __init__(self, nb_layers, in_planes, out_planes, block, stride):\n        super(NetworkBlock, self).__init__()\n        self.layer = self._make_layer(block, in_planes, out_planes, nb_layers, stride)\n\n    def _make_layer(self, block, in_planes, out_planes, nb_layers, stride):\n        layers = []\n        for i in range(int(nb_layers)):\n            layers.append(block(i == 0 and in_planes or out_planes, out_planes, i == 0 and stride or 1))\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.layer(x)\n\n\nclass WideResNet(nn.Module):\n    def __init__(self, depth, num_classes, widen_factor=1):\n        super(WideResNet, self).__init__()\n        n_channels = [16, 16 * widen_factor, 32 * widen_factor, 64 * widen_factor]\n        assert (depth - 4) % 6 == 0\n        n = (depth - 4) / 6\n        block = BasicBlock\n        # 1st conv before any network block\n        self.conv1 = nn.Conv2d(3, n_channels[0], kernel_size=3, stride=1, padding=1, bias=False)\n        # 1st block\n        self.block1 = NetworkBlock(n, n_channels[0], n_channels[1], block, 1)\n        # 2nd block\n        self.block2 = NetworkBlock(n, n_channels[1], n_channels[2], block, 2)\n        # 3rd block\n        self.block3 = NetworkBlock(n, n_channels[2], n_channels[3], block, 2)\n        # global average pooling and classifier\n        self.bn1 = nn.BatchNorm2d(n_channels[3])\n        self.relu = nn.ReLU(inplace=True)\n        self.fc = nn.Linear(n_channels[3], num_classes)\n        self.n_channels = n_channels[3]\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.Linear):\n                m.bias.data.zero_()\n\n    def forward_features(self, x):\n        x = self.conv1(x)\n        x = self.block1(x)\n        x = self.block2(x)\n        x = self.block3(x)\n        x = self.relu(self.bn1(x))\n        x = F.avg_pool2d(x, 8, 1, 0)\n        x = x.view(-1, self.n_channels)\n        return x\n\n    def forward_classifier(self, x):\n        return self.fc(x)\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = self.forward_classifier(x)\n        return x\n\n\ndef wide_resnet_28x10(num_classes):\n    return WideResNet(depth=28, widen_factor=10, num_classes=num_classes)\n\n\nclass SVHNClassificationModel(BaseDiscriminator):\n    def __init__(self, *args, **kwargs):\n        super().__init__()\n        self.base_model = wide_resnet_28x10(num_classes=10)\n        num_features = self.base_model.fc.in_features\n        self.discriminator = nn.Sequential(\n            nn.Linear(num_features, num_features), nn.ReLU(), nn.Linear(num_features, 1)\n        )\n\n    def forward(self, input):\n        x = self.base_model.forward_features(input)\n        return self.base_model.forward_classifier(x), self.discriminator(x).view(-1)\n</code></pre>"},{"location":"examples/","title":"List of examples","text":"<ul> <li>Defining a simple augmentation pipeline for image augmentation</li> <li>Using Albumentations to augment bounding boxes for object detection tasks</li> <li>How to use Albumentations for detection tasks if you need to keep all bounding boxes</li> <li>Using Albumentations for a semantic segmentation task</li> <li>Using Albumentations to augment keypoints</li> <li>Applying the same augmentation with the same parameters to multiple images, masks, bounding boxes, or keypoints</li> <li>Weather augmentations in Albumentations</li> <li>Example of applying XYMasking transform</li> <li>Example of applying MixUp transform</li> <li>Example of applying ChromaticAberration transform</li> <li>Example of applying Morphological transform</li> <li>Example of applying D4 transform</li> <li>Example of applying RandomGridShuffle transform</li> <li>Example of applying OverlayElements transform</li> <li>Example of applying TextImage transform</li> <li>Migrating from torchvision to Albumentations</li> <li>Debugging an augmentation pipeline with ReplayCompose</li> <li>How to save and load parameters of an augmentation pipeline</li> <li>Showcase. Cool augmentation examples on diverse set of images from various real-world tasks.</li> <li>How to save and load transforms to HuggingFace Hub.</li> </ul>"},{"location":"examples/#examples-of-how-to-use-albumentations-with-different-deep-learning-frameworks","title":"Examples of how to use Albumentations with different deep learning frameworks","text":"<ul> <li>PyTorch</li> <li>PyTorch and Albumentations for image classification</li> <li>PyTorch and Albumentations for semantic segmentation</li> <li>TensorFlow 2</li> <li>Using Albumentations with Tensorflow</li> </ul>"},{"location":"examples/example/","title":"Defining a simple augmentation pipeline for image augmentation","text":"<p>This example shows how you can use Albumentations to define a simple augmentation pipeline.</p>"},{"location":"examples/example/#import-the-required-libraries","title":"Import the required libraries","text":"Python<pre><code>import random\n\nimport cv2\nfrom matplotlib import pyplot as plt\n\nimport albumentations as A\n</code></pre>"},{"location":"examples/example/#define-the-visualization-function","title":"Define the visualization function","text":"Python<pre><code>def visualize(image):\n    plt.figure(figsize=(10, 10))\n    plt.axis('off')\n    plt.imshow(image)\n</code></pre>"},{"location":"examples/example/#read-the-image-from-the-disk-and-convert-it-from-the-bgr-color-space-to-the-rgb-color-space","title":"Read the image from the disk and convert it from the BGR color space to the RGB color space","text":"<p>For historical reasons, OpenCV reads an image in BGR format (so color channels of the image have the following order: Blue, Green, Red). Albumentations uses the most common and popular RGB image format. So when using OpenCV, we need to convert the image format to RGB explicitly.</p> Python<pre><code>image = cv2.imread('images/image_3.jpg')\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\nvisualize(image)\n</code></pre> <p></p>"},{"location":"examples/example/#define-a-single-augmentation-pass-the-image-to-it-and-receive-the-augmented-image","title":"Define a single augmentation, pass the image to it and receive the augmented image","text":"<p>We fix the random seed for visualization purposes, so the augmentation will always produce the same result. In a real computer vision pipeline, you shouldn't fix the random seed before applying a transform to the image because, in that case, the pipeline will always output the same image. The purpose of image augmentation is to use different transformations each time.</p> Python<pre><code>transform = A.HorizontalFlip(p=0.5)\nrandom.seed(7)\naugmented_image = transform(image=image)['image']\nvisualize(augmented_image)\n</code></pre> <p></p> Python<pre><code>transform = A.ShiftScaleRotate(p=0.5)\nrandom.seed(7)\naugmented_image = transform(image=image)['image']\nvisualize(augmented_image)\n</code></pre> <p></p>"},{"location":"examples/example/#define-an-augmentation-pipeline-using-compose-pass-the-image-to-it-and-receive-the-augmented-image","title":"Define an augmentation pipeline using <code>Compose</code>, pass the image to it and receive the augmented image","text":"Python<pre><code>transform = A.Compose([\n    A.CLAHE(),\n    A.RandomRotate90(),\n    A.Transpose(),\n    A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.50, rotate_limit=45, p=.75),\n    A.Blur(blur_limit=3),\n    A.OpticalDistortion(),\n    A.GridDistortion(),\n    A.HueSaturationValue(),\n])\nrandom.seed(42)\naugmented_image = transform(image=image)['image']\nvisualize(augmented_image)\n</code></pre> Python<pre><code>transform = A.Compose([\n        A.RandomRotate90(),\n        A.Flip(),\n        A.Transpose(),\n        A.GaussNoise(),\n        A.OneOf([\n            A.MotionBlur(p=.2),\n            A.MedianBlur(blur_limit=3, p=0.1),\n            A.Blur(blur_limit=3, p=0.1),\n        ], p=0.2),\n        A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.2, rotate_limit=45, p=0.2),\n        A.OneOf([\n            A.OpticalDistortion(p=0.3),\n            A.GridDistortion(p=.1),\n        ], p=0.2),\n        A.OneOf([\n            A.CLAHE(clip_limit=2),\n            A.RandomBrightnessContrast(),\n        ], p=0.3),\n        A.HueSaturationValue(p=0.3),\n    ])\nrandom.seed(42)\naugmented_image = transform(image=image)['image']\nvisualize(augmented_image)\n</code></pre>"},{"location":"examples/example_16_bit_tiff/","title":"Working with non-8-bit images","text":"<p>This example shows how you can augment 16-bit TIFF images. 16-bit images are used in satellite imagery. The following technique could also be applied to all non-8-bit images (i.e., 24-bit images, 32-bit images. etc.).</p>"},{"location":"examples/example_16_bit_tiff/#import-the-required-libraries","title":"Import the required libraries","text":"Python<pre><code>import random\n\nimport cv2\nfrom matplotlib import pyplot as plt\n\nimport albumentations as A\n</code></pre>"},{"location":"examples/example_16_bit_tiff/#define-the-visualization-function","title":"Define the visualization function","text":"Python<pre><code>def visualize(image):\n    # Divide all values by 65535 so we can display the image using matplotlib\n    image = image / 65535\n    plt.figure(figsize=(10, 10))\n    plt.axis('off')\n    plt.imshow(image)\n</code></pre>"},{"location":"examples/example_16_bit_tiff/#read-the-16-bit-tiff-image-from-the-disk","title":"Read the 16-bit TIFF image from the disk","text":"Python<pre><code># The image is taken from http://www.brucelindbloom.com/index.html?ReferenceImages.html\n# \u00a9 Bruce Justin Lindbloom\nimage = cv2.imread('images/DeltaE_16bit_gamma2.2.tif', cv2.IMREAD_UNCHANGED)\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\nvisualize(image)\n</code></pre> <p>Note: OpenCV may read incorrectly some TIFF files. Consider using <code>tifffile</code> - https://github.com/blink1073/tifffile</p>"},{"location":"examples/example_16_bit_tiff/#define-an-augmentation-pipeline-that-works-with-16-bit-tiff-images","title":"Define an augmentation pipeline that works with 16-bit TIFF images","text":"<p>Under the hood, Albumentations supports two data types that describe the intensity of pixels:  - <code>np.uint8</code>, an unsigned 8-bit integer that can define values between 0 and 255. - <code>np.float32</code>, a floating-point number with single precision. For <code>np.float32</code> input, Albumentations expects that value will lie in the range between 0.0 and 1.0.</p> <p>Albumentations has a dedicated transformation called <code>ToFloat</code> that takes a NumPy array with data types such as <code>np.uint16</code>, <code>np.uint32</code>, etc. (so any datatype that used values higher than 255 to represent pixel intensity) and converts it to a NumPy array with the <code>np.float32</code> datatype. Additionally, this transformation divides all input values to lie in the range <code>[0.0, 1.0]</code>. By default, if the input data type is <code>np.uint16</code>, all values are divided by 65535, and if the input data type is <code>np.uint32</code>, all values are divided by 4294967295. You can specify your divider in the <code>max_value</code> parameter.</p> <p>The augmentation pipeline for non-8-bit images consists of the following stages:</p> <ul> <li>First, you use the <code>ToFloat</code> transform to convert an input image to float32. All values in the converted image will lie in the range <code>[0.0, 1.0]</code>.</li> <li>Then you use all the necessary image transforms.</li> <li>Optionally you could use the <code>FromFloat</code> transform at the end of the augmentation pipeline to convert the image back to its original data type.</li> </ul> Python<pre><code>transform = A.Compose([\n    A.ToFloat(max_value=65535.0),\n\n    A.RandomRotate90(),\n    A.Flip(),\n    A.OneOf([\n        A.MotionBlur(p=0.2),\n        A.MedianBlur(blur_limit=3, p=0.1),\n        A.Blur(blur_limit=3, p=0.1),\n    ], p=0.2),\n    A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.2, rotate_limit=45, p=0.2),\n    A.OneOf([\n        A.OpticalDistortion(p=0.3),\n        A.GridDistortion(p=0.1),\n    ], p=0.2),\n    A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=0.1, val_shift_limit=0.1, p=0.3),\n\n    A.FromFloat(max_value=65535.0),\n])\n</code></pre> <p>We fix the random seed for visualization purposes, so the augmentation will always produce the same result. In a real computer vision pipeline, you shouldn't fix the random seed before applying a transform to the image because, in that case, the pipeline will always output the same image. The purpose of image augmentation is to use different transformations each time.</p> Python<pre><code>random.seed(7)\naugmented = transform(image=image)\nvisualize(augmented['image'])\n</code></pre> <p></p>"},{"location":"examples/example_OverlayElements/","title":"Overlay Elements","text":"<p>Code for the transform is based on the code from https://github.com/danaaubakirova/doc-augmentation by Dana Aubakirova</p> Python<pre><code>%load_ext autoreload\n%autoreload 2\n</code></pre> <pre><code>The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n</code></pre> Python<pre><code>!pip install -U pillow\n</code></pre> <pre><code>Requirement already satisfied: pillow in /Users/vladimiriglovikov/anaconda3/envs/albumentations/lib/python3.8/site-packages (10.3.0)\n</code></pre> Python<pre><code>%matplotlib inline\n</code></pre> Python<pre><code>import cv2\nfrom matplotlib import pyplot as plt\n</code></pre> Python<pre><code>from PIL import ImageDraw, ImageFont, Image\n</code></pre> Python<pre><code>from pylab import *\n</code></pre> Python<pre><code>import albumentations as A\n</code></pre> Python<pre><code>import json\n</code></pre> Python<pre><code>def visualize(image):\n    plt.figure(figsize=(20, 10))\n    plt.axis('off')\n    plt.imshow(image)\n</code></pre> Python<pre><code>def load_rgb(image_path):\n    image = cv2.imread(image_path)\n    return cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n</code></pre> Python<pre><code>font_path = \"../data/documents/LiberationSerif-Regular.ttf\"\n</code></pre> Python<pre><code>image = load_rgb(\"../data/documents/docs.png\")\n</code></pre> Python<pre><code>with open(\"../data/documents/text.json\") as f:\n    labels = json.load(f)\n</code></pre> Python<pre><code>visualize(image)\n</code></pre> <p></p> Python<pre><code>transform = A.Compose([A.OverlayElements(p=1)])\n</code></pre>"},{"location":"examples/example_OverlayElements/#render-images-to-paste","title":"Render images to paste","text":"Python<pre><code>def render_text(bbox_shape, text, font):\n    bbox_height, bbox_width = bbox_shape\n\n    # Create an empty RGB image with the size of the bounding box\n    bbox_img = Image.new(\"RGB\", (bbox_width, bbox_height), color=\"white\")\n    draw = ImageDraw.Draw(bbox_img)\n\n    # Draw the text in red\n    draw.text((0, 0), text, fill=\"red\", font=font)\n\n    return np.array(bbox_img)\n</code></pre> Python<pre><code>bbox_indices_to_update = np.random.choice(range(len(labels[\"text\"])), 10)\n</code></pre> Python<pre><code>labels.keys()\n</code></pre> <pre><code>dict_keys(['text', 'bbox', 'poly', 'score'])\n</code></pre> Python<pre><code>image_height, image_width = image.shape[:2]\nnum_channels = image.shape[2] if len(image.shape) == 3 else 1\n</code></pre> Python<pre><code>metadata = []\nfor index in bbox_indices_to_update:\n    selected_bbox = labels[\"bbox\"][index]\n\n    # You may apply any transforms you want to text like random deletion, swapping words, applying synonims, etc\n    text = labels[\"text\"][index]\n\n    left, top, width_norm, height_norm = selected_bbox\n\n    bbox_height = int(image_height * height_norm)\n    bbox_width = int(image_width * width_norm)\n\n    font = ImageFont.truetype(font_path, int(0.90 * bbox_height))\n\n    overlay_image = render_text((bbox_height, bbox_width), text, font)\n\n    metadata += [\n        {\n            \"image\": overlay_image,\n            \"bbox\": (left, top, left + width_norm, top + height_norm)\n        }\n    ]\n</code></pre>"},{"location":"examples/example_OverlayElements/#paste-new-text-to-image","title":"Paste new text to image","text":"Python<pre><code>transformed = transform(image=image, overlay_metadata=metadata)\n</code></pre> Python<pre><code>visualize(transformed[\"image\"])\n</code></pre>"},{"location":"examples/example_OverlayElements/#as-a-part-of-the-augmentation-pipeline","title":"As a part of the augmentation pipeline","text":"Python<pre><code>transform_complex = A.Compose([A.OverlayElements(p=1),\n                               A.RandomCrop(p=1, height=1024, width=1024),\n                               A.PlanckianJitter(p=1),\n                               A.Affine(p=1)\n                              ])\n</code></pre> Python<pre><code>transformed = transform_complex(image=image, overlay_metadata=metadata)\n</code></pre> Python<pre><code>visualize(transformed[\"image\"])\n</code></pre> Python<pre><code>\n</code></pre>"},{"location":"examples/example_bboxes/","title":"Using Albumentations to augment bounding boxes for object detection tasks","text":""},{"location":"examples/example_bboxes/#import-the-required-libraries","title":"Import the required libraries","text":"Python<pre><code>%matplotlib inline\n</code></pre> Python<pre><code>import random\n\nimport cv2\nfrom matplotlib import pyplot as plt\n\nimport albumentations as A\n</code></pre>"},{"location":"examples/example_bboxes/#define-functions-to-visualize-bounding-boxes-and-class-labels-on-an-image","title":"Define functions to visualize bounding boxes and class labels on an image","text":"<p>The visualization function is based on https://github.com/facebookresearch/Detectron/blob/master/detectron/utils/vis.py</p> Python<pre><code>BOX_COLOR = (255, 0, 0) # Red\nTEXT_COLOR = (255, 255, 255) # White\n\n\ndef visualize_bbox(img, bbox, class_name, color=BOX_COLOR, thickness=2):\n    \"\"\"Visualizes a single bounding box on the image\"\"\"\n    x_min, y_min, w, h = bbox\n    x_min, x_max, y_min, y_max = int(x_min), int(x_min + w), int(y_min), int(y_min + h)\n\n    cv2.rectangle(img, (x_min, y_min), (x_max, y_max), color=color, thickness=thickness)\n\n    ((text_width, text_height), _) = cv2.getTextSize(class_name, cv2.FONT_HERSHEY_SIMPLEX, 0.35, 1)\n    cv2.rectangle(img, (x_min, y_min - int(1.3 * text_height)), (x_min + text_width, y_min), BOX_COLOR, -1)\n    cv2.putText(\n        img,\n        text=class_name,\n        org=(x_min, y_min - int(0.3 * text_height)),\n        fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n        fontScale=0.35,\n        color=TEXT_COLOR,\n        lineType=cv2.LINE_AA,\n    )\n    return img\n\n\ndef visualize(image, bboxes, category_ids, category_id_to_name):\n    img = image.copy()\n    for bbox, category_id in zip(bboxes, category_ids):\n        class_name = category_id_to_name[category_id]\n        img = visualize_bbox(img, bbox, class_name)\n    plt.figure(figsize=(12, 12))\n    plt.axis('off')\n    plt.imshow(img)\n</code></pre>"},{"location":"examples/example_bboxes/#get-an-image-and-annotations-for-it","title":"Get an image and annotations for it","text":"<p>For this example we will use an image from the COCO dataset that have two associated bounding boxes. The image is available at http://cocodataset.org/#explore?id=386298</p>"},{"location":"examples/example_bboxes/#load-the-image-from-the-disk","title":"Load the image from the disk","text":"Python<pre><code>image = cv2.imread('images/000000386298.jpg')\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n</code></pre>"},{"location":"examples/example_bboxes/#define-two-bounding-boxes-with-coordinates-and-class-labels","title":"Define two bounding boxes with coordinates and class labels","text":"<p>Coordinates for those bounding boxes are declared using the <code>coco</code> format. Each bounding box is described using four values <code>[x_min, y_min, width, height]</code>. For the detailed description of different formats for bounding boxes coordinates, please refer to the documentation article about bounding boxes - https://albumentations.ai/docs/getting_started/bounding_boxes_augmentation/.</p> Python<pre><code>bboxes = [[5.66, 138.95, 147.09, 164.88], [366.7, 80.84, 132.8, 181.84]]\ncategory_ids = [17, 18]\n\n# We will use the mapping from category_id to the class name\n# to visualize the class label for the bounding box on the image\ncategory_id_to_name = {17: 'cat', 18: 'dog'}\n</code></pre>"},{"location":"examples/example_bboxes/#visuaize-the-original-image-with-bounding-boxes","title":"Visuaize the original image with bounding boxes","text":"Python<pre><code>visualize(image, bboxes, category_ids, category_id_to_name)\n</code></pre>"},{"location":"examples/example_bboxes/#define-an-augmentation-pipeline","title":"Define an augmentation pipeline","text":"<p>To make an augmentation pipeline that works with bounding boxes, you need to pass an instance of <code>BboxParams</code> to <code>Compose</code>. In <code>BboxParams</code> you need to specify the format of coordinates for bounding boxes and optionally a few other parameters. For the detailed description of <code>BboxParams</code> please refer to the documentation article about bounding boxes - https://albumentations.ai/docs/getting_started/bounding_boxes_augmentation/.</p> Python<pre><code>transform = A.Compose(\n    [A.HorizontalFlip(p=0.5)],\n    bbox_params=A.BboxParams(format='coco', label_fields=['category_ids']),\n)\n</code></pre> <p>We fix the random seed for visualization purposes, so the augmentation will always produce the same result. In a real computer vision pipeline, you shouldn't fix the random seed before applying a transform to the image because, in that case, the pipeline will always output the same image. The purpose of image augmentation is to use different transformations each time.</p> Python<pre><code>random.seed(7)\ntransformed = transform(image=image, bboxes=bboxes, category_ids=category_ids)\nvisualize(\n    transformed['image'],\n    transformed['bboxes'],\n    transformed['category_ids'],\n    category_id_to_name,\n)\n</code></pre> <p></p>"},{"location":"examples/example_bboxes/#another-example","title":"Another example","text":"Python<pre><code>transform = A.Compose(\n    [A.ShiftScaleRotate(p=0.5)],\n    bbox_params=A.BboxParams(format='coco', label_fields=['category_ids']),\n)\n</code></pre> Python<pre><code>random.seed(7)\ntransformed = transform(image=image, bboxes=bboxes, category_ids=category_ids)\nvisualize(\n    transformed['image'],\n    transformed['bboxes'],\n    transformed['category_ids'],\n    category_id_to_name,\n)\n</code></pre>"},{"location":"examples/example_bboxes/#define-a-complex-augmentation-piepline","title":"Define a complex augmentation piepline","text":"Python<pre><code>transform = A.Compose([\n        A.HorizontalFlip(p=0.5),\n        A.ShiftScaleRotate(p=0.5),\n        A.RandomBrightnessContrast(p=0.3),\n        A.RGBShift(r_shift_limit=30, g_shift_limit=30, b_shift_limit=30, p=0.3),\n    ],\n    bbox_params=A.BboxParams(format='coco', label_fields=['category_ids']),\n)\n</code></pre> Python<pre><code>random.seed(7)\ntransformed = transform(image=image, bboxes=bboxes, category_ids=category_ids)\nvisualize(\n    transformed['image'],\n    transformed['bboxes'],\n    transformed['category_ids'],\n    category_id_to_name,\n)\n</code></pre>"},{"location":"examples/example_bboxes/#min_area-and-min_visibility-parameters","title":"<code>min_area</code> and <code>min_visibility</code> parameters","text":"<p>The size of bounding boxes could change if you apply spatial augmentations, for example, when you crop a part of an image or when you resize an image.</p> <p><code>min_area</code> and <code>min_visibility</code> parameters control what Albumentations should do to the augmented bounding boxes if their size has changed after augmentation. The size of bounding boxes could change if you apply spatial augmentations, for example, when you crop a part of an image or when you resize an image.</p> <p><code>min_area</code> is a value in pixels. If the area of a bounding box after augmentation becomes smaller than <code>min_area</code>, Albumentations will drop that box. So the returned list of augmented bounding boxes won't contain that bounding box.</p> <p><code>min_visibility</code> is a value between 0 and 1. If the ratio of the bounding box area after augmentation to <code>the area of the bounding box before augmentation</code> becomes smaller than <code>min_visibility</code>, Albumentations will drop that box. So if the augmentation process cuts the most of the bounding box, that box won't be present in the returned list of the augmented bounding boxes.</p>"},{"location":"examples/example_bboxes/#define-an-augmentation-pipeline-with-the-default-values-for-min_area-and-min_visibilty","title":"Define an augmentation pipeline with the default values for <code>min_area</code> and <code>min_visibilty</code>","text":"<p>If you don't pass the <code>min_area</code> and <code>min_visibility</code> parameters, Albumentations will use 0 as a default value for them.</p> Python<pre><code>transform = A.Compose(\n    [A.CenterCrop(height=280, width=280, p=1)],\n    bbox_params=A.BboxParams(format='coco', label_fields=['category_ids']),\n)\n</code></pre> Python<pre><code>transformed = transform(image=image, bboxes=bboxes, category_ids=category_ids)\nvisualize(\n    transformed['image'],\n    transformed['bboxes'],\n    transformed['category_ids'],\n    category_id_to_name,\n)\n</code></pre> <p></p> <p>As you see the output contains two bounding boxes.</p>"},{"location":"examples/example_bboxes/#define-an-augmentation-pipeline-with-min_area","title":"Define an augmentation pipeline with <code>min_area</code>","text":"<p>Next, we will set the <code>min_area</code> value to 4500 pixels.</p> Python<pre><code>transform = A.Compose(\n    [A.CenterCrop(height=280, width=280, p=1)],\n    bbox_params=A.BboxParams(format='coco', min_area=4500, label_fields=['category_ids']),\n)\n</code></pre> Python<pre><code>transformed = transform(image=image, bboxes=bboxes, category_ids=category_ids)\nvisualize(\n    transformed['image'],\n    transformed['bboxes'],\n    transformed['category_ids'],\n    category_id_to_name,\n)\n</code></pre> <p></p> <p>The output contains only one bounding box because the area of the second bounding box became lower than 4500 pixels.</p>"},{"location":"examples/example_bboxes/#define-an-augmentation-pipeline-with-min_visibility","title":"Define an augmentation pipeline with <code>min_visibility</code>","text":"<p>Finally, we will set <code>min_visibility</code> to 0.3. So if the area of the output bounding box is less than 30% of the original area, Albumentations won't return that bounding box.</p> Python<pre><code>transform = A.Compose(\n    [A.CenterCrop(height=280, width=280, p=1)],\n    bbox_params=A.BboxParams(format='coco', min_visibility=0.3, label_fields=['category_ids']),\n)\n</code></pre> Python<pre><code>transformed = transform(image=image, bboxes=bboxes, category_ids=category_ids)\nvisualize(\n    transformed['image'],\n    transformed['bboxes'],\n    transformed['category_ids'],\n    category_id_to_name,\n)\n</code></pre> <p></p> <p>The output doesn't contain any bounding box.</p> <p>Note that you can declare both the <code>min_area</code> and <code>min_visibility</code> parameters simultaneously in one <code>BboxParams</code> instance.</p>"},{"location":"examples/example_bboxes2/","title":"How to use Albumentations for detection tasks if you need to keep all bounding boxes","text":"<p>Some augmentations like <code>RandomCrop</code> and <code>CenterCrop</code> may transform an image so that it won't contain all original bounding boxes. This example shows how you can use the transform named <code>RandomSizedBBoxSafeCrop</code> to crop a part of the image but keep all bounding boxes from the original image.</p>"},{"location":"examples/example_bboxes2/#import-the-required-libraries","title":"Import the required libraries","text":"Python<pre><code>import random\n\nimport cv2\nfrom matplotlib import pyplot as plt\n\nimport albumentations as A\n</code></pre>"},{"location":"examples/example_bboxes2/#define-functions-to-visualize-bounding-boxes-and-class-labels-on-an-image","title":"Define functions to visualize bounding boxes and class labels on an image","text":"<p>The visualization function is based on https://github.com/facebookresearch/Detectron/blob/master/detectron/utils/vis.py</p> Python<pre><code>BOX_COLOR = (255, 0, 0) # Red\nTEXT_COLOR = (255, 255, 255) # White\n\n\ndef visualize_bbox(img, bbox, class_name, color=BOX_COLOR, thickness=2):\n    \"\"\"Visualizes a single bounding box on the image\"\"\"\n    x_min, y_min, w, h = bbox\n    x_min, x_max, y_min, y_max = int(x_min), int(x_min + w), int(y_min), int(y_min + h)\n\n    cv2.rectangle(img, (x_min, y_min), (x_max, y_max), color=color, thickness=thickness)\n\n    ((text_width, text_height), _) = cv2.getTextSize(class_name, cv2.FONT_HERSHEY_SIMPLEX, 0.35, 1)\n    cv2.rectangle(img, (x_min, y_min - int(1.3 * text_height)), (x_min + text_width, y_min), BOX_COLOR, -1)\n    cv2.putText(\n        img,\n        text=class_name,\n        org=(x_min, y_min - int(0.3 * text_height)),\n        fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n        fontScale=0.35,\n        color=TEXT_COLOR,\n        lineType=cv2.LINE_AA,\n    )\n    return img\n\n\ndef visualize(image, bboxes, category_ids, category_id_to_name):\n    img = image.copy()\n    for bbox, category_id in zip(bboxes, category_ids):\n        class_name = category_id_to_name[category_id]\n        img = visualize_bbox(img, bbox, class_name)\n    plt.figure(figsize=(12, 12))\n    plt.axis('off')\n    plt.imshow(img)\n</code></pre>"},{"location":"examples/example_bboxes2/#get-an-image-and-annotations-for-it","title":"Get an image and annotations for it","text":"<p>For this example we will use an image from the COCO dataset that have two associated bounding boxes. The image is available at http://cocodataset.org/#explore?id=386298</p>"},{"location":"examples/example_bboxes2/#load-the-image-from-the-disk","title":"Load the image from the disk","text":"Python<pre><code>image = cv2.imread('images/000000386298.jpg')\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n</code></pre>"},{"location":"examples/example_bboxes2/#define-two-bounding-boxes-with-coordinates-and-class-labels","title":"Define two bounding boxes with coordinates and class labels","text":"<p>Coordinates for those bounding boxes are declared using the <code>coco</code> format. Each bounding box is described using four values <code>[x_min, y_min, width, height]</code>. For the detailed description of different formats for bounding boxes coordinates, please refer to the documentation article about bounding boxes - https://albumentations.ai/docs/getting_started/bounding_boxes_augmentation/.</p> Python<pre><code>bboxes = [[5.66, 138.95, 147.09, 164.88], [366.7, 80.84, 132.8, 181.84]]\ncategory_ids = [17, 18]\n\n# We will use the mapping from category_id to the class name\n# to visualize the class label for the bounding box on the image\ncategory_id_to_name = {17: 'cat', 18: 'dog'}\n</code></pre>"},{"location":"examples/example_bboxes2/#visuaize-the-image-with-bounding-boxes","title":"Visuaize the image with bounding boxes","text":"Python<pre><code>visualize(image, bboxes, category_ids, category_id_to_name)\n</code></pre>"},{"location":"examples/example_bboxes2/#using-randomsizedbboxsafecrop-to-keep-all-bounding-boxes-from-the-original-image","title":"Using <code>RandomSizedBBoxSafeCrop</code> to keep all bounding boxes from the original image","text":"<p><code>RandomSizedBBoxSafeCrop</code> crops a random part of the image. It ensures that the cropped part will contain all bounding boxes from the original image. Then the transform rescales the crop to height and width specified by the respective parameters. The <code>erosion_rate</code> parameter controls how much area of the original bounding box could be lost after cropping. <code>erosion_rate = 0.2</code> means that the augmented bounding box's area could be up to 20% smaller than the area of the original bounding box.</p>"},{"location":"examples/example_bboxes2/#define-an-augmentation-pipeline","title":"Define an augmentation pipeline","text":"Python<pre><code>transform = A.Compose(\n    [A.RandomSizedBBoxSafeCrop(width=448, height=336, erosion_rate=0.2)],\n    bbox_params=A.BboxParams(format='coco', label_fields=['category_ids']),\n)\n</code></pre>"},{"location":"examples/example_bboxes2/#augment-the-input-image-with-bounding-boxes","title":"Augment the input image with bounding boxes","text":"<p>We fix the random seed for visualization purposes, so the augmentation will always produce the same result. In a real computer vision pipeline, you shouldn't fix the random seed before applying a transform to the image because, in that case, the pipeline will always output the same image. The purpose of image augmentation is to use different transformations each time.</p> Python<pre><code>random.seed(7)\ntransformed = transform(image=image, bboxes=bboxes, category_ids=category_ids)\nvisualize(\n    transformed['image'],\n    transformed['bboxes'],\n    transformed['category_ids'],\n    category_id_to_name,\n)\n</code></pre> <p></p>"},{"location":"examples/example_bboxes2/#a-few-more-examples-with-different-random-seeds","title":"A few more examples with different random seeds","text":"Python<pre><code>random.seed(3)\ntransformed = transform(image=image, bboxes=bboxes, category_ids=category_ids)\nvisualize(\n    transformed['image'],\n    transformed['bboxes'],\n    transformed['category_ids'],\n    category_id_to_name,\n)\n</code></pre> Python<pre><code>random.seed(444)\ntransformed = transform(image=image, bboxes=bboxes, category_ids=category_ids)\nvisualize(\n    transformed['image'],\n    transformed['bboxes'],\n    transformed['category_ids'],\n    category_id_to_name,\n)\n</code></pre>"},{"location":"examples/example_chromatic_aberration/","title":"Example chromatic aberration","text":"Python<pre><code>import cv2\nfrom matplotlib import pyplot as plt\nimport cv2\n\nimport albumentations as A\n</code></pre> Python<pre><code>def visualize(image):\n    plt.figure(figsize=(10, 5))\n    plt.axis('off')\n    plt.imshow(image)\n</code></pre> Python<pre><code>def load_rgb(image_path):\n    image = cv2.imread(image_path)\n    return cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n</code></pre>"},{"location":"examples/example_chromatic_aberration/#load-the-image-from-the-disk","title":"Load the image from the disk","text":"Python<pre><code>img_path = \"../images/alina_rossoshanska.jpeg\"\nimg = load_rgb(img_path)\n</code></pre>"},{"location":"examples/example_chromatic_aberration/#visualize-the-original-image","title":"Visualize the original image","text":"Python<pre><code>visualize(img)\n</code></pre>"},{"location":"examples/example_chromatic_aberration/#red-blue-mode","title":"Red-blue mode","text":"Python<pre><code>transform = A.Compose([A.ChromaticAberration(mode=\"red_blue\", primary_distortion_limit=0.5, secondary_distortion_limit=0.1, p=1)])\n\nplt.figure(figsize=(15, 10))\n\nnum_images = 12\n\n# Loop through the list of images and plot them with subplot\nfor i in range(num_images):\n    transformed_image = transform(image = img)[\"image\"]\n    plt.subplot(4, 3, i + 1)\n    plt.imshow(transformed_image)\n    plt.axis('off')\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"examples/example_d4/","title":"D4 transform","text":"<p>Geomatric transforms are the most widely used augmentations. Mainly becase they do not get data outside of the original data distribution and because they \"They make intuitive sense\". </p> <p>D4 transform maps orignal image to one of 8 states. </p> <ul> <li><code>e</code> - identity. The original image</li> <li><code>r90</code> - rotation by 90 degrees</li> <li><code>r180</code> - rotation by 180 degrees, which is equal to <code>v * h = h * v</code></li> <li><code>r270</code> - rotation by 270 degrees</li> <li><code>v</code> - vertical flip</li> <li><code>hvt</code> - reflection across anti diagonal, which is equal to <code>t * v * h</code> or <code>t * rot180</code> </li> <li><code>h</code> - horizonal flip</li> <li><code>t</code> - reflection actoss the diagonal</li> </ul> <p>The same transform could be represented as </p> <p>Python<pre><code>A.Compose([A.HorizonatalFlip(p=0.5), A.RandomRotate90(p=1)])\n</code></pre> Hence this is just a convenient notation.</p> <p>The transform is useful in situations where imagery data does not have preferred orientation:</p> <p>For example: - medical images - top view drone and satellite imagery</p> <p>Works for: - image - mask - keypoints - bounding boxes</p> Python<pre><code>%matplotlib inline\n</code></pre> Python<pre><code>import json\n</code></pre> Python<pre><code>import hashlib\n</code></pre> Python<pre><code>import random\nimport numpy as np\nimport cv2\nfrom matplotlib import pyplot as plt\n\nimport albumentations as A\n</code></pre> Python<pre><code>BOX_COLOR = (255, 0, 0)\nTEXT_COLOR = (255, 255, 255)\nKEYPOINT_COLOR = (0, 255, 0)\n</code></pre> Python<pre><code>def visualize_bbox(img, bbox, class_name, bbox_color=BOX_COLOR, thickness=1):\n    \"\"\"Visualizes a single bounding box on the image\"\"\"\n    x_min, y_min, x_max, y_max = (int(x) for x in bbox)\n    cv2.rectangle(img, (x_min, y_min), (x_max, y_max), color=bbox_color, thickness=thickness)\n\n    ((text_width, text_height), _) = cv2.getTextSize(class_name, cv2.FONT_HERSHEY_SIMPLEX, 0.35, 1)\n    cv2.rectangle(img, (x_min, y_min - int(1.3 * text_height)), (x_min + text_width, y_min), bbox_color, -1)\n    cv2.putText(\n        img,\n        text=class_name,\n        org=(x_min, y_min - int(0.3 * text_height)),\n        fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n        fontScale=0.35,\n        color=TEXT_COLOR,\n        lineType=cv2.LINE_AA,\n    )\n    return img\n\ndef vis_keypoints(image, keypoints, color=KEYPOINT_COLOR, diameter=3):\n    image = image.copy()\n    for (x, y) in keypoints:\n        cv2.circle(image, (int(x), int(y)), diameter, color, -1)\n    return image\n</code></pre> Python<pre><code>def visualize_one(image, bboxes, keypoints, category_ids, category_id_to_name, mask):\n    # Create a copy of the image to draw on\n    img = image.copy()\n\n    # Apply each bounding box and corresponding category ID\n    for bbox, category_id in zip(bboxes, category_ids):\n        class_name = category_id_to_name[category_id]\n        img = visualize_bbox(img, bbox, class_name)\n\n    # Apply keypoints if provided\n    if keypoints:\n        img = vis_keypoints(img, keypoints)\n\n    # Setup plot\n    fig, ax = plt.subplots(1, 2, figsize=(6, 3))\n\n    # Show the image with annotations\n    ax[0].imshow(img)    \n    ax[0].axis('off')\n\n    # Show the mask\n    ax[1].imshow(mask, cmap='gray')    \n    ax[1].axis('off')\n\n    plt.tight_layout()\n    plt.show()\n</code></pre> Python<pre><code>def visualize(images, bboxes_list, keypoints_list, category_ids_list, category_id_to_name, masks):\n    if len(images) != 8:\n        raise ValueError(\"This function is specifically designed to handle exactly 8 images.\")\n\n    num_rows = 4\n    num_cols = 4\n\n    fig, axs = plt.subplots(num_cols, num_rows, figsize=(20, 20)) \n\n    for idx, (image, bboxes, keypoints, category_ids, mask) in enumerate(zip(images, bboxes_list, keypoints_list, category_ids_list, masks)):\n        img = image.copy()\n\n        # Process each image: draw bounding boxes and keypoints\n        for bbox, category_id in zip(bboxes, category_ids):\n            class_name = category_id_to_name[category_id]\n            img = visualize_bbox(img, bbox, class_name)\n\n        if keypoints:\n            img = vis_keypoints(img, keypoints)\n\n        # Calculate subplot indices\n        row_index = (idx * 2) // num_rows  # Each pair takes two columns in one row\n        col_index_image = (idx * 2) % num_cols  # Image at even index\n        col_index_mask = (idx * 2 + 1) % num_cols  # Mask at odd index right after image\n\n        # Plot the processed image\n        img_ax = axs[row_index, col_index_image]\n        img_ax.imshow(img)        \n        img_ax.axis('off')\n\n        # Plot the corresponding mask\n        mask_ax = axs[row_index, col_index_mask]\n        mask_ax.imshow(mask, cmap='gray')        \n        mask_ax.axis('off')\n\n    plt.tight_layout()\n    plt.show()\n</code></pre> Python<pre><code>with open(\"../data/road_labels.json\") as f:\n    labels = json.load(f)\n</code></pre> Python<pre><code>bboxes = labels[\"bboxes\"]\nkeypoints = labels[\"keypoints\"]\n</code></pre> Python<pre><code>bgr_image = cv2.imread(\"../data/road.jpeg\")\nimage = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2RGB)\n\nmask = cv2.imread(\"../data/road.png\", 0)\n</code></pre> Python<pre><code># In this example we use only one class, hence category_ids is list equal to the number of bounding boxes with only one value\ncategory_ids = [1] * len(labels[\"bboxes\"])\ncategory_id_to_name = {1: \"car\"}\n</code></pre> Python<pre><code>visualize_one(image, bboxes, keypoints, category_ids, category_id_to_name, mask)\n</code></pre> <p></p> Python<pre><code>transform = A.Compose([\n    A.CenterCrop(height=512, width=256, p=1),\n    A.D4(p=1)],\n                     bbox_params=A.BboxParams(format='pascal_voc', label_fields=['category_ids']),\n                     keypoint_params=A.KeypointParams(format='xy'))\n</code></pre> Python<pre><code>transformed = transform(image=image, bboxes=bboxes, category_ids=category_ids, keypoints=keypoints, mask=mask)\n</code></pre> Python<pre><code>def get_hash(image):\n    image_bytes = image.tobytes()\n    hash_md5 = hashlib.md5()\n    hash_md5.update(image_bytes)    \n    return hash_md5.hexdigest()\n</code></pre> Python<pre><code>transformations_dict = {}\n\nfor _ in range(80):\n    transformed = transform(image=image, bboxes=bboxes, category_ids=category_ids, keypoints=keypoints, mask=mask)\n    image_hash = get_hash(transformed[\"image\"])\n\n    if image_hash in transformations_dict:\n        transformations_dict[image_hash]['count'] += 1\n    else:\n        transformations_dict[image_hash] = {\n            \"count\": 1,\n            \"transformed\": transformed\n        }\n</code></pre> <p>The transform generates all 8 possible variants with the same probability, including identity transform</p> Python<pre><code>len(transformations_dict)\n</code></pre> <pre><code>8\n</code></pre> Python<pre><code>for key in transformations_dict:\n    print(key, transformations_dict[key][\"count\"])\n</code></pre> <pre><code>7c795aa3b49e3e6ddc8aa88b8733e722 14\nc085441d5d9caf2f023ecf00d110128b 11\nc714bc2f34652f4602086e7e40ae220d 13\nea0eff187cd3ace9958c4a5816352cd0 8\n9ab005fad5fc545fe637d9fa6e8f61a6 11\n6f6b1cbc99952ed23a35516925a5f674 5\n28b177074878fe87574650377c205697 12\na47ceaaf314a159365c0092867e881e6 6\n</code></pre> Python<pre><code>transformed_list = [value[\"transformed\"] for value in transformations_dict.values()]\n</code></pre> Python<pre><code>images = [x[\"image\"] for x in transformed_list]\nmasks = [x[\"mask\"] for x in transformed_list]\nbboxes_list = [x[\"bboxes\"] for x in transformed_list]\nkeypoints_list = [x[\"keypoints\"] for x in transformed_list]\n\n\ncategory_ids_list = [[1] * len(x[\"bboxes\"]) for x in transformed_list]\ncategory_id_to_name = {1: \"car\"}\n</code></pre> Python<pre><code>visualize(images, bboxes_list, keypoints_list, category_ids_list, category_id_to_name, masks)\n</code></pre> <p></p> Python<pre><code>\n</code></pre>"},{"location":"examples/example_documents/","title":"Morphological Transform","text":"Python<pre><code>import random\n\nimport cv2\nfrom matplotlib import pyplot as plt\n\nimport albumentations as A\n</code></pre> Python<pre><code>def visualize(image):\n    plt.figure(figsize=(10, 5))\n    plt.axis('off')\n    plt.imshow(image)\n</code></pre> Python<pre><code>def load_rgb(image_path):\n    image = cv2.imread(image_path)\n    return cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n</code></pre> <p>Load the image from the disk</p> Python<pre><code>img_path = \"../images/scan.jpeg\"\nimg = load_rgb(img_path)\n</code></pre>"},{"location":"examples/example_documents/#visualize-the-original-image","title":"Visualize the original image","text":"Python<pre><code>visualize(img)\n</code></pre>"},{"location":"examples/example_documents/#dilation","title":"Dilation","text":"<p>Dilation expands the white (foreground) regions in a binary or grayscale image.</p> Python<pre><code>transform = A.Compose([A.Morphological(p=1, scale=(2, 3), operation='dilation')], p=1)\n</code></pre> Python<pre><code>transformed = transform(image=img)\nvisualize(transformed[\"image\"])\n</code></pre> <p></p>"},{"location":"examples/example_documents/#erosion","title":"Erosion","text":"<p>Erosion shrinks the white (foreground) regions in a binary or grayscale image.</p> Python<pre><code>transform = A.Compose([A.Morphological(p=1, scale=(2, 3), operation='erosion')], p=1)\n</code></pre> Python<pre><code>transformed = transform(image=img)\nvisualize(transformed[\"image\"])\n</code></pre> <p></p>"},{"location":"examples/example_domain_adaptation/","title":"Domain adaptation transforms","text":"<p>It is possible to perform style transfer without the use of Neural networks.</p> <p>Resulting imagews do not look as ideal but could be generated on the fly in a reasonable time.</p> Python<pre><code>import random\n\nimport cv2\nfrom matplotlib import pyplot as plt\nfrom pathlib import Path\nimport numpy as np\nimport cv2\n\nimport albumentations as A\n</code></pre> Python<pre><code>def visualize(image):\n    plt.figure(figsize=(10, 5))\n    plt.axis('off')\n    plt.imshow(image)\n</code></pre> Python<pre><code>def load_rgb(image_path):\n    image = cv2.imread(image_path)\n    return cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n</code></pre> Python<pre><code>image1 = load_rgb(\"../images/park1.jpeg\")\nimage2 = load_rgb(\"../images/rain2.jpeg\")\n</code></pre> Python<pre><code>visualize(image1)\n</code></pre> <p></p> Python<pre><code>visualize(image2)\n</code></pre> <p></p>"},{"location":"examples/example_domain_adaptation/#historgram-matching","title":"Historgram matching","text":"<p>This process adjusts the pixel values of an input image to align its histogram with that of a reference image. When dealing with multi-channel images, this alignment occurs separately for each channel, provided that both the input and the reference image have an identical number of channels.</p> <p>Documentation</p> Python<pre><code>transform = A.Compose([A.HistogramMatching(reference_images=[image2], \n                                           read_fn = lambda x: x, \n                                           p=1,\n                                           blend_ratio=(0.3, 0.3)\n                                          )], p=1)\n</code></pre> Python<pre><code>transformed = transform(image=image1)[\"image\"]\n</code></pre> Python<pre><code>visualize(transformed)\n</code></pre> <p></p>"},{"location":"examples/example_domain_adaptation/#fourier-domain-adaptation-fda","title":"Fourier Domain Adaptation (FDA)","text":"<p>Fourier Domain Adaptation (FDA) for simple \"style transfer\" in the context of unsupervised domain adaptation (UDA).     FDA manipulates the frequency components of images to reduce the domain gap between source and target datasets,      effectively adapting images from one domain to closely resemble those from another without altering their semantic content.</p> <p>This transform is particularly beneficial in scenarios where the training (source) and testing (target) images come from different distributions, such as synthetic versus real images, or day versus night scenes. Unlike traditional domain adaptation methods that may require complex adversarial training, FDA achieves domain alignment by swapping low-frequency components of the Fourier transform between the source and target images. This technique has shown to improve the performance of models on the target domain, particularly for tasks like semantic segmentation, without additional training for domain invariance.</p> <p>Documentation</p> Python<pre><code>transform = A.Compose([A.FDA(reference_images=[image1], read_fn = lambda x: x, p=1, beta_limit=(0.2, 0.2))], p=1)\n</code></pre> Python<pre><code>transformed = transform(image=image1)[\"image\"]\nvisualize(transformed)\n</code></pre> <p></p> Python<pre><code>(transformed - image1).mean()\n</code></pre> <pre><code>55.401878356933594\n</code></pre>"},{"location":"examples/example_domain_adaptation/#pixeldistribution","title":"PixelDistribution","text":"<p>Performs pixel-level domain adaptation by aligning the pixel value distribution of an input image with that of a reference image. This process involves fitting a simple statistical (such as PCA, StandardScaler, or MinMaxScaler) to both the original and the reference images, transforming the original image with the transformation trained on it, and then applying the inverse transformation using the transform fitted on the reference image. The result is an adapted image that retains the original content while mimicking the pixel value distribution of the reference domain.</p> <p>The process can be visualized as two main steps: 1. Adjusting the original image to a standard distribution space using a selected transform. 2. Moving the adjusted image into the distribution space of the reference image by applying the inverse        of the transform fitted on the reference image.</p> <p>This technique is especially useful in scenarios where images from different domains (e.g., synthetic vs. real images, day vs. night scenes) need to be harmonized for better consistency or performance in image processing tasks.</p> <p>Documentation</p> Python<pre><code>transform = A.Compose([A.PixelDistributionAdaptation(reference_images=[image1], read_fn = lambda x: x, p=1,  transform_type=\"pca\")], p=1)\n</code></pre> Python<pre><code>transformed = transform(image=image1)[\"image\"]\nvisualize(transformed)\n</code></pre> <p></p> Python<pre><code>beta = 0.1\n</code></pre> Python<pre><code>height, width = image1.shape[:2]\nborder = int(np.floor(min(height, width) * beta))\ncenter_y, center_x = height // 2, width // 2\n\n# Define region for amplitude substitution\ny1, y2 = center_y - border, center_y + border\n\nx1, x2 = center_x - border, center_x + border\n</code></pre> Python<pre><code>x1, x2, y1, y2\n</code></pre> <pre><code>(205, 307, 205, 307)\n</code></pre> Python<pre><code>def low_freq_mutate_np(amp_src, amp_trg, L, h, w):\n    b = int(np.floor(min(h, w) * L))\n    c_h, c_w = h // 2, w // 2\n    h1, h2 = max(0, c_h - b), min(c_h + b, h - 1)\n    w1, w2 = max(0, c_w - b), min(c_w + b, w - 1)\n    amp_src[h1:h2, w1:w2] = amp_trg[h1:h2, w1:w2]\n    return amp_src\n\ndef fourier_domain_adaptation(src_img, trg_img, beta=0.1):\n    assert src_img.shape == trg_img.shape, \"Source and target images must have the same shape.\"\n    src_img = src_img.astype(np.float32)\n    trg_img = trg_img.astype(np.float32)\n\n    height, width, num_channels = src_img.shape\n\n    # Prepare container for the output image\n    src_in_trg = np.zeros_like(src_img)\n\n    for c in range(num_channels):\n        # Perform FFT on each channel\n        fft_src = np.fft.fft2(src_img[:, :, c])\n        fft_trg = np.fft.fft2(trg_img[:, :, c])\n\n        # Shift the zero frequency component to the center\n        fft_src_shifted = np.fft.fftshift(fft_src)\n        fft_trg_shifted = np.fft.fftshift(fft_trg)\n\n        # Extract amplitude and phase\n        amp_src, pha_src = np.abs(fft_src_shifted), np.angle(fft_src_shifted)\n        amp_trg = np.abs(fft_trg_shifted)\n\n        # Mutate the amplitude part of the source with the target\n\n        mutated_amp = low_freq_mutate_np(amp_src.copy(), amp_trg, beta, height, width)\n\n        # Combine the mutated amplitude with the original phase\n        fft_src_mutated = np.fft.ifftshift(mutated_amp * np.exp(1j * pha_src))\n\n        # Perform inverse FFT\n        src_in_trg_channel = np.fft.ifft2(fft_src_mutated)\n\n        # Store the result in the corresponding channel of the output image\n        src_in_trg[:, :, c] = np.real(src_in_trg_channel)\n\n    return np.clip(src_in_trg, 0, 255)\n</code></pre> Python<pre><code>visualize(fourier_domain_adaptation(image2, image1, 0.01).astype(np.uint8))\n</code></pre> <p></p> Python<pre><code>\n</code></pre> Python<pre><code>\n</code></pre>"},{"location":"examples/example_gridshuffle/","title":"Example gridshuffle","text":"Python<pre><code>%matplotlib inline\n</code></pre>"},{"location":"examples/example_gridshuffle/#randomgridshuffle","title":"RandomGridShuffle","text":"<p>This transformation divides the image into a grid and then permutes these grid cells based on a random mapping.</p> <p>It could be useful when only micro features are important for the model, and memorizing the global structure could be harmful. </p> <p>For example:  - Identifying the type of cell phone used to take a picture based on micro artifacts generated by phone post-processing algorithms, rather than the semantic features of the photo. See more at https://ieeexplore.ieee.org/abstract/document/8622031 - Identifying stress, glucose, hydration levels based on skin images.</p> Python<pre><code>import random\nimport numpy as np\nimport cv2\nfrom matplotlib import pyplot as plt\n\nimport albumentations as A\n</code></pre> Python<pre><code>import json\n</code></pre> Python<pre><code>KEYPOINT_COLOR = (0, 255, 0)\n</code></pre> Python<pre><code>def vis_keypoints(image, keypoints, color=KEYPOINT_COLOR, diameter=3):\n    image = image.copy()\n    for (x, y) in keypoints:\n        cv2.circle(image, (int(x), int(y)), diameter, color, -1)\n    return image\n</code></pre> Python<pre><code>def visualize(image, mask, keypoints):\n    # Create a copy of the image to draw on\n    img = image.copy()\n\n    # Apply keypoints if provided\n    if keypoints:\n        img = vis_keypoints(img, keypoints)\n\n    # Setup plot\n    fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n\n    # Show the image with annotations\n    ax[0].imshow(img)    \n    ax[0].axis('off')\n\n    # Show the mask\n    ax[1].imshow(mask, cmap='gray')    \n    ax[1].axis('off')\n\n    plt.tight_layout()\n    plt.show()\n</code></pre> Python<pre><code>with open(\"../data/road_labels.json\") as f:\n    labels = json.load(f)\n</code></pre> Python<pre><code>keypoints = labels[\"keypoints\"]\n</code></pre> Python<pre><code>bgr_image = cv2.imread(\"../data/road.jpeg\")\nimage = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2RGB)\n\nmask = cv2.imread(\"../data/road.png\", 0)\n</code></pre> Python<pre><code>visualize(image, mask, keypoints)\n</code></pre> <p></p> Python<pre><code>transform = A.Compose([A.RandomGridShuffle(grid=(2, 2), p=1)], keypoint_params=A.KeypointParams(format='xy'))\ntransformed = transform(image=image, keypoints=keypoints, mask=mask)\nvisualize(transformed[\"image\"], transformed[\"mask\"], transformed[\"keypoints\"])\n</code></pre> <p></p> Python<pre><code>transform = A.Compose([A.RandomGridShuffle(grid=(3, 3), p=1)], keypoint_params=A.KeypointParams(format='xy'))\ntransformed = transform(image=image, keypoints=keypoints, mask=mask)\nvisualize(transformed[\"image\"], transformed[\"mask\"], transformed[\"keypoints\"])\n</code></pre> <p></p> Python<pre><code>transform = A.Compose([A.RandomGridShuffle(grid=(5, 7), p=1)], keypoint_params=A.KeypointParams(format='xy'))\ntransformed = transform(image=image, keypoints=keypoints, mask=mask)\nvisualize(transformed[\"image\"], transformed[\"mask\"], transformed[\"keypoints\"])\n</code></pre> <p></p>"},{"location":"examples/example_hfhub/","title":"Example on how load and save from Hugging Face Hub","text":"<p>Source</p> <p>Author: Pavel Iakubovskii</p> Python<pre><code>!pip install -U albumentations\n</code></pre> Python<pre><code>from huggingface_hub import notebook_login\n</code></pre> Python<pre><code>notebook_login()\n</code></pre> Python<pre><code>import albumentations as A\nimport numpy as np\n\ntransform = A.Compose([\n    A.RandomCrop(256, 256),\n    A.HorizontalFlip(),\n    A.RandomBrightnessContrast(),\n    A.RGBShift(),\n    A.Normalize(),\n])\n\nevaluation_transform = A.Compose([\n    A.PadIfNeeded(256, 256),\n    A.Normalize(),\n])\n\ntransform.save_pretrained(\"qubvel-hf/albu\", key=\"train\")\n# ^ this will save the transform to a directory \"qubvel-hf/albu\" with filename \"albumentations_config_train.json\"\n\ntransform.save_pretrained(\"qubvel-hf/albu\", key=\"train\", push_to_hub=True)\n# ^ this will save the transform to a directory \"qubvel-hf/albu\" with filename \"albumentations_config_train.json\"\n# + push the transform to the Hub to the repository \"qubvel-hf/albu\"\n\ntransform.push_to_hub(\"qubvel-hf/albu\", key=\"train\")\n# ^ this will push the transform to the Hub to the repository \"qubvel-hf/albu\" (without saving it locally)\n\nloaded_transform = A.Compose.from_pretrained(\"qubvel-hf/albu\", key=\"train\")\n# ^ this will load the transform from local folder if exist or from the Hub repository \"qubvel-hf/albu\"\n\nevaluation_transform.save_pretrained(\"qubvel-hf/albu\", key=\"eval\", push_to_hub=True)\n# ^ this will save the transform to a directory \"qubvel-hf/albu\" with filename \"albumentations_config_eval.json\"\n\nloaded_evaluation_transform = A.Compose.from_pretrained(\"qubvel-hf/albu\", key=\"eval\")\n# ^ this will load the transform from the Hub repository \"qubvel-hf/albu\"\n</code></pre> Python<pre><code># check\nimport numpy as np\n\nimage = np.random.randint(0, 255, (100, 200, 3), dtype=np.uint8)\n\npreprocessed_image_1 = evaluation_transform(image=image)[\"image\"]\npreprocessed_image_2 = loaded_evaluation_transform(image=image)[\"image\"]\n\nassert np.allclose(preprocessed_image_1, preprocessed_image_2)\n</code></pre>"},{"location":"examples/example_kaggle_salt/","title":"Using Albumentations for a semantic segmentation task","text":"<p>We will use images and data from the TGS Salt Identification Challenge.</p>"},{"location":"examples/example_kaggle_salt/#import-the-required-libraries","title":"Import the required libraries","text":"Python<pre><code>import random\n\nimport cv2\nfrom matplotlib import pyplot as plt\n\nimport albumentations as A\n</code></pre>"},{"location":"examples/example_kaggle_salt/#define-a-function-to-visualize-images-and-masks","title":"Define a function to visualize images and masks","text":"Python<pre><code>def visualize(image, mask, original_image=None, original_mask=None):\n    fontsize = 18\n\n    if original_image is None and original_mask is None:\n        f, ax = plt.subplots(2, 1, figsize=(8, 8))\n\n        ax[0].imshow(image)\n        ax[1].imshow(mask)\n    else:\n        f, ax = plt.subplots(2, 2, figsize=(8, 8))\n\n        ax[0, 0].imshow(original_image)\n        ax[0, 0].set_title('Original image', fontsize=fontsize)\n\n        ax[1, 0].imshow(original_mask)\n        ax[1, 0].set_title('Original mask', fontsize=fontsize)\n\n        ax[0, 1].imshow(image)\n        ax[0, 1].set_title('Transformed image', fontsize=fontsize)\n\n        ax[1, 1].imshow(mask)\n        ax[1, 1].set_title('Transformed mask', fontsize=fontsize)\n</code></pre>"},{"location":"examples/example_kaggle_salt/#read-an-image-and-its-mask-from-the-disk","title":"Read an image and its mask from the disk","text":"Python<pre><code>image = cv2.imread('images/kaggle_salt/0fea4b5049_image.png')\nmask = cv2.imread('images/kaggle_salt/0fea4b5049.png', cv2.IMREAD_GRAYSCALE)\n</code></pre>"},{"location":"examples/example_kaggle_salt/#original-image","title":"Original image","text":"Python<pre><code>print(image.shape, mask.shape)\n</code></pre> <pre><code>(101, 101, 3) (101, 101)\n</code></pre> Python<pre><code>original_height, original_width = image.shape[:2]\n</code></pre> Python<pre><code>visualize(image, mask)\n</code></pre> <p># Padding</p> <p>UNet type architecture requires input image size be divisible by 2^N, where N is the number of maxpooling layers. In the vanilla UNet N=5 \\Longrightarrow, we need to pad input images to the closest divisible by 2^5 = 32 number, which is 128. This operation may be performed using PadIfNeeded transformation. It pads both the image and the mask on all four sides. Padding type (zero, constant, reflection) may be specified. The default padding is reflection padding.</p> Python<pre><code>aug = A.PadIfNeeded(min_height=128, min_width=128, p=1)\n\naugmented = aug(image=image, mask=mask)\n\nimage_padded = augmented['image']\nmask_padded = augmented['mask']\n\nprint(image_padded.shape, mask_padded.shape)\n\nvisualize(image_padded, mask_padded, original_image=image, original_mask=mask)\n</code></pre> <pre><code>(128, 128, 3) (128, 128)\n</code></pre> <p></p>"},{"location":"examples/example_kaggle_salt/#centercrop-and-crop","title":"CenterCrop and Crop","text":"<p>To get to the original image and mask from the padded version, we may use CenterCrop or Crop transformations.</p> Python<pre><code>aug = A.CenterCrop(p=1, height=original_height, width=original_width)\n\naugmented = aug(image=image_padded, mask=mask_padded)\n\nimage_center_cropped = augmented['image']\nmask_center_cropped = augmented['mask']\n\nprint(image_center_cropped.shape, mask_center_cropped.shape)\n\nassert (image - image_center_cropped).sum() == 0\nassert (mask - mask_center_cropped).sum() == 0\n\nvisualize(image_padded, mask_padded, original_image=image_center_cropped, original_mask=mask_center_cropped)\n</code></pre> <pre><code>(101, 101, 3) (101, 101)\n</code></pre> <p></p> Python<pre><code>x_min = (128 - original_width) // 2\ny_min = (128 - original_height) // 2\n\nx_max = x_min + original_width\ny_max = y_min + original_height\n\naug = A.Crop(x_min=x_min, x_max=x_max, y_min=y_min, y_max=y_max, p=1)\n\naugmented = aug(image=image_padded, mask=mask_padded)\n\nimage_cropped = augmented['image']\nmask_cropped = augmented['mask']\n\nprint(image_cropped.shape, mask_cropped.shape)\n\nassert (image - image_cropped).sum() == 0\nassert (mask - mask_cropped).sum() == 0\n\nvisualize(image_cropped, mask_cropped, original_image=image_padded, original_mask=mask_padded)\n</code></pre> <pre><code>(101, 101, 3) (101, 101)\n</code></pre> <p></p>"},{"location":"examples/example_kaggle_salt/#non-destructive-transformations-dehidral-group-d4","title":"Non destructive transformations. Dehidral group D4","text":"<p>For images for which there is no clear notion of top like this one, satellite and aerial imagery or medical imagery is typically a good idea to add transformations that do not add or lose the information.</p> <p>There are eight distinct ways to represent the same square on the plane.</p> <p></p> <p>Combinations of the transformations HorizontalFlip, VerticalFlip, Transpose, RandomRotate90 will be able to get the original image to all eight states.</p> <p>## HorizontalFlip</p> Python<pre><code>aug = A.HorizontalFlip(p=1)\n\naugmented = aug(image=image, mask=mask)\n\nimage_h_flipped = augmented['image']\nmask_h_flipped = augmented['mask']\n\nvisualize(image_h_flipped, mask_h_flipped, original_image=image, original_mask=mask)\n</code></pre> <p></p>"},{"location":"examples/example_kaggle_salt/#verticalflip","title":"VerticalFlip","text":"Python<pre><code>aug = A.VerticalFlip(p=1)\n\naugmented = aug(image=image, mask=mask)\n\nimage_v_flipped = augmented['image']\nmask_v_flipped = augmented['mask']\n\nvisualize(image_v_flipped, mask_v_flipped, original_image=image, original_mask=mask)\n</code></pre>"},{"location":"examples/example_kaggle_salt/#randomrotate90-randomly-rotates-by-0-90-180-270-degrees","title":"RandomRotate90 (Randomly rotates by 0, 90, 180, 270 degrees)","text":"Python<pre><code>aug = A.RandomRotate90(p=1)\n\naugmented = aug(image=image, mask=mask)\n\nimage_rot90 = augmented['image']\nmask_rot90 = augmented['mask']\n\nvisualize(image_rot90, mask_rot90, original_image=image, original_mask=mask)\n</code></pre> <p>## Transpose (switch X and Y axis) </p> Python<pre><code>aug = A.Transpose(p=1)\n\naugmented = aug(image=image, mask=mask)\n\nimage_transposed = augmented['image']\nmask_transposed = augmented['mask']\n\nvisualize(image_transposed, mask_transposed, original_image=image, original_mask=mask)\n</code></pre> <p></p>"},{"location":"examples/example_kaggle_salt/#non-rigid-transformations-elastictransform-griddistortion-opticaldistortion","title":"Non-rigid transformations: ElasticTransform, GridDistortion, OpticalDistortion","text":"<p>In medical imaging problems, non-rigid transformations help to augment the data. It is unclear if they will help with this problem, but let's look at them. We will consider ElasticTransform, GridDistortion, OpticalDistortion.</p> <p>We fix the random seed for visualization purposes, so the augmentation will always produce the same result. In a real computer vision pipeline, you shouldn't fix the random seed before applying a transform to the image because, in that case, the pipeline will always output the same image. The purpose of image augmentation is to use different transformations each time.</p>"},{"location":"examples/example_kaggle_salt/#elastictransform","title":"ElasticTransform","text":"Python<pre><code>aug = A.ElasticTransform(p=1, alpha=120, sigma=120 * 0.05, alpha_affine=120 * 0.03)\n\nrandom.seed(7)\naugmented = aug(image=image, mask=mask)\n\nimage_elastic = augmented['image']\nmask_elastic = augmented['mask']\n\nvisualize(image_elastic, mask_elastic, original_image=image, original_mask=mask)\n</code></pre>"},{"location":"examples/example_kaggle_salt/#griddistortion","title":"GridDistortion","text":"Python<pre><code>aug = A.GridDistortion(p=1)\n\nrandom.seed(7)\naugmented = aug(image=image, mask=mask)\n\nimage_grid = augmented['image']\nmask_grid = augmented['mask']\n\nvisualize(image_grid, mask_grid, original_image=image, original_mask=mask)\n</code></pre>"},{"location":"examples/example_kaggle_salt/#opticaldistortion","title":"OpticalDistortion","text":"Python<pre><code>aug = A.OpticalDistortion(distort_limit=2, shift_limit=0.5, p=1)\n\nrandom.seed(7)\naugmented = aug(image=image, mask=mask)\n\nimage_optical = augmented['image']\nmask_optical = augmented['mask']\n\nvisualize(image_optical, mask_optical, original_image=image, original_mask=mask)\n</code></pre>"},{"location":"examples/example_kaggle_salt/#randomsizedcrop","title":"RandomSizedCrop","text":"<p>One may combine RandomCrop and RandomScale but there is a transformation RandomSizedCrop that allows to combine them into one transformation.</p> Python<pre><code>aug = A.RandomSizedCrop(min_max_height=(50, 101), height=original_height, width=original_width, p=1)\n\nrandom.seed(7)\naugmented = aug(image=image, mask=mask)\n\nimage_scaled = augmented['image']\nmask_scaled = augmented['mask']\n\nvisualize(image_scaled, mask_scaled, original_image=image, original_mask=mask)\n</code></pre> <p></p>"},{"location":"examples/example_kaggle_salt/#lets-try-to-combine-different-transformations","title":"Let's try to combine different transformations","text":"<p>Light non-destructive augmentations.</p> Python<pre><code>aug = A.Compose([\n    A.VerticalFlip(p=0.5),\n    A.RandomRotate90(p=0.5)]\n)\n\nrandom.seed(7)\naugmented = aug(image=image, mask=mask)\n\nimage_light = augmented['image']\nmask_light = augmented['mask']\n\nvisualize(image_light, mask_light, original_image=image, original_mask=mask)\n</code></pre> <p></p>"},{"location":"examples/example_kaggle_salt/#lets-add-non-rigid-transformations-and-randomsizedcrop","title":"Let's add non rigid transformations and RandomSizedCrop","text":""},{"location":"examples/example_kaggle_salt/#medium-augmentations","title":"Medium augmentations","text":"Python<pre><code>aug = A.Compose([\n    A.OneOf([\n        A.RandomSizedCrop(min_max_height=(50, 101), height=original_height, width=original_width, p=0.5),\n        A.PadIfNeeded(min_height=original_height, min_width=original_width, p=0.5)\n    ],p=1),\n    A.VerticalFlip(p=0.5),\n    A.RandomRotate90(p=0.5),\n    A.OneOf([\n        A.ElasticTransform(p=0.5, alpha=120, sigma=120 * 0.05, alpha_affine=120 * 0.03),\n        A.GridDistortion(p=0.5),\n        A.OpticalDistortion(distort_limit=1, shift_limit=0.5, p=1),\n    ], p=0.8)])\n\nrandom.seed(11)\naugmented = aug(image=image, mask=mask)\n\nimage_medium = augmented['image']\nmask_medium = augmented['mask']\n\nvisualize(image_medium, mask_medium, original_image=image, original_mask=mask)\n</code></pre>"},{"location":"examples/example_kaggle_salt/#lets-add-non-spatial-stransformations","title":"Let's add non-spatial stransformations.","text":"<p>Many non-spatial transformations like CLAHE, RandomBrightness, RandomContrast, RandomGamma can be also added. They will be applied only to the image and not the mask.</p> Python<pre><code>aug = A.Compose([\n    A.OneOf([\n        A.RandomSizedCrop(min_max_height=(50, 101), height=original_height, width=original_width, p=0.5),\n        A.PadIfNeeded(min_height=original_height, min_width=original_width, p=0.5)\n    ], p=1),\n    A.VerticalFlip(p=0.5),\n    A.RandomRotate90(p=0.5),\n    A.OneOf([\n        A.ElasticTransform(alpha=120, sigma=120 * 0.05, alpha_affine=120 * 0.03, p=0.5),\n        A.GridDistortion(p=0.5),\n        A.OpticalDistortion(distort_limit=2, shift_limit=0.5, p=1)\n        ], p=0.8),\n    A.CLAHE(p=0.8),\n    A.RandomBrightnessContrast(p=0.8),\n    A.RandomGamma(p=0.8)])\n\nrandom.seed(11)\naugmented = aug(image=image, mask=mask)\n\nimage_heavy = augmented['image']\nmask_heavy = augmented['mask']\n\nvisualize(image_heavy, mask_heavy, original_image=image, original_mask=mask)\n</code></pre> <p></p>"},{"location":"examples/example_keypoints/","title":"Using Albumentations to augment keypoints","text":"<p>In this notebook we will show how to apply Albumentations to the keypoint augmentation problem. Please refer to A list of transforms and their supported targets to see which spatial-level augmentations support keypoints. You can use any pixel-level augmentation to an image with keypoints because pixel-level augmentations don't affect keypoints.</p> <p>Note: by default, augmentations that work with keypoints don't change keypoints' labels after transformation. If keypoints' labels are side-specific, that may pose a problem. For example, if you have a keypoint named <code>left arm</code> and apply a HorizontalFlip augmentation, you will get a keypoint with the same <code>left arm</code> label, but it will now look like a <code>right arm</code> keypoint. See a picture at the end of this article for a visual example.</p> <p>If you work with such type of keypoints, consider using SymmetricKeypoints augmentations from albumentations-experimental that are created precisely to handle that case.</p>"},{"location":"examples/example_keypoints/#import-the-required-libraries","title":"Import the required libraries","text":"Python<pre><code>import random\n\nimport cv2\nfrom matplotlib import pyplot as plt\n\nimport albumentations as A\n</code></pre>"},{"location":"examples/example_keypoints/#define-a-function-to-visualize-keypoints-on-an-image","title":"Define a function to visualize keypoints on an image","text":"Python<pre><code>KEYPOINT_COLOR = (0, 255, 0) # Green\n\ndef vis_keypoints(image, keypoints, color=KEYPOINT_COLOR, diameter=15):\n    image = image.copy()\n\n    for (x, y) in keypoints:\n        cv2.circle(image, (int(x), int(y)), diameter, (0, 255, 0), -1)\n\n    plt.figure(figsize=(8, 8))\n    plt.axis('off')\n    plt.imshow(image)\n</code></pre>"},{"location":"examples/example_keypoints/#get-an-image-and-annotations-for-it","title":"Get an image and annotations for it","text":"Python<pre><code>image = cv2.imread('images/keypoints_image.jpg')\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n</code></pre>"},{"location":"examples/example_keypoints/#define-keypoints","title":"Define keypoints","text":"<p>We will use the <code>xy</code> format for keypoints' coordinates. Each keypoint is defined with two coordinates, <code>x</code> is the position on the x-axis, and <code>y</code> is the position on the y-axis. Please refer to this article with the detailed description of formats for keypoints' coordinates - https://albumentations.ai/docs/getting_started/keypoints_augmentation/</p> Python<pre><code>keypoints = [\n    (100, 100),\n    (720, 410),\n    (1100, 400),\n    (1700, 30),\n    (300, 650),\n    (1570, 590),\n    (560, 800),\n    (1300, 750),\n    (900, 1000),\n    (910, 780),\n    (670, 670),\n    (830, 670),\n    (1000, 670),\n    (1150, 670),\n    (820, 900),\n    (1000, 900),\n]\n</code></pre>"},{"location":"examples/example_keypoints/#visualize-the-original-image-with-keypoints","title":"Visualize the original image with keypoints","text":"Python<pre><code>vis_keypoints(image, keypoints)\n</code></pre>"},{"location":"examples/example_keypoints/#define-a-simple-augmentation-pipeline","title":"Define a simple augmentation pipeline","text":"Python<pre><code>transform = A.Compose(\n    [A.HorizontalFlip(p=1)],\n    keypoint_params=A.KeypointParams(format='xy')\n)\ntransformed = transform(image=image, keypoints=keypoints)\nvis_keypoints(transformed['image'], transformed['keypoints'])\n</code></pre>"},{"location":"examples/example_keypoints/#a-few-more-examples-of-augmentation-pipelines","title":"A few more examples of augmentation pipelines","text":"Python<pre><code>transform = A.Compose(\n    [A.VerticalFlip(p=1)],\n    keypoint_params=A.KeypointParams(format='xy')\n)\ntransformed = transform(image=image, keypoints=keypoints)\nvis_keypoints(transformed['image'], transformed['keypoints'])\n</code></pre> <p>We fix the random seed for visualization purposes, so the augmentation will always produce the same result. In a real computer vision pipeline, you shouldn't fix the random seed before applying a transform to the image because, in that case, the pipeline will always output the same image. The purpose of image augmentation is to use different transformations each time.</p> Python<pre><code>random.seed(7)\ntransform = A.Compose(\n    [A.RandomCrop(width=768, height=768, p=1)],\n    keypoint_params=A.KeypointParams(format='xy')\n)\ntransformed = transform(image=image, keypoints=keypoints)\nvis_keypoints(transformed['image'], transformed['keypoints'])\n</code></pre> <p></p> Python<pre><code>random.seed(7)\ntransform = A.Compose(\n    [A.Rotate(p=0.5)],\n    keypoint_params=A.KeypointParams(format='xy')\n)\ntransformed = transform(image=image, keypoints=keypoints)\nvis_keypoints(transformed['image'], transformed['keypoints'])\n</code></pre> <p></p> Python<pre><code>transform = A.Compose(\n    [A.CenterCrop(height=512, width=512, p=1)],\n    keypoint_params=A.KeypointParams(format='xy')\n)\ntransformed = transform(image=image, keypoints=keypoints)\nvis_keypoints(transformed['image'], transformed['keypoints'])\n</code></pre> <p></p> Python<pre><code>random.seed(7)\ntransform = A.Compose(\n    [A.ShiftScaleRotate(p=0.5)],\n    keypoint_params=A.KeypointParams(format='xy')\n)\ntransformed = transform(image=image, keypoints=keypoints)\nvis_keypoints(transformed['image'], transformed['keypoints'])\n</code></pre> <p></p>"},{"location":"examples/example_keypoints/#an-example-of-complex-augmentation-pipeline","title":"An example of complex augmentation pipeline","text":"Python<pre><code>random.seed(7)\ntransform = A.Compose([\n        A.RandomSizedCrop(min_max_height=(256, 1025), height=512, width=512, p=0.5),\n        A.HorizontalFlip(p=0.5),\n        A.OneOf([\n            A.HueSaturationValue(p=0.5),\n            A.RGBShift(p=0.7)\n        ], p=1),\n        A.RandomBrightnessContrast(p=0.5)\n    ],\n    keypoint_params=A.KeypointParams(format='xy'),\n)\ntransformed = transform(image=image, keypoints=keypoints)\nvis_keypoints(transformed['image'], transformed['keypoints'])\n</code></pre>"},{"location":"examples/example_mixup/","title":"MixUp transform in Albumentations","text":"<p>In that transform we create weighted average of original and reference images. Transform also supports global_labels and masks</p> Python<pre><code>import random\n\nimport cv2\nfrom matplotlib import pyplot as plt\nfrom pathlib import Path\nimport numpy as np\nimport cv2\n\nimport albumentations as A\n</code></pre>"},{"location":"examples/example_mixup/#define-a-function-to-visualize-an-image","title":"Define a function to visualize an image","text":"Python<pre><code>def visualize(image):\n    plt.figure(figsize=(10, 5))\n    plt.axis('off')\n    plt.imshow(image)\n</code></pre> Python<pre><code>def load_rgb(image_path):\n    image = cv2.imread(image_path)\n    return cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n</code></pre>"},{"location":"examples/example_mixup/#load-the-image-from-the-disk","title":"Load the image from the disk","text":"Python<pre><code>img_path = \"../images/woman.jpeg\"\nimg = load_rgb(img_path)\n</code></pre> Python<pre><code>class_id = 0\n</code></pre>"},{"location":"examples/example_mixup/#visualize-the-original-image","title":"Visualize the original image","text":"Python<pre><code>visualize(img)\n</code></pre>"},{"location":"examples/example_mixup/#mixup-transform","title":"Mixup transform","text":"<p>To use transform we need to define reference data that could be any sequence or generator.</p> <p>We also need to defined <code>read_fn</code> that transforms items from <code>reference_data</code> to dictionaries with keys: <code>image</code>, and optional <code>global_label</code>, <code>mask</code>.</p> Python<pre><code>reference_data = [{\n    \"image_path\": \"../images/cat1.jpeg\",\n    \"class_id\": 1},\n                  {\"image_path\": \"../images/tiger.jpeg\",\n    \"class_id\": 2}]\n</code></pre> Python<pre><code>def int_to_onehot(value, num_classes):\n    \"\"\"Convert an array of integers to one-hot representation.\n\n    Args:\n        values (np.ndarray): Array of integers to be converted.\n        num_classes (int): Total number of classes, determines the length of one-hot vectors.\n\n    Returns:\n        np.ndarray: One-hot encoded representation of `values`.\n    \"\"\"\n    # Initialize the one-hot encoded array of shape (num_classes,)\n    one_hot = np.zeros(num_classes, dtype=int)\n\n    # Set the appropriate index to one\n    one_hot[value] = 1\n\n    return one_hot\n</code></pre> Python<pre><code>NUM_CLASSES = 5\n</code></pre> Python<pre><code>target_height = 2500\ntarget_width = 1800\n</code></pre> Python<pre><code># We can process data as we want, including application of augmentations transform.\n\nreference_aug = A.Compose([A.RandomCrop(width=target_width, height=target_height, p=1)], p=1)\n</code></pre> Python<pre><code>def read_fn(item):\n    image = load_rgb(item[\"image_path\"])\n\n    transformed_image = reference_aug(image=image)[\"image\"]\n\n    global_label = int_to_onehot(item[\"class_id\"], NUM_CLASSES)\n    return {\n        \"image\": transformed_image,\n        \"global_label\": global_label\n    }\n</code></pre>"},{"location":"examples/example_mixup/#show-reference-images","title":"Show reference images","text":"Python<pre><code>visualize(read_fn(reference_data[0])[\"image\"])\n</code></pre> Python<pre><code>visualize(read_fn(reference_data[1])[\"image\"])\n</code></pre> Python<pre><code>transform = A.Compose([A.RandomCrop(width=target_width, height=target_height, p=1),\n                                       A.MixUp(reference_data=reference_data,\n                                              read_fn=read_fn, p=1),\n                                      A.HorizontalFlip(p=1)], p=1)\n</code></pre> Python<pre><code>original_global_label = int_to_onehot(class_id, NUM_CLASSES)\n</code></pre> Python<pre><code>transformed = transform(image=img, global_label=original_global_label)\nprint(\"Global label = \", transformed[\"global_label\"])\nprint(\"Mixing coefficient = \", transformed[\"mix_coef\"])\n\nvisualize(transformed[\"image\"])\n</code></pre> <pre><code>Global label =  [0.894349 0.105651 0.       0.       0.      ]\nMixing coefficient =  0.8943490049245845\n</code></pre> Python<pre><code>transformed = transform(image=img, global_label=original_global_label)\nprint(\"Global label = \", transformed[\"global_label\"])\nprint(\"Mixing coefficient = \", transformed[\"mix_coef\"])\n\nvisualize(transformed[\"image\"])\n</code></pre> <pre><code>Global label =  [0.00142189 0.         0.99857811 0.         0.        ]\nMixing coefficient =  0.0014218885525450404\n</code></pre>"},{"location":"examples/example_mixup/#what-if-you-need-to-know-image-that-was-used-for-mixing","title":"What if you need to know image that was used for mixing?","text":"Python<pre><code>mix_data = transformed[\"mix_data\"]\n\nmixing_image = mix_data[\"image\"]\nglobal_label_of_mixing_image = mix_data[\"global_label\"]\n</code></pre>"},{"location":"examples/example_multi_target/","title":"Applying the same augmentation with the same parameters to multiple images, masks, bounding boxes, or keypoints","text":"<p>Sometimes you want to apply the same set of augmentations to multiple input objects of the same type. For example, you might have a set of frames from the video, and you want to augment them in the same way. Or you may have multiple masks for the same image, and you want to apply the same augmentation for all of them.</p> <p>In Albumentations, you can declare additional targets and their types using the <code>additional_targets</code> argument to <code>Compose</code>. </p> <p>For the name of an additional target, you can use any string value that is also a valid argument name in Python. Later, you will use those names to pass additional targets to a transformation pipeline. So you can't use a string that starts with a digit, such as <code>'0image'</code> because it is not a valid Python argument name.</p> <p>The type could be either <code>image</code>, <code>mask</code>, <code>bboxes</code>, or <code>keypoints</code>.</p> <p>An example definition of <code>Compose</code> that supports multiple inputs of the same type may be the following:</p> Text Only<pre><code>import albumentations as A\n\ntransform = A.Compose(\n    [HorizontalFlip(p=0.5), ...],\n    additional_targets={\n        'image1': 'image',\n        'image2': 'image',\n        ...\n        'imageN': 'image',\n\n        'bboxes1': 'bboxes',\n        'bboxes1': 'bboxes',\n        ...\n        'bboxesM': 'bboxes',\n\n        'keypoints1': 'keypoints',\n        'keypoints2': 'keypoints',\n        ...\n        'keypointsK': 'keypoints',\n\n        'mask1': 'mask',\n        'mask2': 'mask',\n        ...\n        'maskL': 'mask'\n    })\n)\n</code></pre> <p>Note: there is also an alternative way to apply the same augmentation to multiple inputs such as images, masks, etc.</p> <p><code>ReplayCompose</code> is a tool that could record augmentation parameters applied to one set of inputs (e.g., an image and an associated mask) and then use the recorded values to augment another set of inputs in the same way.</p> <p>You can read more about <code>ReplayCompose</code> here.</p>"},{"location":"examples/example_multi_target/#import-the-required-libraries","title":"Import the required libraries","text":"Python<pre><code>import random\n\nimport cv2\nfrom matplotlib import pyplot as plt\n\nimport albumentations as A\n</code></pre>"},{"location":"examples/example_multi_target/#define-a-function-to-visualize-an-image","title":"Define a function to visualize an image","text":"Python<pre><code>def visualize(image):\n    plt.figure(figsize=(10, 10))\n    plt.axis('off')\n    plt.imshow(image)\n</code></pre>"},{"location":"examples/example_multi_target/#load-images-from-the-disk","title":"Load images from the disk","text":"Python<pre><code>image = cv2.imread('images/multi_target_1.jpg')\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\nimage0 = cv2.imread('images/multi_target_2.jpg')\nimage0 = cv2.cvtColor(image0, cv2.COLOR_BGR2RGB)\nimage1 = cv2.imread('images/multi_target_3.jpg')\nimage1 = cv2.cvtColor(image1, cv2.COLOR_BGR2RGB)\n</code></pre>"},{"location":"examples/example_multi_target/#show-original-images","title":"Show original images","text":"Python<pre><code>visualize(image)\n</code></pre> Python<pre><code>visualize(image0)\n</code></pre> Python<pre><code>visualize(image1)\n</code></pre>"},{"location":"examples/example_multi_target/#define-an-augmentation-pipeline","title":"Define an augmentation pipeline","text":"<p>The pipeline expects three images as inputs named <code>image</code>, <code>image0</code>, and <code>image1</code>. Then the pipeline will augment those three images in the same way. So it will apply the same set of transformations with the same parameters.</p> Python<pre><code>transform = A.Compose(\n    [A.VerticalFlip(p=1)],\n    additional_targets={'image0': 'image', 'image1': 'image'}\n)\n\n# original object and additional targets of the same type should have the same shape\nimage = image[:503, :723]\nimage1 = image[:503, :723]\n</code></pre> Python<pre><code>transformed = transform(image=image, image0=image0, image1=image1)\n</code></pre> Python<pre><code>visualize(transformed['image'])\n</code></pre> <p></p> Python<pre><code>visualize(transformed['image0'])\n</code></pre> <p></p> Python<pre><code>visualize(transformed['image1'])\n</code></pre> <p></p>"},{"location":"examples/example_multi_target/#an-example-of-more-complex-pipeline","title":"An example of more complex pipeline","text":"Python<pre><code>transform = A.Compose(\n    [\n        A.HorizontalFlip(p=0.5),\n        A.ShiftScaleRotate(p=0.5),\n        A.RandomBrightnessContrast(p=0.2),\n        A.RGBShift(p=0.2),\n    ],\n    additional_targets={'image0': 'image', 'image1': 'image'}\n)\n</code></pre> Python<pre><code>random.seed(42)\ntransformed = transform(image=image, image0=image0, image1=image1)\n</code></pre> Python<pre><code>visualize(transformed['image'])\n</code></pre> Python<pre><code>visualize(transformed['image0'])\n</code></pre> Python<pre><code>visualize(transformed['image1'])\n</code></pre>"},{"location":"examples/example_textimage/","title":"Example textimage","text":"YAML<pre><code>---\ntitle: \"Example on how to apply TextImage Augmentation\"\ndescription: \"An example of using Albumentations to add text to an image, featuring Mooze and Meeko\"\nimage: \"images/cats.jpg\"\n---\n</code></pre>"},{"location":"examples/example_textimage/#example-on-how-to-write-on-top-of-images","title":"Example on how to write on top of images","text":"<p>Note: - Code for the transform is based on the code from https://github.com/danaaubakirova/doc-augmentation by Dana Aubakirova  - Many thanks to Sarah Bieszczad for letting us feature her cats Mooze (small one) and Meeko (the giant) in our project</p> <p>Important!</p> <p>As input this transform takes bounding boxes in the Albumentations format, which normalized Pascal VOC. I.e.</p> <p><code>bbox = [x_min / width, y_min / height, x_max / width, y_max, height]</code></p> <p>For this transform to work we need to install optional dependency <code>pillow</code></p> Python<pre><code>from __future__ import annotations\n</code></pre> Python<pre><code>%load_ext autoreload\n%autoreload 2\n</code></pre> Python<pre><code>!pip install -U pillow\n</code></pre> <pre><code>Requirement already satisfied: pillow in /Users/vladimiriglovikov/anaconda3/envs/albumentations_examples/lib/python3.10/site-packages (10.4.0)\n</code></pre> Python<pre><code>import albumentations as A\nimport cv2\nfrom matplotlib import pyplot as plt\n</code></pre> Python<pre><code>def visualize(image):\n    plt.figure(figsize=(10, 5))\n    plt.axis('off')\n    plt.imshow(image)\n</code></pre> Python<pre><code>bgr_image = cv2.imread(\"images/cats.jpg\")\nimage = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2RGB)\n</code></pre> Python<pre><code>font_path = \"../data/documents/LiberationSerif-Regular.ttf\"\n</code></pre> Python<pre><code>visualize(image)\n</code></pre> <p></p>"},{"location":"examples/example_textimage/#write-text","title":"Write text","text":"Python<pre><code>transform = A.Compose([A.TextImage(font_path=font_path, p=1, font_color=\"yellow\")])\n</code></pre> Python<pre><code>metadata = {\n    \"bbox\": [0.15, 0.9, 0.9, 0.98],\n    \"text\": \"Mooze and Meeko\",\n}\n</code></pre> Python<pre><code>transformed = transform(image=image, textimage_metadata=metadata)\n</code></pre> Python<pre><code>visualize(transformed[\"image\"])\n</code></pre>"},{"location":"examples/example_textimage/#inpaint-background","title":"Inpaint background","text":"<p>We black out parts of the image where insert text and inpaint them. Could be useful when replacing old text with a new one.</p> Python<pre><code>transform = A.Compose([A.TextImage(font_path=font_path, p=1, font_color=(255, 0, 0), clear_bg=True)])\n</code></pre> Python<pre><code>metadata = {\n    \"bbox\": [0.1, 0.3, 0.9, 0.38],\n    \"text\": \"Dangerous Tigers\",\n}\n</code></pre> Python<pre><code>transformed = transform(image=image, textimage_metadata=metadata)\n</code></pre> Python<pre><code>visualize(transformed[\"image\"])\n</code></pre> <p></p>"},{"location":"examples/example_textimage/#write-several-lines","title":"Write several lines","text":"Python<pre><code>transform = A.Compose([A.TextImage(font_path=font_path, p=1, font_color=\"black\", clear_bg=True)])\n</code></pre> Python<pre><code>metadata = [{\n    \"bbox\": [0.02, 0.1, 0.95, 0.17],\n    \"text\": \"Big dreams in small packages...\",\n},\n            {\n    \"bbox\": [0.02, 0.85, 0.95, 0.91],\n    \"text\": \"...and even bigger in bigger ones.\"}\n           ]\n</code></pre> Python<pre><code>transformed = transform(image=image, textimage_metadata=metadata)\n</code></pre> Python<pre><code>visualize(transformed[\"image\"])\n</code></pre>"},{"location":"examples/example_textimage/#augment-text","title":"Augment text","text":"<p>We can insert text as is, or augment it on the fly.</p>"},{"location":"examples/example_textimage/#swap-words","title":"Swap words","text":"Python<pre><code>transform = A.Compose([A.TextImage(font_path=font_path, p=1, font_color=\"white\", augmentations=[\"swap\"])])\n</code></pre> Python<pre><code>metadata = [{\n    \"bbox\": [0.02, 0.1, 0.95, 0.16],\n    \"text\": \"Big dreams in small packages...\",\n},\n            {\n    \"bbox\": [0.02, 0.85, 0.95, 0.91],\n    \"text\": \"...and even bigger in bigger ones.\"}\n           ]\n</code></pre> Python<pre><code>transformed = transform(image=image, textimage_metadata=metadata)\n</code></pre> Python<pre><code>visualize(transformed[\"image\"])\n</code></pre>"},{"location":"examples/example_textimage/#random-deletion","title":"Random Deletion","text":"Python<pre><code>transform = A.Compose([A.TextImage(font_path=font_path, p=1, font_color=\"red\", augmentations=[\"deletion\"])])\n</code></pre> Python<pre><code>metadata = [{\n    \"bbox\": [0.02, 0.1, 0.95, 0.16],\n    \"text\": \"Growing up with a giant...\",\n},\n            {\n    \"bbox\": [0.02, 0.85, 0.95, 0.91],\n    \"text\": \"...is always an adventure..\"}\n           ]\n</code></pre> Python<pre><code>transformed = transform(image=image, textimage_metadata=metadata)\n</code></pre> Python<pre><code>visualize(transformed[\"image\"])\n</code></pre>"},{"location":"examples/example_textimage/#insert-random-stopwords","title":"Insert random stopwords","text":"Python<pre><code>!pip install nltk\n</code></pre> <pre><code>Requirement already satisfied: nltk in /Users/vladimiriglovikov/anaconda3/envs/albumentations_examples/lib/python3.10/site-packages (3.8.1)\nRequirement already satisfied: click in /Users/vladimiriglovikov/anaconda3/envs/albumentations_examples/lib/python3.10/site-packages (from nltk) (8.1.7)\nRequirement already satisfied: joblib in /Users/vladimiriglovikov/anaconda3/envs/albumentations_examples/lib/python3.10/site-packages (from nltk) (1.3.2)\nRequirement already satisfied: regex&gt;=2021.8.3 in /Users/vladimiriglovikov/anaconda3/envs/albumentations_examples/lib/python3.10/site-packages (from nltk) (2024.7.24)\nRequirement already satisfied: tqdm in /Users/vladimiriglovikov/anaconda3/envs/albumentations_examples/lib/python3.10/site-packages (from nltk) (4.66.2)\n</code></pre> Python<pre><code>import nltk\n\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\n</code></pre> <pre><code>[nltk_data] Downloading package stopwords to\n[nltk_data]     /Users/vladimiriglovikov/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n</code></pre> Python<pre><code>stops = stopwords.words('english')\n</code></pre> Python<pre><code>transform = A.Compose([A.TextImage(font_path=font_path, p=1, font_color=\"white\", augmentations=[\"insertion\"], stopwords=stops)])\n</code></pre> Python<pre><code>metadata = {\n    \"bbox\": [0.15, 0.9, 0.9, 0.95],\n    \"text\": \"Mooze and Meeko\",\n}\n</code></pre> Python<pre><code>transformed = transform(image=image, textimage_metadata=metadata)\n</code></pre> Python<pre><code>visualize(transformed[\"image\"])\n</code></pre>"},{"location":"examples/example_textimage/#returning-augmented-text","title":"Returning augmented text","text":"<p>If you need text that was added to the image after \"swap\", \"insertion\" or \"deletion\" you may get it with: </p> Python<pre><code>transform = A.Compose([A.TextImage(font_path=font_path, p=1, font_color=\"white\", augmentations=[\"insertion\", \"swap\"], stopwords=stops)])\n</code></pre> Python<pre><code>metadata = [{\n    \"bbox\": [0.02, 0.1, 0.95, 0.16],\n    \"text\": \"Big dreams in small packages...\",\n},\n            {\n    \"bbox\": [0.02, 0.85, 0.95, 0.91],\n    \"text\": \"...and even bigger in bigger ones.\"}\n           ]\n</code></pre> Python<pre><code>transformed = transform(image=image, textimage_metadata=metadata)\n</code></pre> Python<pre><code>transformed[\"overlay_data\"]\n</code></pre> <pre><code>[{'bbox_coords': (19, 1088, 912, 1164),\n  'text': 'for ...and even bigger in as bigger didn ones.',\n  'original_text': '...and even bigger in bigger ones.',\n  'bbox_index': 1,\n  'font_color': 'white'},\n {'bbox_coords': (19, 128, 912, 204),\n  'text': 'dreams in Big small packages...',\n  'original_text': 'Big dreams in small packages...',\n  'bbox_index': 0,\n  'font_color': 'white'}]\n</code></pre>"},{"location":"examples/example_weather_transforms/","title":"Weather augmentations in Albumentations","text":"<p>This notebook demonstrates weather augmentations that are supported by Albumentations.</p>"},{"location":"examples/example_weather_transforms/#import-the-required-libraries","title":"Import the required libraries","text":"Python<pre><code>import random\n\n\nimport cv2\nfrom matplotlib import pyplot as plt\n\nimport albumentations as A\n</code></pre>"},{"location":"examples/example_weather_transforms/#define-a-function-to-visualize-an-image","title":"Define a function to visualize an image","text":"Python<pre><code>def visualize(image):\n    plt.figure(figsize=(20, 10))\n    plt.axis('off')\n    plt.imshow(image)\n</code></pre>"},{"location":"examples/example_weather_transforms/#load-the-image-from-the-disk","title":"Load the image from the disk","text":"Python<pre><code>image = cv2.imread('images/weather_example.jpg')\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n</code></pre>"},{"location":"examples/example_weather_transforms/#visualize-the-original-image","title":"Visualize the original image","text":"Python<pre><code>visualize(image)\n</code></pre>"},{"location":"examples/example_weather_transforms/#randomrain","title":"RandomRain","text":"<p>We fix the random seed for visualization purposes, so the augmentation will always produce the same result. In a real computer vision pipeline, you shouldn't fix the random seed before applying a transform to the image because, in that case, the pipeline will always output the same image. The purpose of image augmentation is to use different transformations each time.</p> Python<pre><code>transform = A.Compose(\n    [A.RandomRain(brightness_coefficient=0.9, drop_width=1, blur_value=5, p=1)],\n)\nrandom.seed(7)\ntransformed = transform(image=image)\nvisualize(transformed['image'])\n</code></pre> <p></p>"},{"location":"examples/example_weather_transforms/#randomsnow","title":"RandomSnow","text":"Python<pre><code>transform = A.Compose(\n    [A.RandomSnow(brightness_coeff=2.5, snow_point_lower=0.3, snow_point_upper=0.5, p=1)],\n)\nrandom.seed(7)\ntransformed = transform(image=image)\nvisualize(transformed['image'])\n</code></pre>"},{"location":"examples/example_weather_transforms/#randomsunflare","title":"RandomSunFlare","text":"Python<pre><code>transform = A.Compose(\n    [A.RandomSunFlare(flare_roi=(0, 0, 1, 0.5), angle_lower=0.5, p=1)],\n)\nrandom.seed(7)\ntransformed = transform(image=image)\nvisualize(transformed['image'])\n</code></pre>"},{"location":"examples/example_weather_transforms/#randomshadow","title":"RandomShadow","text":"Python<pre><code>transform = A.Compose(\n    [A.RandomShadow(num_shadows_lower=1, num_shadows_upper=1, shadow_dimension=5, shadow_roi=(0, 0.5, 1, 1), p=1)],\n)\nrandom.seed(7)\ntransformed = transform(image=image)\nvisualize(transformed['image'])\n</code></pre>"},{"location":"examples/example_weather_transforms/#randomfog","title":"RandomFog","text":"Python<pre><code>transform = A.Compose(\n    [A.RandomFog(fog_coef_lower=0.7, fog_coef_upper=0.8, alpha_coef=0.1, p=1)],\n)\nrandom.seed(7)\ntransformed = transform(image=image)\nvisualize(transformed['image'])\n</code></pre>"},{"location":"examples/example_xymasking/","title":"Example xymasking","text":""},{"location":"examples/example_xymasking/#xymasking-aug-in-albumentations","title":"XYMasking aug in Albumentations","text":"<p>This transform is a generalization of the TimeMasking and FrequencyMasking from torchaudio.</p> Python<pre><code>import random\n\nimport cv2\nfrom matplotlib import pyplot as plt\n\nimport albumentations as A\n</code></pre> Python<pre><code>import torchaudio\nimport torch\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\n</code></pre> Python<pre><code>def visualize(image):\n    plt.figure(figsize=(10, 5))\n    plt.axis('off')\n    plt.imshow(image)\n</code></pre> Python<pre><code>img = np.load('../images/spectrogram.npy')\n</code></pre> Python<pre><code>img.shape\n</code></pre> <pre><code>(128, 256, 4)\n</code></pre> Python<pre><code>visualize(img[:, :, 0])\n</code></pre> <p></p>"},{"location":"examples/example_xymasking/#one-vertical-stripe-time-masking-with-fixed-width-filled-with-0","title":"One vertical stripe (time masking) with fixed width, filled with 0","text":"Python<pre><code>params1 = {\n    \"num_masks_x\": 1,    \n    \"mask_x_length\": 20,\n    \"fill_value\": 0,    \n\n}\ntransform1 = A.Compose([A.XYMasking(**params1, p=1)])\nvisualize(transform1(image=img[:, :, 0])[\"image\"])\n</code></pre>"},{"location":"examples/example_xymasking/#one-vertical-stripe-time-masking-with-randomly-sampled-width-filled-with-0","title":"One vertical stripe (time masking) with randomly sampled width, filled with 0","text":"Python<pre><code>params2 = {\n    \"num_masks_x\": 1,    \n    \"mask_x_length\": (0, 20), # This line changed from fixed  to a range\n    \"fill_value\": 0,\n}\ntransform2 = A.Compose([A.XYMasking(**params2, p=1)])\nvisualize(transform2(image=img[:, :, 0])[\"image\"])\n</code></pre>"},{"location":"examples/example_xymasking/#analogous-transform-in-torchaudio","title":"Analogous transform in torchaudio\u00b6","text":"Python<pre><code>spectrogram = torchaudio.transforms.Spectrogram()\nmasking = torchaudio.transforms.TimeMasking(time_mask_param=20)\nmasked = masking(torch.from_numpy(img[:, :, 0]))\nvisualize(masked.numpy())\n</code></pre>"},{"location":"examples/example_xymasking/#one-horizontal-stripe-frequency-masking-with-randomly-sampled-width-filled-with-0","title":"One horizontal stripe (frequency masking) with randomly sampled width, filled with 0","text":"Python<pre><code>params3 = {    \n    \"num_masks_y\": 1,    \n    \"mask_y_length\": (0, 20),\n    \"fill_value\": 0,    \n\n}\ntransform3 = A.Compose([A.XYMasking(**params3, p=1)])\nvisualize(transform3(image=img[:, :, 0])[\"image\"])\n</code></pre>"},{"location":"examples/example_xymasking/#analogous-transform-in-torchaudio_1","title":"Analogous transform in torchaudio","text":"Python<pre><code>spectrogram = torchaudio.transforms.Spectrogram()\nmasking = torchaudio.transforms.FrequencyMasking(freq_mask_param=20)\nmasked = masking(torch.from_numpy(img[:, :, 0]))\nvisualize(masked)\n</code></pre>"},{"location":"examples/example_xymasking/#several-vertical-and-horizontal-stripes","title":"Several vertical and horizontal stripes","text":"Python<pre><code>params4 = {    \n    \"num_masks_x\": (2, 4),\n    \"num_masks_y\": 5,    \n    \"mask_y_length\": 8,\n    \"mask_x_length\": (10, 20),\n    \"fill_value\": 0,  \n\n}\ntransform4 = A.Compose([A.XYMasking(**params4, p=1)])\nvisualize(transform4(image=img[:, :, 0])[\"image\"])\n</code></pre>"},{"location":"examples/example_xymasking/#application-to-the-image-with-the-number-of-channels-larger-than-3-and-different-fill-values-for-different-channels","title":"Application to the image with the number of channels larger than 3, and different fill values for different channels","text":"<p>Transform can work with any number of channels supporing image shapes of</p> <ul> <li>Grayscale: (height, width)</li> <li>RGB: (height, width, 3)</li> <li>Multichannel: (heigh, width, num_channels)</li> </ul> <p>For value that is used to fill masking regions you can use:</p> <ul> <li>scalar that will be applied to every channel</li> <li>list of numbers equal to the number of channels, so that every channel will have it's own filling value</li> </ul> Python<pre><code>params5 = {    \n    \"num_masks_x\": (2, 4),\n    \"num_masks_y\": 5,    \n    \"mask_y_length\": 8,\n    \"mask_x_length\": (20, 30),\n    \"fill_value\": (0, 1, 2, 3),  \n\n}\ntransform5 = A.Compose([A.XYMasking(**params5, p=1)])\ntransformed = transform5(image=img)[\"image\"]\n</code></pre> Python<pre><code>fig, axs = plt.subplots(2, 2) \nvmin=0\nvmax=3\n\naxs[0, 0].imshow(transformed[:, :, 0], vmin=vmin, vmax=vmax)\naxs[0, 0].set_title('Channel 0')\naxs[0, 0].axis('off')  # Hide axes for cleaner visualization\n\naxs[0, 1].imshow(transformed[:, :, 1], vmin=vmin, vmax=vmax)\naxs[0, 1].set_title('Channel 1')\naxs[0, 1].axis('off')\n\naxs[1, 0].imshow(transformed[:, :, 2], vmin=vmin, vmax=vmax)\naxs[1, 0].set_title('Channel 2')\naxs[1, 0].axis('off')\n\naxs[1, 1].imshow(transformed[:, :, 3], vmin=vmin, vmax=vmax)\naxs[1, 1].set_title('Channel 3')\naxs[1, 1].axis('off')\n\nplt.tight_layout()\n\nplt.show()\n</code></pre> <p></p> Python<pre><code>\n</code></pre>"},{"location":"examples/migrating_from_torchvision_to_albumentations/","title":"Migrating from torchvision to Albumentations","text":"<p>This notebook shows how you can use Albumentations instead of torchvision to perform data augmentation.</p>"},{"location":"examples/migrating_from_torchvision_to_albumentations/#import-the-required-libraries","title":"Import the required libraries","text":"Python<pre><code>from PIL import Image\nimport cv2\nimport numpy as np\nfrom torch.utils.data import Dataset\nfrom torchvision import transforms\n\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n</code></pre>"},{"location":"examples/migrating_from_torchvision_to_albumentations/#an-example-pipeline-that-uses-torchvision","title":"An example pipeline that uses torchvision","text":"Python<pre><code>class TorchvisionDataset(Dataset):\n    def __init__(self, file_paths, labels, transform=None):\n        self.file_paths = file_paths\n        self.labels = labels\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.file_paths)\n\n    def __getitem__(self, idx):\n        label = self.labels[idx]\n        file_path = self.file_paths[idx]\n\n        # Read an image with PIL\n        image = Image.open(file_path)\n        if self.transform:\n            image = self.transform(image)\n        return image, label\n\n\ntorchvision_transform = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.RandomCrop(224),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225],\n    )\n])\n\n\ntorchvision_dataset = TorchvisionDataset(\n    file_paths=['./images/image_1.jpg', './images/image_2.jpg', './images/image_3.jpg'],\n    labels=[1, 2, 3],\n    transform=torchvision_transform,\n)\n</code></pre>"},{"location":"examples/migrating_from_torchvision_to_albumentations/#the-same-pipeline-with-albumentations","title":"The same pipeline with Albumentations","text":"Python<pre><code>class AlbumentationsDataset(Dataset):\n    \"\"\"__init__ and __len__ functions are the same as in TorchvisionDataset\"\"\"\n    def __init__(self, file_paths, labels, transform=None):\n        self.file_paths = file_paths\n        self.labels = labels\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.file_paths)\n\n    def __getitem__(self, idx):\n        label = self.labels[idx]\n        file_path = self.file_paths[idx]\n\n        # Read an image with OpenCV\n        image = cv2.imread(file_path)\n\n        # By default OpenCV uses BGR color space for color images,\n        # so we need to convert the image to RGB color space.\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        if self.transform:\n            augmented = self.transform(image=image)\n            image = augmented['image']\n        return image, label\n\n\nalbumentations_transform = A.Compose([\n    A.Resize(256, 256),\n    A.RandomCrop(224, 224),\n    A.HorizontalFlip(),\n    A.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225],\n    ),\n    ToTensorV2()\n])\n\n\nalbumentations_dataset = AlbumentationsDataset(\n    file_paths=['./images/image_1.jpg', './images/image_2.jpg', './images/image_3.jpg'],\n    labels=[1, 2, 3],\n    transform=albumentations_transform,\n)\n</code></pre>"},{"location":"examples/migrating_from_torchvision_to_albumentations/#using-albumentations-with-pil","title":"Using albumentations with PIL","text":"<p>You can use PIL instead of OpenCV while working with Albumentations, but in that case, you need to convert a PIL image to a NumPy array before applying transformations. Them you need to convert the augmented image back from a NumPy array to a PIL image.</p> Python<pre><code>class AlbumentationsPilDataset(Dataset):\n    \"\"\"__init__ and __len__ functions are the same as in TorchvisionDataset\"\"\"\n    def __init__(self, file_paths, labels, transform=None):\n        self.file_paths = file_paths\n        self.labels = labels\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.file_paths)\n\n    def __getitem__(self, idx):\n        label = self.labels[idx]\n        file_path = self.file_paths[idx]\n\n        image = Image.open(file_path)\n\n        if self.transform:\n            # Convert PIL image to numpy array\n            image_np = np.array(image)\n            # Apply transformations\n            augmented = self.transform(image=image_np)\n            # Convert numpy array to PIL Image\n            image = Image.fromarray(augmented['image'])\n        return image, label\n\n\nalbumentations_pil_transform = A.Compose([\n    A.Resize(256, 256),\n    A.RandomCrop(224, 224),\n    A.HorizontalFlip(),\n])\n\n\n# Note that this dataset will output PIL images and not numpy arrays nor PyTorch tensors\nalbumentations_pil_dataset = AlbumentationsPilDataset(\n    file_paths=['./images/image_1.jpg', './images/image_2.jpg', './images/image_3.jpg'],\n    labels=[1, 2, 3],\n    transform=albumentations_pil_transform,\n)\n</code></pre>"},{"location":"examples/migrating_from_torchvision_to_albumentations/#albumentations-equivalents-for-torchvision-transforms","title":"Albumentations equivalents for torchvision transforms","text":"torchvision transform Albumentations transform Albumentations example Compose Compose <code>A.Compose([A.Resize(256, 256), A.RandomCrop(224, 224)])</code> CenterCrop CenterCrop <code>A.CenterCrop(256, 256)</code> ColorJitter HueSaturationValue <code>A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.5)</code> Pad PadIfNeeded <code>A.PadIfNeeded(min_height=512, min_width=512)</code> RandomAffine Affine <code>A.Affine(scale=(0.9, 1.1), translate_percent=(0.0, 0.2), rotate=(-45, 45), shear=(-15, 15), mode=cv2.BORDER_REFLECT_101, p=0.5)</code> RandomCrop RandomCrop <code>A.RandomCrop(256, 256)</code> RandomGrayscale ToGray <code>A.ToGray(p=0.5)</code> RandomHorizontalFlip HorizontalFlip <code>A.HorizontalFlip(p=0.5)</code> RandomPerspective Perspective <code>A.Perspective(scale=(0.2, 0.4), fit_output=True, p=0.5)</code> RandomRotation Rotate <code>A.Rotate(limit=45, p=0.5)</code> RandomVerticalFlip VerticalFlip <code>A.VerticalFlip(p=0.5)</code> Resize Resize <code>A.Resize(256, 256)</code> GaussianBlur GaussianBlur <code>A.GaussianBlur(blur_limit=(3, 7), p=0.5)</code> RandomInvert InvertImg <code>A.InvertImg(p=0.5)</code> RandomPosterize Posterize <code>A.Posterize(num_bits=4, p=0.5)</code> RandomSolarize Solarize <code>A.Solarize(threshold=127, p=0.5)</code> RandomAdjustSharpness Sharpen <code>A.Sharpen(alpha=(0.2, 0.5), lightness=(0.5, 1.0), p=0.5)</code> RandomAutocontrast RandomBrightnessContrast <code>A.RandomBrightnessContrast(brightness_limit=0, contrast_limit=0.2, p=0.5)</code> RandomEqualize Equalize <code>A.Equalize(p=0.5)</code> RandomErasing CoarseDropout <code>A.CoarseDropout(min_height=8, max_height=32, min_width=8, max_width=32, p=0.5)</code> Normalize Normalize <code>A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])</code>"},{"location":"examples/pytorch_classification/","title":"PyTorch and Albumentations for image classification","text":"<p>This example shows how to use Albumentations for image classification. We will use the <code>Cats vs. Dogs</code> dataset. The task will be to detect whether an image contains a cat or a dog.</p>"},{"location":"examples/pytorch_classification/#import-the-required-libraries","title":"Import the required libraries","text":"Python<pre><code>from collections import defaultdict\nimport copy\nimport random\nimport os\nimport shutil\nfrom urllib.request import urlretrieve\n\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nimport cv2\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport torch\nimport torch.backends.cudnn as cudnn\nimport torch.nn as nn\nimport torch.optim\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.models as models\n\ncudnn.benchmark = True\n</code></pre>"},{"location":"examples/pytorch_classification/#define-functions-to-download-an-archived-dataset-and-unpack-it","title":"Define functions to download an archived dataset and unpack it","text":"Python<pre><code>class TqdmUpTo(tqdm):\n    def update_to(self, b=1, bsize=1, tsize=None):\n        if tsize is not None:\n            self.total = tsize\n        self.update(b * bsize - self.n)\n\n\ndef download_url(url, filepath):\n    directory = os.path.dirname(os.path.abspath(filepath))\n    os.makedirs(directory, exist_ok=True)\n    if os.path.exists(filepath):\n        print(\"Filepath already exists. Skipping download.\")\n        return\n\n    with TqdmUpTo(unit=\"B\", unit_scale=True, unit_divisor=1024, miniters=1, desc=os.path.basename(filepath)) as t:\n        urlretrieve(url, filename=filepath, reporthook=t.update_to, data=None)\n        t.total = t.n\n\n\ndef extract_archive(filepath):\n    extract_dir = os.path.dirname(os.path.abspath(filepath))\n    shutil.unpack_archive(filepath, extract_dir)\n</code></pre>"},{"location":"examples/pytorch_classification/#set-the-root-directory-for-the-downloaded-dataset","title":"Set the root directory for the downloaded dataset","text":"Python<pre><code>dataset_directory = os.path.join(os.environ[\"HOME\"], \"datasets/cats-vs-dogs\")\n</code></pre>"},{"location":"examples/pytorch_classification/#download-and-extract-the-cats-vs-dogs-dataset","title":"Download and extract the <code>Cats vs. Dogs</code> dataset","text":"Python<pre><code>filepath = os.path.join(dataset_directory, \"kagglecatsanddogs_3367a.zip\")\ndownload_url(\n    url=\"https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_3367a.zip\",\n    filepath=filepath,\n)\nextract_archive(filepath)\n</code></pre> <pre><code>Filepath already exists. Skipping download.\n</code></pre>"},{"location":"examples/pytorch_classification/#split-files-from-the-dataset-into-the-train-and-validation-sets","title":"Split files from the dataset into the train and validation sets","text":"<p>Some files in the dataset are broken, so we will use only those image files that OpenCV could load correctly. We will use 20000 images for training, 4936 images for validation, and 10 images for testing.</p> Python<pre><code>root_directory = os.path.join(dataset_directory, \"PetImages\")\n\ncat_directory = os.path.join(root_directory, \"Cat\")\ndog_directory = os.path.join(root_directory, \"Dog\")\n\ncat_images_filepaths = sorted([os.path.join(cat_directory, f) for f in os.listdir(cat_directory)])\ndog_images_filepaths = sorted([os.path.join(dog_directory, f) for f in os.listdir(dog_directory)])\nimages_filepaths = [*cat_images_filepaths, *dog_images_filepaths]\ncorrect_images_filepaths = [i for i in images_filepaths if cv2.imread(i) is not None]\n\nrandom.seed(42)\nrandom.shuffle(correct_images_filepaths)\ntrain_images_filepaths = correct_images_filepaths[:20000]\nval_images_filepaths = correct_images_filepaths[20000:-10]\ntest_images_filepaths = correct_images_filepaths[-10:]\nprint(len(train_images_filepaths), len(val_images_filepaths), len(test_images_filepaths))\n</code></pre> <pre><code>20000 4936 10\n</code></pre>"},{"location":"examples/pytorch_classification/#define-a-function-to-visualize-images-and-their-labels","title":"Define a function to visualize images and their labels","text":"<p>Let's define a function that will take a list of images' file paths and their labels and visualize them in a grid. Correct labels are colored green, and incorrectly predicted labels are colored red.</p> Python<pre><code>def display_image_grid(images_filepaths, predicted_labels=(), cols=5):\n    rows = len(images_filepaths) // cols\n    figure, ax = plt.subplots(nrows=rows, ncols=cols, figsize=(12, 6))\n    for i, image_filepath in enumerate(images_filepaths):\n        image = cv2.imread(image_filepath)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        true_label = os.path.normpath(image_filepath).split(os.sep)[-2]\n        predicted_label = predicted_labels[i] if predicted_labels else true_label\n        color = \"green\" if true_label == predicted_label else \"red\"\n        ax.ravel()[i].imshow(image)\n        ax.ravel()[i].set_title(predicted_label, color=color)\n        ax.ravel()[i].set_axis_off()\n    plt.tight_layout()\n    plt.show()\n</code></pre> Python<pre><code>display_image_grid(test_images_filepaths)\n</code></pre> <p></p>"},{"location":"examples/pytorch_classification/#define-a-pytorch-dataset-class","title":"Define a PyTorch dataset class","text":"<p>Next, we define a PyTorch dataset. If you are new to PyTorch datasets, please refer to this tutorial - https://pytorch.org/tutorials/beginner/data_loading_tutorial.html.</p> <p>Out task is binary classification - a model needs to predict whether an image contains a cat or a dog. Our labels will mark the probability that an image contains a cat. So the correct label for an image with a cat will be <code>1.0</code>, and the correct label for an image with a dog will be <code>0.0</code>.</p> <p><code>__init__</code> will receive an optional <code>transform</code> argument. It is a transformation function of the Albumentations augmentation pipeline. Then in <code>__getitem__</code>, the Dataset class will use that function to augment an image and return it along with the correct label.</p> Python<pre><code>class CatsVsDogsDataset(Dataset):\n    def __init__(self, images_filepaths, transform=None):\n        self.images_filepaths = images_filepaths\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.images_filepaths)\n\n    def __getitem__(self, idx):\n        image_filepath = self.images_filepaths[idx]\n        image = cv2.imread(image_filepath)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        if os.path.normpath(image_filepath).split(os.sep)[-2] == \"Cat\":\n            label = 1.0\n        else:\n            label = 0.0\n        if self.transform is not None:\n            image = self.transform(image=image)[\"image\"]\n        return image, label\n</code></pre>"},{"location":"examples/pytorch_classification/#use-albumentations-to-define-transformation-functions-for-the-train-and-validation-datasets","title":"Use Albumentations to define transformation functions for the train and validation datasets","text":"<p>We use Albumentations to define augmentation pipelines for training and validation datasets. In both pipelines, we first resize an input image, so its smallest size is 160px, then we take a 128px by 128px crop. For the training dataset, we also apply more augmentations to that crop. Next, we will normalize an image. We first divide all pixel values of an image by 255, so each pixel's value will lie in a range <code>[0.0, 1.0]</code>. Then we will subtract mean pixel values and divide values by the standard deviation. <code>mean</code> and <code>std</code> in augmentation pipelines are taken from the <code>ImageNet</code> dataset. Still, they transfer reasonably well to the <code>Cats vs. Dogs</code> dataset. After that, we will apply <code>ToTensorV2</code> that converts a NumPy array to a PyTorch tensor, which will serve as an input to a neural network.</p> <p>Note that in the validation pipeline we will use <code>A.CenterCrop</code> instead of <code>A.RandomCrop</code> because we want out validation results to be deterministic (so that they will not depend upon a random location of a crop).</p> Python<pre><code>train_transform = A.Compose(\n    [\n        A.SmallestMaxSize(max_size=160),\n        A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.5),\n        A.RandomCrop(height=128, width=128),\n        A.RGBShift(r_shift_limit=15, g_shift_limit=15, b_shift_limit=15, p=0.5),\n        A.RandomBrightnessContrast(p=0.5),\n        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n        ToTensorV2(),\n    ]\n)\ntrain_dataset = CatsVsDogsDataset(images_filepaths=train_images_filepaths, transform=train_transform)\n</code></pre> Python<pre><code>val_transform = A.Compose(\n    [\n        A.SmallestMaxSize(max_size=160),\n        A.CenterCrop(height=128, width=128),\n        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n        ToTensorV2(),\n    ]\n)\nval_dataset = CatsVsDogsDataset(images_filepaths=val_images_filepaths, transform=val_transform)\n</code></pre> <p>Also let's define a function that takes a dataset and visualizes different augmentations applied to the same image.</p> Python<pre><code>def visualize_augmentations(dataset, idx=0, samples=10, cols=5):\n    dataset = copy.deepcopy(dataset)\n    dataset.transform = A.Compose([t for t in dataset.transform if not isinstance(t, (A.Normalize, ToTensorV2))])\n    rows = samples // cols\n    figure, ax = plt.subplots(nrows=rows, ncols=cols, figsize=(12, 6))\n    for i in range(samples):\n        image, _ = dataset[idx]\n        ax.ravel()[i].imshow(image)\n        ax.ravel()[i].set_axis_off()\n    plt.tight_layout()\n    plt.show()    \n</code></pre> Python<pre><code>random.seed(42)\nvisualize_augmentations(train_dataset)\n</code></pre> <p></p>"},{"location":"examples/pytorch_classification/#define-helpers-for-training","title":"Define helpers for training","text":"<p>We define a few helpers for our training pipeline. <code>calculate_accuracy</code> takes model predictions and true labels and will return accuracy for those predictions. <code>MetricMonitor</code> helps to track metrics such as accuracy or loss during training and validation</p> Python<pre><code>def calculate_accuracy(output, target):\n    output = torch.sigmoid(output) &gt;= 0.5\n    target = target == 1.0\n    return torch.true_divide((target == output).sum(dim=0), output.size(0)).item()\n</code></pre> Python<pre><code>class MetricMonitor:\n    def __init__(self, float_precision=3):\n        self.float_precision = float_precision\n        self.reset()\n\n    def reset(self):\n        self.metrics = defaultdict(lambda: {\"val\": 0, \"count\": 0, \"avg\": 0})\n\n    def update(self, metric_name, val):\n        metric = self.metrics[metric_name]\n\n        metric[\"val\"] += val\n        metric[\"count\"] += 1\n        metric[\"avg\"] = metric[\"val\"] / metric[\"count\"]\n\n    def __str__(self):\n        return \" | \".join(\n            [\n                \"{metric_name}: {avg:.{float_precision}f}\".format(\n                    metric_name=metric_name, avg=metric[\"avg\"], float_precision=self.float_precision\n                )\n                for (metric_name, metric) in self.metrics.items()\n            ]\n        )\n</code></pre>"},{"location":"examples/pytorch_classification/#define-training-parameters","title":"Define training parameters","text":"<p>Here we define a few training parameters such as model architecture, learning rate, batch size, epochs, etc</p> Python<pre><code>params = {\n    \"model\": \"resnet50\",\n    \"device\": \"cuda\",\n    \"lr\": 0.001,\n    \"batch_size\": 64,\n    \"num_workers\": 4,\n    \"epochs\": 10,\n}\n</code></pre>"},{"location":"examples/pytorch_classification/#create-all-required-objects-and-functions-for-training-and-validation","title":"Create all required objects and functions for training and validation","text":"Python<pre><code>model = getattr(models, params[\"model\"])(pretrained=False, num_classes=1,)\nmodel = model.to(params[\"device\"])\ncriterion = nn.BCEWithLogitsLoss().to(params[\"device\"])\noptimizer = torch.optim.Adam(model.parameters(), lr=params[\"lr\"])\n</code></pre> Python<pre><code>train_loader = DataLoader(\n    train_dataset, batch_size=params[\"batch_size\"], shuffle=True, num_workers=params[\"num_workers\"], pin_memory=True,\n)\nval_loader = DataLoader(\n    val_dataset, batch_size=params[\"batch_size\"], shuffle=False, num_workers=params[\"num_workers\"], pin_memory=True,\n)\n</code></pre> Python<pre><code>def train(train_loader, model, criterion, optimizer, epoch, params):\n    metric_monitor = MetricMonitor()\n    model.train()\n    stream = tqdm(train_loader)\n    for i, (images, target) in enumerate(stream, start=1):\n        images = images.to(params[\"device\"], non_blocking=True)\n        target = target.to(params[\"device\"], non_blocking=True).float().view(-1, 1)\n        output = model(images)\n        loss = criterion(output, target)\n        accuracy = calculate_accuracy(output, target)\n        metric_monitor.update(\"Loss\", loss.item())\n        metric_monitor.update(\"Accuracy\", accuracy)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        stream.set_description(\n            \"Epoch: {epoch}. Train.      {metric_monitor}\".format(epoch=epoch, metric_monitor=metric_monitor)\n        )\n</code></pre> Python<pre><code>def validate(val_loader, model, criterion, epoch, params):\n    metric_monitor = MetricMonitor()\n    model.eval()\n    stream = tqdm(val_loader)\n    with torch.no_grad():\n        for i, (images, target) in enumerate(stream, start=1):\n            images = images.to(params[\"device\"], non_blocking=True)\n            target = target.to(params[\"device\"], non_blocking=True).float().view(-1, 1)\n            output = model(images)\n            loss = criterion(output, target)\n            accuracy = calculate_accuracy(output, target)\n\n            metric_monitor.update(\"Loss\", loss.item())\n            metric_monitor.update(\"Accuracy\", accuracy)\n            stream.set_description(\n                \"Epoch: {epoch}. Validation. {metric_monitor}\".format(epoch=epoch, metric_monitor=metric_monitor)\n            )\n</code></pre>"},{"location":"examples/pytorch_classification/#train-a-model","title":"Train a model","text":"Python<pre><code>for epoch in range(1, params[\"epochs\"] + 1):\n    train(train_loader, model, criterion, optimizer, epoch, params)\n    validate(val_loader, model, criterion, epoch, params)\n</code></pre> <pre><code>Epoch: 1. Train.      Loss: 0.700 | Accuracy: 0.598: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 313/313 [00:38&lt;00:00,  8.04it/s]\nEpoch: 1. Validation. Loss: 0.684 | Accuracy: 0.663: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 78/78 [00:03&lt;00:00, 23.46it/s]\nEpoch: 2. Train.      Loss: 0.611 | Accuracy: 0.675: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 313/313 [00:37&lt;00:00,  8.24it/s]\nEpoch: 2. Validation. Loss: 0.581 | Accuracy: 0.689: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 78/78 [00:03&lt;00:00, 23.25it/s]\nEpoch: 3. Train.      Loss: 0.513 | Accuracy: 0.752: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 313/313 [00:38&lt;00:00,  8.22it/s]\nEpoch: 3. Validation. Loss: 0.408 | Accuracy: 0.818: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 78/78 [00:03&lt;00:00, 23.61it/s]\nEpoch: 4. Train.      Loss: 0.440 | Accuracy: 0.796: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 313/313 [00:37&lt;00:00,  8.24it/s]\nEpoch: 4. Validation. Loss: 0.374 | Accuracy: 0.829: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 78/78 [00:03&lt;00:00, 22.89it/s]\nEpoch: 5. Train.      Loss: 0.391 | Accuracy: 0.821: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 313/313 [00:37&lt;00:00,  8.25it/s]\nEpoch: 5. Validation. Loss: 0.345 | Accuracy: 0.853: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 78/78 [00:03&lt;00:00, 23.03it/s]\nEpoch: 6. Train.      Loss: 0.343 | Accuracy: 0.845: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 313/313 [00:38&lt;00:00,  8.22it/s]\nEpoch: 6. Validation. Loss: 0.304 | Accuracy: 0.861: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 78/78 [00:03&lt;00:00, 23.88it/s]\nEpoch: 7. Train.      Loss: 0.312 | Accuracy: 0.858: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 313/313 [00:38&lt;00:00,  8.23it/s]\nEpoch: 7. Validation. Loss: 0.259 | Accuracy: 0.886: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 78/78 [00:03&lt;00:00, 23.29it/s]\nEpoch: 8. Train.      Loss: 0.284 | Accuracy: 0.875: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 313/313 [00:38&lt;00:00,  8.21it/s]\nEpoch: 8. Validation. Loss: 0.304 | Accuracy: 0.882: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 78/78 [00:03&lt;00:00, 23.81it/s]\nEpoch: 9. Train.      Loss: 0.265 | Accuracy: 0.884: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 313/313 [00:38&lt;00:00,  8.18it/s]\nEpoch: 9. Validation. Loss: 0.255 | Accuracy: 0.888: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 78/78 [00:03&lt;00:00, 23.78it/s]\nEpoch: 10. Train.      Loss: 0.248 | Accuracy: 0.890: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 313/313 [00:38&lt;00:00,  8.21it/s]\nEpoch: 10. Validation. Loss: 0.222 | Accuracy: 0.909: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 78/78 [00:03&lt;00:00, 23.90it/s]\n</code></pre>"},{"location":"examples/pytorch_classification/#predict-labels-for-images-and-visualize-those-predictions","title":"Predict labels for images and visualize those predictions","text":"<p>Now we have a trained model, so let's try to predict labels for some images and see whether those predictions are correct. First we make the <code>CatsVsDogsInferenceDataset</code> PyTorch dataset. Its code is similar to the training and validation datasets, but the inference dataset returns only an image and not an associated label (because in the real world we usually don't have access to the true labels and want to infer them using our trained model).</p> Python<pre><code>class CatsVsDogsInferenceDataset(Dataset):\n    def __init__(self, images_filepaths, transform=None):\n        self.images_filepaths = images_filepaths\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.images_filepaths)\n\n    def __getitem__(self, idx):\n        image_filepath = self.images_filepaths[idx]\n        image = cv2.imread(image_filepath)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        if self.transform is not None:\n            image = self.transform(image=image)[\"image\"]\n        return image\n\ntest_transform = A.Compose(\n    [\n        A.SmallestMaxSize(max_size=160),\n        A.CenterCrop(height=128, width=128),\n        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n        ToTensorV2(),\n    ]\n)\ntest_dataset = CatsVsDogsInferenceDataset(images_filepaths=test_images_filepaths, transform=test_transform)\ntest_loader = DataLoader(\n    test_dataset, batch_size=params[\"batch_size\"], shuffle=False, num_workers=params[\"num_workers\"], pin_memory=True,\n)\n</code></pre> Python<pre><code>model = model.eval()\npredicted_labels = []\nwith torch.no_grad():\n    for images in test_loader:\n        images = images.to(params[\"device\"], non_blocking=True)\n        output = model(images)\n        predictions = (torch.sigmoid(output) &gt;= 0.5)[:, 0].cpu().numpy()\n        predicted_labels += [\"Cat\" if is_cat else \"Dog\" for is_cat in predictions]\n</code></pre> Python<pre><code>display_image_grid(test_images_filepaths, predicted_labels)\n</code></pre> <p></p> <p>As we see our model predicted correct labels for 7 out of 10 images. If you train the model for more epochs, you will obtain better results.</p>"},{"location":"examples/pytorch_semantic_segmentation/","title":"PyTorch and Albumentations for semantic segmentation","text":"<p>This example shows how to use Albumentations for binary semantic segmentation. We will use the The Oxford-IIIT Pet Dataset . The task will be to classify each pixel of an input image either as <code>pet</code> or <code>background</code>.</p>"},{"location":"examples/pytorch_semantic_segmentation/#install-the-required-libraries","title":"Install the required libraries","text":"<p>We will use TernausNet, a library that provides pretrained UNet models for the semantic segmentation task.</p> Python<pre><code>!pip install ternausnet &gt; /dev/null\n</code></pre>"},{"location":"examples/pytorch_semantic_segmentation/#import-the-required-libraries","title":"Import the required libraries","text":"Python<pre><code>from collections import defaultdict\nimport copy\nimport random\nimport os\nimport shutil\nfrom urllib.request import urlretrieve\n\nimport albumentations as A\nimport albumentations.augmentations.functional as F\nfrom albumentations.pytorch import ToTensorV2\nimport cv2\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport ternausnet.models\nfrom tqdm import tqdm\nimport torch\nimport torch.backends.cudnn as cudnn\nimport torch.nn as nn\nimport torch.optim\nfrom torch.utils.data import Dataset, DataLoader\n\ncudnn.benchmark = True\n</code></pre>"},{"location":"examples/pytorch_semantic_segmentation/#define-functions-to-download-an-archived-dataset-and-unpack-it","title":"Define functions to download an archived dataset and unpack it","text":"Python<pre><code>class TqdmUpTo(tqdm):\n    def update_to(self, b=1, bsize=1, tsize=None):\n        if tsize is not None:\n            self.total = tsize\n        self.update(b * bsize - self.n)\n\n\ndef download_url(url, filepath):\n    directory = os.path.dirname(os.path.abspath(filepath))\n    os.makedirs(directory, exist_ok=True)\n    if os.path.exists(filepath):\n        print(\"Dataset already exists on the disk. Skipping download.\")\n        return\n\n    with TqdmUpTo(unit=\"B\", unit_scale=True, unit_divisor=1024, miniters=1, desc=os.path.basename(filepath)) as t:\n        urlretrieve(url, filename=filepath, reporthook=t.update_to, data=None)\n        t.total = t.n\n\n\ndef extract_archive(filepath):\n    extract_dir = os.path.dirname(os.path.abspath(filepath))\n    shutil.unpack_archive(filepath, extract_dir)\n</code></pre>"},{"location":"examples/pytorch_semantic_segmentation/#set-the-root-directory-for-the-downloaded-dataset","title":"Set the root directory for the downloaded dataset","text":"Python<pre><code>dataset_directory = os.path.join(os.environ[\"HOME\"], \"datasets/oxford-iiit-pet\")\n</code></pre>"},{"location":"examples/pytorch_semantic_segmentation/#download-and-extract-the-cats-vs-dogs-dataset","title":"Download and extract the <code>Cats vs. Dogs</code> dataset","text":"Python<pre><code>filepath = os.path.join(dataset_directory, \"images.tar.gz\")\ndownload_url(\n    url=\"https://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz\", filepath=filepath,\n)\nextract_archive(filepath)\n</code></pre> <pre><code>Dataset already exists on the disk. Skipping download.\n</code></pre> Python<pre><code>filepath = os.path.join(dataset_directory, \"annotations.tar.gz\")\ndownload_url(\n    url=\"https://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz\", filepath=filepath,\n)\nextract_archive(filepath)\n</code></pre> <pre><code>Dataset already exists on the disk. Skipping download.\n</code></pre>"},{"location":"examples/pytorch_semantic_segmentation/#split-files-from-the-dataset-into-the-train-and-validation-sets","title":"Split files from the dataset into the train and validation sets","text":"<p>Some files in the dataset are broken, so we will use only those image files that OpenCV could load correctly. We will use 6000 images for training, 1374 images for validation, and 10 images for testing.</p> Python<pre><code>root_directory = os.path.join(dataset_directory)\nimages_directory = os.path.join(root_directory, \"images\")\nmasks_directory = os.path.join(root_directory, \"annotations\", \"trimaps\")\n\nimages_filenames = list(sorted(os.listdir(images_directory)))\ncorrect_images_filenames = [i for i in images_filenames if cv2.imread(os.path.join(images_directory, i)) is not None]\n\nrandom.seed(42)\nrandom.shuffle(correct_images_filenames)\n\ntrain_images_filenames = correct_images_filenames[:6000]\nval_images_filenames = correct_images_filenames[6000:-10]\ntest_images_filenames = images_filenames[-10:]\n\nprint(len(train_images_filenames), len(val_images_filenames), len(test_images_filenames))\n</code></pre> <pre><code>6000 1374 10\n</code></pre>"},{"location":"examples/pytorch_semantic_segmentation/#define-a-function-to-preprocess-a-mask","title":"Define a function to preprocess a mask","text":"<p>The dataset contains pixel-level trimap segmentation. For each image, there is an associated PNG file with a mask. The size of a mask equals to the size of the related image. Each pixel in a mask image can take one of three values: <code>1</code>, <code>2</code>, or <code>3</code>. <code>1</code> means that this pixel of an image belongs to the class <code>pet</code>, <code>2</code> - to the class <code>background</code>, <code>3</code> - to the class <code>border</code>. Since this example demonstrates a task of binary segmentation (that is assigning one of two classes to each pixel), we will preprocess the mask, so it will contain only two uniques values: <code>0.0</code> if a pixel is a background and <code>1.0</code> if a pixel is a pet or a border.</p> Python<pre><code>def preprocess_mask(mask):\n    mask = mask.astype(np.float32)\n    mask[mask == 2.0] = 0.0\n    mask[(mask == 1.0) | (mask == 3.0)] = 1.0\n    return mask\n</code></pre>"},{"location":"examples/pytorch_semantic_segmentation/#define-a-function-to-visualize-images-and-their-labels","title":"Define a function to visualize images and their labels","text":"<p>Let's define a visualization function that will take a list of images' file names, a path to the directory with images, a path to the directory with masks, and an optional argument with predicted masks (we will use this argument later to show predictions of a model).</p> Python<pre><code>def display_image_grid(images_filenames, images_directory, masks_directory, predicted_masks=None):\n    cols = 3 if predicted_masks else 2\n    rows = len(images_filenames)\n    figure, ax = plt.subplots(nrows=rows, ncols=cols, figsize=(10, 24))\n    for i, image_filename in enumerate(images_filenames):\n        image = cv2.imread(os.path.join(images_directory, image_filename))\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n        mask = cv2.imread(os.path.join(masks_directory, image_filename.replace(\".jpg\", \".png\")), cv2.IMREAD_UNCHANGED,)\n        mask = preprocess_mask(mask)\n        ax[i, 0].imshow(image)\n        ax[i, 1].imshow(mask, interpolation=\"nearest\")\n\n        ax[i, 0].set_title(\"Image\")\n        ax[i, 1].set_title(\"Ground truth mask\")\n\n        ax[i, 0].set_axis_off()\n        ax[i, 1].set_axis_off()\n\n        if predicted_masks:\n            predicted_mask = predicted_masks[i]\n            ax[i, 2].imshow(predicted_mask, interpolation=\"nearest\")\n            ax[i, 2].set_title(\"Predicted mask\")\n            ax[i, 2].set_axis_off()\n    plt.tight_layout()\n    plt.show()\n</code></pre> Python<pre><code>display_image_grid(test_images_filenames, images_directory, masks_directory)\n</code></pre> <p></p>"},{"location":"examples/pytorch_semantic_segmentation/#image-sizes-for-training-and-prediction","title":"Image sizes for training and prediction","text":"<p>Often, images that you use for training and inference have different heights and widths and different aspect ratios. That fact brings two challenges to a deep learning pipeline: - PyTorch requires all images in a batch to have the same height and width. - If a neural network is not fully convolutional, you have to use the same width and height for all images during training and inference. Fully convolutional architectures, such as UNet, can work with images of any size.</p> <p>There are three common ways to deal with those challenges: 1. Resize all images and masks to a fixed size (e.g., 256x256 pixels) during training. After a model predicts a mask with that fixed size during inference, resize the mask to the original image size. This approach is simple, but it has a few drawbacks:   - The predicted mask is smaller than the image, and the mask may lose some context and important details of the original image.    - This approach may be problematic if images in your dataset have different aspect ratios. For example, suppose you are resizing an image with the size 1024x512 pixels (so an image with an aspect ratio of 2:1) to 256x256 pixels (1:1 aspect ratio). In that case, this transformation will distort the image and may also affect the quality of predictions. 2. If you use a fully convolutional neural network, you can train a model with image crops, but use original images for inference. This option usually provides the best tradeoff between quality, speed of training, and hardware requirements. 3. Do not alter the sizes of images and use source images both for training and inference. With this approach, you won't lose any information. However, original images could be quite large, so they may require a lot of GPU memory. Also, this approach requires more training time to obtain good results.</p> <p>Some architectures, such as UNet, require that an image's size must be divisible by a downsampling factor of a network (usually 32), so you may also need to pad an image with borders. Albumentations provides a particular transformation for that case.</p> <p>The following example shows how different types of images look.</p> Python<pre><code>example_image_filename = correct_images_filenames[0]\nimage = cv2.imread(os.path.join(images_directory, example_image_filename))\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\nresized_image = F.resize(image, height=256, width=256)\npadded_image = F.pad(image, min_height=512, min_width=512)\npadded_constant_image = F.pad(image, min_height=512, min_width=512, border_mode=cv2.BORDER_CONSTANT)\ncropped_image = F.center_crop(image, crop_height=256, crop_width=256)\n</code></pre> Python<pre><code>figure, ax = plt.subplots(nrows=1, ncols=5, figsize=(18, 10))\nax.ravel()[0].imshow(image)\nax.ravel()[0].set_title(\"Original image\")\nax.ravel()[1].imshow(resized_image)\nax.ravel()[1].set_title(\"Resized image\")\nax.ravel()[2].imshow(cropped_image)\nax.ravel()[2].set_title(\"Cropped image\")\nax.ravel()[3].imshow(padded_image)\nax.ravel()[3].set_title(\"Image padded with reflection\")\nax.ravel()[4].imshow(padded_constant_image)\nax.ravel()[4].set_title(\"Image padded with constant padding\")\nplt.tight_layout()\nplt.show()\n</code></pre> <p></p> <p>In this tutorial, we will explore all three approaches for dealing with image sizes.</p>"},{"location":"examples/pytorch_semantic_segmentation/#approach-1-resize-all-images-and-masks-to-a-fixed-size-eg-256x256-pixels","title":"Approach 1. Resize all images and masks to a fixed size (e.g., 256x256 pixels).","text":""},{"location":"examples/pytorch_semantic_segmentation/#define-a-pytorch-dataset-class","title":"Define a PyTorch dataset class","text":"<p>Next, we define a PyTorch dataset. If you are new to PyTorch datasets, please refer to this tutorial - https://pytorch.org/tutorials/beginner/data_loading_tutorial.html.</p> <p><code>__init__</code> will receive an optional <code>transform</code> argument. It is a transformation function of the Albumentations augmentation pipeline. Then in <code>__getitem__</code>, the Dataset class will use that function to augment an image and a mask and return their augmented versions.</p> Python<pre><code>class OxfordPetDataset(Dataset):\n    def __init__(self, images_filenames, images_directory, masks_directory, transform=None):\n        self.images_filenames = images_filenames\n        self.images_directory = images_directory\n        self.masks_directory = masks_directory\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.images_filenames)\n\n    def __getitem__(self, idx):\n        image_filename = self.images_filenames[idx]\n        image = cv2.imread(os.path.join(self.images_directory, image_filename))\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        mask = cv2.imread(\n            os.path.join(self.masks_directory, image_filename.replace(\".jpg\", \".png\")), cv2.IMREAD_UNCHANGED,\n        )\n        mask = preprocess_mask(mask)\n        if self.transform is not None:\n            transformed = self.transform(image=image, mask=mask)\n            image = transformed[\"image\"]\n            mask = transformed[\"mask\"]\n        return image, mask\n</code></pre> <p>Next, we create augmentation pipelines for the training and validation datasets. Note that we use <code>A.Resize(256, 256)</code> to resize input images and masks to the size 256x256 pixels.</p> Python<pre><code>train_transform = A.Compose(\n    [\n        A.Resize(256, 256),\n        A.ShiftScaleRotate(shift_limit=0.2, scale_limit=0.2, rotate_limit=30, p=0.5),\n        A.RGBShift(r_shift_limit=25, g_shift_limit=25, b_shift_limit=25, p=0.5),\n        A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.5),\n        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n        ToTensorV2(),\n    ]\n)\ntrain_dataset = OxfordPetDataset(train_images_filenames, images_directory, masks_directory, transform=train_transform,)\n\nval_transform = A.Compose(\n    [A.Resize(256, 256), A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)), ToTensorV2()]\n)\nval_dataset = OxfordPetDataset(val_images_filenames, images_directory, masks_directory, transform=val_transform,)\n</code></pre> <p>Let's define a function that takes a dataset and visualizes different augmentations applied to the same image and the associated mask.</p> Python<pre><code>def visualize_augmentations(dataset, idx=0, samples=5):\n    dataset = copy.deepcopy(dataset)\n    dataset.transform = A.Compose([t for t in dataset.transform if not isinstance(t, (A.Normalize, ToTensorV2))])\n    figure, ax = plt.subplots(nrows=samples, ncols=2, figsize=(10, 24))\n    for i in range(samples):\n        image, mask = dataset[idx]\n        ax[i, 0].imshow(image)\n        ax[i, 1].imshow(mask, interpolation=\"nearest\")\n        ax[i, 0].set_title(\"Augmented image\")\n        ax[i, 1].set_title(\"Augmented mask\")\n        ax[i, 0].set_axis_off()\n        ax[i, 1].set_axis_off()\n    plt.tight_layout()\n    plt.show()\n</code></pre> Python<pre><code>random.seed(42)\nvisualize_augmentations(train_dataset, idx=55)\n</code></pre> <p></p>"},{"location":"examples/pytorch_semantic_segmentation/#define-helpers-for-training","title":"Define helpers for training","text":"<p><code>MetricMonitor</code> helps to track metrics such as accuracy or loss during training and validation.</p> Python<pre><code>class MetricMonitor:\n    def __init__(self, float_precision=3):\n        self.float_precision = float_precision\n        self.reset()\n\n    def reset(self):\n        self.metrics = defaultdict(lambda: {\"val\": 0, \"count\": 0, \"avg\": 0})\n\n    def update(self, metric_name, val):\n        metric = self.metrics[metric_name]\n\n        metric[\"val\"] += val\n        metric[\"count\"] += 1\n        metric[\"avg\"] = metric[\"val\"] / metric[\"count\"]\n\n    def __str__(self):\n        return \" | \".join(\n            [\n                \"{metric_name}: {avg:.{float_precision}f}\".format(\n                    metric_name=metric_name, avg=metric[\"avg\"], float_precision=self.float_precision\n                )\n                for (metric_name, metric) in self.metrics.items()\n            ]\n        )\n</code></pre>"},{"location":"examples/pytorch_semantic_segmentation/#define-functions-for-training-and-validation","title":"Define functions for training and validation","text":"Python<pre><code>def train(train_loader, model, criterion, optimizer, epoch, params):\n    metric_monitor = MetricMonitor()\n    model.train()\n    stream = tqdm(train_loader)\n    for i, (images, target) in enumerate(stream, start=1):\n        images = images.to(params[\"device\"], non_blocking=True)\n        target = target.to(params[\"device\"], non_blocking=True)\n        output = model(images).squeeze(1)\n        loss = criterion(output, target)\n        metric_monitor.update(\"Loss\", loss.item())\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        stream.set_description(\n            \"Epoch: {epoch}. Train.      {metric_monitor}\".format(epoch=epoch, metric_monitor=metric_monitor)\n        )\n</code></pre> Python<pre><code>def validate(val_loader, model, criterion, epoch, params):\n    metric_monitor = MetricMonitor()\n    model.eval()\n    stream = tqdm(val_loader)\n    with torch.no_grad():\n        for i, (images, target) in enumerate(stream, start=1):\n            images = images.to(params[\"device\"], non_blocking=True)\n            target = target.to(params[\"device\"], non_blocking=True)\n            output = model(images).squeeze(1)\n            loss = criterion(output, target)\n            metric_monitor.update(\"Loss\", loss.item())\n            stream.set_description(\n                \"Epoch: {epoch}. Validation. {metric_monitor}\".format(epoch=epoch, metric_monitor=metric_monitor)\n            )\n</code></pre> Python<pre><code>def create_model(params):\n    model = getattr(ternausnet.models, params[\"model\"])(pretrained=True)\n    model = model.to(params[\"device\"])\n    return model\n</code></pre> Python<pre><code>def train_and_validate(model, train_dataset, val_dataset, params):\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=params[\"batch_size\"],\n        shuffle=True,\n        num_workers=params[\"num_workers\"],\n        pin_memory=True,\n    )\n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=params[\"batch_size\"],\n        shuffle=False,\n        num_workers=params[\"num_workers\"],\n        pin_memory=True,\n    )\n    criterion = nn.BCEWithLogitsLoss().to(params[\"device\"])\n    optimizer = torch.optim.Adam(model.parameters(), lr=params[\"lr\"])\n    for epoch in range(1, params[\"epochs\"] + 1):\n        train(train_loader, model, criterion, optimizer, epoch, params)\n        validate(val_loader, model, criterion, epoch, params)\n    return model\n</code></pre> Python<pre><code>def predict(model, params, test_dataset, batch_size):\n    test_loader = DataLoader(\n        test_dataset, batch_size=batch_size, shuffle=False, num_workers=params[\"num_workers\"], pin_memory=True,\n    )\n    model.eval()\n    predictions = []\n    with torch.no_grad():\n        for images, (original_heights, original_widths) in test_loader:\n            images = images.to(params[\"device\"], non_blocking=True)\n            output = model(images)\n            probabilities = torch.sigmoid(output.squeeze(1))\n            predicted_masks = (probabilities &gt;= 0.5).float() * 1\n            predicted_masks = predicted_masks.cpu().numpy()\n            for predicted_mask, original_height, original_width in zip(\n                predicted_masks, original_heights.numpy(), original_widths.numpy()\n            ):\n                predictions.append((predicted_mask, original_height, original_width))\n    return predictions\n</code></pre>"},{"location":"examples/pytorch_semantic_segmentation/#define-training-parameters","title":"Define training parameters","text":"<p>Here we define a few training parameters such as model architecture, learning rate, batch size, epochs, etc.</p> Python<pre><code>params = {\n    \"model\": \"UNet11\",\n    \"device\": \"cuda\",\n    \"lr\": 0.001,\n    \"batch_size\": 16,\n    \"num_workers\": 4,\n    \"epochs\": 10,\n}\n</code></pre>"},{"location":"examples/pytorch_semantic_segmentation/#train-a-model","title":"Train a model","text":"Python<pre><code>model = create_model(params)\nmodel = train_and_validate(model, train_dataset, val_dataset, params)\n</code></pre> <pre><code>Epoch: 1. Train.      Loss: 0.415: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 375/375 [01:42&lt;00:00,  3.66it/s]\nEpoch: 1. Validation. Loss: 0.210: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 86/86 [00:09&lt;00:00,  9.55it/s]\nEpoch: 2. Train.      Loss: 0.257: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 375/375 [01:40&lt;00:00,  3.75it/s]\nEpoch: 2. Validation. Loss: 0.178: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 86/86 [00:08&lt;00:00, 10.62it/s]\nEpoch: 3. Train.      Loss: 0.221: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 375/375 [01:39&lt;00:00,  3.75it/s]\nEpoch: 3. Validation. Loss: 0.168: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 86/86 [00:08&lt;00:00, 10.58it/s]\nEpoch: 4. Train.      Loss: 0.209: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 375/375 [01:40&lt;00:00,  3.75it/s]\nEpoch: 4. Validation. Loss: 0.156: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 86/86 [00:08&lt;00:00, 10.57it/s]\nEpoch: 5. Train.      Loss: 0.190: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 375/375 [01:40&lt;00:00,  3.75it/s]\nEpoch: 5. Validation. Loss: 0.149: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 86/86 [00:08&lt;00:00, 10.57it/s]\nEpoch: 6. Train.      Loss: 0.179: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 375/375 [01:39&lt;00:00,  3.75it/s]\nEpoch: 6. Validation. Loss: 0.155: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 86/86 [00:08&lt;00:00, 10.55it/s]\nEpoch: 7. Train.      Loss: 0.175: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 375/375 [01:40&lt;00:00,  3.75it/s]\nEpoch: 7. Validation. Loss: 0.147: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 86/86 [00:08&lt;00:00, 10.59it/s]\nEpoch: 8. Train.      Loss: 0.167: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 375/375 [01:40&lt;00:00,  3.75it/s]\nEpoch: 8. Validation. Loss: 0.146: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 86/86 [00:08&lt;00:00, 10.61it/s]\nEpoch: 9. Train.      Loss: 0.165: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 375/375 [01:40&lt;00:00,  3.75it/s]\nEpoch: 9. Validation. Loss: 0.131: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 86/86 [00:08&lt;00:00, 10.56it/s]\nEpoch: 10. Train.      Loss: 0.156: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 375/375 [01:40&lt;00:00,  3.75it/s]\nEpoch: 10. Validation. Loss: 0.140: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 86/86 [00:08&lt;00:00, 10.60it/s]\n</code></pre>"},{"location":"examples/pytorch_semantic_segmentation/#predict-labels-for-images-and-visualize-those-predictions","title":"Predict labels for images and visualize those predictions","text":"<p>Now we have a trained model, so let's try to predict masks for some images. Note that the <code>__getitem__</code> method returns not only an image but also the original height and width of an image. We will use those values to resize a predicted mask from the size of 256x256 pixels to the original image's size.</p> Python<pre><code>class OxfordPetInferenceDataset(Dataset):\n    def __init__(self, images_filenames, images_directory, transform=None):\n        self.images_filenames = images_filenames\n        self.images_directory = images_directory\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.images_filenames)\n\n    def __getitem__(self, idx):\n        image_filename = self.images_filenames[idx]\n        image = cv2.imread(os.path.join(self.images_directory, image_filename))\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        original_size = tuple(image.shape[:2])\n        if self.transform is not None:\n            transformed = self.transform(image=image)\n            image = transformed[\"image\"]\n        return image, original_size\n</code></pre> Python<pre><code>test_transform = A.Compose(\n    [A.Resize(256, 256), A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)), ToTensorV2()]\n)\ntest_dataset = OxfordPetInferenceDataset(test_images_filenames, images_directory, transform=test_transform,)\n</code></pre> Python<pre><code>predictions = predict(model, params, test_dataset, batch_size=16)\n</code></pre> <p>Next, we will resize the predicted masks with the size of 256x256 pixels to the original images' size.</p> Python<pre><code>predicted_masks = []\nfor predicted_256x256_mask, original_height, original_width in predictions:\n    full_sized_mask = F.resize(\n        predicted_256x256_mask, height=original_height, width=original_width, interpolation=cv2.INTER_NEAREST\n    )\n    predicted_masks.append(full_sized_mask)\n</code></pre> Python<pre><code>display_image_grid(test_images_filenames, images_directory, masks_directory, predicted_masks=predicted_masks)\n</code></pre> <p></p>"},{"location":"examples/pytorch_semantic_segmentation/#approach-2-train-on-crops-predict-masks-for-full-sized-images","title":"Approach 2. Train on crops, predict masks for full-sized images","text":"<p>We will reuse most of the code from the previous example.</p> <p>Heights and widths of the same images in the dataset are less than the crop size (256x256 pixels), so we first apply <code>A.PadIfNeeded(min_height=256, min_width=256)</code> which will pad an image if its height or width is less than 256 pixels.</p> Python<pre><code>train_transform = A.Compose(\n    [\n        A.PadIfNeeded(min_height=256, min_width=256),\n        A.RandomCrop(256, 256),\n        A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.5),\n        A.RGBShift(r_shift_limit=15, g_shift_limit=15, b_shift_limit=15, p=0.5),\n        A.RandomBrightnessContrast(p=0.5),\n        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n        ToTensorV2(),\n    ]\n)\ntrain_dataset = OxfordPetDataset(train_images_filenames, images_directory, masks_directory, transform=train_transform,)\n</code></pre> Python<pre><code>val_transform = A.Compose(\n    [\n        A.PadIfNeeded(min_height=256, min_width=256),\n        A.CenterCrop(256, 256),\n        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n        ToTensorV2(),\n    ]\n)\nval_dataset = OxfordPetDataset(val_images_filenames, images_directory, masks_directory, transform=val_transform,)\n</code></pre> Python<pre><code>params = {\n    \"model\": \"UNet11\",\n    \"device\": \"cuda\",\n    \"lr\": 0.001,\n    \"batch_size\": 16,\n    \"num_workers\": 4,\n    \"epochs\": 10,\n}\n</code></pre> Python<pre><code>model = create_model(params)\nmodel = train_and_validate(model, train_dataset, val_dataset, params)\n</code></pre> <pre><code>Epoch: 1. Train.      Loss: 0.445: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 375/375 [01:40&lt;00:00,  3.75it/s]\nEpoch: 1. Validation. Loss: 0.279: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 86/86 [00:08&lt;00:00, 10.49it/s]\nEpoch: 2. Train.      Loss: 0.311: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 375/375 [01:39&lt;00:00,  3.75it/s]\nEpoch: 2. Validation. Loss: 0.238: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 86/86 [00:08&lt;00:00, 10.51it/s]\nEpoch: 3. Train.      Loss: 0.259: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 375/375 [01:39&lt;00:00,  3.75it/s]\nEpoch: 3. Validation. Loss: 0.206: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 86/86 [00:08&lt;00:00, 10.54it/s]\nEpoch: 4. Train.      Loss: 0.244: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 375/375 [01:39&lt;00:00,  3.75it/s]\nEpoch: 4. Validation. Loss: 0.211: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 86/86 [00:08&lt;00:00, 10.54it/s]\nEpoch: 5. Train.      Loss: 0.224: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 375/375 [01:40&lt;00:00,  3.74it/s]\nEpoch: 5. Validation. Loss: 0.270: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 86/86 [00:08&lt;00:00, 10.47it/s]\nEpoch: 6. Train.      Loss: 0.207: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 375/375 [01:40&lt;00:00,  3.75it/s]\nEpoch: 6. Validation. Loss: 0.169: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 86/86 [00:08&lt;00:00, 10.56it/s]\nEpoch: 7. Train.      Loss: 0.212: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 375/375 [01:40&lt;00:00,  3.75it/s]\nEpoch: 7. Validation. Loss: 0.169: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 86/86 [00:08&lt;00:00, 10.56it/s]\nEpoch: 8. Train.      Loss: 0.189: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 375/375 [01:40&lt;00:00,  3.75it/s]\nEpoch: 8. Validation. Loss: 0.201: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 86/86 [00:08&lt;00:00, 10.52it/s]\nEpoch: 9. Train.      Loss: 0.185: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 375/375 [01:39&lt;00:00,  3.75it/s]\nEpoch: 9. Validation. Loss: 0.162: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 86/86 [00:08&lt;00:00, 10.54it/s]\nEpoch: 10. Train.      Loss: 0.187: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 375/375 [01:39&lt;00:00,  3.75it/s]\nEpoch: 10. Validation. Loss: 0.159: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 86/86 [00:08&lt;00:00, 10.49it/s]\n</code></pre> <p>All images in the test dataset have a max side with size 500 pixels. Since PyTorch requires that all images in a batch must have the same dimensions, and also UNet requires that the size of an image will be divisible by 16, we will apply <code>A.PadIfNeeded(min_height=512, min_width=512, border_mode=cv2.BORDER_CONSTANT)</code>. That augmentation will pad image borders with zeros so the image size will become 512x512 pixels.</p> Python<pre><code>test_transform = A.Compose(\n    [\n        A.PadIfNeeded(min_height=512, min_width=512, border_mode=cv2.BORDER_CONSTANT),\n        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n        ToTensorV2(),\n    ]\n)\ntest_dataset = OxfordPetInferenceDataset(test_images_filenames, images_directory, transform=test_transform,)\n</code></pre> Python<pre><code>predictions = predict(model, params, test_dataset, batch_size=16)\n</code></pre> <p>Since we received masks for padded images, we need to crop a part of the original image size from the padded mask.</p> Python<pre><code>predicted_masks = []\nfor predicted_padded_mask, original_height, original_width in predictions:\n    cropped_mask = F.center_crop(predicted_padded_mask, original_height, original_width)\n    predicted_masks.append(cropped_mask)\n</code></pre> Python<pre><code>display_image_grid(test_images_filenames, images_directory, masks_directory, predicted_masks=predicted_masks)\n</code></pre> <p></p>"},{"location":"examples/pytorch_semantic_segmentation/#approach-3-use-original-images","title":"Approach 3. Use original images.","text":"<p>We could also use original images without resizing or cropping them. However, there is a problem with this dataset. A few images in the dataset are so large that even with <code>batch_size=1</code>, they require more than 11Gb of GPU memory for training. So as a tradeoff, we will first apply the <code>A.LongestMaxSize(512)</code> augmentation that will ensure that an image's largest size is no more than 512 pixels. That augmentation will affect only 137 out of 7384 dataset images.</p> <p>Next will use <code>A.PadIfNeeded(min_height=512, min_width=512)</code> to ensure that all images in a batch will have size 512x512 pixels.</p> Python<pre><code>train_transform = A.Compose(\n    [\n        A.LongestMaxSize(512),\n        A.PadIfNeeded(min_height=512, min_width=512),\n        A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.5),\n        A.RGBShift(r_shift_limit=15, g_shift_limit=15, b_shift_limit=15, p=0.5),\n        A.RandomBrightnessContrast(p=0.5),\n        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n        ToTensorV2(),\n    ]\n)\ntrain_dataset = OxfordPetDataset(train_images_filenames, images_directory, masks_directory, transform=train_transform,)\n\nval_transform = A.Compose(\n    [\n        A.LongestMaxSize(512),\n        A.PadIfNeeded(min_height=512, min_width=512, border_mode=cv2.BORDER_CONSTANT),\n        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n        ToTensorV2(),\n    ]\n)\nval_dataset = OxfordPetDataset(val_images_filenames, images_directory, masks_directory, transform=val_transform,)\n</code></pre> Python<pre><code>params = {\n    \"model\": \"UNet11\",\n    \"device\": \"cuda\",\n    \"lr\": 0.001,\n    \"batch_size\": 8,\n    \"num_workers\": 4,\n    \"epochs\": 10,\n}\n</code></pre> Python<pre><code>model = create_model(params)\nmodel = train_and_validate(model, train_dataset, val_dataset, params)\n</code></pre> <pre><code>Epoch: 1. Train.      Loss: 0.442: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 750/750 [06:58&lt;00:00,  1.79it/s]\nEpoch: 1. Validation. Loss: 0.225: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 172/172 [00:35&lt;00:00,  4.80it/s]\nEpoch: 2. Train.      Loss: 0.283: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 750/750 [06:54&lt;00:00,  1.81it/s]\nEpoch: 2. Validation. Loss: 0.188: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 172/172 [00:34&lt;00:00,  4.99it/s]\nEpoch: 3. Train.      Loss: 0.234: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 750/750 [06:53&lt;00:00,  1.81it/s]\nEpoch: 3. Validation. Loss: 0.154: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 172/172 [00:34&lt;00:00,  4.96it/s]\nEpoch: 4. Train.      Loss: 0.211: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 750/750 [06:53&lt;00:00,  1.81it/s]\nEpoch: 4. Validation. Loss: 0.136: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 172/172 [00:34&lt;00:00,  4.99it/s]\nEpoch: 5. Train.      Loss: 0.196: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 750/750 [06:53&lt;00:00,  1.81it/s]\nEpoch: 5. Validation. Loss: 0.131: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 172/172 [00:34&lt;00:00,  4.96it/s]\nEpoch: 6. Train.      Loss: 0.187: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 750/750 [06:53&lt;00:00,  1.81it/s]\nEpoch: 6. Validation. Loss: 0.151: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 172/172 [00:34&lt;00:00,  4.98it/s]\nEpoch: 7. Train.      Loss: 0.177: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 750/750 [06:53&lt;00:00,  1.81it/s]\nEpoch: 7. Validation. Loss: 0.127: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 172/172 [00:34&lt;00:00,  4.98it/s]\nEpoch: 8. Train.      Loss: 0.171: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 750/750 [06:53&lt;00:00,  1.81it/s]\nEpoch: 8. Validation. Loss: 0.113: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 172/172 [00:34&lt;00:00,  4.99it/s]\nEpoch: 9. Train.      Loss: 0.162: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 750/750 [06:54&lt;00:00,  1.81it/s]\nEpoch: 9. Validation. Loss: 0.143: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 172/172 [00:34&lt;00:00,  4.94it/s]\nEpoch: 10. Train.      Loss: 0.157: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 750/750 [06:53&lt;00:00,  1.81it/s]\nEpoch: 10. Validation. Loss: 0.115: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 172/172 [00:34&lt;00:00,  4.97it/s]\n</code></pre> <p>Next, we will use the same code that we were using in Approach 2 to make predictions.</p> Python<pre><code>test_transform = A.Compose(\n    [\n        A.PadIfNeeded(min_height=512, min_width=512, border_mode=cv2.BORDER_CONSTANT),\n        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n        ToTensorV2(),\n    ]\n)\ntest_dataset = OxfordPetInferenceDataset(test_images_filenames, images_directory, transform=test_transform,)\n</code></pre> Python<pre><code>predictions = predict(model, params, test_dataset, batch_size=16)\n</code></pre> Python<pre><code>predicted_masks = []\nfor predicted_padded_mask, original_height, original_width in predictions:\n    cropped_mask = F.center_crop(predicted_padded_mask, original_height, original_width)\n    predicted_masks.append(cropped_mask)\n</code></pre> Python<pre><code>display_image_grid(test_images_filenames, images_directory, masks_directory, predicted_masks=predicted_masks)\n</code></pre> <p></p>"},{"location":"examples/replay/","title":"Debugging an augmentation pipeline with ReplayCompose","text":"<p>An augmentation pipeline has a lot of randomness inside it. It applies augmentations with some probabilities, and it samples parameters for those augmentations (such as a rotation angle or a level of changing brightness) from a random distribution.</p> <p>It could be very useful for debugging purposes to see which augmentations were applied to the image and look at the parameters of those augmentations.</p> <p><code>ReplayCompose</code> tracks augmentation parameters. You can inspect those parameters or reapply them to another image.</p>"},{"location":"examples/replay/#import-the-required-libraries","title":"Import the required libraries","text":"Python<pre><code>import random\n\nimport cv2\nimport matplotlib.pyplot as plt\nimport albumentations as A\n</code></pre>"},{"location":"examples/replay/#define-the-visualization-function","title":"Define the visualization function","text":"Python<pre><code>def visualize(image):\n    plt.figure(figsize=(10, 10))\n    plt.axis('off')\n    plt.imshow(image)\n</code></pre>"},{"location":"examples/replay/#load-an-image-from-the-disk","title":"Load an image from the disk","text":"Python<pre><code>image = cv2.imread('images/parrot.jpg')\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\nvisualize(image)\n</code></pre>"},{"location":"examples/replay/#declare-an-augmentation-pipeline-using-replaycompose","title":"Declare an augmentation pipeline using <code>ReplayCompose</code>","text":"Python<pre><code>transform = A.ReplayCompose([\n    A.Resize(512, 512, always_apply=True),\n    A.RandomCrop(200, 200, always_apply=True),\n    A.OneOf([\n        A.RGBShift(),\n        A.HueSaturationValue()\n    ]),\n])\n</code></pre> <p>We fix the random seed for visualization purposes, so the augmentation will always produce the same result. In a real computer vision pipeline, you shouldn't fix the random seed before applying a transform to the image because, in that case, the pipeline will always output the same image. The purpose of image augmentation is to use different transformations each time.</p> Python<pre><code>random.seed(7)\ndata = transform(image=image)\nvisualize(data['image'])\n</code></pre> <p></p> <p><code>data['replay']</code> contains information about augmentations that ere applied to the image. If <code>applied</code> is <code>True</code>, then the augmentation was applied to the image. <code>params</code> contains information about parameters that were used to transform the image.</p> Python<pre><code>data['replay']\n</code></pre> <pre><code>{'__class_fullname__': 'ReplayCompose',\n 'params': None,\n 'transforms': [{'__class_fullname__': 'Resize',\n   'always_apply': True,\n   'p': 1,\n   'height': 512,\n   'width': 512,\n   'interpolation': 1,\n   'params': {},\n   'applied': True},\n  {'__class_fullname__': 'RandomCrop',\n   'always_apply': True,\n   'p': 1.0,\n   'height': 200,\n   'width': 200,\n   'params': {'h_start': 0.07243628666754276, 'w_start': 0.5358820043066892},\n   'applied': True},\n  {'__class_fullname__': 'OneOf',\n   'params': None,\n   'transforms': [{'__class_fullname__': 'RGBShift',\n     'always_apply': False,\n     'p': 0.5,\n     'r_shift_limit': (-20, 20),\n     'g_shift_limit': (-20, 20),\n     'b_shift_limit': (-20, 20),\n     'params': None,\n     'applied': False},\n    {'__class_fullname__': 'HueSaturationValue',\n     'always_apply': False,\n     'p': 0.5,\n     'hue_shift_limit': (-20, 20),\n     'sat_shift_limit': (-30, 30),\n     'val_shift_limit': (-20, 20),\n     'params': {'hue_shift': -2.654172653504567,\n      'sat_shift': -25.808674585522866,\n      'val_shift': -16.371479466245397},\n     'applied': True}],\n   'applied': True}],\n 'bbox_params': None,\n 'keypoint_params': None,\n 'additional_targets': {},\n 'is_check_shapes': True,\n 'applied': True}\n</code></pre>"},{"location":"examples/replay/#using-replaycomposereplay-to-apply-the-same-augmentations-to-another-image","title":"Using <code>ReplayCompose.replay</code> to apply the same augmentations to another image","text":"<p>To apply the same set of augmentations to a new target, you can use the <code>ReplayCompose.replay</code> function.</p>"},{"location":"examples/replay/#load-new-images","title":"Load new images","text":"Python<pre><code>image2 = cv2.imread('images/image_2.jpg')\nimage2 = cv2.cvtColor(image2, cv2.COLOR_BGR2RGB)\nvisualize(image2)\n</code></pre> Python<pre><code>image3 = cv2.imread('images/image_3.jpg')\nimage3 = cv2.cvtColor(image3, cv2.COLOR_BGR2RGB)\nvisualize(image3)\n</code></pre>"},{"location":"examples/replay/#apply-augmentations-from-datareplay-to-those-images","title":"Apply augmentations from data['replay'] to those images","text":"Python<pre><code>image2_data = A.ReplayCompose.replay(data['replay'], image=image2)\nvisualize(image2_data['image'])\n</code></pre> Python<pre><code>image3_data = A.ReplayCompose.replay(data['replay'], image=image3)\nvisualize(image3_data['image'])\n</code></pre>"},{"location":"examples/serialization/","title":"How to save and load parameters of an augmentation pipeline","text":"<p>Reproducibility is very important in deep learning. Data scientists and machine learning engineers need a way to save all parameters of deep learning pipelines such as model, optimizer, input datasets, and augmentation parameters and to be able to recreate the same pipeline using that data. Albumentations has built-in functionality to serialize the augmentation parameters and save them. Then you can use those parameters to recreate an augmentation pipeline.</p>"},{"location":"examples/serialization/#import-the-required-libraries","title":"Import the required libraries","text":"Python<pre><code>import random\n\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nimport albumentations as A\n</code></pre>"},{"location":"examples/serialization/#define-the-visualization-function","title":"Define the visualization function","text":"Python<pre><code>def visualize(image):\n    plt.figure(figsize=(6, 6))\n    plt.axis('off')\n    plt.imshow(image)\n</code></pre>"},{"location":"examples/serialization/#load-an-image-from-the-disk","title":"Load an image from the disk","text":"Python<pre><code>image = cv2.imread('images/parrot.jpg')\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\nvisualize(image)\n</code></pre>"},{"location":"examples/serialization/#define-an-augmentation-pipeline-that-we-want-to-serialize","title":"Define an augmentation pipeline that we want to serialize","text":"Python<pre><code>transform = A.Compose([\n    A.Perspective(),\n    A.RandomCrop(768, 768),\n    A.OneOf([\n        A.RGBShift(),\n        A.HueSaturationValue()\n    ]),\n])\n</code></pre> <p>We can pass an instance of augmentation to the <code>print</code> function, and it will print the string representation of it.</p> Python<pre><code>print(transform)\n</code></pre> <pre><code>Compose([\n  Perspective(always_apply=False, p=0.5, scale=(0.05, 0.1), keep_size=True, pad_mode=0, pad_val=0, mask_pad_val=0, fit_output=False, interpolation=1),\n  RandomCrop(always_apply=False, p=1.0, height=768, width=768),\n  OneOf([\n    RGBShift(always_apply=False, p=0.5, r_shift_limit=(-20, 20), g_shift_limit=(-20, 20), b_shift_limit=(-20, 20)),\n    HueSaturationValue(always_apply=False, p=0.5, hue_shift_limit=(-20, 20), sat_shift_limit=(-30, 30), val_shift_limit=(-20, 20)),\n  ], p=0.5),\n], p=1.0, bbox_params=None, keypoint_params=None, additional_targets={}, is_check_shapes=True)\n</code></pre> <p>Next, we will fix the random seed to make augmentation reproducible for visualization purposes and augment an example image.</p> Python<pre><code>random.seed(42)\nnp.random.seed(42)\ntransformed = transform(image=image)\nvisualize(transformed['image'])\n</code></pre> <p></p>"},{"location":"examples/serialization/#serializing-an-augmentation-pipeline-to-a-json-or-yaml-file","title":"Serializing an augmentation pipeline to a JSON or YAML file","text":"<p>To save the serialized representation of an augmentation pipeline to a JSON file, use the <code>save</code> function from Albumentations.</p> Python<pre><code>A.save(transform, '/tmp/transform.json')\n</code></pre> <p>To load a serialized representation from a JSON file, use the <code>load</code> function from Albumentations.</p> Python<pre><code>loaded_transform = A.load('/tmp/transform.json')\n</code></pre> Python<pre><code>print(loaded_transform)\n</code></pre> <pre><code>Compose([\n  Perspective(always_apply=False, p=0.5, scale=(0.05, 0.1), keep_size=True, pad_mode=0, pad_val=0, mask_pad_val=0, fit_output=False, interpolation=1),\n  RandomCrop(always_apply=False, p=1.0, height=768, width=768),\n  OneOf([\n    RGBShift(always_apply=False, p=0.5, r_shift_limit=(-20, 20), g_shift_limit=(-20, 20), b_shift_limit=(-20, 20)),\n    HueSaturationValue(always_apply=False, p=0.5, hue_shift_limit=(-20, 20), sat_shift_limit=(-30, 30), val_shift_limit=(-20, 20)),\n  ], p=0.5),\n], p=1.0, bbox_params=None, keypoint_params=None, additional_targets={}, is_check_shapes=True)\n</code></pre> <p>Next, we will use the same random seed as before and apply the loaded augmentation pipeline to the same image.</p> Python<pre><code>random.seed(42)\ntransformed_from_loaded_transform = loaded_transform(image=image)\nvisualize(transformed_from_loaded_transform['image'])\n</code></pre> <p></p> Python<pre><code>assert np.array_equal(transformed['image'], transformed_from_loaded_transform['image'])\n</code></pre> <p>As you see, it produced the same result.</p>"},{"location":"examples/serialization/#using-yaml-insted-of-json","title":"Using YAML insted of JSON","text":"<p>You can also use YAML instead of JSON for serializing and deserializing of augmentation pipelines. To do that add the <code>data_format='yaml'</code> argument to  the <code>save</code> and <code>load</code> functions.</p> Python<pre><code>A.save(transform, '/tmp/transform.yml', data_format='yaml')\nloaded_transform = A.load('/tmp/transform.yml', data_format='yaml')\n</code></pre> Python<pre><code>print(loaded_transform)\n</code></pre> <pre><code>Compose([\n  Perspective(always_apply=False, p=0.5, scale=(0.05, 0.1), keep_size=True, pad_mode=0, pad_val=0, mask_pad_val=0, fit_output=False, interpolation=1),\n  RandomCrop(always_apply=False, p=1.0, height=768, width=768),\n  OneOf([\n    RGBShift(always_apply=False, p=0.5, r_shift_limit=(-20, 20), g_shift_limit=(-20, 20), b_shift_limit=(-20, 20)),\n    HueSaturationValue(always_apply=False, p=0.5, hue_shift_limit=(-20, 20), sat_shift_limit=(-30, 30), val_shift_limit=(-20, 20)),\n  ], p=0.5),\n], p=1.0, bbox_params=None, keypoint_params=None, additional_targets={}, is_check_shapes=True)\n</code></pre>"},{"location":"examples/serialization/#serializing-an-augmentation-pipeline-to-a-python-dictionary","title":"Serializing an augmentation pipeline to a Python dictionary","text":"<p>If you need more control over a serialized pipeline, e.g., you want to save a serialized version to a database or send it to a server you can use the <code>to_dict</code> and <code>from_dict</code> functions. <code>to_dict</code> returns a Python dictionary that describes a pipeline. The dictionary will contain only primitive data types such as dictionaries, lists, strings, integers, and floats. To construct a pipeline from a dictionary, you need to call <code>from_dict</code>.</p> Python<pre><code>transform_dict = A.to_dict(transform)\nloaded_transform = A.from_dict(transform_dict)\n</code></pre> Python<pre><code>print(loaded_transform)\n</code></pre> <pre><code>Compose([\n  Perspective(always_apply=False, p=0.5, scale=(0.05, 0.1), keep_size=True, pad_mode=0, pad_val=0, mask_pad_val=0, fit_output=False, interpolation=1),\n  RandomCrop(always_apply=False, p=1.0, height=768, width=768),\n  OneOf([\n    RGBShift(always_apply=False, p=0.5, r_shift_limit=(-20, 20), g_shift_limit=(-20, 20), b_shift_limit=(-20, 20)),\n    HueSaturationValue(always_apply=False, p=0.5, hue_shift_limit=(-20, 20), sat_shift_limit=(-30, 30), val_shift_limit=(-20, 20)),\n  ], p=0.5),\n], p=1.0, bbox_params=None, keypoint_params=None, additional_targets={}, is_check_shapes=True)\n</code></pre>"},{"location":"examples/serialization/#serializing-and-deserializing-lambda-transforms","title":"Serializing and deserializing Lambda transforms","text":"<p>Lambda transforms use custom transformation functions provided by a user. For those types of transforms, Albumentations saves only the name and the position in the augmentation pipeline. To deserialize an augmentation pipeline with Lambda transforms, you need to manually provide all Lambda transform instances using the <code>lambda_transforms</code> argument.</p> <p>Let's define a function that we will use to transform an image. </p> Python<pre><code>def hflip_image(image, **kwargs):\n    return cv2.flip(image, 1)\n</code></pre> <p>Next, we create a Lambda transform that will apply the <code>hflip_image</code> function to input images. Note that to make the transform serializable, you need to pass the <code>name</code> argument.</p> Python<pre><code>hflip = A.Lambda(name='hflip_image', image=hflip_image, p=0.5)\ntransform = A.Compose([hflip])\nprint(transform)\n</code></pre> <pre><code>Compose([\n  Lambda(name='hflip_image', image=&lt;function hflip_image at 0x2a7e34d30&gt;, mask=&lt;function noop at 0x13a7c0d30&gt;, keypoint=&lt;function noop at 0x13a7c0d30&gt;, bbox=&lt;function noop at 0x13a7c0d30&gt;, always_apply=False, p=0.5),\n], p=1.0, bbox_params=None, keypoint_params=None, additional_targets={}, is_check_shapes=True)\n</code></pre> <p>To check that transform is working, we will apply to an image.</p> Python<pre><code>random.seed(7)\ntransformed = transform(image=image)\nvisualize(transformed['image'])\n</code></pre> <p></p> <p>To serialize a pipeline with a Lambda transform, use the <code>save</code> function as before.</p> Python<pre><code>A.save(transform, '/tmp/lambda_transform.json')\n</code></pre> <p>To deserialize a pipeline that contains Lambda transforms, you need to pass names and instances of all Lambda transforms in a pipeline through the <code>lambda_transforms</code> argument.</p> Python<pre><code>loaded_transform = A.load('/tmp/lambda_transform.json', nonserializable={'hflip_image': hflip})\n</code></pre> Python<pre><code>print(loaded_transform)\n</code></pre> <pre><code>Compose([\n  Lambda(name='hflip_image', image=&lt;function hflip_image at 0x2a7e34d30&gt;, mask=&lt;function noop at 0x13a7c0d30&gt;, keypoint=&lt;function noop at 0x13a7c0d30&gt;, bbox=&lt;function noop at 0x13a7c0d30&gt;, always_apply=False, p=0.5),\n], p=1.0, bbox_params=None, keypoint_params=None, additional_targets={}, is_check_shapes=True)\n</code></pre> <p>Verify that the deserialized pipeline produces the same output.</p> Python<pre><code>random.seed(7)\ntransformed_from_loaded_transform = loaded_transform(image=image)\nassert np.array_equal(transformed['image'], transformed_from_loaded_transform['image'])\n</code></pre> <p>To serialize and deserialize Lambda transforms to and from dictionaries use <code>to_dict</code> and <code>from_dict</code>.</p> Python<pre><code>transform_dict = A.to_dict(transform)\nprint(transform_dict)\n</code></pre> <pre><code>{'__version__': '1.4.0', 'transform': {'__class_fullname__': 'Compose', 'p': 1.0, 'transforms': [{'__class_fullname__': 'Lambda', '__name__': 'hflip_image'}], 'bbox_params': None, 'keypoint_params': None, 'additional_targets': {}, 'is_check_shapes': True}}\n</code></pre> Python<pre><code>loaded_transform = A.from_dict(transform_dict, nonserializable={'hflip_image': hflip})\n</code></pre>"},{"location":"examples/showcase/","title":"Showcase. Cool augmentation examples on diverse set of images from various real-world tasks.","text":""},{"location":"examples/showcase/#import-libraries-and-define-helper-functions","title":"Import libraries and define helper functions","text":""},{"location":"examples/showcase/#import-the-required-libraries","title":"Import the required libraries","text":"Python<pre><code>import os\nimport random\n\nimport albumentations as A\nimport cv2\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom skimage.color import label2rgb\n</code></pre>"},{"location":"examples/showcase/#define-visualization-functions","title":"Define visualization functions","text":"Python<pre><code>BOX_COLOR = (255, 0, 0) # Red\nTEXT_COLOR = (255, 255, 255) # White\n\n\ndef visualize_bbox(img, bbox, color=BOX_COLOR, thickness=2, **kwargs):\n    x_min, y_min, w, h = bbox\n    x_min, x_max, y_min, y_max = int(x_min), int(x_min + w), int(y_min), int(y_min + h)\n    cv2.rectangle(img, (x_min, y_min), (x_max, y_max), color=color, thickness=thickness)\n    return img\n\ndef visualize_titles(img, bbox, title, font_thickness = 2, font_scale=0.35, **kwargs):\n    x_min, y_min = bbox[:2]\n    x_min = int(x_min)\n    y_min = int(y_min)\n    ((text_width, text_height), _) = cv2.getTextSize(title, cv2.FONT_HERSHEY_SIMPLEX, font_scale, font_thickness)\n    cv2.rectangle(img, (x_min, y_min - int(1.3 * text_height)), (x_min + text_width, y_min), BOX_COLOR, -1)\n    cv2.putText(img, title, (x_min, y_min - int(0.3 * text_height)), cv2.FONT_HERSHEY_SIMPLEX, font_scale, TEXT_COLOR,\n                font_thickness, lineType=cv2.LINE_AA)\n    return img\n\n\ndef augment_and_show(aug, image, mask=None, bboxes=[], categories=[], category_id_to_name=[], filename=None,\n                     font_scale_orig=0.35, font_scale_aug=0.35, show_title=True, **kwargs):\n\n    if mask is None:\n        augmented = aug(image=image, bboxes=bboxes, category_ids=categories                       )\n    else:\n        augmented = aug(image=image, mask=mask, bboxes=bboxes, category_ids=categories)\n\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    image_aug = cv2.cvtColor(augmented['image'], cv2.COLOR_BGR2RGB)\n\n    for bbox in bboxes:\n        visualize_bbox(image, bbox, **kwargs)\n\n    for bbox in augmented['bboxes']:\n        visualize_bbox(image_aug, bbox, **kwargs)\n\n    if show_title:\n        for bbox,cat_id in zip(bboxes, categories):\n            visualize_titles(image, bbox, category_id_to_name[cat_id], font_scale=font_scale_orig, **kwargs)\n        for bbox,cat_id in zip(augmented['bboxes'], augmented['category_ids']):\n            visualize_titles(image_aug, bbox, category_id_to_name[cat_id], font_scale=font_scale_aug, **kwargs)\n\n\n    if mask is None:\n        f, ax = plt.subplots(1, 2, figsize=(16, 8))\n\n        ax[0].imshow(image)\n        ax[0].set_title('Original image')\n\n        ax[1].imshow(image_aug)\n        ax[1].set_title('Augmented image')\n    else:\n        f, ax = plt.subplots(2, 2, figsize=(16, 16))\n\n        if len(mask.shape) != 3:\n            mask = label2rgb(mask, bg_label=0)\n            mask_aug = label2rgb(augmented['mask'], bg_label=0)\n        else:\n            mask = cv2.cvtColor(mask, cv2.COLOR_BGR2RGB)\n            mask_aug = cv2.cvtColor(augmented['mask'], cv2.COLOR_BGR2RGB)\n\n        ax[0, 0].imshow(image)\n        ax[0, 0].set_title('Original image')\n\n        ax[0, 1].imshow(image_aug)\n        ax[0, 1].set_title('Augmented image')\n\n        ax[1, 0].imshow(mask, interpolation='nearest')\n        ax[1, 0].set_title('Original mask')\n\n        ax[1, 1].imshow(mask_aug, interpolation='nearest')\n        ax[1, 1].set_title('Augmented mask')\n\n    f.tight_layout()\n\n    if filename is not None:\n        f.savefig(filename)\n\n    if mask is None:\n        return augmented['image'], None, augmented['bboxes']\n\n    return augmented['image'], augmented['mask'], augmented['bboxes']\n\ndef find_in_dir(dirname):\n    return [os.path.join(dirname, fname) for fname in sorted(os.listdir(dirname))]\n</code></pre>"},{"location":"examples/showcase/#color-augmentations","title":"Color augmentations","text":"Python<pre><code>image = cv2.imread('images/parrot.jpg')\n</code></pre> Python<pre><code>random.seed(42)\n\nbbox_params = A.BboxParams(format='coco', label_fields=['category_ids'])\n\nlight = A.Compose([\n    A.RandomBrightnessContrast(p=1),\n    A.RandomGamma(p=1),\n    A.CLAHE(p=1),\n], p=1, bbox_params=bbox_params)\n\nmedium = A.Compose([\n    A.CLAHE(p=1),\n    A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=50, val_shift_limit=50, p=1),\n], p=1, bbox_params=bbox_params)\n\n\nstrong = A.Compose([\n    A.RGBShift(p=1),\n     A.Blur(p=1),\n     A.GaussNoise(p=1),\n     A.ElasticTransform(p=1),\n], p=1, bbox_params=bbox_params)\n</code></pre> Python<pre><code>r = augment_and_show(light, image)\n</code></pre> Python<pre><code>r = augment_and_show(medium, image)\n</code></pre> Python<pre><code>r = augment_and_show(strong, image)\n</code></pre>"},{"location":"examples/showcase/#inria-aerial-image-labeling-dataset","title":"Inria Aerial Image Labeling Dataset","text":"Python<pre><code>random.seed(42)\n\nimage, mask = cv2.imread('images/inria/inria_tyrol_w4_image.jpg'), cv2.imread('images/inria/inria_tyrol_w4_mask.tif', cv2.IMREAD_GRAYSCALE)\nimage, mask = image[:1024, :1024], mask[:1024,:1024]\n\nlight = A.Compose([\n     A.RandomSizedCrop((512-100, 512+100), 512, 512),\n     A.ShiftScaleRotate(),\n     A.RGBShift(),\n     A.Blur(),\n     A.GaussNoise(),\n     A.ElasticTransform(),\n     A.MaskDropout((10,15), p=1),\n], p=1, bbox_params=A.BboxParams(format='coco', label_fields=['category_ids']))\n\nr = augment_and_show(light, image, mask)\n</code></pre>"},{"location":"examples/showcase/#2018-data-science-bowl","title":"2018 Data Science Bowl","text":"Python<pre><code>random.seed(42)\n\nimage = cv2.imread(\n    'images/dsb2018/1a11552569160f0b1ea10bedbd628ce6c14f29edec5092034c2309c556df833e/images/1a11552569160f0b1ea10bedbd628ce6c14f29edec5092034c2309c556df833e.png')\nmasks = [cv2.imread(x, cv2.IMREAD_GRAYSCALE) for x in\n         find_in_dir('images/dsb2018/1a11552569160f0b1ea10bedbd628ce6c14f29edec5092034c2309c556df833e/masks')]\nbboxes = [cv2.boundingRect(cv2.findNonZero(mask)) for mask in masks]\nlabel_image = np.zeros_like(masks[0])\nfor i, mask in enumerate(masks):\n    label_image += (mask &gt; 0).astype(np.uint8) * i\n\nlight = A.Compose([\n    A.RGBShift(),\n    A.InvertImg(),\n    A.Blur(),\n    A.GaussNoise(),\n    A.Flip(),\n    A.RandomRotate90(),\n    A.RandomSizedCrop((512 - 100, 512 + 100), 512, 512),\n], bbox_params={'format':'coco', 'min_area': 1, 'min_visibility': 0.5, 'label_fields': ['category_ids']}, p=1)\n\nlabel_ids = [0] * len(bboxes)\nlabel_names = ['Nuclei']\n\nr = augment_and_show(light, image, label_image, bboxes, label_ids, label_names, show_title=False)\n</code></pre>"},{"location":"examples/showcase/#mapilary-vistas","title":"Mapilary Vistas","text":"Python<pre><code>from PIL import Image\n\nimage = cv2.imread('images/vistas/_HnWguqEbRCphUquTMrCCA.jpg')\nlabels = cv2.imread('images/vistas/_HnWguqEbRCphUquTMrCCA_labels.png', cv2.IMREAD_COLOR)\ninstances = np.array(Image.open('images/vistas/_HnWguqEbRCphUquTMrCCA_instances.png'),dtype=np.uint16)\nIGNORED = 65 * 256\n\ninstances[(instances//256 != 55) &amp; (instances//256 != 44) &amp; (instances//256 != 50)] = IGNORED\n\nimage = image[1000:2500, 1000:2500]\nlabels = labels[1000:2500, 1000:2500]\ninstances = instances[1000:2500, 1000:2500]\n\nbboxes = [cv2.boundingRect(cv2.findNonZero((instances == instance_id).astype(np.uint8))) for instance_id in np.unique(instances) if instance_id != IGNORED]\ninstance_labels = [instance_id // 256 for instance_id in np.unique(instances) if instance_id != IGNORED]\n\ntitles = [\"Bird\",\n\"Ground Animal\",\n\"Curb\",\n\"Fence\",\n\"Guard Rail\",\n\"Barrier\",\n\"Wall\",\n\"Bike Lane\",\n\"Crosswalk - Plain\",\n\"Curb Cut\",\n\"Parking\",\n\"Pedestrian Area\",\n\"Rail Track\",\n\"Road\",\n\"Service Lane\",\n\"Sidewalk\",\n\"Bridge\",\n\"Building\",\n\"Tunnel\",\n\"Person\",\n\"Bicyclist\",\n\"Motorcyclist\",\n\"Other Rider\",\n\"Lane Marking - Crosswalk\",\n\"Lane Marking - General\",\n\"Mountain\",\n\"Sand\",\n\"Sky\",\n\"Snow\",\n\"Terrain\",\n\"Vegetation\",\n\"Water\",\n\"Banner\",\n\"Bench\",\n\"Bike Rack\",\n\"Billboard\",\n\"Catch Basin\",\n\"CCTV Camera\",\n\"Fire Hydrant\",\n\"Junction Box\",\n\"Mailbox\",\n\"Manhole\",\n\"Phone Booth\",\n\"Pothole\",\n\"Street Light\",\n\"Pole\",\n\"Traffic Sign Frame\",\n\"Utility Pole\",\n\"Traffic Light\",\n\"Traffic Sign (Back)\",\n\"Traffic Sign (Front)\",\n\"Trash Can\",\n\"Bicycle\",\n\"Boat\",\n\"Bus\",\n\"Car\",\n\"Caravan\",\n\"Motorcycle\",\n\"On Rails\",\n\"Other Vehicle\",\n\"Trailer\",\n\"Truck\",\n\"Wheeled Slow\",\n\"Car Mount\",\n\"Ego Vehicle\",\n\"Unlabeled\"]\nbbox_params = A.BboxParams(format='coco', min_area=1, min_visibility=0.5, label_fields=['category_ids'])\n\nlight = A.Compose([\n    A.HorizontalFlip(p=1),\n    A.RandomSizedCrop((800 - 100, 800 + 100), 600, 600),\n    A.GaussNoise(var_limit=(100, 150), p=1),\n], bbox_params=bbox_params,  p=1)\n\nmedium = A.Compose([\n    A.HorizontalFlip(p=1),\n    A.RandomSizedCrop((800 - 100, 800 + 100), 600, 600),\n    A.MotionBlur(blur_limit=17, p=1),\n], bbox_params=bbox_params, p=1)\n\n\nstrong = A.Compose([\n    A.HorizontalFlip(p=1),\n    A.RandomSizedCrop((800 - 100, 800 + 100), 600, 600),\n    A.RGBShift(p=1),\n    A.Blur(blur_limit=11, p=1),\n    A.RandomBrightnessContrast(p=1),\n    A.CLAHE(p=1),\n], bbox_params=bbox_params, p=1)\n</code></pre> Python<pre><code>random.seed(13)\nr = augment_and_show(light, image, labels, bboxes, instance_labels, titles, thickness=2,\n                     font_scale_orig=2,\n                     font_scale_aug=1)\n</code></pre> Python<pre><code>random.seed(13)\nr = augment_and_show(medium, image, labels, bboxes, instance_labels, titles, thickness=2, font_scale_orig=2, font_scale_aug=1)\n</code></pre> Python<pre><code>random.seed(13)\nr = augment_and_show(strong, image, labels, bboxes, instance_labels, titles, thickness=2, font_scale_orig=2, font_scale_aug=1)\n</code></pre>"},{"location":"examples/tensorflow-example/","title":"Using Albumentations with Tensorflow","text":"<p>Author: Ayushman Buragohain</p> Python<pre><code>!pip install -q -U albumentations\n!echo \"$(pip freeze | grep albumentations) is successfully installed\"\n</code></pre> <pre><code>albumentations==1.4.0 is successfully installed\n</code></pre>"},{"location":"examples/tensorflow-example/#recommended-update-the-version-of-tensorflow_datasets-if-you-want-to-use-it","title":"[Recommended] Update the version of tensorflow_datasets if you want to use it","text":"<ul> <li>We'll we using an example from <code>tensorflow_datasets</code>.</li> </ul> Python<pre><code>! pip install --upgrade tensorflow_datasets tensorflow\n</code></pre> <pre><code>Requirement already satisfied: tensorflow_datasets in /Users/vladimiriglovikov/anaconda3/envs/albumentations_examples/lib/python3.10/site-packages (4.9.4)\nRequirement already satisfied: tensorflow in /Users/vladimiriglovikov/anaconda3/envs/albumentations_examples/lib/python3.10/site-packages (2.15.0)\nRequirement already satisfied: absl-py in /Users/vladimiriglovikov/anaconda3/envs/albumentations_examples/lib/python3.10/site-packages (from tensorflow_datasets) (1.4.0)\nRequirement already satisfied: click in /Users/vladimiriglovikov/anaconda3/envs/albumentations_examples/lib/python3.10/site-packages (from tensorflow_datasets) (8.1.7)\nRequirement already satisfied: dm-tree in /Users/vladimiriglovikov/anaconda3/envs/albumentations_examples/lib/python3.10/site-packages (from tensorflow_datasets) (0.1.8)\nRequirement already satisfied: etils&gt;=0.9.0 in /Users/vladimiriglovikov/anaconda3/envs/albumentations_examples/lib/python3.10/site-packages (from etils[enp,epath,etree]&gt;=0.9.0-&gt;tensorflow_datasets) (1.7.0)\nRequirement already satisfied: numpy in /Users/vladimiriglovikov/anaconda3/envs/albumentations_examples/lib/python3.10/site-packages (from tensorflow_datasets) (1.26.4)\nRequirement already satisfied: promise in /Users/vladimiriglovikov/anaconda3/envs/albumentations_examples/lib/python3.10/site-packages (from tensorflow_datasets) (2.3)\nRequirement already satisfied: protobuf&gt;=3.20 in /Users/vladimiriglovikov/anaconda3/envs/albumentations_examples/lib/python3.10/site-packages (from tensorflow_datasets) (3.20.3)\nRequirement already satisfied: psutil in /Users/vladimiriglovikov/anaconda3/envs/albumentations_examples/lib/python3.10/site-packages (from tensorflow_datasets) (5.9.0)\nRequirement already satisfied: requests&gt;=2.19.0 in /Users/vladimiriglovikov/anaconda3/envs/albumentations_examples/lib/python3.10/site-packages (from tensorflow_datasets) (2.31.0)\nRequirement already satisfied: tensorflow-metadata in /Users/vladimiriglovikov/anaconda3/envs/albumentations_examples/lib/python3.10/site-packages (from tensorflow_datasets) (1.14.0)\nRequirement already satisfied: termcolor in /Users/vladimiriglovikov/anaconda3/envs/albumentations_examples/lib/python3.10/site-packages (from tensorflow_datasets) (2.4.0)\nRequirement already satisfied: toml in /Users/vladimiriglovikov/anaconda3/envs/albumentations_examples/lib/python3.10/site-packages (from tensorflow_datasets) (0.10.2)\nRequirement already satisfied: tqdm in /Users/vladimiriglovikov/anaconda3/envs/albumentations_examples/lib/python3.10/site-packages (from tensorflow_datasets) (4.66.2)\nRequirement already satisfied: wrapt in /Users/vladimiriglovikov/anaconda3/envs/albumentations_examples/lib/python3.10/site-packages (from tensorflow_datasets) (1.14.1)\nRequirement already satisfied: tensorflow-macos==2.15.0 in /Users/vladimiriglovikov/anaconda3/envs/albumentations_examples/lib/python3.10/site-packages (from tensorflow) (2.15.0)\nRequirement already satisfied: astunparse&gt;=1.6.0 in /Users/vladimiriglovikov/anaconda3/envs/albumentations_examples/lib/python3.10/site-packages (from tensorflow-macos==2.15.0-&gt;tensorflow) (1.6.3)\nRequirement already satisfied: flatbuffers&gt;=23.5.26 in /Users/vladimiriglovikov/anaconda3/envs/albumentations_examples/lib/python3.10/site-packages (from tensorflow-macos==2.15.0-&gt;tensorflow) (23.5.26)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,&gt;=0.2.1 in /Users/vladimiriglovikov/anaconda3/envs/albumentations_examples/lib/python3.10/site-packages (from tensorflow-macos==2.15.0-&gt;tensorflow) (0.5.4)\nRequirement already satisfied: google-pasta&gt;=0.1.1 in /Users/vladimiriglovikov/anaconda3/envs/albumentations_examples/lib/python3.10/site-packages (from tensorflow-macos==2.15.0-&gt;tensorflow) (0.2.0)\nRequirement already satisfied: h5py&gt;=2.9.0 in /Users/vladimiriglovikov/anaconda3/envs/albumentations_examples/lib/python3.10/site-packages (from tensorflow-macos==2.15.0-&gt;tensorflow) (3.10.0)\nRequirement already satisfied: libclang&gt;=13.0.0 in /Users/vladimiriglovikov/anaconda3/envs/albumentations_examples/lib/python3.10/site-packages (from tensorflow-macos==2.15.0-&gt;tensorflow) (16.0.6)\nRequirement already satisfied: ml-dtypes~=0.2.0 in /Users/vladimiriglovikov/anaconda3/envs/albumentations_examples/lib/python3.10/site-packages (from tensorflow-macos==2.15.0-&gt;tensorflow) (0.2.0)\nRequirement already satisfied: opt-einsum&gt;=2.3.2 in /Users/vladimiriglovikov/anaconda3/envs/albumentations_examples/lib/python3.10/site-packages (from tensorflow-macos==2.15.0-&gt;tensorflow) (3.3.0)\nRequirement already satisfied: packaging in /Users/vladimiriglovikov/anaconda3/envs/albumentations_examples/lib/python3.10/site-packages (from tensorflow-macos==2.15.0-&gt;tensorflow) (23.2)\nRequirement already satisfied: setuptools in /Users/vladimiriglovikov/anaconda3/envs/albumentations_examples/lib/python3.10/site-packages (from tensorflow-macos==2.15.0-&gt;tensorflow) (68.2.2)\nRequirement already satisfied: six&gt;=1.12.0 in /Users/vladimiriglovikov/anaconda3/envs/albumentations_examples/lib/python3.10/site-packages (from tensorflow-macos==2.15.0-&gt;tensorflow) (1.16.0)\nRequirement already satisfied: typing-extensions&gt;=3.6.6 in /Users/vladimiriglovikov/anaconda3/envs/albumentations_examples/lib/python3.10/site-packages (from tensorflow-macos==2.15.0-&gt;tensorflow) (4.9.0)\nRequirement already satisfied: tensorflow-io-gcs-filesystem&gt;=0.23.1 in /Users/vladimiriglovikov/anaconda3/envs/albumentations_examples/lib/python3.10/site-packages (from tensorflow-macos==2.15.0-&gt;tensorflow) (0.36.0)\nRequirement already satisfied: grpcio&lt;2.0,&gt;=1.24.3 in /Users/vladimiriglovikov/anaconda3/envs/albumentations_examples/lib/python3.10/site-packages (from tensorflow-macos==2.15.0-&gt;tensorflow) (1.60.1)\nRequirement already satisfied: tensorboard&lt;2.16,&gt;=2.15 in /Users/vladimiriglovikov/anaconda3/envs/albumentations_examples/lib/python3.10/site-packages (from tensorflow-macos==2.15.0-&gt;tensorflow) (2.15.2)\nRequirement already satisfied: tensorflow-estimator&lt;2.16,&gt;=2.15.0 in /Users/vladimiriglovikov/anaconda3/envs/albumentations_examples/lib/python3.10/site-packages (from tensorflow-macos==2.15.0-&gt;tensorflow) (2.15.0)\nRequirement already satisfied: keras&lt;2.16,&gt;=2.15.0 in /Users/vladimiriglovikov/anaconda3/envs/albumentations_examples/lib/python3.10/site-packages (from tensorflow-macos==2.15.0-&gt;tensorflow) (2.15.0)\nRequirement already satisfied: fsspec in /Users/vladimiriglovikov/anaconda3/envs/albumentations_examples/lib/python3.10/site-packages (from etils[enp,epath,etree]&gt;=0.9.0-&gt;tensorflow_datasets) (2024.2.0)\nRequirement already satisfied: importlib_resources in /Users/vladimiriglovikov/anaconda3/envs/albumentations_examples/lib/python3.10/site-packages (from etils[enp,epath,etree]&gt;=0.9.0-&gt;tensorflow_datasets) (6.1.1)\nRequirement already satisfied: zipp in /Users/vladimiriglovikov/anaconda3/envs/albumentations_examples/lib/python3.10/site-packages (from etils[enp,epath,etree]&gt;=0.9.0-&gt;tensorflow_datasets) (3.17.0)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /Users/vladimiriglovikov/anaconda3/envs/albumentations_examples/lib/python3.10/site-packages (from requests&gt;=2.19.0-&gt;tensorflow_datasets) (3.3.2)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /Users/vladimiriglovikov/anaconda3/envs/albumentations_examples/lib/python3.10/site-packages (from requests&gt;=2.19.0-&gt;tensorflow_datasets) (3.6)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /Users/vladimiriglovikov/anaconda3/envs/albumentations_examples/lib/python3.10/site-packages (from requests&gt;=2.19.0-&gt;tensorflow_datasets) (2.2.0)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /Users/vladimiriglovikov/anaconda3/envs/albumentations_examples/lib/python3.10/site-packages (from requests&gt;=2.19.0-&gt;tensorflow_datasets) (2024.2.2)\nRequirement already satisfied: googleapis-common-protos&lt;2,&gt;=1.52.0 in /Users/vladimiriglovikov/anaconda3/envs/albumentations_examples/lib/python3.10/site-packages (from tensorflow-metadata-&gt;tensorflow_datasets) (1.62.0)\nRequirement already satisfied: wheel&lt;1.0,&gt;=0.23.0 in /Users/vladimiriglovikov/anaconda3/envs/albumentations_examples/lib/python3.10/site-packages (from astunparse&gt;=1.6.0-&gt;tensorflow-macos==2.15.0-&gt;tensorflow) (0.41.2)\nRequirement already satisfied: google-auth&lt;3,&gt;=1.6.3 in /Users/vladimiriglovikov/anaconda3/envs/albumentations_examples/lib/python3.10/site-packages (from tensorboard&lt;2.16,&gt;=2.15-&gt;tensorflow-macos==2.15.0-&gt;tensorflow) (2.28.0)\nRequirement already satisfied: google-auth-oauthlib&lt;2,&gt;=0.5 in /Users/vladimiriglovikov/anaconda3/envs/albumentations_examples/lib/python3.10/site-packages (from tensorboard&lt;2.16,&gt;=2.15-&gt;tensorflow-macos==2.15.0-&gt;tensorflow) (1.2.0)\nRequirement already satisfied: markdown&gt;=2.6.8 in /Users/vladimiriglovikov/anaconda3/envs/albumentations_examples/lib/python3.10/site-packages (from tensorboard&lt;2.16,&gt;=2.15-&gt;tensorflow-macos==2.15.0-&gt;tensorflow) (3.5.2)\nRequirement already satisfied: tensorboard-data-server&lt;0.8.0,&gt;=0.7.0 in /Users/vladimiriglovikov/anaconda3/envs/albumentations_examples/lib/python3.10/site-packages (from tensorboard&lt;2.16,&gt;=2.15-&gt;tensorflow-macos==2.15.0-&gt;tensorflow) (0.7.2)\nRequirement already satisfied: werkzeug&gt;=1.0.1 in /Users/vladimiriglovikov/anaconda3/envs/albumentations_examples/lib/python3.10/site-packages (from tensorboard&lt;2.16,&gt;=2.15-&gt;tensorflow-macos==2.15.0-&gt;tensorflow) (3.0.1)\nRequirement already satisfied: cachetools&lt;6.0,&gt;=2.0.0 in /Users/vladimiriglovikov/anaconda3/envs/albumentations_examples/lib/python3.10/site-packages (from google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard&lt;2.16,&gt;=2.15-&gt;tensorflow-macos==2.15.0-&gt;tensorflow) (5.3.2)\nRequirement already satisfied: pyasn1-modules&gt;=0.2.1 in /Users/vladimiriglovikov/anaconda3/envs/albumentations_examples/lib/python3.10/site-packages (from google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard&lt;2.16,&gt;=2.15-&gt;tensorflow-macos==2.15.0-&gt;tensorflow) (0.3.0)\nRequirement already satisfied: rsa&lt;5,&gt;=3.1.4 in /Users/vladimiriglovikov/anaconda3/envs/albumentations_examples/lib/python3.10/site-packages (from google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard&lt;2.16,&gt;=2.15-&gt;tensorflow-macos==2.15.0-&gt;tensorflow) (4.9)\nRequirement already satisfied: requests-oauthlib&gt;=0.7.0 in /Users/vladimiriglovikov/anaconda3/envs/albumentations_examples/lib/python3.10/site-packages (from google-auth-oauthlib&lt;2,&gt;=0.5-&gt;tensorboard&lt;2.16,&gt;=2.15-&gt;tensorflow-macos==2.15.0-&gt;tensorflow) (1.3.1)\nRequirement already satisfied: MarkupSafe&gt;=2.1.1 in /Users/vladimiriglovikov/anaconda3/envs/albumentations_examples/lib/python3.10/site-packages (from werkzeug&gt;=1.0.1-&gt;tensorboard&lt;2.16,&gt;=2.15-&gt;tensorflow-macos==2.15.0-&gt;tensorflow) (2.1.5)\nRequirement already satisfied: pyasn1&lt;0.6.0,&gt;=0.4.6 in /Users/vladimiriglovikov/anaconda3/envs/albumentations_examples/lib/python3.10/site-packages (from pyasn1-modules&gt;=0.2.1-&gt;google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard&lt;2.16,&gt;=2.15-&gt;tensorflow-macos==2.15.0-&gt;tensorflow) (0.5.1)\nRequirement already satisfied: oauthlib&gt;=3.0.0 in /Users/vladimiriglovikov/anaconda3/envs/albumentations_examples/lib/python3.10/site-packages (from requests-oauthlib&gt;=0.7.0-&gt;google-auth-oauthlib&lt;2,&gt;=0.5-&gt;tensorboard&lt;2.16,&gt;=2.15-&gt;tensorflow-macos==2.15.0-&gt;tensorflow) (3.2.2)\n</code></pre>"},{"location":"examples/tensorflow-example/#run-the-example","title":"Run the example","text":"Python<pre><code># necessary imports\nimport tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow_datasets as tfds\nfrom functools import partial\nimport albumentations as A\n\nAUTOTUNE = tf.data.experimental.AUTOTUNE\n</code></pre> Python<pre><code>tfds.__version__\n</code></pre> <pre><code>'4.9.4'\n</code></pre> Python<pre><code># load in the tf_flowers dataset\ndata, info= tfds.load(name=\"tf_flowers\", split=\"train\", as_supervised=True, with_info=True)\ndata\n</code></pre> <pre><code>&lt;_PrefetchDataset element_spec=(TensorSpec(shape=(None, None, 3), dtype=tf.uint8, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))&gt;\n</code></pre> Python<pre><code>info\n</code></pre> <pre><code>tfds.core.DatasetInfo(\n    name='tf_flowers',\n    full_name='tf_flowers/3.0.1',\n    description=\"\"\"\n    A large set of images of flowers\n    \"\"\",\n    homepage='https://www.tensorflow.org/tutorials/load_data/images',\n    data_dir='/Users/vladimiriglovikov/tensorflow_datasets/tf_flowers/3.0.1',\n    file_format=tfrecord,\n    download_size=218.21 MiB,\n    dataset_size=221.83 MiB,\n    features=FeaturesDict({\n        'image': Image(shape=(None, None, 3), dtype=uint8),\n        'label': ClassLabel(shape=(), dtype=int64, num_classes=5),\n    }),\n    supervised_keys=('image', 'label'),\n    disable_shuffling=False,\n    splits={\n        'train': &lt;SplitInfo num_examples=3670, num_shards=2&gt;,\n    },\n    citation=\"\"\"@ONLINE {tfflowers,\n    author = \"The TensorFlow Team\",\n    title = \"Flowers\",\n    month = \"jan\",\n    year = \"2019\",\n    url = \"http://download.tensorflow.org/example_images/flower_photos.tgz\" }\"\"\",\n)\n</code></pre>"},{"location":"examples/tensorflow-example/#an-example-pipeline-using-tfimage","title":"An Example Pipeline Using <code>tf.image</code>","text":""},{"location":"examples/tensorflow-example/#process-data","title":"Process Data","text":"Python<pre><code>def process_image(image, label, img_size):\n    # cast and normalize image\n    image = tf.image.convert_image_dtype(image, tf.float32)\n    # apply simple augmentations\n    image = tf.image.random_flip_left_right(image)\n    image = tf.image.resize(image,[img_size, img_size])\n    return image, label\n\nds_tf = data.map(partial(process_image, img_size=120), num_parallel_calls=AUTOTUNE).batch(30).prefetch(AUTOTUNE)\nds_tf\n</code></pre> <pre><code>&lt;_PrefetchDataset element_spec=(TensorSpec(shape=(None, 120, 120, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int64, name=None))&gt;\n</code></pre>"},{"location":"examples/tensorflow-example/#view-images-from-the-dataset","title":"View images from the dataset","text":"Python<pre><code>def view_image(ds):\n    image, label = next(iter(ds)) # extract 1 batch from the dataset\n    image = image.numpy()\n    label = label.numpy()\n\n    fig = plt.figure(figsize=(22, 22))\n    for i in range(20):\n        ax = fig.add_subplot(4, 5, i+1, xticks=[], yticks=[])\n        ax.imshow(image[i])\n        ax.set_title(f\"Label: {label[i]}\")\n</code></pre> Python<pre><code>view_image(ds_tf)\n</code></pre> <pre><code>2024-02-18 14:57:07.678378: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n</code></pre> <p>Using <code>tf.image</code> is very efficient to create a pipeline but the disadvantage is that with <code>tf.image</code> we can only apply limited amounts of augmentations to our <code>input data</code>. One way to solve is issue is to use <code>tf.keras</code> <code>ImageDataGenerator</code> class but <code>albumentations</code> is faster.</p>"},{"location":"examples/tensorflow-example/#an-example-pipeline-using-albumentations","title":"An Example Pipeline using <code>albumentations</code>","text":"<p>To integrate <code>albumentations</code> into our tensorflow pipeline we can create two functions : - Pipeline to apply <code>augmentation</code>. - a function that calls the above function and pass in our data through the pipeline. We can then wrap our 2nd Function under <code>tf.numpy_function</code> .</p> <p>italicized text## Create Pipeline to Process data</p> Python<pre><code># Instantiate augments\n# we can apply as many augments we want and adjust the values accordingly\n# here I have chosen the augments and their arguments at random\ntransforms = A.Compose([\n            A.Rotate(limit=40),\n            A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.5),\n            A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.5),\n            A.HorizontalFlip(),\n        ])\n</code></pre> Python<pre><code>def aug_fn(image, img_size):\n    data = {\"image\":image}\n    aug_data = transforms(**data)\n    aug_img = aug_data[\"image\"]\n    aug_img = tf.cast(aug_img/255.0, tf.float32)\n    return tf.image.resize(aug_img, size=[img_size, img_size])\n</code></pre> Python<pre><code>def process_data(image, label, img_size):\n    aug_img = tf.numpy_function(func=aug_fn, inp=[image, img_size], Tout=tf.float32)\n    return aug_img, label\n</code></pre> Python<pre><code># create dataset\nds_alb = data.map(partial(process_data, img_size=120),\n                  num_parallel_calls=AUTOTUNE).prefetch(AUTOTUNE)\nds_alb\n</code></pre> <pre><code>&lt;_PrefetchDataset element_spec=(TensorSpec(shape=&lt;unknown&gt;, dtype=tf.float32, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))&gt;\n</code></pre>"},{"location":"examples/tensorflow-example/#restoring-dataset-shapes","title":"Restoring dataset shapes.","text":"<p>The datasets loses its shape after applying a tf.numpy_function, so this is necessary for the sequential model and when inheriting from the model class.</p> Python<pre><code>def set_shapes(img, label, img_shape=(120,120,3)):\n    img.set_shape(img_shape)\n    label.set_shape([])\n    return img, label\n</code></pre> Python<pre><code>ds_alb = ds_alb.map(set_shapes, num_parallel_calls=AUTOTUNE).batch(32).prefetch(AUTOTUNE)\nds_alb\n</code></pre> <pre><code>&lt;_PrefetchDataset element_spec=(TensorSpec(shape=(None, 120, 120, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int64, name=None))&gt;\n</code></pre>"},{"location":"examples/tensorflow-example/#view-images-from-the-dataset_1","title":"View images from the dataset","text":"Python<pre><code>view_image(ds_alb)\n</code></pre> <pre><code>2024-02-18 14:57:09.800432: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n</code></pre> <p>We can then pass in this dataset to out model and call <code>fit</code> on our model</p>"},{"location":"examples/tensorflow-example/#note","title":"<code>Note</code>:","text":"<p>Some <code>API's</code> of <code>tensorflow.keras.Model</code> might not work, if you dont map the dataset with the set_shapes function.</p>"},{"location":"examples/tensorflow-example/#what-works-without-setting-shapes","title":"What works without setting shapes :","text":"Python<pre><code>from tensorflow.keras import models, layers\nfrom tensorflow import keras\n\n# Running the Model in eager mode using Sequential API\n\ndef create_model(input_shape):\n    return models.Sequential([\n        layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n        layers.MaxPooling2D((2, 2)),\n        layers.Conv2D(64, (3, 3), activation='relu'),\n        layers.MaxPooling2D((2, 2)),\n        layers.Conv2D(64, (3, 3), activation='relu'),\n        layers.Flatten(),\n        layers.Dense(64, activation='relu'),\n        layers.Dense(5, activation='softmax')])\n\nmodel = create_model((120,120,3))\n\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics='accuracy', run_eagerly=True)\nmodel.fit(ds_alb, epochs=2)\n</code></pre> <pre><code>Epoch 1/2\n115/115 [==============================] - 22s 190ms/step - loss: 1.3435 - accuracy: 0.4322\nEpoch 2/2\n115/115 [==============================] - 21s 180ms/step - loss: 1.1221 - accuracy: 0.5529\n\n\n\n\n\n&lt;keras.src.callbacks.History at 0x2d9f3b130&gt;\n</code></pre> Python<pre><code># Functional API\n\ninput = keras.Input(shape=(120, 120, 3))\nx = keras.layers.Conv2D(32, (3, 3), activation=\"relu\")(input)\nx = keras.layers.MaxPooling2D((2, 2))(x)\nx = keras.layers.Conv2D(64, (3, 3), activation='relu')(x)\nx = keras.layers.MaxPooling2D((2, 2))(x)\nx = keras.layers.Conv2D(64, (3, 3), activation='relu')(x)\nx = keras.layers.Flatten()(x)\nx = keras.layers.Dense(64, activation='relu')(x)\nx = keras.layers.Dense(5, activation='softmax')(x)\n\nmodel = keras.Model(inputs=input, outputs=x)\n\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics='accuracy')\nmodel.fit(ds_alb, epochs=2)\n</code></pre> <pre><code>Epoch 1/2\n115/115 [==============================] - 12s 102ms/step - loss: 1.4318 - accuracy: 0.3807\nEpoch 2/2\n115/115 [==============================] - 12s 104ms/step - loss: 1.1839 - accuracy: 0.5183\n\n\n\n\n\n&lt;keras.src.callbacks.History at 0x2d9ee2d40&gt;\n</code></pre> Python<pre><code># Transfer Learning [freeze base model layers]: Sequential API\n\nbase_model = keras.applications.ResNet50(include_top=False, input_shape=(120, 120, 3), weights=\"imagenet\")\nbase_model.trainable = False\n\nmodel = keras.models.Sequential([\n        base_model,\n        keras.layers.Conv2D(32, (1, 1), activation=\"relu\"),\n        keras.layers.Dropout(0.2),\n        keras.layers.Flatten(),\n        keras.layers.Dense(64, activation='relu'),\n        keras.layers.Dense(5, activation='softmax'),\n])\n\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics='accuracy')\nmodel.fit(ds_alb, epochs=2)\n</code></pre> <pre><code>Epoch 1/2\n115/115 [==============================] - 30s 239ms/step - loss: 1.5254 - accuracy: 0.3188\nEpoch 2/2\n115/115 [==============================] - 29s 254ms/step - loss: 1.4476 - accuracy: 0.3768\n\n\n\n\n\n&lt;keras.src.callbacks.History at 0x2e53a5ed0&gt;\n</code></pre> Python<pre><code># Transfer Learning [unfreeze all layers]: Sequential API\n\nbase_model = keras.applications.ResNet50(include_top=False, input_shape=(120, 120, 3), weights=\"imagenet\")\nbase_model.trainable = True\n\nmodel = keras.models.Sequential([\n        base_model,\n        keras.layers.Conv2D(32, (1, 1), activation=\"relu\"),\n        keras.layers.Flatten(),\n        keras.layers.Dense(64, activation='relu'),\n        keras.layers.Dense(5, activation='softmax'),\n])\n\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics='accuracy')\nmodel.fit(ds_alb, epochs=2)\n</code></pre> <pre><code>Epoch 1/2\n115/115 [==============================] - 133s 1s/step - loss: 1.1360 - accuracy: 0.5924\nEpoch 2/2\n115/115 [==============================] - 143s 1s/step - loss: 0.7753 - accuracy: 0.7278\n\n\n\n\n\n&lt;keras.src.callbacks.History at 0x2e5be11e0&gt;\n</code></pre> Python<pre><code># Transfer Learning [freeze all layers of feature extractor]: Functional API\n\nbase_model = keras.applications.ResNet50(include_top=False, input_shape=(120, 120, 3), weights=\"imagenet\")\nbase_model.trainable = False\n\ninput = keras.Input(shape=(120, 120, 3))\nx = base_model(input, training=False)\nx = keras.layers.Conv2D(32, (1, 1), activation=\"relu\")(x)\nx = keras.layers.Dropout(0.2)(x)\nx = keras.layers.Flatten()(x)\nx = keras.layers.Dense(64, activation='relu')(x)\nx = keras.layers.Dense(5, activation='softmax')(x)\n\nmodel = keras.Model(inputs=input, outputs=x)\n\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics='accuracy')\nmodel.fit(ds_alb, epochs=2)\n</code></pre> <pre><code>Epoch 1/2\n115/115 [==============================] - 37s 289ms/step - loss: 1.5292 - accuracy: 0.3240\nEpoch 2/2\n115/115 [==============================] - 29s 251ms/step - loss: 1.4471 - accuracy: 0.3779\n\n\n\n\n\n&lt;keras.src.callbacks.History at 0x2de238820&gt;\n</code></pre> Python<pre><code># Transfer Learning [freeze all layers of feature extractor]: Subclass API\n\nbase_model = keras.applications.ResNet50(include_top=False, input_shape=(120, 120, 3), weights=\"imagenet\")\nbase_model.trainable = False\n\nclass MyModel(keras.Model):\n    def __init__(self, base_model):\n        super(MyModel, self).__init__()\n        self.base = base_model\n        self.layer_1 = keras.layers.Flatten()\n        self.layer_2 = keras.layers.Dense(64, activation='relu')\n        self.layer_3 = keras.layers.Dense(5, activation='softmax')\n\n    @tf.function\n    def call(self, xb):\n        x = self.base(xb)\n        x = self.layer_1(x)\n        x = self.layer_2(x)\n        return self.layer_3(x)\n\n\n\nmodel = MyModel(base_model=base_model)\n\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics='accuracy')\n\nmodel.fit(ds_alb, epochs=2)\n</code></pre> <pre><code>Epoch 1/2\n115/115 [==============================] - 34s 266ms/step - loss: 1.6205 - accuracy: 0.2736\nEpoch 2/2\n115/115 [==============================] - 31s 266ms/step - loss: 1.5317 - accuracy: 0.3025\n\n\n\n\n\n&lt;keras.src.callbacks.History at 0x2de884220&gt;\n</code></pre> Python<pre><code># Transfer Learning using [unfreeze all layers of feature extractor]: Subclass API\n\nbase_model = keras.applications.ResNet50(include_top=False, input_shape=(120, 120, 3), weights=\"imagenet\")\nbase_model.trainable = True\n\nclass MyModel(keras.Model):\n    def __init__(self, base_model):\n        super(MyModel, self).__init__()\n        self.base = base_model\n        self.layer_1 = keras.layers.Flatten()\n        self.layer_2 = keras.layers.Dense(64, activation='relu')\n        self.layer_3 = keras.layers.Dense(5, activation='softmax')\n\n    @tf.function\n    def call(self, xb):\n        x = self.base(xb)\n        x = self.layer_1(x)\n        x = self.layer_2(x)\n        return self.layer_3(x)\n\n\n\nmodel = MyModel(base_model=base_model)\n\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics='accuracy')\n\nmodel.fit(ds_alb, epochs=2)\n</code></pre> <pre><code>Epoch 1/2\n115/115 [==============================] - 137s 1s/step - loss: 1.7294 - accuracy: 0.5883\nEpoch 2/2\n115/115 [==============================] - 139s 1s/step - loss: 1.6056 - accuracy: 0.5390\n\n\n\n\n\n&lt;keras.src.callbacks.History at 0x2e09d9c00&gt;\n</code></pre>"},{"location":"examples/tensorflow-example/#what-works-only-if-you-set-the-shapes-of-the-dataset","title":"What works only if you set the shapes of the dataset :","text":"Python<pre><code># Using Sequential API without transfer learning &amp; Eager Execution\n\ndef create_model(input_shape):\n    return models.Sequential([\n        layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n        layers.MaxPooling2D((2, 2)),\n        layers.Conv2D(64, (3, 3), activation='relu'),\n        layers.MaxPooling2D((2, 2)),\n        layers.Conv2D(64, (3, 3), activation='relu'),\n        layers.Flatten(),\n        layers.Dense(64, activation='relu'),\n        layers.Dense(5, activation='softmax')])\n\nmodel = create_model((120,120,3))\n\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics='accuracy')\nmodel.fit(ds_alb, epochs=2)\n</code></pre> <pre><code>Epoch 1/2\n115/115 [==============================] - 12s 103ms/step - loss: 1.3967 - accuracy: 0.3831\nEpoch 2/2\n115/115 [==============================] - 12s 100ms/step - loss: 1.1674 - accuracy: 0.5272\n\n\n\n\n\n&lt;keras.src.callbacks.History at 0x3dc67a4a0&gt;\n</code></pre> Python<pre><code># Using Subclass API without transfer learning &amp; Eager Execution\n\nclass MyModel(keras.Model):\n    def __init__(self):\n        super(MyModel, self).__init__()\n        self.conv1 = keras.layers.Conv2D(32, (3, 3), activation='relu')\n        self.pool1 = keras.layers.MaxPooling2D((2, 2))\n        self.conv2 = keras.layers.Conv2D(64, (3, 3), activation='relu')\n        self.pool2 = keras.layers.MaxPooling2D((2, 2))\n        self.conv3 = keras.layers.Conv2D(64, (3, 3), activation='relu')\n        self.flat = keras.layers.Flatten()\n        self.dense1 = keras.layers.Dense(64, activation='relu')\n        self.dense2 = keras.layers.Dense(5, activation='softmax')\n\n    def call(self, xb):\n        x = self.conv1(xb)\n        x = self.pool1(x)\n        x = self.conv2(x)\n        x = self.pool2(x)\n        x = self.conv3(x)\n        x = self.flat(x)\n        x = self.dense1(x)\n        x = self.dense2(x)\n        return x\n\n\nmodel = MyModel()\n\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics='accuracy')\n\nmodel.fit(ds_alb, epochs=2)\n</code></pre> <pre><code>Epoch 1/2\n115/115 [==============================] - 12s 102ms/step - loss: 1.4053 - accuracy: 0.3921\nEpoch 2/2\n115/115 [==============================] - 13s 111ms/step - loss: 1.1591 - accuracy: 0.5360\n\n\n\n\n\n&lt;keras.src.callbacks.History at 0x3dc67a320&gt;\n</code></pre>"},{"location":"external_resources/blog_posts_podcasts_talks/","title":"Blog posts, podcasts, talks, and videos about Albumentations","text":""},{"location":"external_resources/blog_posts_podcasts_talks/#blog-posts","title":"Blog posts","text":"<ul> <li>Custom Image Augmentation with Keras. Solving CIFAR-10 with Albumentations and TPU on Google Colab..</li> <li>Road detection using segmentation models and albumentations libraries on Keras.</li> <li>Image Data Augmentation for TensorFlow 2, Keras and PyTorch with Albumentations in Python</li> <li>Explore image augmentations using a convenient tool</li> <li>Image Augmentation using PyTorch and Albumentations</li> <li>Employing the albumentation library in PyTorch workflows. Bonus: Helper for selecting appropriate values!</li> <li>Overview of Albumentations: Open-source library for advanced image augmentations</li> </ul>"},{"location":"external_resources/blog_posts_podcasts_talks/#podcasts-talks-and-videos","title":"Podcasts, talks, and videos","text":"<ul> <li>PyConBY 2020: Eugene Khvedchenya - Albumentations: Fast and Flexible image augmentations</li> <li>Albumentations Framework: a fast image augmentations library | Interview with Dr. Vladimir Iglovikov</li> <li>Image Data Augmentation for TensorFlow 2, Keras and PyTorch with Albumentations in Python</li> <li>Bengali.AI competition - Ch 5. Image augmentations using albumentations</li> <li>Albumentations Tutorial for Data Augmentation</li> </ul>"},{"location":"external_resources/books/","title":"Books that mention Albumentations","text":"<ul> <li>Deep Learning For Dummies. John Paul Mueller, Luca Massaron. May 2019.</li> <li>Data Science Programming All-in-One For Dummies. John Paul Mueller, Luca Massaron. January 2020.</li> <li>PyTorch Computer Vision Cookbook. Michael Avendi. March 2020.</li> <li>Approaching (Almost) Any Machine Learning Problem. Abhishek Thakur. June 2020.</li> </ul>"},{"location":"external_resources/online_courses/","title":"Online classes that cover Albumentations","text":""},{"location":"external_resources/online_courses/#udemy","title":"Udemy","text":"<ul> <li>Modern Computer Vision &amp; Deep Learning with Python &amp; PyTorch</li> <li>Deep Learning for Image Segmentation with Python &amp; Pytorch</li> <li>Deep Learning Masterclass with TensorFlow 2 Over 20 Projects</li> <li>Master Deep Learning for Computer Vision in TensorFlow</li> <li>Deep Learning : Image Classification with Tensorflow in 2024</li> <li>Deep learning with PyTorch | Medical Imaging Competitions</li> <li>Veri Art\u0131r\u0131m\u0131: Albumentations ile Projelerle Veri Art\u0131r\u0131m\u0131</li> <li>Mastering Advanced Representation Learning (CV)</li> </ul>"},{"location":"external_resources/online_courses/#coursera","title":"Coursera","text":"<ul> <li>Deep Learning with PyTorch : Image Segmentation</li> <li>Facial Keypoint Detection with PyTorch</li> <li>Deep Learning with PyTorch : Object Localization</li> <li>Aerial Image Segmentation with PyTorch</li> </ul>"},{"location":"getting_started/bounding_boxes_augmentation/","title":"Bounding boxes augmentation for object detection","text":""},{"location":"getting_started/bounding_boxes_augmentation/#different-annotations-formats","title":"Different annotations formats","text":"<p>Bounding boxes are rectangles that mark objects on an image. There are multiple formats of bounding boxes annotations. Each format uses its specific representation of bounding boxes coordinates. Albumentations supports four formats: <code>pascal_voc</code>, <code>albumentations</code>, <code>coco</code>, and <code>yolo</code> .</p> <p>Let's take a look at each of those formats and how they represent coordinates of bounding boxes.</p> <p>As an example, we will use an image from the dataset named Common Objects in Context. It contains one bounding box that marks a cat. The image width is 640 pixels, and its height is 480 pixels. The width of the bounding box is 322 pixels, and its height is 117 pixels.</p> <p>The bounding box has the following <code>(x, y)</code> coordinates of its corners: top-left is <code>(x_min, y_min)</code> or <code>(98px, 345px)</code>, top-right is <code>(x_max, y_min)</code> or <code>(420px, 345px)</code>, bottom-left is <code>(x_min, y_max)</code> or <code>(98px, 462px)</code>, bottom-right is <code>(x_max, y_max)</code> or <code>(420px, 462px)</code>. As you see, coordinates of the bounding box's corners are calculated with respect to the top-left corner of the image which has <code>(x, y)</code> coordinates <code>(0, 0)</code>.</p> <p> An example image with a bounding box from the COCO dataset</p>"},{"location":"getting_started/bounding_boxes_augmentation/#pascal_voc","title":"pascal_voc","text":"<p><code>pascal_voc</code> is a format used by the Pascal VOC dataset. Coordinates of a bounding box are encoded with four values in pixels: <code>[x_min, y_min, x_max, y_max]</code>.  <code>x_min</code> and <code>y_min</code> are coordinates of the top-left corner of the bounding box. <code>x_max</code> and <code>y_max</code> are coordinates of bottom-right corner of the bounding box.</p> <p>Coordinates of the example bounding box in this format are <code>[98, 345, 420, 462]</code>.</p>"},{"location":"getting_started/bounding_boxes_augmentation/#albumentations","title":"albumentations","text":"<p><code>albumentations</code> is similar to <code>pascal_voc</code>, because it also uses four values <code>[x_min, y_min, x_max, y_max]</code> to represent a bounding box. But unlike <code>pascal_voc</code>, <code>albumentations</code> uses normalized values. To normalize values, we divide coordinates in pixels for the x- and y-axis by the width and the height of the image.</p> <p>Coordinates of the example bounding box in this format are <code>[98 / 640, 345 / 480, 420 / 640, 462 / 480]</code> which are <code>[0.153125, 0.71875, 0.65625, 0.9625]</code>.</p> <p>Albumentations uses this format internally to work with bounding boxes and augment them.</p>"},{"location":"getting_started/bounding_boxes_augmentation/#coco","title":"coco","text":"<p><code>coco</code> is a format used by the Common Objects in Context COCO dataset.</p> <p>In <code>coco</code>, a bounding box is defined by four values in pixels <code>[x_min, y_min, width, height]</code>. They are coordinates of the top-left corner along with the width and height of the bounding box.</p> <p>Coordinates of the example bounding box in this format are <code>[98, 345, 322, 117]</code>.</p>"},{"location":"getting_started/bounding_boxes_augmentation/#yolo","title":"yolo","text":"<p>In <code>yolo</code>, a bounding box is represented by four values <code>[x_center, y_center, width, height]</code>. <code>x_center</code> and <code>y_center</code> are the normalized coordinates of the center of the bounding box. To make coordinates normalized, we take pixel values of x and y, which marks the center of the bounding box on the x- and y-axis. Then we divide the value of x by the width of the image and value of y by the height of the image. <code>width</code> and <code>height</code> represent the width and the height of the bounding box. They are normalized as well.</p> <p>Coordinates of the example bounding box in this format are <code>[((420 + 98) / 2) / 640, ((462 + 345) / 2) / 480, 322 / 640, 117 / 480]</code> which are <code>[0.4046875, 0.840625, 0.503125, 0.24375]</code>.</p> <p> How different formats represent coordinates of a bounding box</p>"},{"location":"getting_started/bounding_boxes_augmentation/#bounding-boxes-augmentation","title":"Bounding boxes augmentation","text":"<p>Just like with images and masks augmentation, the process of augmenting bounding boxes consists of 4 steps.</p> <ol> <li>You import the required libraries.</li> <li>You define an augmentation pipeline.</li> <li>You read images and bounding boxes from the disk.</li> <li>You pass an image and bounding boxes to the augmentation pipeline and receive augmented images and boxes.</li> </ol> <p>Note</p> <p>Some transforms in Albumentation don't support bounding boxes. If you try to use them you will get an exception. Please refer to this article to check whether a transform can augment bounding boxes.</p>"},{"location":"getting_started/bounding_boxes_augmentation/#step-1-import-the-required-libraries","title":"Step 1. Import the required libraries.","text":"Python<pre><code>import albumentations as A\nimport cv2\n</code></pre>"},{"location":"getting_started/bounding_boxes_augmentation/#step-2-define-an-augmentation-pipeline","title":"Step 2. Define an augmentation pipeline.","text":"<p>Here an example of a minimal declaration of an augmentation pipeline that works with bounding boxes.</p> Python<pre><code>transform = A.Compose([\n    A.RandomCrop(width=450, height=450),\n    A.HorizontalFlip(p=0.5),\n    A.RandomBrightnessContrast(p=0.2),\n], bbox_params=A.BboxParams(format='coco'))\n</code></pre> <p>Note that unlike image and masks augmentation, <code>Compose</code> now has an additional parameter <code>bbox_params</code>. You need to pass an instance of <code>A.BboxParams</code> to that argument. <code>A.BboxParams</code> specifies settings for working with bounding boxes. <code>format</code> sets the format for bounding boxes coordinates.</p> <p>It can either be <code>pascal_voc</code>, <code>albumentations</code>, <code>coco</code> or <code>yolo</code>. This value is required because Albumentation needs to know the coordinates' source format for bounding boxes to apply augmentations correctly.</p> <p>Besides <code>format</code>, <code>A.BboxParams</code> supports a few more settings.</p> <p>Here is an example of <code>Compose</code> that shows all available settings with <code>A.BboxParams</code>:</p> Python<pre><code>transform = A.Compose([\n    A.RandomCrop(width=450, height=450),\n    A.HorizontalFlip(p=0.5),\n    A.RandomBrightnessContrast(p=0.2),\n], bbox_params=A.BboxParams(format='coco', min_area=1024, min_visibility=0.1, label_fields=['class_labels']))\n</code></pre>"},{"location":"getting_started/bounding_boxes_augmentation/#min_area-and-min_visibility","title":"<code>min_area</code> and <code>min_visibility</code>","text":"<p><code>min_area</code> and <code>min_visibility</code> parameters control what Albumentations should do to the augmented bounding boxes if their size has changed after augmentation. The size of bounding boxes could change if you apply spatial augmentations, for example, when you crop a part of an image or when you resize an image.</p> <p><code>min_area</code> is a value in pixels. If the area of a bounding box after augmentation becomes smaller than <code>min_area</code>, Albumentations will drop that box. So the returned list of augmented bounding boxes won't contain that bounding box.</p> <p><code>min_visibility</code> is a value between 0 and 1. If the ratio of the bounding box area after augmentation to <code>the area of the bounding box before augmentation</code> becomes smaller than <code>min_visibility</code>, Albumentations will drop that box. So if the augmentation process cuts the most of the bounding box, that box won't be present in the returned list of the augmented bounding boxes.</p> <p>Here is an example image that contains two bounding boxes. Bounding boxes coordinates are declared using the <code>coco</code> format.</p> <p> An example image with two bounding boxes</p> <p>First, we apply the <code>CenterCrop</code> augmentation without declaring parameters <code>min_area</code> and <code>min_visibility</code>. The augmented image contains two bounding boxes.</p> <p> An example image with two bounding boxes after applying augmentation</p> <p>Next, we apply the same <code>CenterCrop</code> augmentation, but now we also use the <code>min_area</code> parameter. Now, the augmented image contains only one bounding box, because the other bounding box's area after augmentation became smaller than <code>min_area</code>, so Albumentations dropped that bounding box.</p> <p> An example image with one bounding box after applying augmentation with 'min_area'</p> <p>Finally, we apply the <code>CenterCrop</code> augmentation with the <code>min_visibility</code>. After that augmentation, the resulting image doesn't contain any bounding box, because visibility of all bounding boxes after augmentation are below threshold set by <code>min_visibility</code>.</p> <p> An example image with zero bounding boxes after applying augmentation with 'min_visibility'</p>"},{"location":"getting_started/bounding_boxes_augmentation/#class-labels-for-bounding-boxes","title":"Class labels for bounding boxes","text":"<p>Besides coordinates, each bounding box should have an associated class label that tells which object lies inside the bounding box. There are two ways to pass a label for a bounding box.</p> <p>Let's say you have an example image with three objects: <code>dog</code>, <code>cat</code>, and <code>sports ball</code>. Bounding boxes coordinates in the <code>coco</code> format for those objects are <code>[23, 74, 295, 388]</code>, <code>[377, 294, 252, 161]</code>, and <code>[333, 421, 49, 49]</code>.</p> <p> An example image with 3 bounding boxes from the COCO dataset</p>"},{"location":"getting_started/bounding_boxes_augmentation/#1-you-can-pass-labels-along-with-bounding-boxes-coordinates-by-adding-them-as-additional-values-to-the-list-of-coordinates","title":"1. You can pass labels along with bounding boxes coordinates by adding them as additional values to the list of coordinates.","text":"<p>For the image above, bounding boxes with class labels will become <code>[23, 74, 295, 388, 'dog']</code>, <code>[377, 294, 252, 161, 'cat']</code>, and <code>[333, 421, 49, 49, 'sports ball']</code>.</p> <p>Class labels could be of any type: integer, string, or any other Python data type. For example, integer values as class labels will look the following: <code>[23, 74, 295, 388, 18]</code>, <code>[377, 294, 252, 161, 17]</code>, and <code>[333, 421, 49, 49, 37].</code></p> <p>Also, you can use multiple class values for each bounding box, for example <code>[23, 74, 295, 388, 'dog', 'animal']</code>, <code>[377, 294, 252, 161, 'cat', 'animal']</code>, and <code>[333, 421, 49, 49, 'sports ball', 'item']</code>.</p>"},{"location":"getting_started/bounding_boxes_augmentation/#2you-can-pass-labels-for-bounding-boxes-as-a-separate-list-the-preferred-way","title":"2.You can pass labels for bounding boxes as a separate list (the preferred way).","text":"<p>For example, if you have three bounding boxes like <code>[23, 74, 295, 388]</code>, <code>[377, 294, 252, 161]</code>, and <code>[333, 421, 49, 49]</code> you can create a separate list with values like <code>['cat', 'dog', 'sports ball']</code>, or <code>[18, 17, 37]</code> that contains class labels for those bounding boxes. Next, you pass that list with class labels as a separate argument to the <code>transform</code> function. Albumentations needs to know the names of all those lists with class labels to join them with augmented bounding boxes correctly. Then, if a bounding box is dropped after augmentation because it is no longer visible, Albumentations will drop the class label for that box as well. Use <code>label_fields</code> parameter to set names for all arguments in <code>transform</code> that will contain label descriptions for bounding boxes (more on that in Step 4).</p>"},{"location":"getting_started/bounding_boxes_augmentation/#step-3-read-images-and-bounding-boxes-from-the-disk","title":"Step 3. Read images and bounding boxes from the disk.","text":"<p>Read an image from the disk.</p> Python<pre><code>image = cv2.imread(\"/path/to/image.jpg\")\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n</code></pre> <p>Bounding boxes can be stored on the disk in different serialization formats: JSON, XML, YAML, CSV, etc. So the code to read bounding boxes depends on the actual format of data on the disk.</p> <p>After you read the data from the disk, you need to prepare bounding boxes for Albumentations.</p> <p>Albumentations expects that bounding boxes will be represented as a list of lists. Each list contains information about a single bounding box. A bounding box definition should have at list four elements that represent the coordinates of that bounding box. The actual meaning of those four values depends on the format of bounding boxes (either <code>pascal_voc</code>, <code>albumentations</code>, <code>coco</code>, or <code>yolo</code>). Besides four coordinates, each definition of a bounding box may contain one or more extra values. You can use those extra values to store additional information about the bounding box, such as a class label of the object inside the box. During augmentation, Albumentations will not process those extra values. The library will return them as is along with the updated coordinates of the augmented bounding box.</p>"},{"location":"getting_started/bounding_boxes_augmentation/#step-4-pass-an-image-and-bounding-boxes-to-the-augmentation-pipeline-and-receive-augmented-images-and-boxes","title":"Step 4. Pass an image and bounding boxes to the augmentation pipeline and receive augmented images and boxes.","text":"<p>As discussed in Step 2, there are two ways of passing class labels along with bounding boxes coordinates:</p>"},{"location":"getting_started/bounding_boxes_augmentation/#1-pass-class-labels-along-with-coordinates","title":"1. Pass class labels along with coordinates","text":"<p>So, if you have coordinates of three bounding boxes that look like this:</p> Python<pre><code>bboxes = [\n    [23, 74, 295, 388],\n    [377, 294, 252, 161],\n    [333, 421, 49, 49],\n]\n</code></pre> <p>you can add a class label for each bounding box as an additional element of the list along with four coordinates. So now a list with bounding boxes and their coordinates will look the following:</p> Python<pre><code>bboxes = [\n    [23, 74, 295, 388, 'dog'],\n    [377, 294, 252, 161, 'cat'],\n    [333, 421, 49, 49, 'sports ball'],\n]\n</code></pre> <p>or with multiple labels per each bounding box: Python<pre><code>bboxes = [\n    [23, 74, 295, 388, 'dog', 'animal'],\n    [377, 294, 252, 161, 'cat', 'animal'],\n    [333, 421, 49, 49, 'sports ball', 'item'],\n]\n</code></pre></p> <p>You can use any data type for declaring class labels. It can be string, integer, or any other Python data type.</p> <p>Next, you pass an image and bounding boxes for it to the <code>transform</code> function and receive the augmented image and bounding boxes.</p> Python<pre><code>transformed = transform(image=image, bboxes=bboxes)\ntransformed_image = transformed['image']\ntransformed_bboxes = transformed['bboxes']\n</code></pre> <p> Example input and output data for bounding boxes augmentation</p>"},{"location":"getting_started/bounding_boxes_augmentation/#2-pass-class-labels-in-a-separate-argument-to-transform-the-preferred-way","title":"2. Pass class labels in a separate argument to <code>transform</code> (the preferred way).","text":"<p>Let's say you have coordinates of three bounding boxes Python<pre><code>bboxes = [\n    [23, 74, 295, 388],\n    [377, 294, 252, 161],\n    [333, 421, 49, 49],\n]\n</code></pre></p> <p>You can create a separate list that contains class labels for those bounding boxes:</p> Python<pre><code>class_labels = ['cat', 'dog', 'parrot']\n</code></pre> <p>Then you pass both bounding boxes and class labels to <code>transform</code>. Note that to pass class labels, you need to use the name of the argument that you declared in <code>label_fields</code> when creating an instance of Compose in step 2. In our case, we set the name of the argument to <code>class_labels</code>.</p> Python<pre><code>transformed = transform(image=image, bboxes=bboxes, class_labels=class_labels)\ntransformed_image = transformed['image']\ntransformed_bboxes = transformed['bboxes']\ntransformed_class_labels = transformed['class_labels']\n</code></pre> <p> Example input and output data for bounding boxes augmentation with a separate argument for class labels</p> <p>Note that <code>label_fields</code> expects a list, so you can set multiple fields that contain labels for your bounding boxes. So if you declare Compose like</p> Python<pre><code>transform = A.Compose([\n    A.RandomCrop(width=450, height=450),\n    A.HorizontalFlip(p=0.5),\n    A.RandomBrightnessContrast(p=0.2),\n], bbox_params=A.BboxParams(format='coco', label_fields=['class_labels', 'class_categories'])))\n</code></pre> <p>you can use those multiple arguments to pass info about class labels, like</p> Python<pre><code>class_labels = ['cat', 'dog', 'parrot']\nclass_categories = ['animal', 'animal', 'item']\n\ntransformed = transform(image=image, bboxes=bboxes, class_labels=class_labels, class_categories=class_categories)\ntransformed_image = transformed['image']\ntransformed_bboxes = transformed['bboxes']\ntransformed_class_labels = transformed['class_labels']\ntransformed_class_categories = transformed['class_categories']\n</code></pre>"},{"location":"getting_started/bounding_boxes_augmentation/#examples","title":"Examples","text":"<ul> <li>Using Albumentations to augment bounding boxes for object detection tasks</li> <li>How to use Albumentations for detection tasks if you need to keep all bounding boxes</li> <li>Showcase. Cool augmentation examples on diverse set of images from various real-world tasks.</li> </ul>"},{"location":"getting_started/image_augmentation/","title":"Image augmentation for classification","text":"<p>We can divide the process of image augmentation into four steps:</p> <ol> <li>Import albumentations and a library to read images from the disk (e.g., OpenCV).</li> <li>Define an augmentation pipeline.</li> <li>Read images from the disk.</li> <li>Pass images to the augmentation pipeline and receive augmented images.</li> </ol>"},{"location":"getting_started/image_augmentation/#step-1-import-the-required-libraries","title":"Step 1. Import the required libraries.","text":"<ul> <li>Import Albumentations</li> </ul> Python<pre><code>import albumentations as A\n</code></pre> <ul> <li>Import a library to read images from the disk. In this example, we will use OpenCV. It is an open-source computer vision library that supports many image formats. Albumentations has OpenCV as a dependency, so you already have OpenCV installed.</li> </ul> Python<pre><code>import cv2\n</code></pre>"},{"location":"getting_started/image_augmentation/#step-2-define-an-augmentation-pipeline","title":"Step 2. Define an augmentation pipeline.","text":"<p>To define an augmentation pipeline, you need to create an instance of the <code>Compose</code> class. As an argument to the <code>Compose</code> class, you need to pass a list of augmentations you want to apply. A call to <code>Compose</code> will return a transform function that will perform image augmentation.</p> <p>Let's look at an example:</p> Python<pre><code>transform = A.Compose([\n    A.RandomCrop(width=256, height=256),\n    A.HorizontalFlip(p=0.5),\n    A.RandomBrightnessContrast(p=0.2),\n])\n</code></pre> <p>In the example, <code>Compose</code> receives a list with three augmentations: <code>A.RandomCrop</code>, <code>A.HorizontalFlip</code>, and <code>A.RandomBrighntessContrast</code>. You can find the full list of all available augmentations in the GitHub repository and in the API Docs. A demo playground that demonstrates how augmentations will transform the input image is available at https://demo.albumentations.ai.</p> <p>To create an augmentation, you create an instance of the required augmentation class and pass augmentation parameters to it. <code>A.RandomCrop</code> receives two parameters, <code>height</code> and <code>width</code>. <code>A.RandomCrop(width=256, height=256)</code> means that <code>A.RandomCrop</code> will take an input image, extract a random patch with size 256 by 256 pixels from it and then pass the result to the next augmentation in the pipeline (in this case to <code>A.HorizontalFlip</code>).</p> <p><code>A.HorizontalFlip</code> in this example has one parameter named <code>p</code>. <code>p</code> is a special parameter that is supported by almost all augmentations. It controls the probability of applying the augmentation. <code>p=0.5</code> means that with a probability of 50%, the transform will flip the image horizontally, and with a probability of 50%, the transform won't modify the input image.</p> <p><code>A.RandomBrighntessContrast</code> in the example also has one parameter, <code>p</code>. With a probability of 20%, this augmentation will change the brightness and contrast of the image received from <code>A.HorizontalFlip</code>. And with a probability of 80%, it will keep the received image unchanged.</p> <p> A visualized version of the augmentation pipeline. You pass an image to it, the image goes through all transformations, and then you receive an augmented image from the pipeline.</p>"},{"location":"getting_started/image_augmentation/#step-3-read-images-from-the-disk","title":"Step 3. Read images from the disk.","text":"<p>To pass an image to the augmentation pipeline, you need to read it from the disk. The pipeline expects to receive an image in the form of a NumPy array. If it is a color image, it should have three channels in the following order: Red, Green, Blue (so a regular RGB image).</p> <p>To read images from the disk, you can use OpenCV - a popular library for image processing. It supports a lot of input formats and is installed along with Albumentations since Albumentations utilizes that library under the hood for a lot of augmentations.</p> <p>To import OpenCV</p> Python<pre><code>import cv2\n</code></pre> <p>To read an image with OpenCV</p> <p>Python<pre><code>image = cv2.imread(\"/path/to/image.jpg\")\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n</code></pre> Note the usage of <code>cv2.cvtColor</code>. For historical reasons, OpenCV reads an image in BGR format (so color channels of the image have the following order: Blue, Green, Red). Albumentations uses the most common and popular RGB image format. So when using OpenCV, we need to convert the image format to RGB explicitly.</p> <p>Besides OpenCV, you can use other image processing libraries.</p>"},{"location":"getting_started/image_augmentation/#pillow","title":"Pillow","text":"<p>Pillow is a popular Python image processing library.</p> <ul> <li>Install Pillow</li> </ul> Bash<pre><code>    pip install pillow\n</code></pre> <ul> <li>Import Pillow and NumPy (we need NumPy to convert a Pillow image to a NumPy array. NumPy is already installed along with Albumentations).</li> </ul> Python<pre><code>from PIL import Image\nimport numpy as np\n</code></pre> <ul> <li>Read an image with Pillow and convert it to a NumPy array. Python<pre><code>pillow_image = Image.open(\"image.jpg\")\nimage = np.array(pillow_image)\n</code></pre></li> </ul>"},{"location":"getting_started/image_augmentation/#step-4-pass-images-to-the-augmentation-pipeline-and-receive-augmented-images","title":"Step 4. Pass images to the augmentation pipeline and receive augmented images.","text":"<p>To pass an image to the augmentation pipeline you need to call the <code>transform</code> function created by a call to <code>A.Compose</code> at Step 2. In the <code>image</code> argument to that function, you need to pass an image that you want to augment.</p> Python<pre><code>transformed = transform(image=image)\n</code></pre> <p><code>transform</code> will return a dictionary with a single key <code>image</code>. Value at that key will contain an augmented image.</p> Python<pre><code>transformed_image = transformed[\"image\"]\n</code></pre> <p>To augment the next image, you need to call <code>transform</code> again and pass a new image as the <code>image</code> argument:</p> Python<pre><code>another_transformed_image = transform(image=another_image)[\"image\"]\n</code></pre> <p>Each augmentation will change the input image with the probability set by the parameter <code>p</code>. Also, many augmentations have parameters that control the magnitude of changes that will be applied to an image. For example, <code>A.RandomBrightnessContrast</code> has two parameters: <code>brightness_limit</code> that controls the magnitude of adjusting brightness and <code>contrast_limit</code> that controls the magnitude of adjusting contrast. The bigger the value, the more the augmentation will change an image. During augmentation, a magnitude of the transformation is sampled from a uniform distribution limited by <code>brightness_limit</code> and <code>contrast_limit</code>. That means that if you make multiple calls to <code>transform</code> with the same input image, you will get a different output image each time.</p> Python<pre><code>transform = A.Compose([\n    A.RandomBrightnessContrast(brightness_limit=1, contrast_limit=1, p=1.0),\n])\ntransformed_image_1 = transform(image=image)['image']\ntransformed_image_2 = transform(image=image)['image']\ntransformed_image_3 = transform(image=image)['image']\n</code></pre> <p></p>"},{"location":"getting_started/image_augmentation/#examples","title":"Examples","text":"<ul> <li>Defining a simple augmentation pipeline for image augmentation</li> <li>Working with non-8-bit images</li> <li>Weather augmentations in Albumentations</li> <li>Showcase. Cool augmentation examples on diverse set of images from various real-world tasks.</li> </ul>"},{"location":"getting_started/installation/","title":"Installation","text":"<p>Albumentations requires Python 3.8 or higher.</p>"},{"location":"getting_started/installation/#install-the-latest-stable-version-from-pypi","title":"Install the latest stable version from PyPI","text":"Bash<pre><code>pip install -U albumentations\n</code></pre>"},{"location":"getting_started/installation/#install-the-latest-version-from-the-master-branch-on-github","title":"Install the latest version from the master branch on GitHub","text":"Bash<pre><code>pip install -U git+https://github.com/albumentations-team/albumentations\n</code></pre>"},{"location":"getting_started/installation/#note-on-opencv-dependencies","title":"Note on OpenCV dependencies","text":"<p>By default, pip downloads a wheel distribution of Albumentations. This distribution has <code>opencv-python-headless</code> as its dependency.</p> <p>If you already have some OpenCV distribution (such as <code>opencv-python-headless</code>, <code>opencv-python</code>, <code>opencv-contrib-python</code> or <code>opencv-contrib-python-headless</code>) installed in your Python environment, you can force Albumentations to use it by providing the <code>--no-binary qudida,albumentations</code> argument to pip, e.g.</p> Bash<pre><code>pip install -U albumentations\n</code></pre> <p>pip will use the following logic to determine the required OpenCV distribution:</p> <ol> <li>If your Python environment already contains <code>opencv-python</code>, <code>opencv-contrib-python</code>, <code>opencv-contrib-python-headless</code> or <code>opencv-python-headless</code> pip will use it.</li> <li>If your Python environment doesn't contain any OpenCV distribution from step 1, pip will download <code>opencv-python-headless</code>.</li> </ol>"},{"location":"getting_started/installation/#install-the-latest-stable-version-from-conda-forge","title":"Install the latest stable version from conda-forge","text":"<p>If you are using Anaconda or Miniconda you can install Albumentations from conda-forge:</p> Bash<pre><code>conda install -c conda-forge albumentations\n</code></pre>"},{"location":"getting_started/keypoints_augmentation/","title":"Keypoints augmentation","text":"<p>Computer vision tasks such as human pose estimation, face detection, and emotion recognition usually work with keypoints on the image.</p> <p>In the case of pose estimation, keypoints mark human joints such as shoulder, elbow, wrist, knee, etc.</p> <p> Keypoints annotations along with visualized edges between keypoints. Images are from the COCO dataset.</p> <p>In the case of face detection, keypoints mark important areas of the face such as eyes, nose, corners of the mouth, etc.</p> <p> Facial keypoints. Source: the \"Facial Keypoints Detection\" competition on Kaggle.</p> <p>To define a keypoint, you usually need two values, x and y coordinates of the keypoint. Coordinates of the keypoint are calculated with respect to the top-left corner of the image which has <code>(x, y)</code> coordinates <code>(0, 0)</code>. Often keypoints have associated labels such as <code>right_elbow</code>, <code>left_wrist</code>, etc.</p> <p> An example image with five keypoints from the COCO dataset</p> <p>Some classical computer vision algorithms, such as SIFT, may use four values to describe a keypoint. In addition to the x and y coordinates, there are keypoint scale and keypoint angle. Albumentations support those values as well.</p> <p> A keypoint may also has associated scale and angle values</p> <p>Keypoint angles are counter-clockwise. For example, in the following image, the angle value is 65\u00b0. You can read more about angle of rotation in the Wikipedia article. </p>"},{"location":"getting_started/keypoints_augmentation/#supported-formats-for-keypoints-coordinates","title":"Supported formats for keypoints' coordinates.","text":"<ul> <li> <p><code>xy</code>. A keypoint is defined by x and y coordinates in pixels.</p> </li> <li> <p><code>yx</code>. A keypoint is defined by y and x coordinates in pixels.</p> </li> <li> <p><code>xya</code>. A keypoint is defined by x and y coordinates in pixels and the angle.</p> </li> <li> <p><code>xys</code>. A keypoint is defined by x and y coordinates in pixels, and the scale.</p> </li> <li> <p><code>xyas</code>. A keypoint is defined by x and y coordinates in pixels, the angle, and the scale.</p> </li> <li> <p><code>xysa</code>. A keypoint is defined by x and y coordinates in pixels, the scale, and the angle.</p> </li> </ul>"},{"location":"getting_started/keypoints_augmentation/#augmenting-keypoints","title":"Augmenting keypoints","text":"<p>The process of augmenting keypoints looks very similar to the bounding boxes augmentation. It consists of 4 steps.</p> <ol> <li>You import the required libraries.</li> <li>You define an augmentation pipeline.</li> <li>You read images and keypoints from the disk.</li> <li>You pass an image and keypoints to the augmentation pipeline and receive augmented images and keypoints.</li> </ol> <p>Note</p> <p>Some transforms in Albumentation don't support keypoints. If you try to use them you will get an exception. Please refer to this article to check whether a transform can augment keypoints.</p>"},{"location":"getting_started/keypoints_augmentation/#step-1-import-the-required-libraries","title":"Step 1. Import the required libraries.","text":"Python<pre><code>import albumentations as A\nimport cv2\n</code></pre>"},{"location":"getting_started/keypoints_augmentation/#step-2-define-an-augmentation-pipeline","title":"Step 2. Define an augmentation pipeline.","text":"<p>Here an example of a minimal declaration of an augmentation pipeline that works with keypoints.</p> Python<pre><code>transform = A.Compose([\n    A.RandomCrop(width=330, height=330),\n    A.RandomBrightnessContrast(p=0.2),\n], keypoint_params=A.KeypointParams(format='xy'))\n</code></pre> <p>Note that just like with bounding boxes, <code>Compose</code> has an additional parameter that defines the format for keypoints' coordinates. In the case of keypoints, it is called <code>keypoint_params</code>. Here we pass an instance of <code>A.KeypointParams</code> that says that <code>xy</code> coordinates format should be used.</p> <p>Besides <code>format</code>, <code>A.KeypointParams</code> supports a few more settings.</p> <p>Here is an example of <code>Compose</code> that shows all available settings with <code>A.KeypointParams</code></p> Python<pre><code>transform = A.Compose([\n    A.RandomCrop(width=330, height=330),\n    A.RandomBrightnessContrast(p=0.2),\n], keypoint_params=A.KeypointParams(format='xy', label_fields=['class_labels'], remove_invisible=True, angle_in_degrees=True))\n</code></pre>"},{"location":"getting_started/keypoints_augmentation/#label_fields","title":"<code>label_fields</code>","text":"<p>In some computer vision tasks, keypoints have not only coordinates but associated labels as well. For example, in pose estimation, each keypoint has a label such as <code>elbow</code>, <code>knee</code> or <code>wrist</code>. You need to pass those labels in a separate argument (or arguments, because you can use multiple fields) to the <code>transform</code> function that will augment keypoints. <code>label_fields</code> defines names of those fields. Step 4 describes how you need to use the <code>transform</code> function.</p>"},{"location":"getting_started/keypoints_augmentation/#remove_invisible","title":"<code>remove_invisible</code>","text":"<p>After the augmentation, some keypoints may become invisible because they will be located outside of the augmented image's visible area. For example, if you crop a part of the image, all the keypoints outside of the cropped area will become invisible. If <code>remove_invisible</code> is set to <code>True</code>, Albumentations won't return invisible keypoints. <code>remove_invisible</code> is set to <code>True</code> by default, so if you don't pass that argument, Albumentations won't return invisible keypoints.</p>"},{"location":"getting_started/keypoints_augmentation/#angle_in_degrees","title":"<code>angle_in_degrees</code>","text":"<p>If <code>angle_in_degrees</code> is set to <code>True</code> (this is the default value), then Albumentations expects that the angle value in formats <code>xya</code>, <code>xyas</code>, and <code>xysa</code> is defined in angles. If <code>angle_in_degrees</code> is set to <code>False</code>, Albumentations expects that the angle value is specified in radians.</p> <p>This setting doesn't affect <code>xy</code> and <code>yx</code> formats, because those formats don't use angles.</p>"},{"location":"getting_started/keypoints_augmentation/#3-read-images-and-keypoints-from-the-disk","title":"3. Read images and keypoints from the disk.","text":"<p>Read an image from the disk.</p> <p>Python<pre><code>image = cv2.imread(\"/path/to/image.jpg\")\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n</code></pre> Keypoints can be stored on the disk in different serialization formats: JSON, XML, YAML, CSV, etc. So the code to read keypoints depends on the actual format of data on the disk.</p> <p>After you read the data from the disk, you need to prepare keypoints for Albumentations.</p> <p>Albumentations expects that keypoint will be represented as a list of lists. Each list contains information about a single keypoint. A definition of keypoint should have two to four elements depending on the selected format of keypoints. The first two elements are x and y coordinates of a keypoint in pixels (or y and x coordinates in the <code>yx</code> format). The third and fourth elements may be the angle and the scale of keypoint if you select a format that uses those values.</p>"},{"location":"getting_started/keypoints_augmentation/#step-4-pass-an-image-and-keypoints-to-the-augmentation-pipeline-and-receive-augmented-images-and-boxes","title":"Step 4. Pass an image and keypoints to the augmentation pipeline and receive augmented images and boxes.","text":"<p>Let's say you have an example image with five keypoints.</p> <p>A list with those five keypoints' coordinates in the <code>xy</code> format will look the following:</p> Python<pre><code>keypoints = [\n    (264, 203),\n    (86, 88),\n    (254, 160),\n    (193, 103),\n    (65, 341),\n]\n</code></pre> <p>Then you pass those keypoints to the <code>transform</code> function along with the image and receive the augmented versions of image and keypoints.</p> Python<pre><code>transformed = transform(image=image, keypoints=keypoints)\ntransformed_image = transformed['image']\ntransformed_keypoints = transformed['keypoints']\n</code></pre> <p> The augmented image with augmented keypoints</p> <p>If you set <code>remove_invisible</code> to <code>False</code> in <code>keypoint_params</code>, then Albumentations will return all keypoints, even if they lie outside the visible area. In the example image below, you can see that the keypoint for the right hip is located outside the image, but Albumentations still returned it. The area outside the image is highlighted in yellow.</p> <p> When <code>remove_invisible</code> is set to <code>False</code> Albumentations will return all keypoints, even those located outside the image</p> <p>If keypoints have associated class labels, you need to create a list that contains those labels:</p> Python<pre><code>class_labels = [\n    'left_elbow',\n    'right_elbow',\n    'left_wrist',\n    'right_wrist',\n    'right_hip',\n]\n</code></pre> <p>Also, you need to declare the name of the argument to <code>transform</code> that will contain those labels. For declaration, you need to use the <code>label_fields</code> parameters of <code>A.KeypointParams</code>.</p> <p>For example, we could use the <code>class_labels</code> name for the argument with labels.</p> Python<pre><code>transform = A.Compose([\n    A.RandomCrop(width=330, height=330),\n    A.RandomBrightnessContrast(p=0.2),\n], keypoint_params=A.KeypointParams(format='xy', label_fields=['class_labels']))\n</code></pre> <p>Next, you pass both keypoints' coordinates and class labels to <code>transform</code>.</p> Python<pre><code>transformed = transform(image=image, keypoints=keypoints, class_labels=class_labels)\ntransformed_image = transformed['image']\ntransformed_keypoints = transformed['keypoints']\ntransformed_class_labels = transformed['class_labels']\n</code></pre> <p>Note that <code>label_fields</code> expects a list, so you can set multiple fields that contain labels for your keypoints. So if you declare Compose like</p> Python<pre><code>transform = A.Compose([\n    A.RandomCrop(width=330, height=330),\n    A.RandomBrightnessContrast(p=0.2),\n], keypoint_params=A.KeypointParams(format='xy', label_fields=['class_labels', 'class_sides']))\n</code></pre> <p>you can use those multiple arguments to pass info about class labels, like</p> Python<pre><code>class_labels = [\n    'left_elbow',\n    'right_elbow',\n    'left_wrist',\n    'right_wrist',\n    'right_hip',\n]\n\nclass_sides = ['left', 'right', 'left', 'right', 'right']\n\ntransformed = transform(image=image, keypoints=keypoints, class_labels=class_labels, class_sides=class_sides)\ntransformed_class_sides = transformed['class_sides']\ntransformed_class_labels = transformed['class_labels']\ntransformed_keypoints = transformed['keypoints']\ntransformed_image = transformed['image']\n</code></pre> <p> Example input and output data for keypoints augmentation with two separate arguments for class labels</p> <p>Note</p> <p>Some augmentations may affect class labels and make them incorrect. For example, the <code>HorizontalFlip</code> augmentation mirrors the input image. When you apply that augmentation to keypoints that mark the side of body parts (left or right), those keypoints will point to the wrong side (since <code>left</code> on the mirrored image becomes <code>right</code>). So when you are creating an augmentation pipeline look carefully which augmentations could be applied to the input data.</p> <p> <code>HorizontalFlip</code> may make keypoints' labels incorrect</p>"},{"location":"getting_started/keypoints_augmentation/#examples","title":"Examples","text":"<ul> <li>Using Albumentations to augment keypoints</li> </ul>"},{"location":"getting_started/mask_augmentation/","title":"Mask augmentation for segmentation","text":"<p>For instance and semantic segmentation tasks, you need to augment both the input image and one or more output masks.</p> <p>Albumentations ensures that the input image and the output mask will receive the same set of augmentations with the same parameters.</p> <p>The process of augmenting images and masks looks very similar to the regular image-only augmentation.</p> <ol> <li>You import the required libraries.</li> <li>You define an augmentation pipeline.</li> <li>You read images and masks from the disk.</li> <li>You pass an image and one or more masks to the augmentation pipeline and receive augmented images and masks.</li> </ol>"},{"location":"getting_started/mask_augmentation/#steps-1-and-2-import-the-required-libraries-and-define-an-augmentation-pipeline","title":"Steps 1 and 2. Import the required libraries and define an augmentation pipeline.","text":"<p>Image augmentation for classification described Steps 1 and 2 in great detail. These are the same steps for the simultaneous augmentation of images and masks.</p> Python<pre><code>import albumentations as A\nimport cv2\n\ntransform = A.Compose([\n    A.RandomCrop(width=256, height=256),\n    A.HorizontalFlip(p=0.5),\n    A.RandomBrightnessContrast(p=0.2),\n])\n</code></pre>"},{"location":"getting_started/mask_augmentation/#step-3-read-images-and-masks-from-the-disk","title":"Step 3. Read images and masks from the disk.","text":"<ul> <li>Reading an image</li> </ul> Python<pre><code>image = cv2.imread(\"/path/to/image.jpg\")\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n</code></pre> <ul> <li>For semantic segmentation, you usually read one mask per image. Albumentations expects the mask to be a NumPy array. The height and width of the mask should have the same values as the height and width of the image.</li> </ul> Python<pre><code>mask = cv2.imread(\"/path/to/mask.png\")\n</code></pre> <ul> <li>For instance segmentation, you sometimes need to read multiple masks per image. Then you create a list that contains all the masks.</li> </ul> Python<pre><code>mask_1 = cv2.imread(\"/path/to/mask_1.png\")\nmask_2 = cv2.imread(\"/path/to/mask_2.png\")\nmask_3 = cv2.imread(\"/path/to/mask_3.png\")\nmasks = [mask_1, mask_2, mask_3]\n</code></pre> <p>Some datasets use other formats to store masks. For example, they can use Run-Length Encoding or Polygon coordinates. In that case, you need to convert a mask to a NumPy before augmenting it with Albumentations. Often dataset authors provide special libraries and tools to simplify the conversion.</p>"},{"location":"getting_started/mask_augmentation/#step-4-pass-image-and-masks-to-the-augmentation-pipeline-and-receive-augmented-images-and-masks","title":"Step 4. Pass image and masks to the augmentation pipeline and receive augmented images and masks.","text":"<p>If the image has one associated mask, you need to call <code>transform</code> with two arguments: <code>image</code> and <code>mask</code>. In <code>image</code> you should pass the input image, in <code>mask</code> you should pass the output mask. <code>transform</code> will return a dictionary with two keys: <code>image</code> will contain the augmented image, and <code>mask</code> will contain the augmented mask.</p> Python<pre><code>transformed = transform(image=image, mask=mask)\ntransformed_image = transformed['image']\ntransformed_mask = transformed['mask']\n</code></pre> <p></p> <p>An image and a mask before and after augmentation. Inria Aerial Image Labeling dataset contains aerial photos as well as their segmentation masks. Each pixel of the mask is marked as 1 if the pixel belongs to the class <code>building</code> and 0 otherwise.</p> <p>If the image has multiple associated masks, you should use the <code>masks</code> argument instead of <code>mask</code>. In <code>masks</code> you should pass a list of masks.</p> Python<pre><code>transformed = transform(image=image, masks=masks)\ntransformed_image = transformed['image']\ntransformed_masks = transformed['masks']\n</code></pre>"},{"location":"getting_started/mask_augmentation/#examples","title":"Examples","text":"<ul> <li>Using Albumentations for a semantic segmentation task</li> <li>Showcase. Cool augmentation examples on diverse set of images from various real-world tasks.</li> </ul>"},{"location":"getting_started/setting_probabilities/","title":"Setting probabilities for transforms in an augmentation pipeline","text":"<p>Each augmentation in Albumentations has a parameter named <code>p</code> that sets the probability of applying that augmentation to input data.</p> <p>The following augmentations have the default value of <code>p</code> set 1 (which means that by default they will be applied to each instance of input data): <code>Compose</code>, <code>ReplayCompose</code>, <code>CenterCrop</code>, <code>Crop</code>, <code>CropNonEmptyMaskIfExists</code>, <code>FromFloat</code>, <code>CenterCrop</code>, <code>Crop</code>, <code>CropNonEmptyMaskIfExists</code>, <code>FromFloat</code>, <code>IAACropAndPad</code>, <code>Lambda</code>, <code>LongestMaxSize</code>, <code>Normalize</code>, <code>PadIfNeeded</code>, <code>RandomCrop</code>, <code>RandomCropNearBBox</code>, <code>RandomResizedCrop</code>, <code>RandomSizedBBoxSafeCrop</code>, <code>RandomSizedCrop</code>, <code>Resize</code>, <code>SmallestMaxSize</code>, <code>ToFloat</code>.</p> <p>All other augmentations have the default value of <code>p</code> set 0.5, which means that by default, they will be applied to 50% of instances of input data.</p> <p>Let's take a look at the example:</p> Python<pre><code>import albumentations as A\nimport cv2\n\np1 = 0.95\np2 = 0.85\np3 = 0.75\n\n\ntransform = A.Compose([\n    A.RandomRotate90(p=p2),\n    A.OneOf([\n        A.IAAAdditiveGaussianNoise(p=0.9),\n        A.GaussNoise(p=0.6),\n    ], p=p3)\n], p=p1)\n\nimage = cv2.imread('some/image.jpg')\nimage = cv2.cvtColor(cv2.COLOR_BGR2RGB)\n\ntransformed = transform(image=image)\ntransformed_image = transformed['image']\n</code></pre> <p>We declare an augmentation pipeline. In this pipeline, we use three placeholder values to set probabilities: <code>p1</code>, <code>p2</code>, and <code>p3</code>. Let's take a closer look at them.</p>"},{"location":"getting_started/setting_probabilities/#p1","title":"<code>p1</code>","text":"<p><code>p1</code> sets the probability that the augmentation pipeline will apply augmentations at all.</p> <p>If <code>p1</code> is set to 0, then augmentations inside <code>Compose</code> will never be applied to the input image, so the augmentation pipeline will always return the input image unchanged.</p> <p>If <code>p1</code> is set to 1, then all augmentations inside <code>Compose</code> will have a chance to be applied. The example above contains two augmentations inside <code>Compose</code>: <code>RandomRotate90</code> and the <code>OneOf</code> block with two child augmentations (more on their probabilities later). Any value of <code>p1</code> between 0 and 1 means that augmentations inside <code>Compose</code> could be applied with the probability between 0 and 100%.</p> <p>If <code>p1</code> equals to 1 or <code>p1</code> is less than 1, but the random generator decides to apply augmentations inside Compose probabilities <code>p2</code> and <code>p3</code> come into play.</p>"},{"location":"getting_started/setting_probabilities/#p2","title":"<code>p2</code>","text":"<p>Each augmentation inside <code>Compose</code> has a probability of being applied. <code>p2</code> sets the probability of applying <code>RandomRotate90</code>. In the example above, <code>p2</code> equals 0.85, so <code>RandomRotate90</code> has an 85% chance to be applied to the input image.</p>"},{"location":"getting_started/setting_probabilities/#p3","title":"<code>p3</code>","text":"<p><code>p3</code> sets the probability of applying the <code>OneOf</code> block. If the random generator decided to apply <code>RandomRotate90</code> at the previous step, then <code>OneOf</code> will receive data augmented by it. If the random generator decided not to apply <code>RandomRotate90</code> then <code>OneOf</code> will receive the input data (that was passed to <code>Compose</code>) since <code>RandomRotate90</code> is skipped.</p> <p>The <code>OneOf</code>block applies one of the augmentations inside it. That means that if the random generator chooses to apply <code>OneOf</code> then one child augmentation from it will be applied to the input data.</p> <p>To decide which augmentation within the <code>OneOf</code> block is used, Albumentations uses the following rule:</p> <p>The <code>OneOf</code> block normalizes the probabilities of all augmentations inside it, so their probabilities sum up to 1. Next, <code>OneOf</code> chooses one of the augmentations inside it with a chance defined by its normalized probability and applies it to the input data. In the example above <code>IAAAdditiveGaussianNoise</code> has probability 0.9 and <code>GaussNoise</code> probability 0.6. After normalization, they become 0.6 and 0.4. Which means that <code>OneOf</code> will decide that it should use <code>IAAAdditiveGaussianNoise</code> with probability 0.6 and <code>GaussNoise</code> otherwise.</p>"},{"location":"getting_started/setting_probabilities/#example-calculations","title":"Example calculations","text":"<p>Thus, each augmentation in the example above will be applied with the probability:</p> <ul> <li><code>RandomRotate90</code>: <code>p1</code> * <code>p2</code></li> <li><code>IAAAdditiveGaussianNoise</code>: <code>p1</code> * <code>p3</code> * (0.9 / (0.9 + 0.6))</li> <li><code>GaussianNoise</code>: <code>p1</code> * <code>p3</code> * (0.6 / (0.9 + 0.6))</li> </ul>"},{"location":"getting_started/simultaneous_augmentation/","title":"Simultaneous augmentation of multiple targets: masks, bounding boxes, keypoints","text":"<p>Albumentations can apply the same set of transformations to the input images and all the targets that are passed to <code>transform</code>: masks, bounding boxes, and keypoints.</p> <p>Please refer to articles Image augmentation for classification, Mask augmentation for segmentation, Bounding boxes augmentation for object detection, and Keypoints augmentation for the detailed description of each data type.</p> <p>Note</p> <p>Some transforms in Albumentation don't support bounding boxes or keypoints. If you try to use them you will get an exception. Please refer to this article to check whether a transform can augment bounding boxes and keypoints.</p> <p>Below is an example, how you can simultaneously augment the input image, mask, bounding boxes with their labels, and keypoints with their labels. Note that the only required argument to <code>transform</code> is <code>image</code>; all other arguments are optional, and you can combine them in any way.</p>"},{"location":"getting_started/simultaneous_augmentation/#step-1-define-compose-with-parameters-that-specify-formats-for-bounding-boxes-and-keypoints","title":"Step 1. Define <code>Compose</code> with parameters that specify formats for bounding boxes and keypoints.","text":"Python<pre><code>transform = A.Compose(\n  [A.RandomCrop(width=330, height=330), A.RandomBrightnessContrast(p=0.2)],\n  bbox_params=A.BboxParams(format=\"coco\", label_fields=[\"bbox_classes\"]),\n  keypoint_params=A.KeypointParams(format=\"xy\", label_fields=[\"keypoints_classes\"]),\n)\n</code></pre>"},{"location":"getting_started/simultaneous_augmentation/#step-2-load-all-required-data-from-the-disk","title":"Step 2. Load all required data from the disk","text":"<p>Please refer to articles Image augmentation for classification, Mask augmentation for segmentation, Bounding boxes augmentation for object detection, and Keypoints augmentation for more information about loading the input data.</p> <p>For example, here is an image from the COCO dataset. that has one associated mask, one bounding box with the class label <code>person</code>, and five keypoints that define body parts.</p> <p> An example image with mask, bounding boxes and keypoints</p>"},{"location":"getting_started/simultaneous_augmentation/#step-3-pass-all-targets-to-transform-and-receive-their-augmented-versions","title":"Step 3. Pass all targets to <code>transform</code> and receive their augmented versions","text":"Python<pre><code>transformed = transform(\n  image=img,\n  mask=mask,\n  bboxes=bboxes,\n  bbox_classes=bbox_classes,\n  keypoints=keypoints,\n  keypoints_classes=keypoints_classes,\n)\ntransformed_image = transformed[\"image\"]\ntransformed_mask = transformed[\"mask\"]\ntransformed_bboxes = transformed[\"bboxes\"]\ntransformed_bbox_classes = transformed[\"bbox_classes\"]\ntransformed_keypoints = transformed[\"keypoints\"]\ntransformed_keypoints_classes = transformed[\"keypoints_classes\"]\n</code></pre> <p> The augmented version of the image and its targets</p>"},{"location":"getting_started/simultaneous_augmentation/#examples","title":"Examples","text":"<ul> <li>Showcase. Cool augmentation examples on diverse set of images from various real-world tasks.</li> </ul>"},{"location":"getting_started/transforms_and_targets/","title":"A list of transforms and their supported targets","text":"<p>We can split all transforms into two groups: pixel-level transforms, and spatial-level transforms. Pixel-level transforms will change just an input image and will leave any additional targets such as masks, bounding boxes, and keypoints unchanged. Spatial-level transforms will simultaneously change both an input image as well as additional targets such as masks, bounding boxes, and keypoints. For the additional information, please refer to this section of \"Why you need a dedicated library for image augmentation\".</p>"},{"location":"getting_started/transforms_and_targets/#pixel-level-transforms","title":"Pixel-level transforms","text":"<p>Here is a list of all available pixel-level transforms. You can apply a pixel-level transform to any target, and under the hood, the transform will change only the input image and return any other input targets such as masks, bounding boxes, or keypoints unchanged.</p> <ul> <li>AdvancedBlur</li> <li>Blur</li> <li>CLAHE</li> <li>ChannelDropout</li> <li>ChannelShuffle</li> <li>ChromaticAberration</li> <li>ColorJitter</li> <li>Defocus</li> <li>Downscale</li> <li>Emboss</li> <li>Equalize</li> <li>FDA</li> <li>FancyPCA</li> <li>FromFloat</li> <li>GaussNoise</li> <li>GaussianBlur</li> <li>GlassBlur</li> <li>HistogramMatching</li> <li>HueSaturationValue</li> <li>ISONoise</li> <li>ImageCompression</li> <li>InvertImg</li> <li>MedianBlur</li> <li>MotionBlur</li> <li>MultiplicativeNoise</li> <li>Normalize</li> <li>PixelDistributionAdaptation</li> <li>PlanckianJitter</li> <li>Posterize</li> <li>RGBShift</li> <li>RandomBrightnessContrast</li> <li>RandomFog</li> <li>RandomGamma</li> <li>RandomGravel</li> <li>RandomRain</li> <li>RandomShadow</li> <li>RandomSnow</li> <li>RandomSunFlare</li> <li>RandomToneCurve</li> <li>RingingOvershoot</li> <li>Sharpen</li> <li>Solarize</li> <li>Spatter</li> <li>Superpixels</li> <li>TemplateTransform</li> <li>TextImage</li> <li>ToFloat</li> <li>ToGray</li> <li>ToRGB</li> <li>ToSepia</li> <li>UnsharpMask</li> <li>ZoomBlur</li> </ul>"},{"location":"getting_started/transforms_and_targets/#spatial-level-transforms","title":"Spatial-level transforms","text":"<p>Here is a table with spatial-level transforms and targets they support. If you try to apply a spatial-level transform to an unsupported target, Albumentations will raise an error.</p> Transform Image Mask BBoxes Keypoints Global Label Affine \u2713 \u2713 \u2713 \u2713 BBoxSafeRandomCrop \u2713 \u2713 \u2713 \u2713 CenterCrop \u2713 \u2713 \u2713 \u2713 CoarseDropout \u2713 \u2713 \u2713 Crop \u2713 \u2713 \u2713 \u2713 CropAndPad \u2713 \u2713 \u2713 \u2713 CropNonEmptyMaskIfExists \u2713 \u2713 \u2713 \u2713 D4 \u2713 \u2713 \u2713 \u2713 ElasticTransform \u2713 \u2713 \u2713 GridDistortion \u2713 \u2713 \u2713 GridDropout \u2713 \u2713 GridElasticDeform \u2713 \u2713 HorizontalFlip \u2713 \u2713 \u2713 \u2713 Lambda \u2713 \u2713 \u2713 \u2713 \u2713 LongestMaxSize \u2713 \u2713 \u2713 \u2713 MaskDropout \u2713 \u2713 MixUp \u2713 \u2713 \u2713 Morphological \u2713 \u2713 NoOp \u2713 \u2713 \u2713 \u2713 \u2713 OpticalDistortion \u2713 \u2713 \u2713 OverlayElements \u2713 \u2713 PadIfNeeded \u2713 \u2713 \u2713 \u2713 Perspective \u2713 \u2713 \u2713 \u2713 PiecewiseAffine \u2713 \u2713 \u2713 \u2713 PixelDropout \u2713 \u2713 RandomCrop \u2713 \u2713 \u2713 \u2713 RandomCropFromBorders \u2713 \u2713 \u2713 \u2713 RandomGridShuffle \u2713 \u2713 \u2713 RandomResizedCrop \u2713 \u2713 \u2713 \u2713 RandomRotate90 \u2713 \u2713 \u2713 \u2713 RandomScale \u2713 \u2713 \u2713 \u2713 RandomSizedBBoxSafeCrop \u2713 \u2713 \u2713 \u2713 RandomSizedCrop \u2713 \u2713 \u2713 \u2713 Resize \u2713 \u2713 \u2713 \u2713 Rotate \u2713 \u2713 \u2713 \u2713 SafeRotate \u2713 \u2713 \u2713 \u2713 ShiftScaleRotate \u2713 \u2713 \u2713 \u2713 SmallestMaxSize \u2713 \u2713 \u2713 \u2713 Transpose \u2713 \u2713 \u2713 \u2713 VerticalFlip \u2713 \u2713 \u2713 \u2713 XYMasking \u2713 \u2713 \u2713"},{"location":"integrations/","title":"Integrations","text":"<p>Here are some examples of how to use Albumentations with different deep learning frameworks and tools:</p> <ul> <li>HuggingFace</li> <li>FiftyOne</li> <li>Roboflow</li> </ul>"},{"location":"integrations/fiftyone/","title":"FiftyOne integration","text":""},{"location":"integrations/fiftyone/#introduction","title":"Introduction","text":"<p>FiftyOne is an open-source visualization and analysis tool for machine learning datasets, particularly useful in computer vision projects. It facilitates detailed dataset examination and the fine-tuning of model performance.</p> <p>Albumentations could be used in FiftyOne via the FiftyOne Plugin.</p> <p>With the FiftyOne Albumentations plugin, you can transform any and all labels of type Detections, Keypoints, Segmentation, and Heatmap, or just the <code>images</code> themselves.</p> <p>Info</p> <p>This tutorial is almost entirely based on the FiftyOne Documentation and serves as an overview of the functionality of the FiftyOne Albumentations plugin.</p> <p>For more up to date information check the original source.</p> <p>This integration guide will focus on the setup process and the functionality of the plugin.</p> <p>For a tutorial on how to curate your augmentations, check out the Data Augmentation Tutorial as FiftyOne Documentation.</p>"},{"location":"integrations/fiftyone/#overview","title":"Overview","text":"<p>Albumentations supports 80+ transforms spanning pixel-level, geometric transformations, and more.</p> <p>As of April 29, 2024 FiftyOne supports:</p> <ul> <li>AdvancedBlur</li> <li>GridDropout</li> <li>MaskDropout</li> <li>PiecewiseAffine</li> <li>RandomGravel</li> <li>RandomGridShuffle</li> <li>RandomShadow</li> <li>RandomSunFlare</li> <li>Rotate</li> </ul>"},{"location":"integrations/fiftyone/#functionality","title":"Functionality","text":"<p>The FiftyOne Albumentations plugin provides the following functionality:</p> <ul> <li>Apply Albumentations transformations to your dataset, your current view, or selected samples</li> <li>Visualize the effects of these transformations directly within the FiftyOne App</li> <li>View samples generated by the last applied transformation</li> <li>Save augmented samples to the dataset</li> <li>Get info about the last applied transformation</li> <li>Save transformation pipelines to the dataset for reproducibility</li> </ul>"},{"location":"integrations/fiftyone/#setup","title":"Setup","text":"<p>Make sure you have FiftyOne and Albumentations installed:</p> Bash<pre><code>pip install -U fiftyone albumentations\n</code></pre> <p>Next, install the FiftyOne Albumentations plugin:</p> Bash<pre><code>fiftyone plugins download https://github.com/jacobmarks/fiftyone-albumentations-plugin\n</code></pre> <p>Note</p> <p>If you have the FiftyOne Plugin Utils plugin installed, you can also install the Albumentations plugin via the <code>install_plugin</code> operator, selecting the Albumentations plugin from the community dropdown menu.</p> <p>You will also need to load (and download if necessary) a dataset to apply the augmentations to. For this guide, we'll use the the quickstart dataset:</p> Python<pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\n## only take 5 samples for quick demonstration\ndataset = foz.load_zoo_dataset(\"quickstart\", max_samples=5)\n\n# only keep the ground truth labels\ndataset.select_fields(\"ground_truth\").keep_fields()\n\nsession = fo.launch_app(dataset)\n</code></pre> <p>Note</p> <p>The quickstart dataset only contains Detections labels. If you want to test Albumentations transformations on other label types, here are some quick examples to get you started, using FiftyOne's Hugging Face Transformers and Ultralytics integrations: Bash<pre><code>pip install -U transformers ultralytics\n</code></pre> Python<pre><code>import fiftyone as fo\nimport fiftyone.zoo as foz\n\nfrom ultralytics import YOLO\n\n# Keypoints\nmodel = YOLO(\"yolov8l-pose.pt\")\ndataset.apply_model(model, label_field=\"keypoints\")\n\n# Instance Segmentation\nmodel = YOLO(\"yolov8l-seg.pt\")\ndataset.apply_model(model, label_field=\"instances\")\n\n# Semantic Segmentation\nmodel = foz.load_zoo_model(\n    \"segmentation-transformer-torch\",\n    name_or_path=\"Intel/dpt-large-ade\",\n)\ndataset.apply_model(model, label_field=\"mask\")\n\n# Heatmap\nmodel = foz.load_zoo_model(\n    \"depth-estimation-transformer-torch\",\n    name_or_path=\"LiheYoung/depth-anything-small-hf\",\n)\ndataset.apply_model(model, label_field=\"depth_map\")\n</code></pre></p>"},{"location":"integrations/fiftyone/#apply-transformations","title":"Apply transformations","text":"<p>To apply Albumentations transformations to your dataset, you can use the augment_with_albumentations operator. Press the backtick key to open the operator modal, and select the <code>augment_with_albumentations</code> operator from the dropdown menu.</p> <p>You can then configure the transformations to apply:</p> <ul> <li>Number of augmentations per sample: The number of augmented samples to generate for each input sample. The default is 1, which is sufficient for deterministic transformations, but for probabilistic transformations, you may want to generate multiple samples to see the range of possible outputs.</li> <li>Number of transforms: The number of transformations to compose into the pipeline to be applied to each sample. The default is 1, but you can set this as high as you'd like \u2014 the more transformations, the more complex the augmentations will be. You will be able to configure each transform separately.</li> <li>Target view: The view to which the transformations will be applied. The default is <code>dataset</code>, but you can also apply the transformations to the current view or to currently selected samples within the app.</li> <li>Execution mode: If you set <code>delegated=False</code>, the operation will be executed immediately. If you set <code>delegated=True</code>, the operation will be queued as a job, which you can then run in the background from your terminal with:</li> </ul> Bash<pre><code>fiftyone delegated launch\n</code></pre> <p>For each transformation, you can select either a \"primitive\" transformation from the Albumentations library, or a \"saved\" transformation pipeline that you have previously saved to the dataset. These saved pipelines can consist of one or more transformations.</p> <p>When you apply a primitive transformation, you can configure the parameters of the transformation directly within the app. The available parameters, their default values, types, and docstrings are all integrated directly from the Albumentations library.</p> <p></p> <p>When you apply a saved pipeline, there will not be any parameters to configure.</p> <p></p>"},{"location":"integrations/fiftyone/#visualize-transformations","title":"Visualize transformations","text":"<p>Once you've applied the transformations, you can visualize the effects of the transformations directly within the FiftyOne App. All augmented samples will be added to the dataset, and will be tagged as <code>augmented</code> so that you can easily filter for just augmented or non-augmented samples in the app.</p> <p></p> <p>You can also filter for augmented samples programmatically with the match_tags() method:</p> Python<pre><code># get just the augmented samples\naugmented_view = dataset.match_tags(\"augmented\")\n\n# get just the non-augmented samples\nnon_augmented_view = dataset.match_tags(\"augmented\", bool=False)\n</code></pre> <p>However, matching on these tags will return all samples that have been generated by an augmentation, not just the samples that were generated by the last applied transformation \u2014 as you will see shortly, we can save augmentations to the dataset. To get just the samples generated by the last applied transformation, you can use the view_last_albumentations_run operator:</p> <p></p> <p>Note</p> <p>For all samples added to the dataset by the FiftyOne Albumentations plugin, there will be a field <code>\"transform\"</code>, which contains the information not just about the pipeline that was applied, but also about the specific parameters that were used for this application of the pipeline. For example, if you had a HorizontalFlip transformation with an application probability of <code>p=0.5</code>, the contents of the <code>\"transform\"</code> field tell you whether or not this transformation was applied to the sample!</p>"},{"location":"integrations/fiftyone/#save-augmentations","title":"Save augmentations","text":"<p>By default all augmentations are temporary, as the FiftyOne Albumentations plugin is primarily designed for rapid prototyping and experimentation. This means that when you generated a new batch of augmented samples, the previous batch of augmented samples will be removed from the dataset, and the image files will be deleted from disk.</p> <p>However, if you want to save the augmented samples to the dataset, you can use the save_albumentations_augmentations operator, which will save the augmented samples to the dataset while keeping the augmented tag on the samples.</p> <p></p>"},{"location":"integrations/fiftyone/#get-last-transformation-info","title":"Get last transformation info","text":"<p>When you apply a transformation pipeline to samples in your dataset using the FiftyOne Albumentations plugin, this information is captured and stored using FiftyOne's custom runs. This means that you can easily access the information about the last applied transformation.</p> <p>In the FiftyOne App, you can use the get_last_albumentations_run_info operator to display a formatted summary of the relevant information:</p> <p></p> <p>Note</p> <p>You can also access this information programmatically by getting info about the custom run that the information is stored in. For the Albumentations plugin, this info is stored via the key <code>'_last_albumentations_run'</code>:</p> Python<pre><code>last_run_info = dataset.get_run_info(\"_last_albumentations_run\")\nprint(last_run_info)\n</code></pre>"},{"location":"integrations/fiftyone/#save-transformations","title":"Save transformations","text":"<p>If you are satisfied with the transformation pipeline you have created, you can save the entire composition of transformations to the dataset, hyperparameters and all. This means that after your rapid prototyping phase, you can easily move to a more reproducible workflow, and you can share your transformations or port them to other datasets.</p> <p>To save a transformation pipeline, you can use the save_albumentations_transform operator:</p> <p>After doing so, you will be able to view the information about this saved transformation pipeline using the get_albumentations_run_info operator:</p> <p></p> <p>Additionally, you will have access to this saved transformation pipeline under the \"saved\" tab for each transformation in the augment_with_albumentations operator modal.</p>"},{"location":"integrations/huggingface/","title":"HuggingFace","text":"<ul> <li>Image classification</li> <li>Object Detection</li> </ul>"},{"location":"integrations/huggingface/image_classification_albumentations/","title":"Fine-tuning for Image Classification with \ud83e\udd17 Transformers","text":"<p>This notebook shows how to fine-tune any pretrained Vision model for Image Classification on a custom dataset. The idea is to add a randomly initialized classification head on top of a pre-trained encoder, and fine-tune the model altogether on a labeled dataset.</p>"},{"location":"integrations/huggingface/image_classification_albumentations/#imagefolder-feature","title":"ImageFolder feature","text":"<p>This notebook leverages the ImageFolder feature to easily run the notebook on a custom dataset (namely, EuroSAT in this tutorial). You can either load a <code>Dataset</code> from local folders or from local/remote files, like zip or tar.</p>"},{"location":"integrations/huggingface/image_classification_albumentations/#any-model","title":"Any model","text":"<p>This notebook is built to run on any image classification dataset with any vision model checkpoint from the Model Hub as long as that model has a version with a Image Classification head, such as: * ViT * Swin Transformer * ConvNeXT</p> <ul> <li>in short, any model supported by AutoModelForImageClassification.</li> </ul>"},{"location":"integrations/huggingface/image_classification_albumentations/#albumentations","title":"Albumentations","text":"<p>In this notebook, we are going to leverage the Albumentations library for data augmentation. Note that we have other versions of this notebook available as well with other libraries including:</p> <ul> <li>Torchvision's Transforms</li> <li>Kornia</li> <li>imgaug. </li> </ul> <p>Depending on the model and the GPU you are using, you might need to adjust the batch size to avoid out-of-memory errors. Set those two parameters, then the rest of the notebook should run smoothly.</p> <p>In this notebook, we'll fine-tune from the https://huggingface.co/facebook/convnext-tiny-224 checkpoint, but note that there are many, many more available on the hub.</p> Python<pre><code>model_checkpoint = \"facebook/convnext-tiny-224\" # pre-trained model from which to fine-tune\nbatch_size = 32 # batch size for training and evaluation\n</code></pre> <p>Before we start, let's install the <code>datasets</code>, <code>transformers</code> and <code>albumentations</code> libraries.</p> Python<pre><code>!pip install -q datasets transformers\n</code></pre> <pre><code>\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 325 kB 8.7 MB/s \n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.0 MB 67.0 MB/s \n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 77 kB 8.1 MB/s \n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.1 MB 48.8 MB/s \n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 136 kB 72.0 MB/s \n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 212 kB 72.9 MB/s \n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 127 kB 75.0 MB/s \n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 895 kB 67.3 MB/s \n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6.5 MB 56.3 MB/s \n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 596 kB 76.4 MB/s \n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 144 kB 76.3 MB/s \n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 94 kB 3.3 MB/s \n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 271 kB 77.3 MB/s \n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n\u001b[?25h\n</code></pre> Python<pre><code>!pip install -q albumentations\n</code></pre> <pre><code>\u001b[?25l\n</code></pre> <p>\u001b[K     |\u258c                               | 10 kB 26.1 MB/s eta 0:00:01 \u001b[K     |\u2588                               | 20 kB 27.6 MB/s eta 0:00:01 \u001b[K     |\u2588\u258b                              | 30 kB 11.8 MB/s eta 0:00:01 \u001b[K     |\u2588\u2588                              | 40 kB 8.9 MB/s eta 0:00:01 \u001b[K     |\u2588\u2588\u258b                             | 51 kB 6.7 MB/s eta 0:00:01 \u001b[K     |\u2588\u2588\u2588\u258f                            | 61 kB 7.9 MB/s eta 0:00:01 \u001b[K     |\u2588\u2588\u2588\u258b                            | 71 kB 8.0 MB/s eta 0:00:01 \u001b[K     |\u2588\u2588\u2588\u2588\u258f                           | 81 kB 7.4 MB/s eta 0:00:01 \u001b[K     |\u2588\u2588\u2588\u2588\u258a                           | 92 kB 8.2 MB/s eta 0:00:01 \u001b[K     |\u2588\u2588\u2588\u2588\u2588\u258f                          | 102 kB 8.4 MB/s eta 0:00:01 \u001b[K     |\u2588\u2588\u2588\u2588\u2588\u258a                          | 112 kB 8.4 MB/s eta 0:00:01 \u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u258e                         | 122 kB 8.4 MB/s eta 0:00:01 \u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u258a                         | 133 kB 8.4 MB/s eta 0:00:01 \u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e                        | 143 kB 8.4 MB/s eta 0:00:01 \u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589                        | 153 kB 8.4 MB/s eta 0:00:01 \u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e                       | 163 kB 8.4 MB/s eta 0:00:01 \u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589                       | 174 kB 8.4 MB/s eta 0:00:01 \u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d                      | 184 kB 8.4 MB/s eta 0:00:01 \u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589                      | 194 kB 8.4 MB/s eta 0:00:01 \u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d                     | 204 kB 8.4 MB/s eta 0:00:01 \u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                     | 215 kB 8.4 MB/s eta 0:00:01 \u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d                    | 225 kB 8.4 MB/s eta 0:00:01 \u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                    | 235 kB 8.4 MB/s eta 0:00:01 \u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c                   | 245 kB 8.4 MB/s eta 0:00:01 \u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                   | 256 kB 8.4 MB/s eta 0:00:01 \u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c                  | 266 kB 8.4 MB/s eta 0:00:01 \u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                  | 276 kB 8.4 MB/s eta 0:00:01 \u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c                 | 286 kB 8.4 MB/s eta 0:00:01 \u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                 | 296 kB 8.4 MB/s eta 0:00:01 \u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b                | 307 kB 8.4 MB/s eta 0:00:01 \u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                | 317 kB 8.4 MB/s eta 0:00:01 \u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b               | 327 kB 8.4 MB/s eta 0:00:01 \u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f              | 337 kB 8.4 MB/s eta 0:00:01 \u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b              | 348 kB 8.4 MB/s eta 0:00:01 \u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f             | 358 kB 8.4 MB/s eta 0:00:01 \u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a             | 368 kB 8.4 MB/s eta 0:00:01 \u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f            | 378 kB 8.4 MB/s eta 0:00:01 \u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a            | 389 kB 8.4 MB/s eta 0:00:01 \u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e           | 399 kB 8.4 MB/s eta 0:00:01 \u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a           | 409 kB 8.4 MB/s eta 0:00:01 \u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e          | 419 kB 8.4 MB/s eta 0:00:01 \u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589          | 430 kB 8.4 MB/s eta 0:00:01 \u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e         | 440 kB 8.4 MB/s eta 0:00:01 \u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589         | 450 kB 8.4 MB/s eta 0:00:01 \u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d        | 460 kB 8.4 MB/s eta 0:00:01 \u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589        | 471 kB 8.4 MB/s eta 0:00:01 \u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d       | 481 kB 8.4 MB/s eta 0:00:01 \u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588       | 491 kB 8.4 MB/s eta 0:00:01 \u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d      | 501 kB 8.4 MB/s eta 0:00:01 \u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588      | 512 kB 8.4 MB/s eta 0:00:01 \u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c     | 522 kB 8.4 MB/s eta 0:00:01 \u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588     | 532 kB 8.4 MB/s eta 0:00:01 \u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c    | 542 kB 8.4 MB/s eta 0:00:01 \u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588    | 552 kB 8.4 MB/s eta 0:00:01 \u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 563 kB 8.4 MB/s eta 0:00:01 \u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 573 kB 8.4 MB/s eta 0:00:01 \u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 583 kB 8.4 MB/s eta 0:00:01 \u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 593 kB 8.4 MB/s eta 0:00:01 \u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 604 kB 8.4 MB/s eta 0:00:01 \u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 614 kB 8.4 MB/s eta 0:00:01 \u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 624 kB 8.4 MB/s eta 0:00:01 \u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 631 kB 8.4 MB/s      \u001b[?25h  Building wheel for imgaug (setup.py) ... \u001b[?25l\u001b[?25hdone</p> <p>If you're opening this notebook locally, make sure your environment has an install from the last version of those libraries.</p> <p>To be able to share your model with the community and generate results like the one shown in the picture below via the inference API, there are a few more steps to follow.</p> <p>First you have to store your authentication token from the Hugging Face website (sign up here if you haven't already!) then execute the following cell and input your token:</p> Python<pre><code>from huggingface_hub import notebook_login\n\nnotebook_login()\n</code></pre> <pre><code>Login successful\nYour token has been saved to /root/.huggingface/token\n\u001b[1m\u001b[31mAuthenticated through git-credential store but this isn't the helper defined on your machine.\nYou might have to re-authenticate when pushing to the Hugging Face Hub. Run the following command in your terminal in case you want to set this credential helper as the default\n\ngit config --global credential.helper store\u001b[0m\n</code></pre> <p>Then you need to install Git-LFS to upload your model checkpoints:</p> Python<pre><code>%%capture\n!sudo apt -qq install git-lfs\n!git config --global credential.helper store\n</code></pre> <p>We also quickly upload some telemetry - this tells us which examples and software versions are getting used so we know where to prioritize our maintenance efforts. We don't collect (or care about) any personally identifiable information, but if you'd prefer not to be counted, feel free to skip this step or delete this cell entirely.</p> Python<pre><code>from transformers.utils import send_example_telemetry\n\nsend_example_telemetry(\"image_classification_albumentations_notebook\", framework=\"pytorch\")\n</code></pre>"},{"location":"integrations/huggingface/image_classification_albumentations/#fine-tuning-a-model-on-an-image-classification-task","title":"Fine-tuning a model on an image classification task","text":"<p>In this notebook, we will see how to fine-tune one of the \ud83e\udd17 Transformers vision models on an Image Classification dataset.</p> <p>Given an image, the goal is to predict an appropriate class for it, like \"tiger\". The screenshot below is taken from a ViT fine-tuned on ImageNet-1k - try out the inference widget!</p> <p></p>"},{"location":"integrations/huggingface/image_classification_albumentations/#loading-the-dataset","title":"Loading the dataset","text":"<p>We will use the \ud83e\udd17 Datasets library's ImageFolder feature to download our custom dataset into a DatasetDict.</p> <p>In this case, the EuroSAT dataset is hosted remotely, so we provide the <code>data_files</code> argument. Alternatively, if you have local folders with images, you can load them using the <code>data_dir</code> argument. </p> Python<pre><code>from datasets import load_dataset \n\n# load a custom dataset from local/remote files using the ImageFolder feature\n\n# option 1: local/remote files (supporting the following formats: tar, gzip, zip, xz, rar, zstd)\ndataset = load_dataset(\"imagefolder\", data_files=\"https://madm.dfki.de/files/sentinel/EuroSAT.zip\")\n\n# note that you can also provide several splits:\n# dataset = load_dataset(\"imagefolder\", data_files={\"train\": [\"path/to/file1\", \"path/to/file2\"], \"test\": [\"path/to/file3\", \"path/to/file4\"]})\n\n# note that you can push your dataset to the hub very easily (and reload afterwards using load_dataset)!\n# dataset.push_to_hub(\"nielsr/eurosat\")\n# dataset.push_to_hub(\"nielsr/eurosat\", private=True)\n\n# option 2: local folder\n# dataset = load_dataset(\"imagefolder\", data_dir=\"path_to_folder\")\n\n# option 3: just load any existing dataset from the hub ...\n# dataset = load_dataset(\"cifar10\")\n</code></pre> <pre><code>Using custom data configuration default-0537267e6f812d56\n\n\nDownloading and preparing dataset image_folder/default to /root/.cache/huggingface/datasets/image_folder/default-0537267e6f812d56/0.0.0/ee92df8e96c6907f3c851a987be3fd03d4b93b247e727b69a8e23ac94392a091...\n\n\n\nDownloading data files: 0it [00:00, ?it/s]\n\n\n\nDownloading data files:   0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n\n\nDownloading data:   0%|          | 0.00/94.3M [00:00&lt;?, ?B/s]\n\n\n\nExtracting data files:   0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n\n\nGenerating train split: 0 examples [00:00, ? examples/s]\n\n\nDataset image_folder downloaded and prepared to /root/.cache/huggingface/datasets/image_folder/default-0537267e6f812d56/0.0.0/ee92df8e96c6907f3c851a987be3fd03d4b93b247e727b69a8e23ac94392a091. Subsequent calls will reuse this data.\n\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n</code></pre> <p>Let us also load the Accuracy metric, which we'll use to evaluate our model both during and after training.</p> Python<pre><code>from datasets import load_metric\n\nmetric = load_metric(\"accuracy\")\n</code></pre> <pre><code>Downloading builder script:   0%|          | 0.00/1.41k [00:00&lt;?, ?B/s]\n</code></pre> <p>The <code>dataset</code> object itself is a <code>DatasetDict</code>, which contains one key per split (in this case, only \"train\" for a training split).</p> Python<pre><code>dataset\n</code></pre> <pre><code>DatasetDict({\n    train: Dataset({\n        features: ['image', 'label'],\n        num_rows: 27000\n    })\n})\n</code></pre> <p>To access an actual element, you need to select a split first, then give an index:</p> Python<pre><code>example = dataset[\"train\"][10]\nexample\n</code></pre> <pre><code>{'image': &lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=64x64 at 0x7FD62DA6B2D0&gt;,\n 'label': 2}\n</code></pre> <p>Each example consists of an image and a corresponding label. We can also verify this by checking the features of the dataset:</p> Python<pre><code>dataset[\"train\"].features\n</code></pre> <pre><code>{'image': Image(decode=True, id=None),\n 'label': ClassLabel(num_classes=10, names=['AnnualCrop', 'Forest', 'HerbaceousVegetation', 'Highway', 'Industrial', 'Pasture', 'PermanentCrop', 'Residential', 'River', 'SeaLake'], id=None)}\n</code></pre> <p>The cool thing is that we can directly view the image (as the 'image' field is an Image feature), as follows:</p> Python<pre><code>example['image']\n</code></pre> <p></p> <p>Let's make it a little bigger as the images in the EuroSAT dataset are of low resolution (64x64 pixels):</p> Python<pre><code>example['image'].resize((200, 200))\n</code></pre> <p></p> <p>Let's check the corresponding label:</p> Python<pre><code>example['label']\n</code></pre> <pre><code>2\n</code></pre> <p>As you can see, the <code>label</code> field is not an actual string label. By default the <code>ClassLabel</code> fields are encoded into integers for convenience:</p> Python<pre><code>dataset[\"train\"].features[\"label\"]\n</code></pre> <pre><code>ClassLabel(num_classes=10, names=['AnnualCrop', 'Forest', 'HerbaceousVegetation', 'Highway', 'Industrial', 'Pasture', 'PermanentCrop', 'Residential', 'River', 'SeaLake'], id=None)\n</code></pre> <p>Let's create an <code>id2label</code> dictionary to decode them back to strings and see what they are. The inverse <code>label2id</code> will be useful too, when we load the model later.</p> Python<pre><code>labels = dataset[\"train\"].features[\"label\"].names\nlabel2id, id2label = dict(), dict()\nfor i, label in enumerate(labels):\n    label2id[label] = i\n    id2label[i] = label\n\nid2label[2]\n</code></pre> <pre><code>'HerbaceousVegetation'\n</code></pre>"},{"location":"integrations/huggingface/image_classification_albumentations/#preprocessing-the-data","title":"Preprocessing the data","text":"<p>Before we can feed these images to our model, we need to preprocess them. </p> <p>Preprocessing images typically comes down to (1) resizing them to a particular size (2) normalizing the color channels (R,G,B) using a mean and standard deviation. These are referred to as image transformations.</p> <p>In addition, one typically performs what is called data augmentation during training (like random cropping and flipping) to make the model more robust and achieve higher accuracy. Data augmentation is also a great technique to increase the size of the training data.</p> <p>We will use <code>Albumentations</code> for the image transformations/data augmentation in this tutorial, but note that one can use any other package (like torchvision's transforms, imgaug, Kornia, etc.).</p> <p>To make sure we (1) resize to the appropriate size (2) use the appropriate image mean and standard deviation for the model architecture we are going to use, we instantiate what is called an image processor with the <code>AutoImageProcessor.from_pretrained</code> method.</p> <p>This image processor is a minimal preprocessor that can be used to prepare images for inference.</p> Python<pre><code>from transformers import AutoImageProcessor\n\nimage_processor = AutoImageProcessor.from_pretrained(model_checkpoint)\nimage_processor\n</code></pre> <pre><code>Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n\n\n\n\n\nConvNextImageProcessor {\n  \"crop_pct\": 0.875,\n  \"do_normalize\": true,\n  \"do_rescale\": true,\n  \"do_resize\": true,\n  \"image_mean\": [\n    0.485,\n    0.456,\n    0.406\n  ],\n  \"image_processor_type\": \"ConvNextImageProcessor\",\n  \"image_std\": [\n    0.229,\n    0.224,\n    0.225\n  ],\n  \"resample\": 3,\n  \"rescale_factor\": 0.00392156862745098,\n  \"size\": {\n    \"shortest_edge\": 224\n  }\n}\n</code></pre> <p>The Datasets library is made for processing data very easily. We can write custom functions, which can then be applied on an entire dataset (either using <code>.map()</code> or <code>.set_transform()</code>).</p> <p>Here we define 2 separate functions, one for training (which includes data augmentation) and one for validation (which only includes resizing, center cropping and normalizing). </p> Python<pre><code>import cv2\nimport albumentations as A\nimport numpy as np\n\nif \"height\" in image_processor.size:\n    size = (image_processor.size[\"height\"], image_processor.size[\"width\"])\n    crop_size = size\n    max_size = None\nelif \"shortest_edge\" in image_processor.size:\n    size = image_processor.size[\"shortest_edge\"]\n    crop_size = (size, size)\n    max_size = image_processor.size.get(\"longest_edge\")\n\ntrain_transforms = A.Compose([\n    A.Resize(height=size, width=size),\n    A.RandomRotate90(),\n    A.HorizontalFlip(p=0.5),\n    A.RandomBrightnessContrast(p=0.2),\n    A.Normalize(),\n])\n\nval_transforms = A.Compose([\n    A.Resize(height=size, width=size),\n    A.Normalize(),\n])\n\ndef preprocess_train(examples):\n    examples[\"pixel_values\"] = [\n        train_transforms(image=np.array(image))[\"image\"] for image in examples[\"image\"]\n    ]\n\n    return examples\n\ndef preprocess_val(examples):\n    examples[\"pixel_values\"] = [\n        val_transforms(image=np.array(image))[\"image\"] for image in examples[\"image\"]\n    ]\n\n    return examples\n</code></pre> <p>Next, we can preprocess our dataset by applying these functions. We will use the <code>set_transform</code> functionality, which allows to apply the functions above on-the-fly (meaning that they will only be applied when the images are loaded in RAM).</p> Python<pre><code># split up training into training + validation\nsplits = dataset[\"train\"].train_test_split(test_size=0.1)\ntrain_ds = splits['train']\nval_ds = splits['test']\n</code></pre> Python<pre><code>train_ds.set_transform(preprocess_train)\nval_ds.set_transform(preprocess_val)\n</code></pre> <p>Let's check the first example:</p> Python<pre><code>train_ds[0]\n</code></pre> <pre><code>{'image': &lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=64x64 at 0x7FD610178490&gt;,\n 'label': 5,\n 'pixel_values': array([[[-1.415789  , -0.53011197, -0.37525052],\n         [-1.415789  , -0.53011197, -0.37525052],\n         [-1.415789  , -0.53011197, -0.37525052],\n         ...,\n         [-1.34729   , -0.897759  , -0.37525052],\n         [-1.34729   , -0.897759  , -0.37525052],\n         [-1.34729   , -0.897759  , -0.37525052]],\n\n        [[-1.415789  , -0.53011197, -0.37525052],\n         [-1.415789  , -0.53011197, -0.37525052],\n         [-1.415789  , -0.53011197, -0.37525052],\n         ...,\n         [-1.34729   , -0.897759  , -0.37525052],\n         [-1.34729   , -0.897759  , -0.37525052],\n         [-1.34729   , -0.897759  , -0.37525052]],\n\n        [[-1.415789  , -0.53011197, -0.37525052],\n         [-1.415789  , -0.53011197, -0.37525052],\n         [-1.415789  , -0.53011197, -0.37525052],\n         ...,\n         [-1.3986642 , -0.93277305, -0.4101089 ],\n         [-1.3986642 , -0.93277305, -0.4101089 ],\n         [-1.3986642 , -0.93277305, -0.4101089 ]],\n\n        ...,\n\n        [[-1.5014129 , -0.582633  , -0.35782132],\n         [-1.5014129 , -0.582633  , -0.35782132],\n         [-1.5014129 , -0.582633  , -0.35782132],\n         ...,\n         [-1.4842881 , -0.98529404, -0.5146841 ],\n         [-1.4671633 , -1.0028011 , -0.49725488],\n         [-1.4671633 , -1.0028011 , -0.49725488]],\n\n        [[-1.5356623 , -0.565126  , -0.3403921 ],\n         [-1.5356623 , -0.565126  , -0.3403921 ],\n         [-1.5356623 , -0.565126  , -0.35782132],\n         ...,\n         [-1.4842881 , -0.98529404, -0.5146841 ],\n         [-1.4671633 , -1.0028011 , -0.49725488],\n         [-1.4671633 , -1.0028011 , -0.49725488]],\n\n        [[-1.5356623 , -0.565126  , -0.3403921 ],\n         [-1.5356623 , -0.565126  , -0.3403921 ],\n         [-1.5356623 , -0.565126  , -0.35782132],\n         ...,\n         [-1.4842881 , -0.98529404, -0.5146841 ],\n         [-1.4671633 , -1.0028011 , -0.49725488],\n         [-1.4671633 , -1.0028011 , -0.49725488]]], dtype=float32)}\n</code></pre>"},{"location":"integrations/huggingface/image_classification_albumentations/#training-the-model","title":"Training the model","text":"<p>Now that our data is ready, we can download the pretrained model and fine-tune it. For classification we use the <code>AutoModelForImageClassification</code> class. Like with the image processor, the <code>from_pretrained</code> method will download and cache the model for us. As the label ids and the number of labels are dataset dependent, we pass <code>num_labels</code>, <code>label2id</code>, and <code>id2label</code> alongside the <code>model_checkpoint</code> he\u00a3re.</p> <p>NOTE: in case you're planning to fine-tune an already fine-tuned checkpoint, like facebook/convnext-tiny-224 (which has already been fine-tuned on ImageNet-1k), then you need to provide the additional argument <code>ignore_mismatched_sizes=True</code> to the <code>from_pretrained</code> method. This will make sure the output head is thrown away and replaced by a new, randomly initialized classification head that includes a custom number of output neurons.</p> Python<pre><code>from transformers import AutoModelForImageClassification, TrainingArguments, Trainer\n\nnum_labels = len(id2label)\nmodel = AutoModelForImageClassification.from_pretrained(\n    model_checkpoint, \n    label2id=label2id,\n    id2label=id2label,\n    ignore_mismatched_sizes = True, # provide this in case you'd like to fine-tune an already fine-tuned checkpoint\n)\n</code></pre> <pre><code>Downloading:   0%|          | 0.00/68.0k [00:00&lt;?, ?B/s]\n\n\n\nDownloading:   0%|          | 0.00/109M [00:00&lt;?, ?B/s]\n\n\nSome weights of ConvNextForImageClassification were not initialized from the model checkpoint at facebook/convnext-tiny-224 and are newly initialized because the shapes did not match:\n- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([10, 768]) in the model instantiated\n- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([10]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n</code></pre> <p>The warning is telling us we are throwing away some weights (the weights and bias of the <code>pooler</code> layer) and randomly initializing some other (the weights and bias of the <code>classifier</code> layer). This is expected in this case, because we are adding a new head for which we don't have pretrained weights, so the library warns us we should fine-tune this model before using it for inference, which is exactly what we are going to do.</p> <p>To instantiate a <code>Trainer</code>, we will need to define the training configuration and the evaluation metric. The most important is the <code>TrainingArguments</code>, which is a class that contains all the attributes to customize the training. It requires one folder name, which will be used to save the checkpoints of the model.</p> <p>Most of the training arguments are pretty self-explanatory, but one that is quite important here is <code>remove_unused_columns=False</code>. This one will drop any features not used by the model's call function. By default it's <code>True</code> because usually it's ideal to drop unused feature columns, making it easier to unpack inputs into the model's call function. But, in our case, we need the unused features ('img' in particular) in order to create 'pixel_values'.</p> Python<pre><code>model_name = model_checkpoint.split(\"/\")[-1]\n\nargs = TrainingArguments(\n    f\"{model_name}-finetuned-eurosat-albumentations\",\n    remove_unused_columns=False,\n    evaluation_strategy = \"epoch\",\n    save_strategy = \"epoch\",\n    learning_rate=5e-5,\n    per_device_train_batch_size=batch_size,\n    gradient_accumulation_steps=4,\n    per_device_eval_batch_size=batch_size,\n    num_train_epochs=3,\n    warmup_ratio=0.1,\n    logging_steps=10,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"accuracy\",\n    push_to_hub=True,\n)\n</code></pre> <p>Here we set the evaluation to be done at the end of each epoch, tweak the learning rate, use the <code>batch_size</code> defined at the top of the notebook and customize the number of epochs for training, as well as the weight decay. Since the best model might not be the one at the end of training, we ask the <code>Trainer</code> to load the best model it saved (according to <code>metric_name</code>) at the end of training.</p> <p>The last argument <code>push_to_hub</code> allows the Trainer to push the model to the Hub regularly during training. Remove it if you didn't follow the installation steps at the top of the notebook. If you want to save your model locally with a name that is different from the name of the repository, or if you want to push your model under an organization and not your name space, use the <code>hub_model_id</code> argument to set the repo name (it needs to be the full name, including your namespace: for instance <code>\"nielsr/vit-finetuned-cifar10\"</code> or <code>\"huggingface/nielsr/vit-finetuned-cifar10\"</code>).</p> <p>Next, we need to define a function for how to compute the metrics from the predictions, which will just use the <code>metric</code> we loaded earlier. The only preprocessing we have to do is to take the argmax of our predicted logits:</p> Python<pre><code>import numpy as np\n\n# the compute_metrics function takes a Named Tuple as input:\n# predictions, which are the logits of the model as Numpy arrays,\n# and label_ids, which are the ground-truth labels as Numpy arrays.\ndef compute_metrics(eval_pred):\n    \"\"\"Computes accuracy on a batch of predictions\"\"\"\n    predictions = np.argmax(eval_pred.predictions, axis=1)\n    return metric.compute(predictions=predictions, references=eval_pred.label_ids)\n</code></pre> <p>We also define a <code>collate_fn</code>, which will be used to batch examples together. Each batch consists of 2 keys, namely <code>pixel_values</code> and <code>labels</code>.</p> Python<pre><code>import torch\n\ndef collate_fn(examples):\n    images = []\n    labels = []\n    for example in examples:\n        image = np.moveaxis(example[\"pixel_values\"], source=2, destination=0)\n        images.append(torch.from_numpy(image))\n        labels.append(example[\"label\"])\n\n    pixel_values = torch.stack(images)\n    labels = torch.tensor(labels)\n    return {\"pixel_values\": pixel_values, \"labels\": labels}\n</code></pre> <p>Then we just need to pass all of this along with our datasets to the <code>Trainer</code>:</p> Python<pre><code>trainer = Trainer(\n    model,\n    args,\n    train_dataset=train_ds,\n    eval_dataset=val_ds,\n    tokenizer=image_processor,\n    compute_metrics=compute_metrics,\n    data_collator=collate_fn,\n)\n</code></pre> <pre><code>/content/convnext-tiny-224-finetuned-eurosat-albumentations is already a clone of https://huggingface.co/nielsr/convnext-tiny-224-finetuned-eurosat-albumentations. Make sure you pull the latest changes with `repo.git_pull()`.\n</code></pre> <p>You might wonder why we pass along the <code>image_processor</code> as a tokenizer when we already preprocessed our data. This is only to make sure the image processor configuration file (stored as JSON) will also be uploaded to the repo on the hub.</p> <p>Now we can finetune our model by calling the <code>train</code> method:</p> Python<pre><code>trainer.train()\n</code></pre> <pre><code>/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 24300\n  Num Epochs = 3\n  Instantaneous batch size per device = 32\n  Total train batch size (w. parallel, distributed &amp; accumulation) = 128\n  Gradient Accumulation steps = 4\n  Total optimization steps = 570\n\n\n\n\n&lt;div&gt;\n\n  &lt;progress value='570' max='570' style='width:300px; height:20px; vertical-align: middle;'&gt;&lt;/progress&gt;\n  [570/570 15:59, Epoch 3/3]\n&lt;/div&gt;\n&lt;table border=\"1\" class=\"dataframe\"&gt;\n</code></pre> Epoch Training Loss Validation Loss Accuracy 1 0.141000 0.149633 0.954444 2 0.073600 0.095782 0.971852 3 0.056800 0.072716 0.974815 <p><p></p> <pre><code>***** Running Evaluation *****\n  Num examples = 2700\n  Batch size = 32\nSaving model checkpoint to convnext-tiny-224-finetuned-eurosat-albumentations/checkpoint-190\nConfiguration saved in convnext-tiny-224-finetuned-eurosat-albumentations/checkpoint-190/config.json\nModel weights saved in convnext-tiny-224-finetuned-eurosat-albumentations/checkpoint-190/pytorch_model.bin\nFeature extractor saved in convnext-tiny-224-finetuned-eurosat-albumentations/checkpoint-190/preprocessor_config.json\nFeature extractor saved in convnext-tiny-224-finetuned-eurosat-albumentations/preprocessor_config.json\n***** Running Evaluation *****\n  Num examples = 2700\n  Batch size = 32\nSaving model checkpoint to convnext-tiny-224-finetuned-eurosat-albumentations/checkpoint-380\nConfiguration saved in convnext-tiny-224-finetuned-eurosat-albumentations/checkpoint-380/config.json\nModel weights saved in convnext-tiny-224-finetuned-eurosat-albumentations/checkpoint-380/pytorch_model.bin\nFeature extractor saved in convnext-tiny-224-finetuned-eurosat-albumentations/checkpoint-380/preprocessor_config.json\nFeature extractor saved in convnext-tiny-224-finetuned-eurosat-albumentations/preprocessor_config.json\n***** Running Evaluation *****\n  Num examples = 2700\n  Batch size = 32\nSaving model checkpoint to convnext-tiny-224-finetuned-eurosat-albumentations/checkpoint-570\nConfiguration saved in convnext-tiny-224-finetuned-eurosat-albumentations/checkpoint-570/config.json\nModel weights saved in convnext-tiny-224-finetuned-eurosat-albumentations/checkpoint-570/pytorch_model.bin\nFeature extractor saved in convnext-tiny-224-finetuned-eurosat-albumentations/checkpoint-570/preprocessor_config.json\nFeature extractor saved in convnext-tiny-224-finetuned-eurosat-albumentations/preprocessor_config.json\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from convnext-tiny-224-finetuned-eurosat-albumentations/checkpoint-570 (score: 0.9748148148148148).\n\n\n\n\n\nTrainOutput(global_step=570, training_loss=0.34729809766275843, metrics={'train_runtime': 961.6293, 'train_samples_per_second': 75.809, 'train_steps_per_second': 0.593, 'total_flos': 1.8322098956292096e+18, 'train_loss': 0.34729809766275843, 'epoch': 3.0})\n</code></pre> <p>We can check with the <code>evaluate</code> method that our <code>Trainer</code> did reload the best model properly (if it was not the last one):</p> Python<pre><code>metrics = trainer.evaluate()\nprint(metrics)\n</code></pre> <pre><code>***** Running Evaluation *****\n  Num examples = 2700\n  Batch size = 32\n</code></pre>    [85/85 00:12]  <pre><code>{'eval_loss': 0.0727163776755333, 'eval_accuracy': 0.9748148148148148, 'eval_runtime': 13.0419, 'eval_samples_per_second': 207.026, 'eval_steps_per_second': 6.517, 'epoch': 3.0}\n</code></pre> <p>You can now upload the result of the training to the Hub, just execute this instruction (note that the Trainer will automatically create a model card for you, as well as adding Tensorboard metrics - see the \"Training metrics\" tab!):</p> Python<pre><code>trainer.push_to_hub()\n</code></pre> <pre><code>Saving model checkpoint to convnext-tiny-224-finetuned-eurosat-albumentations\nConfiguration saved in convnext-tiny-224-finetuned-eurosat-albumentations/config.json\nModel weights saved in convnext-tiny-224-finetuned-eurosat-albumentations/pytorch_model.bin\nFeature extractor saved in convnext-tiny-224-finetuned-eurosat-albumentations/preprocessor_config.json\n\n\n\nUpload file runs/Apr12_12-03-24_1ad162e1ead9/events.out.tfevents.1649765159.1ad162e1ead9.73.4:  24%|##4       \u2026\n\n\n\nUpload file runs/Apr12_12-03-24_1ad162e1ead9/events.out.tfevents.1649767032.1ad162e1ead9.73.6: 100%|##########\u2026\n\n\nTo https://huggingface.co/nielsr/convnext-tiny-224-finetuned-eurosat-albumentations\n   c500b3f..2143b42  main -&gt; main\n\nTo https://huggingface.co/nielsr/convnext-tiny-224-finetuned-eurosat-albumentations\n   2143b42..71339cf  main -&gt; main\n\n\n\n\n\n\n'https://huggingface.co/nielsr/convnext-tiny-224-finetuned-eurosat-albumentations/commit/2143b423b5cacdde6daebd3ee2b5971ecab463f6'\n</code></pre> <p>You can now share this model with all your friends, family, favorite pets: they can all load it with the identifier <code>\"your-username/the-name-you-picked\"</code> so for instance:</p> Python<pre><code>from transformers import AutoModelForImageClassification, AutoImageProcessor\n\nimage_processor = AutoImageProcessor.from_pretrained(\"nielsr/my-awesome-model\")\nmodel = AutoModelForImageClassification.from_pretrained(\"nielsr/my-awesome-model\")\n</code></pre>"},{"location":"integrations/huggingface/image_classification_albumentations/#inference","title":"Inference","text":"<p>Let's say you have a new image, on which you'd like to make a prediction. Let's load a satellite image of a highway (that's not part of the EuroSAT dataset), and see how the model does.</p> Python<pre><code>from PIL import Image\nimport requests\n\nurl = 'https://huggingface.co/nielsr/convnext-tiny-224-finetuned-eurosat-albumentations/resolve/main/highway.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\nimage\n</code></pre> <p></p> <p>We'll load the image processor and model from the hub (here, we use the Auto Classes, which will make sure the appropriate classes will be loaded automatically based on the <code>config.json</code> and <code>preprocessor_config.json</code> files of the repo on the hub):</p> Python<pre><code>from transformers import AutoModelForImageClassification, AutoImageProcessor\n\nrepo_name = \"nielsr/convnext-tiny-224-finetuned-eurosat-albumentations\"\n\nimage_processor = AutoImageProcessor.from_pretrained(repo_name)\nmodel = AutoModelForImageClassification.from_pretrained(repo_name)\n</code></pre> <pre><code>https://huggingface.co/nielsr/convnext-tiny-224-finetuned-eurosat-albumentations/resolve/main/preprocessor_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp04g0zg5n\n\n\n\nDownloading:   0%|          | 0.00/266 [00:00&lt;?, ?B/s]\n\n\nstoring https://huggingface.co/nielsr/convnext-tiny-224-finetuned-eurosat-albumentations/resolve/main/preprocessor_config.json in cache at /root/.cache/huggingface/transformers/38b41a2c904b6ce5bb10403bf902ee4263144d862c5a602c83cd120c0c1ba0e6.37be7274d6b5860aee104bb1fbaeb0722fec3850a85bb2557ae9491f17f89433\ncreating metadata file for /root/.cache/huggingface/transformers/38b41a2c904b6ce5bb10403bf902ee4263144d862c5a602c83cd120c0c1ba0e6.37be7274d6b5860aee104bb1fbaeb0722fec3850a85bb2557ae9491f17f89433\nloading feature extractor configuration file https://huggingface.co/nielsr/convnext-tiny-224-finetuned-eurosat-albumentations/resolve/main/preprocessor_config.json from cache at /root/.cache/huggingface/transformers/38b41a2c904b6ce5bb10403bf902ee4263144d862c5a602c83cd120c0c1ba0e6.37be7274d6b5860aee104bb1fbaeb0722fec3850a85bb2557ae9491f17f89433\nFeature extractor ConvNextFeatureExtractor {\n  \"crop_pct\": 0.875,\n  \"do_normalize\": true,\n  \"do_resize\": true,\n  \"feature_extractor_type\": \"ConvNextFeatureExtractor\",\n  \"image_mean\": [\n    0.485,\n    0.456,\n    0.406\n  ],\n  \"image_std\": [\n    0.229,\n    0.224,\n    0.225\n  ],\n  \"resample\": 3,\n  \"size\": 224\n}\n\nhttps://huggingface.co/nielsr/convnext-tiny-224-finetuned-eurosat-albumentations/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpbf9y4q39\n\n\n\nDownloading:   0%|          | 0.00/1.03k [00:00&lt;?, ?B/s]\n\n\nstoring https://huggingface.co/nielsr/convnext-tiny-224-finetuned-eurosat-albumentations/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/25088566ab29cf0ff360b05880b5f20cdc0c79ab995056a1fb4f98212d021154.4637c3f271a8dfbcfe5c4ee777270112d841a5af95814f0fd086c3c2761e7370\ncreating metadata file for /root/.cache/huggingface/transformers/25088566ab29cf0ff360b05880b5f20cdc0c79ab995056a1fb4f98212d021154.4637c3f271a8dfbcfe5c4ee777270112d841a5af95814f0fd086c3c2761e7370\nloading configuration file https://huggingface.co/nielsr/convnext-tiny-224-finetuned-eurosat-albumentations/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/25088566ab29cf0ff360b05880b5f20cdc0c79ab995056a1fb4f98212d021154.4637c3f271a8dfbcfe5c4ee777270112d841a5af95814f0fd086c3c2761e7370\nModel config ConvNextConfig {\n  \"_name_or_path\": \"nielsr/convnext-tiny-224-finetuned-eurosat-albumentations\",\n  \"architectures\": [\n    \"ConvNextForImageClassification\"\n  ],\n  \"depths\": [\n    3,\n    3,\n    9,\n    3\n  ],\n  \"drop_path_rate\": 0.0,\n  \"hidden_act\": \"gelu\",\n  \"hidden_sizes\": [\n    96,\n    192,\n    384,\n    768\n  ],\n  \"id2label\": {\n    \"0\": \"AnnualCrop\",\n    \"1\": \"Forest\",\n    \"2\": \"HerbaceousVegetation\",\n    \"3\": \"Highway\",\n    \"4\": \"Industrial\",\n    \"5\": \"Pasture\",\n    \"6\": \"PermanentCrop\",\n    \"7\": \"Residential\",\n    \"8\": \"River\",\n    \"9\": \"SeaLake\"\n  },\n  \"image_size\": 224,\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"AnnualCrop\": 0,\n    \"Forest\": 1,\n    \"HerbaceousVegetation\": 2,\n    \"Highway\": 3,\n    \"Industrial\": 4,\n    \"Pasture\": 5,\n    \"PermanentCrop\": 6,\n    \"Residential\": 7,\n    \"River\": 8,\n    \"SeaLake\": 9\n  },\n  \"layer_norm_eps\": 1e-12,\n  \"layer_scale_init_value\": 1e-06,\n  \"model_type\": \"convnext\",\n  \"num_channels\": 3,\n  \"num_stages\": 4,\n  \"patch_size\": 4,\n  \"problem_type\": \"single_label_classification\",\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.18.0\"\n}\n\nhttps://huggingface.co/nielsr/convnext-tiny-224-finetuned-eurosat-albumentations/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpzr_9yxjo\n\n\n\nDownloading:   0%|          | 0.00/106M [00:00&lt;?, ?B/s]\n\n\nstoring https://huggingface.co/nielsr/convnext-tiny-224-finetuned-eurosat-albumentations/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/3f4bcce35d3279d19b07fb762859d89bce636d8f0235685031ef6494800b9769.d611c768c0b0939188b05c3d505f0b36c97aa57649d4637e3384992d3c5c0b89\ncreating metadata file for /root/.cache/huggingface/transformers/3f4bcce35d3279d19b07fb762859d89bce636d8f0235685031ef6494800b9769.d611c768c0b0939188b05c3d505f0b36c97aa57649d4637e3384992d3c5c0b89\nloading weights file https://huggingface.co/nielsr/convnext-tiny-224-finetuned-eurosat-albumentations/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/3f4bcce35d3279d19b07fb762859d89bce636d8f0235685031ef6494800b9769.d611c768c0b0939188b05c3d505f0b36c97aa57649d4637e3384992d3c5c0b89\nAll model checkpoint weights were used when initializing ConvNextForImageClassification.\n\nAll the weights of ConvNextForImageClassification were initialized from the model checkpoint at nielsr/convnext-tiny-224-finetuned-eurosat-albumentations.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use ConvNextForImageClassification for predictions without further training.\n</code></pre> Python<pre><code># prepare image for the model\nencoding = image_processor(image.convert(\"RGB\"), return_tensors=\"pt\")\nprint(encoding.pixel_values.shape)\n</code></pre> <pre><code>torch.Size([1, 3, 224, 224])\n</code></pre> Python<pre><code>import torch\n\n# forward pass\nwith torch.no_grad():\n    outputs = model(**encoding)\n    logits = outputs.logits\n</code></pre> Python<pre><code>predicted_class_idx = logits.argmax(-1).item()\nprint(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n</code></pre> <pre><code>Predicted class: Highway\n</code></pre> <p>Looks like our model got it correct! </p>"},{"location":"integrations/huggingface/image_classification_albumentations/#pipeline-api","title":"Pipeline API","text":"<p>An alternative way to quickly perform inference with any model on the hub is by leveraging the Pipeline API, which abstracts away all the steps we did manually above for us. It will perform the preprocessing, forward pass and postprocessing all in a single object. </p> <p>Let's showcase this for our trained model:</p> Python<pre><code>from transformers import pipeline\n\npipe = pipeline(\"image-classification\", \"nielsr/convnext-tiny-224-finetuned-eurosat-albumentations\")\n</code></pre> <pre><code>loading configuration file https://huggingface.co/nielsr/convnext-tiny-224-finetuned-eurosat-albumentations/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/25088566ab29cf0ff360b05880b5f20cdc0c79ab995056a1fb4f98212d021154.4637c3f271a8dfbcfe5c4ee777270112d841a5af95814f0fd086c3c2761e7370\nModel config ConvNextConfig {\n  \"_name_or_path\": \"nielsr/convnext-tiny-224-finetuned-eurosat-albumentations\",\n  \"architectures\": [\n    \"ConvNextForImageClassification\"\n  ],\n  \"depths\": [\n    3,\n    3,\n    9,\n    3\n  ],\n  \"drop_path_rate\": 0.0,\n  \"hidden_act\": \"gelu\",\n  \"hidden_sizes\": [\n    96,\n    192,\n    384,\n    768\n  ],\n  \"id2label\": {\n    \"0\": \"AnnualCrop\",\n    \"1\": \"Forest\",\n    \"2\": \"HerbaceousVegetation\",\n    \"3\": \"Highway\",\n    \"4\": \"Industrial\",\n    \"5\": \"Pasture\",\n    \"6\": \"PermanentCrop\",\n    \"7\": \"Residential\",\n    \"8\": \"River\",\n    \"9\": \"SeaLake\"\n  },\n  \"image_size\": 224,\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"AnnualCrop\": 0,\n    \"Forest\": 1,\n    \"HerbaceousVegetation\": 2,\n    \"Highway\": 3,\n    \"Industrial\": 4,\n    \"Pasture\": 5,\n    \"PermanentCrop\": 6,\n    \"Residential\": 7,\n    \"River\": 8,\n    \"SeaLake\": 9\n  },\n  \"layer_norm_eps\": 1e-12,\n  \"layer_scale_init_value\": 1e-06,\n  \"model_type\": \"convnext\",\n  \"num_channels\": 3,\n  \"num_stages\": 4,\n  \"patch_size\": 4,\n  \"problem_type\": \"single_label_classification\",\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.18.0\"\n}\n\nloading configuration file https://huggingface.co/nielsr/convnext-tiny-224-finetuned-eurosat-albumentations/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/25088566ab29cf0ff360b05880b5f20cdc0c79ab995056a1fb4f98212d021154.4637c3f271a8dfbcfe5c4ee777270112d841a5af95814f0fd086c3c2761e7370\nModel config ConvNextConfig {\n  \"_name_or_path\": \"nielsr/convnext-tiny-224-finetuned-eurosat-albumentations\",\n  \"architectures\": [\n    \"ConvNextForImageClassification\"\n  ],\n  \"depths\": [\n    3,\n    3,\n    9,\n    3\n  ],\n  \"drop_path_rate\": 0.0,\n  \"hidden_act\": \"gelu\",\n  \"hidden_sizes\": [\n    96,\n    192,\n    384,\n    768\n  ],\n  \"id2label\": {\n    \"0\": \"AnnualCrop\",\n    \"1\": \"Forest\",\n    \"2\": \"HerbaceousVegetation\",\n    \"3\": \"Highway\",\n    \"4\": \"Industrial\",\n    \"5\": \"Pasture\",\n    \"6\": \"PermanentCrop\",\n    \"7\": \"Residential\",\n    \"8\": \"River\",\n    \"9\": \"SeaLake\"\n  },\n  \"image_size\": 224,\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"AnnualCrop\": 0,\n    \"Forest\": 1,\n    \"HerbaceousVegetation\": 2,\n    \"Highway\": 3,\n    \"Industrial\": 4,\n    \"Pasture\": 5,\n    \"PermanentCrop\": 6,\n    \"Residential\": 7,\n    \"River\": 8,\n    \"SeaLake\": 9\n  },\n  \"layer_norm_eps\": 1e-12,\n  \"layer_scale_init_value\": 1e-06,\n  \"model_type\": \"convnext\",\n  \"num_channels\": 3,\n  \"num_stages\": 4,\n  \"patch_size\": 4,\n  \"problem_type\": \"single_label_classification\",\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.18.0\"\n}\n\nloading weights file https://huggingface.co/nielsr/convnext-tiny-224-finetuned-eurosat-albumentations/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/3f4bcce35d3279d19b07fb762859d89bce636d8f0235685031ef6494800b9769.d611c768c0b0939188b05c3d505f0b36c97aa57649d4637e3384992d3c5c0b89\nAll model checkpoint weights were used when initializing ConvNextForImageClassification.\n\nAll the weights of ConvNextForImageClassification were initialized from the model checkpoint at nielsr/convnext-tiny-224-finetuned-eurosat-albumentations.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use ConvNextForImageClassification for predictions without further training.\nloading feature extractor configuration file https://huggingface.co/nielsr/convnext-tiny-224-finetuned-eurosat-albumentations/resolve/main/preprocessor_config.json from cache at /root/.cache/huggingface/transformers/38b41a2c904b6ce5bb10403bf902ee4263144d862c5a602c83cd120c0c1ba0e6.37be7274d6b5860aee104bb1fbaeb0722fec3850a85bb2557ae9491f17f89433\nFeature extractor ConvNextFeatureExtractor {\n  \"crop_pct\": 0.875,\n  \"do_normalize\": true,\n  \"do_resize\": true,\n  \"feature_extractor_type\": \"ConvNextFeatureExtractor\",\n  \"image_mean\": [\n    0.485,\n    0.456,\n    0.406\n  ],\n  \"image_std\": [\n    0.229,\n    0.224,\n    0.225\n  ],\n  \"resample\": 3,\n  \"size\": 224\n}\n</code></pre> Python<pre><code>pipe(image)\n</code></pre> <pre><code>[{'label': 'Highway', 'score': 0.5163754224777222},\n {'label': 'River', 'score': 0.11824000626802444},\n {'label': 'AnnualCrop', 'score': 0.05467210337519646},\n {'label': 'PermanentCrop', 'score': 0.05066365748643875},\n {'label': 'Industrial', 'score': 0.049283623695373535}]\n</code></pre> <p>As we can see, it does not only show the class label with the highest probability, but does return the top 5 labels, with their corresponding scores. Note that the pipelines also work with local models and image_processor:</p> Python<pre><code>pipe = pipeline(\"image-classification\", \n                model=model,\n                feature_extractor=image_processor)\n</code></pre> Python<pre><code>pipe(image)\n</code></pre> <pre><code>[{'label': 'Highway', 'score': 0.5163754224777222},\n {'label': 'River', 'score': 0.11824000626802444},\n {'label': 'AnnualCrop', 'score': 0.05467210337519646},\n {'label': 'PermanentCrop', 'score': 0.05066365748643875},\n {'label': 'Industrial', 'score': 0.049283623695373535}]\n</code></pre> Python<pre><code>\n</code></pre>"},{"location":"integrations/huggingface/object_detection/","title":"Hugging Face Object Detection","text":""},{"location":"integrations/huggingface/object_detection/#object-detection","title":"Object detection","text":"<p>Object detection is the computer vision task of detecting instances (such as humans, buildings, or cars) in an image. Object detection models receive an image as input and output coordinates of the bounding boxes and associated labels of the detected objects. An image can contain multiple objects, each with its own bounding box and a label (e.g. it can have a car and a building), and each object can be present in different parts of an image (e.g. the image can have several cars). This task is commonly used in autonomous driving for detecting things like pedestrians, road signs, and traffic lights. Other applications include counting objects in images, image search, and more.</p> <p>In this guide, you will learn how to:</p> <ol> <li>Finetune DETR, a model that combines a convolutional  backbone with an encoder-decoder Transformer, on the CPPE-5  dataset.</li> <li>Use your finetuned model for inference.</li> </ol> <p> <p>To see all architectures and checkpoints compatible with this task, we recommend checking the task-page</p> <p></p> <p>Before you begin, make sure you have all the necessary libraries installed:</p> Bash<pre><code>pip install -q datasets transformers accelerate timm\npip install -q -U albumentations&gt;=1.4.5 torchmetrics pycocotools\n</code></pre> <p>You'll use \ud83e\udd17 Datasets to load a dataset from the Hugging Face Hub, \ud83e\udd17 Transformers to train your model, and <code>albumentations</code> to augment the data.</p> <p>We encourage you to share your model with the community. Log in to your Hugging Face account to upload it to the Hub. When prompted, enter your token to log in:</p> Python<pre><code>&gt;&gt;&gt; from huggingface_hub import notebook_login\n\n&gt;&gt;&gt; notebook_login()\n</code></pre> <p>To get started, we'll define global constants, namely the model name and image size. For this tutorial, we'll use the conditional DETR model due to its faster convergence. Feel free to select any object detection model available in the <code>transformers</code> library.</p> Python<pre><code>&gt;&gt;&gt; MODEL_NAME = \"microsoft/conditional-detr-resnet-50\"  # or \"facebook/detr-resnet-50\"\n&gt;&gt;&gt; IMAGE_SIZE = 480\n</code></pre>"},{"location":"integrations/huggingface/object_detection/#load-the-cppe-5-dataset","title":"Load the CPPE-5 dataset","text":"<p>The CPPE-5 dataset contains images with annotations identifying medical personal protective equipment (PPE) in the context of the COVID-19 pandemic.</p> <p>Start by loading the dataset and creating a <code>validation</code> split from <code>train</code>:</p> Python<pre><code>&gt;&gt;&gt; from datasets import load_dataset\n\n&gt;&gt;&gt; cppe5 = load_dataset(\"cppe-5\")\n\n&gt;&gt;&gt; if \"validation\" not in cppe5:\n...     split = cppe5[\"train\"].train_test_split(0.15, seed=1337)\n...     cppe5[\"train\"] = split[\"train\"]\n...     cppe5[\"validation\"] = split[\"test\"]\n\n&gt;&gt;&gt; cppe5\nDatasetDict({\n    train: Dataset({\n        features: ['image_id', 'image', 'width', 'height', 'objects'],\n        num_rows: 850\n    })\n    test: Dataset({\n        features: ['image_id', 'image', 'width', 'height', 'objects'],\n        num_rows: 29\n    })\n    validation: Dataset({\n        features: ['image_id', 'image', 'width', 'height', 'objects'],\n        num_rows: 150\n    })\n})\n</code></pre> <p>You'll see that this dataset has 1000 images for train and validation sets and a test set with 29 images.</p> <p>To get familiar with the data, explore what the examples look like.</p> Python<pre><code>&gt;&gt;&gt; cppe5[\"train\"][0]\n{\n  'image_id': 366,\n  'image': &lt;PIL.PngImagePlugin.PngImageFile image mode=RGBA size=500x290&gt;,\n  'width': 500,\n  'height': 500,\n  'objects': {\n    'id': [1932, 1933, 1934],\n    'area': [27063, 34200, 32431],\n    'bbox': [[29.0, 11.0, 97.0, 279.0],\n      [201.0, 1.0, 120.0, 285.0],\n      [382.0, 0.0, 113.0, 287.0]],\n    'category': [0, 0, 0]\n  }\n}\n</code></pre> <p>The examples in the dataset have the following fields: - <code>image_id</code>: the example image id - <code>image</code>: a <code>PIL.Image.Image</code> object containing the image - <code>width</code>: width of the image - <code>height</code>: height of the image - <code>objects</code>: a dictionary containing bounding box metadata for the objects in the image:   - <code>id</code>: the annotation id   - <code>area</code>: the area of the bounding box   - <code>bbox</code>: the object's bounding box (in the COCO format )   - <code>category</code>: the object's category, with possible values including <code>Coverall (0)</code>, <code>Face_Shield (1)</code>, <code>Gloves (2)</code>, <code>Goggles (3)</code> and <code>Mask (4)</code></p> <p>You may notice that the <code>bbox</code> field follows the COCO format, which is the format that the DETR model expects. However, the grouping of the fields inside <code>objects</code> differs from the annotation format DETR requires. You will need to apply some preprocessing transformations before using this data for training.</p> <p>To get an even better understanding of the data, visualize an example in the dataset.</p> Python<pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import os\n&gt;&gt;&gt; from PIL import Image, ImageDraw\n\n&gt;&gt;&gt; image = cppe5[\"train\"][2][\"image\"]\n&gt;&gt;&gt; annotations = cppe5[\"train\"][2][\"objects\"]\n&gt;&gt;&gt; draw = ImageDraw.Draw(image)\n\n&gt;&gt;&gt; categories = cppe5[\"train\"].features[\"objects\"].feature[\"category\"].names\n\n&gt;&gt;&gt; id2label = {index: x for index, x in enumerate(categories, start=0)}\n&gt;&gt;&gt; label2id = {v: k for k, v in id2label.items()}\n\n&gt;&gt;&gt; for i in range(len(annotations[\"id\"])):\n...     box = annotations[\"bbox\"][i]\n...     class_idx = annotations[\"category\"][i]\n...     x, y, w, h = tuple(box)\n...     # Check if coordinates are normalized or not\n...     if max(box) &gt; 1.0:\n...         # Coordinates are un-normalized, no need to re-scale them\n...         x1, y1 = int(x), int(y)\n...         x2, y2 = int(x + w), int(y + h)\n...     else:\n...         # Coordinates are normalized, re-scale them\n...         x1 = int(x * width)\n...         y1 = int(y * height)\n...         x2 = int((x + w) * width)\n...         y2 = int((y + h) * height)\n...     draw.rectangle((x, y, x + w, y + h), outline=\"red\", width=1)\n...     draw.text((x, y), id2label[class_idx], fill=\"white\")\n\n&gt;&gt;&gt; image\n</code></pre> <p>To visualize the bounding boxes with associated labels, you can get the labels from the dataset's metadata, specifically the <code>category</code> field. You'll also want to create dictionaries that map a label id to a label class (<code>id2label</code>) and the other way around (<code>label2id</code>). You can use them later when setting up the model. Including these maps will make your model reusable by others if you share it on the Hugging Face Hub. Please note that, the part of above code that draws the bounding boxes assume that it is in <code>COCO</code> format <code>(x_min, y_min, width, height)</code>. It has to be adjusted to work for other formats like <code>(x_min, y_min, x_max, y_max)</code>.</p> <p>As a final step of getting familiar with the data, explore it for potential issues. One common problem with datasets for object detection is bounding boxes that \"stretch\" beyond the edge of the image. Such \"runaway\" bounding boxes can raise errors during training and should be addressed. There are a few examples with this issue in this dataset. To keep things simple in this guide, we will set <code>clip=True</code> for <code>BboxParams</code> in transformations below.</p>"},{"location":"integrations/huggingface/object_detection/#preprocess-the-data","title":"Preprocess the data","text":"<p>To finetune a model, you must preprocess the data you plan to use to match precisely the approach used for the pre-trained model. [<code>AutoImageProcessor</code>] takes care of processing image data to create <code>pixel_values</code>, <code>pixel_mask</code>, and <code>labels</code> that a DETR model can train with. The image processor has some attributes that you won't have to worry about:</p> <ul> <li><code>image_mean = [0.485, 0.456, 0.406 ]</code></li> <li><code>image_std = [0.229, 0.224, 0.225]</code></li> </ul> <p>These are the mean and standard deviation used to normalize images during the model pre-training. These values are crucial to replicate when doing inference or finetuning a pre-trained image model.</p> <p>Instantiate the image processor from the same checkpoint as the model you want to finetune.</p> Python<pre><code>&gt;&gt;&gt; from transformers import AutoImageProcessor\n\n&gt;&gt;&gt; MAX_SIZE = IMAGE_SIZE\n\n&gt;&gt;&gt; image_processor = AutoImageProcessor.from_pretrained(\n...     MODEL_NAME,\n...     do_resize=True,\n...     size={\"max_height\": MAX_SIZE, \"max_width\": MAX_SIZE},\n...     do_pad=True,\n...     pad_size={\"height\": MAX_SIZE, \"width\": MAX_SIZE},\n... )\n</code></pre> <p>Before passing the images to the <code>image_processor</code>, apply two preprocessing transformations to the dataset: - Augmenting images - Reformatting annotations to meet DETR expectations</p> <p>First, to make sure the model does not overfit on the training data, you can apply image augmentation with any data augmentation library. Here we use Albumentations. This library ensures that transformations affect the image and update the bounding boxes accordingly. The \ud83e\udd17 Datasets library documentation has a detailed guide on how to augment images for object detection, and it uses the exact same dataset as an example. Apply some geometric and color transformations to the image. For additional augmentation options, explore the Albumentations Demo Space.</p> Python<pre><code>&gt;&gt;&gt; import albumentations as A\n\n&gt;&gt;&gt; train_augment_and_transform = A.Compose(\n...     [\n...         A.Perspective(p=0.1),\n...         A.HorizontalFlip(p=0.5),\n...         A.RandomBrightnessContrast(p=0.5),\n...         A.HueSaturationValue(p=0.1),\n...     ],\n...     bbox_params=A.BboxParams(format=\"coco\", label_fields=[\"category\"], clip=True, min_area=25),\n... )\n\n&gt;&gt;&gt; validation_transform = A.Compose(\n...     [A.NoOp()],\n...     bbox_params=A.BboxParams(format=\"coco\", label_fields=[\"category\"], clip=True),\n... )\n</code></pre> <p>The <code>image_processor</code> expects the annotations to be in the following format: <code>{'image_id': int, 'annotations': List[Dict]}</code>,  where each dictionary is a COCO object annotation. Let's add a function to reformat annotations for a single example:</p> Python<pre><code>&gt;&gt;&gt; def format_image_annotations_as_coco(image_id, categories, areas, bboxes):\n...     \"\"\"Format one set of image annotations to the COCO format\n\n...     Args:\n...         image_id (str): image id. e.g. \"0001\"\n...         categories (List[int]): list of categories/class labels corresponding to provided bounding boxes\n...         areas (List[float]): list of corresponding areas to provided bounding boxes\n...         bboxes (List[Tuple[float]]): list of bounding boxes provided in COCO format\n...             ([center_x, center_y, width, height] in absolute coordinates)\n\n...     Returns:\n...         dict: {\n...             \"image_id\": image id,\n...             \"annotations\": list of formatted annotations\n...         }\n...     \"\"\"\n...     annotations = []\n...     for category, area, bbox in zip(categories, areas, bboxes):\n...         formatted_annotation = {\n...             \"image_id\": image_id,\n...             \"category_id\": category,\n...             \"iscrowd\": 0,\n...             \"area\": area,\n...             \"bbox\": list(bbox),\n...         }\n...         annotations.append(formatted_annotation)\n\n...     return {\n...         \"image_id\": image_id,\n...         \"annotations\": annotations,\n...     }\n</code></pre> <p>Now you can combine the image and annotation transformations to use on a batch of examples:</p> Python<pre><code>&gt;&gt;&gt; def augment_and_transform_batch(examples, transform, image_processor, return_pixel_mask=False):\n...     \"\"\"Apply augmentations and format annotations in COCO format for object detection task\"\"\"\n\n...     images = []\n...     annotations = []\n...     for image_id, image, objects in zip(examples[\"image_id\"], examples[\"image\"], examples[\"objects\"]):\n...         image = np.array(image.convert(\"RGB\"))\n\n...         # apply augmentations\n...         output = transform(image=image, bboxes=objects[\"bbox\"], category=objects[\"category\"])\n...         images.append(output[\"image\"])\n\n...         # format annotations in COCO format\n...         formatted_annotations = format_image_annotations_as_coco(\n...             image_id, output[\"category\"], objects[\"area\"], output[\"bboxes\"]\n...         )\n...         annotations.append(formatted_annotations)\n\n...     # Apply the image processor transformations: resizing, rescaling, normalization\n...     result = image_processor(images=images, annotations=annotations, return_tensors=\"pt\")\n\n...     if not return_pixel_mask:\n...         result.pop(\"pixel_mask\", None)\n\n...     return result\n</code></pre> <p>Apply this preprocessing function to the entire dataset using \ud83e\udd17 Datasets [<code>~datasets.Dataset.with_transform</code>] method. This method applies transformations on the fly when you load an element of the dataset.</p> <p>At this point, you can check what an example from the dataset looks like after the transformations. You should see a tensor with <code>pixel_values</code>, a tensor with <code>pixel_mask</code>, and <code>labels</code>.</p> Python<pre><code>&gt;&gt;&gt; from functools import partial\n\n&gt;&gt;&gt; # Make transform functions for batch and apply for dataset splits\n&gt;&gt;&gt; train_transform_batch = partial(\n...     augment_and_transform_batch, transform=train_augment_and_transform, image_processor=image_processor\n... )\n&gt;&gt;&gt; validation_transform_batch = partial(\n...     augment_and_transform_batch, transform=validation_transform, image_processor=image_processor\n... )\n\n&gt;&gt;&gt; cppe5[\"train\"] = cppe5[\"train\"].with_transform(train_transform_batch)\n&gt;&gt;&gt; cppe5[\"validation\"] = cppe5[\"validation\"].with_transform(validation_transform_batch)\n&gt;&gt;&gt; cppe5[\"test\"] = cppe5[\"test\"].with_transform(validation_transform_batch)\n\n&gt;&gt;&gt; cppe5[\"train\"][15]\n{'pixel_values': tensor([[[ 1.9235,  1.9407,  1.9749,  ..., -0.7822, -0.7479, -0.6965],\n          [ 1.9578,  1.9749,  1.9920,  ..., -0.7993, -0.7650, -0.7308],\n          [ 2.0092,  2.0092,  2.0263,  ..., -0.8507, -0.8164, -0.7822],\n          ...,\n          [ 0.0741,  0.0741,  0.0741,  ...,  0.0741,  0.0741,  0.0741],\n          [ 0.0741,  0.0741,  0.0741,  ...,  0.0741,  0.0741,  0.0741],\n          [ 0.0741,  0.0741,  0.0741,  ...,  0.0741,  0.0741,  0.0741]],\n\n          [[ 1.6232,  1.6408,  1.6583,  ...,  0.8704,  1.0105,  1.1331],\n          [ 1.6408,  1.6583,  1.6758,  ...,  0.8529,  0.9930,  1.0980],\n          [ 1.6933,  1.6933,  1.7108,  ...,  0.8179,  0.9580,  1.0630],\n          ...,\n          [ 0.2052,  0.2052,  0.2052,  ...,  0.2052,  0.2052,  0.2052],\n          [ 0.2052,  0.2052,  0.2052,  ...,  0.2052,  0.2052,  0.2052],\n          [ 0.2052,  0.2052,  0.2052,  ...,  0.2052,  0.2052,  0.2052]],\n\n          [[ 1.8905,  1.9080,  1.9428,  ..., -0.1487, -0.0964, -0.0615],\n          [ 1.9254,  1.9428,  1.9603,  ..., -0.1661, -0.1138, -0.0790],\n          [ 1.9777,  1.9777,  1.9951,  ..., -0.2010, -0.1138, -0.0790],\n          ...,\n          [ 0.4265,  0.4265,  0.4265,  ...,  0.4265,  0.4265,  0.4265],\n          [ 0.4265,  0.4265,  0.4265,  ...,  0.4265,  0.4265,  0.4265],\n          [ 0.4265,  0.4265,  0.4265,  ...,  0.4265,  0.4265,  0.4265]]]),\n  'labels': {'image_id': tensor([688]), 'class_labels': tensor([3, 4, 2, 0, 0]), 'boxes': tensor([[0.4700, 0.1933, 0.1467, 0.0767],\n          [0.4858, 0.2600, 0.1150, 0.1000],\n          [0.4042, 0.4517, 0.1217, 0.1300],\n          [0.4242, 0.3217, 0.3617, 0.5567],\n          [0.6617, 0.4033, 0.5400, 0.4533]]), 'area': tensor([ 4048.,  4140.,  5694., 72478., 88128.]), 'iscrowd': tensor([0, 0, 0, 0, 0]), 'orig_size': tensor([480, 480])}}\n</code></pre> <p>You have successfully augmented the individual images and prepared their annotations. However, preprocessing isn't complete yet. In the final step, create a custom <code>collate_fn</code> to batch images together. Pad images (which are now <code>pixel_values</code>) to the largest image in a batch, and create a corresponding <code>pixel_mask</code> to indicate which pixels are real (1) and which are padding (0).</p> Python<pre><code>&gt;&gt;&gt; import torch\n\n&gt;&gt;&gt; def collate_fn(batch):\n...     data = {}\n...     data[\"pixel_values\"] = torch.stack([x[\"pixel_values\"] for x in batch])\n...     data[\"labels\"] = [x[\"labels\"] for x in batch]\n...     if \"pixel_mask\" in batch[0]:\n...         data[\"pixel_mask\"] = torch.stack([x[\"pixel_mask\"] for x in batch])\n...     return data\n</code></pre>"},{"location":"integrations/huggingface/object_detection/#preparing-function-to-compute-map","title":"Preparing function to compute mAP","text":"<p>Object detection models are commonly evaluated with a set of COCO-style metrics. We are going to use <code>torchmetrics</code> to compute <code>mAP</code> (mean average precision) and <code>mAR</code> (mean average recall) metrics and will wrap it to <code>compute_metrics</code> function in order to use in [<code>Trainer</code>] for evaluation.</p> <p>Intermediate format of boxes used for training is <code>YOLO</code> (normalized) but we will compute metrics for boxes in <code>Pascal VOC</code> (absolute) format in order to correctly handle box areas. Let's define a function that converts bounding boxes to <code>Pascal VOC</code> format:</p> Python<pre><code>&gt;&gt;&gt; from transformers.image_transforms import center_to_corners_format\n\n&gt;&gt;&gt; def convert_bbox_yolo_to_pascal(boxes, image_size):\n...     \"\"\"\n...     Convert bounding boxes from YOLO format (x_center, y_center, width, height) in range [0, 1]\n...     to Pascal VOC format (x_min, y_min, x_max, y_max) in absolute coordinates.\n\n...     Args:\n...         boxes (torch.Tensor): Bounding boxes in YOLO format\n...         image_size (Tuple[int, int]): Image size in format (height, width)\n\n...     Returns:\n...         torch.Tensor: Bounding boxes in Pascal VOC format (x_min, y_min, x_max, y_max)\n...     \"\"\"\n...     # convert center to corners format\n...     boxes = center_to_corners_format(boxes)\n\n...     # convert to absolute coordinates\n...     height, width = image_size\n...     boxes = boxes * torch.tensor([[width, height, width, height]])\n\n...     return boxes\n</code></pre> <p>Then, in <code>compute_metrics</code> function we collect <code>predicted</code> and <code>target</code> bounding boxes, scores and labels from evaluation loop results and pass it to the scoring function.</p> Python<pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from dataclasses import dataclass\n&gt;&gt;&gt; from torchmetrics.detection.mean_ap import MeanAveragePrecision\n\n\n&gt;&gt;&gt; @dataclass\n&gt;&gt;&gt; class ModelOutput:\n...     logits: torch.Tensor\n...     pred_boxes: torch.Tensor\n\n\n&gt;&gt;&gt; @torch.no_grad()\n&gt;&gt;&gt; def compute_metrics(evaluation_results, image_processor, threshold=0.0, id2label=None):\n...     \"\"\"\n...     Compute mean average mAP, mAR and their variants for the object detection task.\n\n...     Args:\n...         evaluation_results (EvalPrediction): Predictions and targets from evaluation.\n...         threshold (float, optional): Threshold to filter predicted boxes by confidence. Defaults to 0.0.\n...         id2label (Optional[dict], optional): Mapping from class id to class name. Defaults to None.\n\n...     Returns:\n...         Mapping[str, float]: Metrics in a form of dictionary {&lt;metric_name&gt;: &lt;metric_value&gt;}\n...     \"\"\"\n\n...     predictions, targets = evaluation_results.predictions, evaluation_results.label_ids\n\n...     # For metric computation we need to provide:\n...     #  - targets in a form of list of dictionaries with keys \"boxes\", \"labels\"\n...     #  - predictions in a form of list of dictionaries with keys \"boxes\", \"scores\", \"labels\"\n\n...     image_sizes = []\n...     post_processed_targets = []\n...     post_processed_predictions = []\n\n...     # Collect targets in the required format for metric computation\n...     for batch in targets:\n...         # collect image sizes, we will need them for predictions post processing\n...         batch_image_sizes = torch.tensor(np.array([x[\"orig_size\"] for x in batch]))\n...         image_sizes.append(batch_image_sizes)\n...         # collect targets in the required format for metric computation\n...         # boxes were converted to YOLO format needed for model training\n...         # here we will convert them to Pascal VOC format (x_min, y_min, x_max, y_max)\n...         for image_target in batch:\n...             boxes = torch.tensor(image_target[\"boxes\"])\n...             boxes = convert_bbox_yolo_to_pascal(boxes, image_target[\"orig_size\"])\n...             labels = torch.tensor(image_target[\"class_labels\"])\n...             post_processed_targets.append({\"boxes\": boxes, \"labels\": labels})\n\n...     # Collect predictions in the required format for metric computation,\n...     # model produce boxes in YOLO format, then image_processor convert them to Pascal VOC format\n...     for batch, target_sizes in zip(predictions, image_sizes):\n...         batch_logits, batch_boxes = batch[1], batch[2]\n...         output = ModelOutput(logits=torch.tensor(batch_logits), pred_boxes=torch.tensor(batch_boxes))\n...         post_processed_output = image_processor.post_process_object_detection(\n...             output, threshold=threshold, target_sizes=target_sizes\n...         )\n...         post_processed_predictions.extend(post_processed_output)\n\n...     # Compute metrics\n...     metric = MeanAveragePrecision(box_format=\"xyxy\", class_metrics=True)\n...     metric.update(post_processed_predictions, post_processed_targets)\n...     metrics = metric.compute()\n\n...     # Replace list of per class metrics with separate metric for each class\n...     classes = metrics.pop(\"classes\")\n...     map_per_class = metrics.pop(\"map_per_class\")\n...     mar_100_per_class = metrics.pop(\"mar_100_per_class\")\n...     for class_id, class_map, class_mar in zip(classes, map_per_class, mar_100_per_class):\n...         class_name = id2label[class_id.item()] if id2label is not None else class_id.item()\n...         metrics[f\"map_{class_name}\"] = class_map\n...         metrics[f\"mar_100_{class_name}\"] = class_mar\n\n...     metrics = {k: round(v.item(), 4) for k, v in metrics.items()}\n\n...     return metrics\n\n\n&gt;&gt;&gt; eval_compute_metrics_fn = partial(\n...     compute_metrics, image_processor=image_processor, id2label=id2label, threshold=0.0\n... )\n</code></pre>"},{"location":"integrations/huggingface/object_detection/#training-the-detection-model","title":"Training the detection model","text":"<p>You have done most of the heavy lifting in the previous sections, so now you are ready to train your model! The images in this dataset are still quite large, even after resizing. This means that finetuning this model will require at least one GPU.</p> <p>Training involves the following steps: 1. Load the model with [<code>AutoModelForObjectDetection</code>] using the same checkpoint as in the preprocessing. 2. Define your training hyperparameters in [<code>TrainingArguments</code>]. 3. Pass the training arguments to [<code>Trainer</code>] along with the model, dataset, image processor, and data collator. 4. Call [<code>~Trainer.train</code>] to finetune your model.</p> <p>When loading the model from the same checkpoint that you used for the preprocessing, remember to pass the <code>label2id</code> and <code>id2label</code> maps that you created earlier from the dataset's metadata. Additionally, we specify <code>ignore_mismatched_sizes=True</code> to replace the existing classification head with a new one.</p> Python<pre><code>&gt;&gt;&gt; from transformers import AutoModelForObjectDetection\n\n&gt;&gt;&gt; model = AutoModelForObjectDetection.from_pretrained(\n...     MODEL_NAME,\n...     id2label=id2label,\n...     label2id=label2id,\n...     ignore_mismatched_sizes=True,\n... )\n</code></pre> <p>In the [<code>TrainingArguments</code>] use <code>output_dir</code> to specify where to save your model, then configure hyperparameters as you see fit. For <code>num_train_epochs=30</code> training will take about 35 minutes in Google Colab T4 GPU, increase the number of epoch to get better results.</p> <p>Important notes:  - Do not remove unused columns because this will drop the image column. Without the image column, you can't create <code>pixel_values</code>. For this reason, set <code>remove_unused_columns</code> to <code>False</code>.  - Set <code>eval_do_concat_batches=False</code> to get proper evaluation results. Images have different number of target boxes, if batches are concatenated we will not be able to determine which boxes belongs to particular image.</p> <p>If you wish to share your model by pushing to the Hub, set <code>push_to_hub</code> to <code>True</code> (you must be signed in to Hugging Face to upload your model).</p> Python<pre><code>&gt;&gt;&gt; from transformers import TrainingArguments\n\n&gt;&gt;&gt; training_args = TrainingArguments(\n...     output_dir=\"detr_finetuned_cppe5\",\n...     num_train_epochs=30,\n...     fp16=False,\n...     per_device_train_batch_size=8,\n...     dataloader_num_workers=4,\n...     learning_rate=5e-5,\n...     lr_scheduler_type=\"cosine\",\n...     weight_decay=1e-4,\n...     max_grad_norm=0.01,\n...     metric_for_best_model=\"eval_map\",\n...     greater_is_better=True,\n...     load_best_model_at_end=True,\n...     eval_strategy=\"epoch\",\n...     save_strategy=\"epoch\",\n...     save_total_limit=2,\n...     remove_unused_columns=False,\n...     eval_do_concat_batches=False,\n...     push_to_hub=True,\n... )\n</code></pre> <p>Finally, bring everything together, and call [<code>~transformers.Trainer.train</code>]:</p> Python<pre><code>&gt;&gt;&gt; from transformers import Trainer\n\n&gt;&gt;&gt; trainer = Trainer(\n...     model=model,\n...     args=training_args,\n...     train_dataset=cppe5[\"train\"],\n...     eval_dataset=cppe5[\"validation\"],\n...     tokenizer=image_processor,\n...     data_collator=collate_fn,\n...     compute_metrics=eval_compute_metrics_fn,\n... )\n\n&gt;&gt;&gt; trainer.train()\n</code></pre>    [3210/3210 26:07, Epoch 30/30]  Epoch Training Loss Validation Loss Map Map 50 Map 75 Map Small Map Medium Map Large Mar 1 Mar 10 Mar 100 Mar Small Mar Medium Mar Large Map Coverall Mar 100 Coverall Map Face Shield Mar 100 Face Shield Map Gloves Mar 100 Gloves Map Goggles Mar 100 Goggles Map Mask Mar 100 Mask 1 No log 2.629903 0.008900 0.023200 0.006500 0.001300 0.002800 0.020500 0.021500 0.070400 0.101400 0.007600 0.106200 0.096100 0.036700 0.232000 0.000300 0.019000 0.003900 0.125400 0.000100 0.003100 0.003500 0.127600 2 No log 3.479864 0.014800 0.034600 0.010800 0.008600 0.011700 0.012500 0.041100 0.098700 0.130000 0.056000 0.062200 0.111900 0.053500 0.447300 0.010600 0.100000 0.000200 0.022800 0.000100 0.015400 0.009700 0.064400 3 No log 2.107622 0.041700 0.094000 0.034300 0.024100 0.026400 0.047400 0.091500 0.182800 0.225800 0.087200 0.199400 0.210600 0.150900 0.571200 0.017300 0.101300 0.007300 0.180400 0.002100 0.026200 0.031000 0.250200 4 No log 2.031242 0.055900 0.120600 0.046900 0.013800 0.038100 0.090300 0.105900 0.225600 0.266100 0.130200 0.228100 0.330000 0.191000 0.572100 0.010600 0.157000 0.014600 0.235300 0.001700 0.052300 0.061800 0.313800 5 3.889400 1.883433 0.089700 0.201800 0.067300 0.022800 0.065300 0.129500 0.136000 0.272200 0.303700 0.112900 0.312500 0.424600 0.300200 0.585100 0.032700 0.202500 0.031300 0.271000 0.008700 0.126200 0.075500 0.333800 6 3.889400 1.807503 0.118500 0.270900 0.090200 0.034900 0.076700 0.152500 0.146100 0.297800 0.325400 0.171700 0.283700 0.545900 0.396900 0.554500 0.043000 0.262000 0.054500 0.271900 0.020300 0.230800 0.077600 0.308000 7 3.889400 1.716169 0.143500 0.307700 0.123200 0.045800 0.097800 0.258300 0.165300 0.327700 0.352600 0.140900 0.336700 0.599400 0.442900 0.620700 0.069400 0.301300 0.081600 0.292000 0.011000 0.230800 0.112700 0.318200 8 3.889400 1.679014 0.153000 0.355800 0.127900 0.038700 0.115600 0.291600 0.176000 0.322500 0.349700 0.135600 0.326100 0.643700 0.431700 0.582900 0.069800 0.265800 0.088600 0.274600 0.028300 0.280000 0.146700 0.345300 9 3.889400 1.618239 0.172100 0.375300 0.137600 0.046100 0.141700 0.308500 0.194000 0.356200 0.386200 0.162400 0.359200 0.677700 0.469800 0.623900 0.102100 0.317700 0.099100 0.290200 0.029300 0.335400 0.160200 0.364000 10 1.599700 1.572512 0.179500 0.400400 0.147200 0.056500 0.141700 0.316700 0.213100 0.357600 0.381300 0.197900 0.344300 0.638500 0.466900 0.623900 0.101300 0.311400 0.104700 0.279500 0.051600 0.338500 0.173000 0.353300 11 1.599700 1.528889 0.192200 0.415000 0.160800 0.053700 0.150500 0.378000 0.211500 0.371700 0.397800 0.204900 0.374600 0.684800 0.491900 0.632400 0.131200 0.346800 0.122000 0.300900 0.038400 0.344600 0.177500 0.364400 12 1.599700 1.517532 0.198300 0.429800 0.159800 0.066400 0.162900 0.383300 0.220700 0.382100 0.405400 0.214800 0.383200 0.672900 0.469000 0.610400 0.167800 0.379700 0.119700 0.307100 0.038100 0.335400 0.196800 0.394200 13 1.599700 1.488849 0.209800 0.452300 0.172300 0.094900 0.171100 0.437800 0.222000 0.379800 0.411500 0.203800 0.397300 0.707500 0.470700 0.620700 0.186900 0.407600 0.124200 0.306700 0.059300 0.355400 0.207700 0.367100 14 1.599700 1.482210 0.228900 0.482600 0.187800 0.083600 0.191800 0.444100 0.225900 0.376900 0.407400 0.182500 0.384800 0.700600 0.512100 0.640100 0.175000 0.363300 0.144300 0.300000 0.083100 0.363100 0.229900 0.370700 15 1.326800 1.475198 0.216300 0.455600 0.174900 0.088500 0.183500 0.424400 0.226900 0.373400 0.404300 0.199200 0.396400 0.677800 0.496300 0.633800 0.166300 0.392400 0.128900 0.312900 0.085200 0.312300 0.205000 0.370200 16 1.326800 1.459697 0.233200 0.504200 0.192200 0.096000 0.202000 0.430800 0.239100 0.382400 0.412600 0.219500 0.403100 0.670400 0.485200 0.625200 0.196500 0.410100 0.135700 0.299600 0.123100 0.356900 0.225300 0.371100 17 1.326800 1.407340 0.243400 0.511900 0.204500 0.121000 0.215700 0.468000 0.246200 0.394600 0.424200 0.225900 0.416100 0.705200 0.494900 0.638300 0.224900 0.430400 0.157200 0.317900 0.115700 0.369200 0.224200 0.365300 18 1.326800 1.419522 0.245100 0.521500 0.210000 0.116100 0.211500 0.489900 0.255400 0.391600 0.419700 0.198800 0.421200 0.701400 0.501800 0.634200 0.226700 0.410100 0.154400 0.321400 0.105900 0.352300 0.236700 0.380400 19 1.158600 1.398764 0.253600 0.519200 0.213600 0.135200 0.207700 0.491900 0.257300 0.397300 0.428000 0.241400 0.401800 0.703500 0.509700 0.631100 0.236700 0.441800 0.155900 0.330800 0.128100 0.352300 0.237500 0.384000 20 1.158600 1.390591 0.248800 0.520200 0.216600 0.127500 0.211400 0.471900 0.258300 0.407000 0.429100 0.240300 0.407600 0.708500 0.505800 0.623400 0.235500 0.431600 0.150000 0.325000 0.125700 0.375400 0.227200 0.390200 21 1.158600 1.360608 0.262700 0.544800 0.222100 0.134700 0.230000 0.487500 0.269500 0.413300 0.436300 0.236200 0.419100 0.709300 0.514100 0.637400 0.257200 0.450600 0.165100 0.338400 0.139400 0.372300 0.237700 0.382700 22 1.158600 1.368296 0.262800 0.542400 0.236400 0.137400 0.228100 0.498500 0.266500 0.409000 0.433000 0.239900 0.418500 0.697500 0.520500 0.641000 0.257500 0.455700 0.162600 0.334800 0.140200 0.353800 0.233200 0.379600 23 1.158600 1.368176 0.264800 0.541100 0.233100 0.138200 0.223900 0.498700 0.272300 0.407400 0.434400 0.233100 0.418300 0.702000 0.524400 0.642300 0.262300 0.444300 0.159700 0.335300 0.140500 0.366200 0.236900 0.384000 24 1.049700 1.355271 0.269700 0.549200 0.239100 0.134700 0.229900 0.519200 0.274800 0.412700 0.437600 0.245400 0.417200 0.711200 0.523200 0.644100 0.272100 0.440500 0.166700 0.341500 0.137700 0.373800 0.249000 0.388000 25 1.049700 1.355180 0.272500 0.547900 0.243800 0.149700 0.229900 0.523100 0.272500 0.415700 0.442200 0.256200 0.420200 0.705800 0.523900 0.639600 0.271700 0.451900 0.166300 0.346900 0.153700 0.383100 0.247000 0.389300 26 1.049700 1.349337 0.275600 0.556300 0.246400 0.146700 0.234800 0.516300 0.274200 0.418300 0.440900 0.248700 0.418900 0.705800 0.523200 0.636500 0.274700 0.440500 0.172400 0.349100 0.155600 0.384600 0.252300 0.393800 27 1.049700 1.350782 0.275200 0.548700 0.246800 0.147300 0.236400 0.527200 0.280100 0.416200 0.442600 0.253400 0.424000 0.710300 0.526600 0.640100 0.273200 0.445600 0.167000 0.346900 0.160100 0.387700 0.249200 0.392900 28 1.049700 1.346533 0.277000 0.552800 0.252900 0.147400 0.240000 0.527600 0.280900 0.420900 0.444100 0.255500 0.424500 0.711200 0.530200 0.646800 0.277400 0.441800 0.170900 0.346900 0.156600 0.389200 0.249600 0.396000 29 0.993700 1.346575 0.277100 0.554800 0.252900 0.148400 0.239700 0.523600 0.278400 0.420000 0.443300 0.256300 0.424000 0.705600 0.529600 0.647300 0.273900 0.439200 0.174300 0.348700 0.157600 0.386200 0.250100 0.395100 30 0.993700 1.346446 0.277400 0.554700 0.252700 0.147900 0.240800 0.523600 0.278800 0.420400 0.443300 0.256100 0.424200 0.705500 0.530100 0.646800 0.275600 0.440500 0.174500 0.348700 0.157300 0.386200 0.249200 0.394200 <p>  If you have set `push_to_hub` to `True` in the `training_args`, the training checkpoints are pushed to the Hugging Face Hub. Upon training completion, push the final model to the Hub as well by calling the [`~transformers.Trainer.push_to_hub`] method.  Python<pre><code>&gt;&gt;&gt; trainer.push_to_hub()\n</code></pre>  ## Evaluate  Python<pre><code>&gt;&gt;&gt; from pprint import pprint\n\n&gt;&gt;&gt; metrics = trainer.evaluate(eval_dataset=cppe5[\"test\"], metric_key_prefix=\"test\")\n&gt;&gt;&gt; pprint(metrics)\n{'epoch': 30.0,\n  'test_loss': 1.0877351760864258,\n  'test_map': 0.4116,\n  'test_map_50': 0.741,\n  'test_map_75': 0.3663,\n  'test_map_Coverall': 0.5937,\n  'test_map_Face_Shield': 0.5863,\n  'test_map_Gloves': 0.3416,\n  'test_map_Goggles': 0.1468,\n  'test_map_Mask': 0.3894,\n  'test_map_large': 0.5637,\n  'test_map_medium': 0.3257,\n  'test_map_small': 0.3589,\n  'test_mar_1': 0.323,\n  'test_mar_10': 0.5237,\n  'test_mar_100': 0.5587,\n  'test_mar_100_Coverall': 0.6756,\n  'test_mar_100_Face_Shield': 0.7294,\n  'test_mar_100_Gloves': 0.4721,\n  'test_mar_100_Goggles': 0.4125,\n  'test_mar_100_Mask': 0.5038,\n  'test_mar_large': 0.7283,\n  'test_mar_medium': 0.4901,\n  'test_mar_small': 0.4469,\n  'test_runtime': 1.6526,\n  'test_samples_per_second': 17.548,\n  'test_steps_per_second': 2.42}\n</code></pre>  These results can be further improved by adjusting the hyperparameters in [`TrainingArguments`]. Give it a go!  ## Inference  Now that you have finetuned a model, evaluated it, and uploaded it to the Hugging Face Hub, you can use it for inference.  Python<pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; import requests\n\n&gt;&gt;&gt; from PIL import Image, ImageDraw\n&gt;&gt;&gt; from transformers import AutoImageProcessor, AutoModelForObjectDetection\n\n&gt;&gt;&gt; url = \"https://images.pexels.com/photos/8413299/pexels-photo-8413299.jpeg?auto=compress&amp;cs=tinysrgb&amp;w=630&amp;h=375&amp;dpr=2\"\n&gt;&gt;&gt; image = Image.open(requests.get(url, stream=True).raw)\n</code></pre>  Load model and image processor from the Hugging Face Hub (skip to use already trained in this session): Python<pre><code>&gt;&gt;&gt; device = \"cuda\"\n&gt;&gt;&gt; model_repo = \"qubvel-hf/detr_finetuned_cppe5\"\n\n&gt;&gt;&gt; image_processor = AutoImageProcessor.from_pretrained(model_repo)\n&gt;&gt;&gt; model = AutoModelForObjectDetection.from_pretrained(model_repo)\n&gt;&gt;&gt; model = model.to(device)\n</code></pre>  And detect bounding boxes:  Python<pre><code>&gt;&gt;&gt; with torch.no_grad():\n...     inputs = image_processor(images=[image], return_tensors=\"pt\")\n...     outputs = model(**inputs.to(device))\n...     target_sizes = torch.tensor([[image.size[1], image.size[0]]])\n...     results = image_processor.post_process_object_detection(outputs, threshold=0.3, target_sizes=target_sizes)[0]\n\n&gt;&gt;&gt; for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n...     box = [round(i, 2) for i in box.tolist()]\n...     print(\n...         f\"Detected {model.config.id2label[label.item()]} with confidence \"\n...         f\"{round(score.item(), 3)} at location {box}\"\n...     )\nDetected Gloves with confidence 0.683 at location [244.58, 124.33, 300.35, 185.13]\nDetected Mask with confidence 0.517 at location [143.73, 64.58, 219.57, 125.89]\nDetected Gloves with confidence 0.425 at location [179.15, 155.57, 262.4, 226.35]\nDetected Coverall with confidence 0.407 at location [307.13, -1.18, 477.82, 318.06]\nDetected Coverall with confidence 0.391 at location [68.61, 126.66, 309.03, 318.89]\n</code></pre>  Let's plot the result:  Python<pre><code>&gt;&gt;&gt; draw = ImageDraw.Draw(image)\n\n&gt;&gt;&gt; for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n...     box = [round(i, 2) for i in box.tolist()]\n...     x, y, x2, y2 = tuple(box)\n...     draw.rectangle((x, y, x2, y2), outline=\"red\", width=1)\n...     draw.text((x, y), model.config.id2label[label.item()], fill=\"white\")\n\n&gt;&gt;&gt; image\n</code></pre>"},{"location":"integrations/roboflow/train-rt-detr-on-custom-dataset-with-transformers/","title":"Train rt detr on custom dataset with transformers","text":""},{"location":"integrations/roboflow/train-rt-detr-on-custom-dataset-with-transformers/#how-to-train-rt-detr-on-custom-dataset","title":"How to Train RT-DETR on Custom Dataset","text":"<p>RT-DETR, short for \"Real-Time DEtection TRansformer\", is a computer vision model developed by Peking University and Baidu. In their paper, \"DETRs Beat YOLOs on Real-time Object Detection\" the authors claim that RT-DETR can outperform YOLO models in object detection, both in terms of speed and accuracy. The model has been released under the Apache 2.0 license, making it a great option, especially for enterprise projects.</p> <p></p> <p>Recently, RT-DETR was added to the <code>transformers</code> library, significantly simplifying its fine-tuning process. In this tutorial, we will show you how to train RT-DETR on a custom dataset.</p>"},{"location":"integrations/roboflow/train-rt-detr-on-custom-dataset-with-transformers/#setup","title":"Setup","text":""},{"location":"integrations/roboflow/train-rt-detr-on-custom-dataset-with-transformers/#configure-your-api-keys","title":"Configure your API keys","text":"<p>To fine-tune RT-DETR, you need to provide your HuggingFace Token and Roboflow API key. Follow these steps:</p> <ul> <li>Open your <code>HuggingFace Settings</code> page. Click <code>Access Tokens</code> then <code>New Token</code> to generate new token.</li> <li>Go to your <code>Roboflow Settings</code> page. Click <code>Copy</code>. This will place your private key in the clipboard.</li> <li>In Colab, go to the left pane and click on <code>Secrets</code> (\ud83d\udd11).<ul> <li>Store HuggingFace Access Token under the name <code>HF_TOKEN</code>.</li> <li>Store Roboflow API Key under the name <code>ROBOFLOW_API_KEY</code>.</li> </ul> </li> </ul>"},{"location":"integrations/roboflow/train-rt-detr-on-custom-dataset-with-transformers/#select-the-runtime","title":"Select the runtime","text":"<p>Let's make sure that we have access to GPU. We can use <code>nvidia-smi</code> command to do that. In case of any problems navigate to <code>Edit</code> -&gt; <code>Notebook settings</code> -&gt; <code>Hardware accelerator</code>, set it to <code>L4 GPU</code>, and then click <code>Save</code>.</p> Python<pre><code>!nvidia-smi\n</code></pre> <pre><code>Thu Jul 11 09:20:53 2024       \n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n| N/A   65C    P8              11W /  70W |      0MiB / 15360MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n\n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n|  No running processes found                                                           |\n+---------------------------------------------------------------------------------------+\n</code></pre> <p>NOTE: To make it easier for us to manage datasets, images and models we create a <code>HOME</code> constant.</p> Python<pre><code>import os\nHOME = os.getcwd()\nprint(\"HOME:\", HOME)\n</code></pre> <pre><code>HOME: /content\n</code></pre>"},{"location":"integrations/roboflow/train-rt-detr-on-custom-dataset-with-transformers/#install-dependencies","title":"Install dependencies","text":"Python<pre><code>!pip install -q git+https://github.com/huggingface/transformers.git\n!pip install -q git+https://github.com/roboflow/supervision.git\n!pip install -q accelerate\n!pip install -q roboflow\n!pip install -q torchmetrics\n!pip install -q \"albumentations&gt;=1.4.5\"\n</code></pre> <pre><code>  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for supervision (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m314.1/314.1 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m71.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m76.2/76.2 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m178.7/178.7 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m868.8/868.8 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m165.3/165.3 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m14.9/14.9 MB\u001b[0m \u001b[31m83.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m13.4/13.4 MB\u001b[0m \u001b[31m92.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m313.5/313.5 kB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\n</code></pre>"},{"location":"integrations/roboflow/train-rt-detr-on-custom-dataset-with-transformers/#imports","title":"Imports","text":"Python<pre><code>import torch\nimport requests\n\nimport numpy as np\nimport supervision as sv\nimport albumentations as A\n\nfrom PIL import Image\nfrom pprint import pprint\nfrom roboflow import Roboflow\nfrom dataclasses import dataclass, replace\nfrom google.colab import userdata\nfrom torch.utils.data import Dataset\nfrom transformers import (\n    AutoImageProcessor,\n    AutoModelForObjectDetection,\n    TrainingArguments,\n    Trainer\n)\nfrom torchmetrics.detection.mean_ap import MeanAveragePrecision\n</code></pre>"},{"location":"integrations/roboflow/train-rt-detr-on-custom-dataset-with-transformers/#inference-with-pre-trained-rt-detr-model","title":"Inference with pre-trained RT-DETR model","text":"Python<pre><code># @title Load model\n\nCHECKPOINT = \"PekingU/rtdetr_r50vd_coco_o365\"\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel = AutoModelForObjectDetection.from_pretrained(CHECKPOINT).to(DEVICE)\nprocessor = AutoImageProcessor.from_pretrained(CHECKPOINT)\n</code></pre> <pre><code>config.json:   0%|          | 0.00/5.11k [00:00&lt;?, ?B/s]\n\n\n\nmodel.safetensors:   0%|          | 0.00/172M [00:00&lt;?, ?B/s]\n\n\n\npreprocessor_config.json:   0%|          | 0.00/841 [00:00&lt;?, ?B/s]\n</code></pre> Python<pre><code># @title Run inference\n\nURL = \"https://media.roboflow.com/notebooks/examples/dog.jpeg\"\n\nimage = Image.open(requests.get(URL, stream=True).raw)\ninputs = processor(image, return_tensors=\"pt\").to(DEVICE)\n\nwith torch.no_grad():\n    outputs = model(**inputs)\n\nw, h = image.size\nresults = processor.post_process_object_detection(\n    outputs, target_sizes=[(h, w)], threshold=0.3)\n</code></pre> Python<pre><code># @title Display result with NMS\n\ndetections = sv.Detections.from_transformers(results[0])\nlabels = [\n    model.config.id2label[class_id]\n    for class_id\n    in detections.class_id\n]\n\nannotated_image = image.copy()\nannotated_image = sv.BoundingBoxAnnotator().annotate(annotated_image, detections)\nannotated_image = sv.LabelAnnotator().annotate(annotated_image, detections, labels=labels)\nannotated_image.thumbnail((600, 600))\nannotated_image\n</code></pre> Python<pre><code># @title Display result with NMS\n\ndetections = sv.Detections.from_transformers(results[0]).with_nms(threshold=0.1)\nlabels = [\n    model.config.id2label[class_id]\n    for class_id\n    in detections.class_id\n]\n\nannotated_image = image.copy()\nannotated_image = sv.BoundingBoxAnnotator().annotate(annotated_image, detections)\nannotated_image = sv.LabelAnnotator().annotate(annotated_image, detections, labels=labels)\nannotated_image.thumbnail((600, 600))\nannotated_image\n</code></pre>"},{"location":"integrations/roboflow/train-rt-detr-on-custom-dataset-with-transformers/#fine-tune-rt-detr-on-custom-dataset","title":"Fine-tune RT-DETR on custom dataset","text":"Python<pre><code># @title Download dataset from Roboflow Universe\n\nROBOFLOW_API_KEY = userdata.get('ROBOFLOW_API_KEY')\nrf = Roboflow(api_key=ROBOFLOW_API_KEY)\n\nproject = rf.workspace(\"roboflow-jvuqo\").project(\"poker-cards-fmjio\")\nversion = project.version(4)\ndataset = version.download(\"coco\")\n</code></pre> <pre><code>loading Roboflow workspace...\nloading Roboflow project...\n\n\nDownloading Dataset Version Zip in poker-cards-4 to coco:: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 39123/39123 [00:01&lt;00:00, 27288.54it/s]\n\n\n\n\n\nExtracting Dataset Version Zip to poker-cards-4 in coco:: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 907/907 [00:00&lt;00:00, 2984.59it/s]\n</code></pre> Python<pre><code>ds_train = sv.DetectionDataset.from_coco(\n    images_directory_path=f\"{dataset.location}/train\",\n    annotations_path=f\"{dataset.location}/train/_annotations.coco.json\",\n)\nds_valid = sv.DetectionDataset.from_coco(\n    images_directory_path=f\"{dataset.location}/valid\",\n    annotations_path=f\"{dataset.location}/valid/_annotations.coco.json\",\n)\nds_test = sv.DetectionDataset.from_coco(\n    images_directory_path=f\"{dataset.location}/test\",\n    annotations_path=f\"{dataset.location}/test/_annotations.coco.json\",\n)\n\nprint(f\"Number of training images: {len(ds_train)}\")\nprint(f\"Number of validation images: {len(ds_valid)}\")\nprint(f\"Number of test images: {len(ds_test)}\")\n</code></pre> <pre><code>Number of training images: 811\nNumber of validation images: 44\nNumber of test images: 44\n</code></pre> Python<pre><code># @title Display dataset sample\n\nGRID_SIZE = 5\n\ndef annotate(image, annotations, classes):\n    labels = [\n        classes[class_id]\n        for class_id\n        in annotations.class_id\n    ]\n\n    bounding_box_annotator = sv.BoundingBoxAnnotator()\n    label_annotator = sv.LabelAnnotator(text_scale=1, text_thickness=2)\n\n    annotated_image = image.copy()\n    annotated_image = bounding_box_annotator.annotate(annotated_image, annotations)\n    annotated_image = label_annotator.annotate(annotated_image, annotations, labels=labels)\n    return annotated_image\n\nannotated_images = []\nfor i in range(GRID_SIZE * GRID_SIZE):\n    _, image, annotations = ds_train[i]\n    annotated_image = annotate(image, annotations, ds_train.classes)\n    annotated_images.append(annotated_image)\n\ngrid = sv.create_tiles(\n    annotated_images,\n    grid_size=(GRID_SIZE, GRID_SIZE),\n    single_tile_size=(400, 400),\n    tile_padding_color=sv.Color.WHITE,\n    tile_margin_color=sv.Color.WHITE\n)\nsv.plot_image(grid, size=(10, 10))\n</code></pre>"},{"location":"integrations/roboflow/train-rt-detr-on-custom-dataset-with-transformers/#preprocess-the-data","title":"Preprocess the data","text":"<p>To finetune a model, you must preprocess the data you plan to use to match precisely the approach used for the pre-trained model. AutoImageProcessor takes care of processing image data to create <code>pixel_values</code>, <code>pixel_mask</code>, and <code>labels</code> that a DETR model can train with. The image processor has some attributes that you won't have to worry about:</p> <ul> <li><code>image_mean = [0.485, 0.456, 0.406 ]</code></li> <li><code>image_std = [0.229, 0.224, 0.225]</code></li> </ul> <p>These are the mean and standard deviation used to normalize images during the model pre-training. These values are crucial to replicate when doing inference or finetuning a pre-trained image model.</p> <p>Instantiate the image processor from the same checkpoint as the model you want to finetune.</p> Python<pre><code>IMAGE_SIZE = 480\n\nprocessor = AutoImageProcessor.from_pretrained(\n    CHECKPOINT,\n    do_resize=True,\n    size={\"width\": IMAGE_SIZE, \"height\": IMAGE_SIZE},\n)\n</code></pre> <p>Before passing the images to the <code>processor</code>, apply two preprocessing transformations to the dataset:</p> <ul> <li>Augmenting images</li> <li>Reformatting annotations to meet RT-DETR expectations</li> </ul> <p>First, to make sure the model does not overfit on the training data, you can apply image augmentation with any data augmentation library. Here we use Albumentations. This library ensures that transformations affect the image and update the bounding boxes accordingly.</p> Python<pre><code>train_augmentation_and_transform = A.Compose(\n    [\n        A.Perspective(p=0.1),\n        A.HorizontalFlip(p=0.5),\n        A.RandomBrightnessContrast(p=0.5),\n        A.HueSaturationValue(p=0.1),\n    ],\n    bbox_params=A.BboxParams(\n        format=\"pascal_voc\",\n        label_fields=[\"category\"],\n        clip=True,\n        min_area=25\n    ),\n)\n\nvalid_transform = A.Compose(\n    [A.NoOp()],\n    bbox_params=A.BboxParams(\n        format=\"pascal_voc\",\n        label_fields=[\"category\"],\n        clip=True,\n        min_area=1\n    ),\n)\n</code></pre> Python<pre><code># @title Visualize some augmented images\n\nIMAGE_COUNT = 5\n\nfor i in range(IMAGE_COUNT):\n    _, image, annotations = ds_train[i]\n\n    output = train_augmentation_and_transform(\n        image=image,\n        bboxes=annotations.xyxy,\n        category=annotations.class_id\n    )\n\n    augmented_image = output[\"image\"]\n    augmented_annotations = replace(\n        annotations,\n        xyxy=np.array(output[\"bboxes\"]),\n        class_id=np.array(output[\"category\"])\n    )\n\n    annotated_images = [\n        annotate(image, annotations, ds_train.classes),\n        annotate(augmented_image, augmented_annotations, ds_train.classes)\n    ]\n    grid = sv.create_tiles(\n        annotated_images,\n        titles=['original', 'augmented'],\n        titles_scale=0.5,\n        single_tile_size=(400, 400),\n        tile_padding_color=sv.Color.WHITE,\n        tile_margin_color=sv.Color.WHITE\n    )\n    sv.plot_image(grid, size=(6, 6))\n</code></pre> <p></p> <p></p> <p></p> <p></p> <p></p> <p>The <code>processor</code> expects the annotations to be in the following format: <code>{'image_id': int, 'annotations': List[Dict]}</code>, where each dictionary is a COCO object annotation. Let's add a function to reformat annotations for a single example:</p> Python<pre><code>class PyTorchDetectionDataset(Dataset):\n    def __init__(self, dataset: sv.DetectionDataset, processor, transform: A.Compose = None):\n        self.dataset = dataset\n        self.processor = processor\n        self.transform = transform\n\n    @staticmethod\n    def annotations_as_coco(image_id, categories, boxes):\n        annotations = []\n        for category, bbox in zip(categories, boxes):\n            x1, y1, x2, y2 = bbox\n            formatted_annotation = {\n                \"image_id\": image_id,\n                \"category_id\": category,\n                \"bbox\": [x1, y1, x2 - x1, y2 - y1],\n                \"iscrowd\": 0,\n                \"area\": (x2 - x1) * (y2 - y1),\n            }\n            annotations.append(formatted_annotation)\n\n        return {\n            \"image_id\": image_id,\n            \"annotations\": annotations,\n        }\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, idx):\n        _, image, annotations = self.dataset[idx]\n\n        # Convert image to RGB numpy array\n        image = image[:, :, ::-1]\n        boxes = annotations.xyxy\n        categories = annotations.class_id\n\n        if self.transform:\n            transformed = self.transform(\n                image=image,\n                bboxes=boxes,\n                category=categories\n            )\n            image = transformed[\"image\"]\n            boxes = transformed[\"bboxes\"]\n            categories = transformed[\"category\"]\n\n\n        formatted_annotations = self.annotations_as_coco(\n            image_id=idx, categories=categories, boxes=boxes)\n        result = self.processor(\n            images=image, annotations=formatted_annotations, return_tensors=\"pt\")\n\n        # Image processor expands batch dimension, lets squeeze it\n        result = {k: v[0] for k, v in result.items()}\n\n        return result\n</code></pre> <p>Now you can combine the image and annotation transformations to use on a batch of examples:</p> Python<pre><code>pytorch_dataset_train = PyTorchDetectionDataset(\n    ds_train, processor, transform=train_augmentation_and_transform)\npytorch_dataset_valid = PyTorchDetectionDataset(\n    ds_valid, processor, transform=valid_transform)\npytorch_dataset_test = PyTorchDetectionDataset(\n    ds_test, processor, transform=valid_transform)\n\npytorch_dataset_train[15]\n</code></pre> <pre><code>{'pixel_values': tensor([[[0.0745, 0.0745, 0.0745,  ..., 0.2431, 0.2471, 0.2471],\n          [0.0745, 0.0745, 0.0745,  ..., 0.2510, 0.2549, 0.2549],\n          [0.0667, 0.0706, 0.0706,  ..., 0.2588, 0.2588, 0.2588],\n          ...,\n          [0.0118, 0.0118, 0.0118,  ..., 0.0510, 0.0549, 0.0510],\n          [0.0157, 0.0196, 0.0235,  ..., 0.0549, 0.0627, 0.0549],\n          [0.0235, 0.0275, 0.0314,  ..., 0.0549, 0.0627, 0.0549]],\n\n         [[0.0549, 0.0549, 0.0549,  ..., 0.3137, 0.3176, 0.3176],\n          [0.0549, 0.0549, 0.0549,  ..., 0.3216, 0.3255, 0.3255],\n          [0.0471, 0.0510, 0.0510,  ..., 0.3294, 0.3294, 0.3294],\n          ...,\n          [0.0000, 0.0000, 0.0000,  ..., 0.0353, 0.0392, 0.0353],\n          [0.0000, 0.0000, 0.0039,  ..., 0.0392, 0.0471, 0.0392],\n          [0.0000, 0.0039, 0.0078,  ..., 0.0392, 0.0471, 0.0392]],\n\n         [[0.0431, 0.0431, 0.0431,  ..., 0.3922, 0.3961, 0.3961],\n          [0.0431, 0.0431, 0.0431,  ..., 0.4000, 0.4039, 0.4039],\n          [0.0353, 0.0392, 0.0392,  ..., 0.4078, 0.4078, 0.4078],\n          ...,\n          [0.0000, 0.0000, 0.0000,  ..., 0.0314, 0.0353, 0.0314],\n          [0.0000, 0.0000, 0.0039,  ..., 0.0353, 0.0431, 0.0353],\n          [0.0000, 0.0039, 0.0078,  ..., 0.0353, 0.0431, 0.0353]]]),\n 'labels': {'size': tensor([480, 480]), 'image_id': tensor([15]), 'class_labels': tensor([36,  4, 44, 52, 48]), 'boxes': tensor([[0.7891, 0.4437, 0.2094, 0.3562],\n         [0.3984, 0.6484, 0.3187, 0.3906],\n         [0.5891, 0.4070, 0.2219, 0.3859],\n         [0.3484, 0.2812, 0.2625, 0.4094],\n         [0.1602, 0.5023, 0.2672, 0.4109]]), 'area': tensor([17185.5000, 28687.5000, 19729.1250, 24759.0000, 25297.3125]), 'iscrowd': tensor([0, 0, 0, 0, 0]), 'orig_size': tensor([640, 640])}}\n</code></pre> <p>You have successfully augmented the images and prepared their annotations. In the final step, create a custom collate_fn to batch images together.</p> Python<pre><code>def collate_fn(batch):\n    data = {}\n    data[\"pixel_values\"] = torch.stack([x[\"pixel_values\"] for x in batch])\n    data[\"labels\"] = [x[\"labels\"] for x in batch]\n    return data\n</code></pre>"},{"location":"integrations/roboflow/train-rt-detr-on-custom-dataset-with-transformers/#preparing-function-to-compute-map","title":"Preparing function to compute mAP","text":"Python<pre><code>id2label = {id: label for id, label in enumerate(ds_train.classes)}\nlabel2id = {label: id for id, label in enumerate(ds_train.classes)}\n\n\n@dataclass\nclass ModelOutput:\n    logits: torch.Tensor\n    pred_boxes: torch.Tensor\n\n\nclass MAPEvaluator:\n\n    def __init__(self, image_processor, threshold=0.00, id2label=None):\n        self.image_processor = image_processor\n        self.threshold = threshold\n        self.id2label = id2label\n\n    def collect_image_sizes(self, targets):\n        \"\"\"Collect image sizes across the dataset as list of tensors with shape [batch_size, 2].\"\"\"\n        image_sizes = []\n        for batch in targets:\n            batch_image_sizes = torch.tensor(np.array([x[\"size\"] for x in batch]))\n            image_sizes.append(batch_image_sizes)\n        return image_sizes\n\n    def collect_targets(self, targets, image_sizes):\n        post_processed_targets = []\n        for target_batch, image_size_batch in zip(targets, image_sizes):\n            for target, (height, width) in zip(target_batch, image_size_batch):\n                boxes = target[\"boxes\"]\n                boxes = sv.xcycwh_to_xyxy(boxes)\n                boxes = boxes * np.array([width, height, width, height])\n                boxes = torch.tensor(boxes)\n                labels = torch.tensor(target[\"class_labels\"])\n                post_processed_targets.append({\"boxes\": boxes, \"labels\": labels})\n        return post_processed_targets\n\n    def collect_predictions(self, predictions, image_sizes):\n        post_processed_predictions = []\n        for batch, target_sizes in zip(predictions, image_sizes):\n            batch_logits, batch_boxes = batch[1], batch[2]\n            output = ModelOutput(logits=torch.tensor(batch_logits), pred_boxes=torch.tensor(batch_boxes))\n            post_processed_output = self.image_processor.post_process_object_detection(\n                output, threshold=self.threshold, target_sizes=target_sizes\n            )\n            post_processed_predictions.extend(post_processed_output)\n        return post_processed_predictions\n\n    @torch.no_grad()\n    def __call__(self, evaluation_results):\n\n        predictions, targets = evaluation_results.predictions, evaluation_results.label_ids\n\n        image_sizes = self.collect_image_sizes(targets)\n        post_processed_targets = self.collect_targets(targets, image_sizes)\n        post_processed_predictions = self.collect_predictions(predictions, image_sizes)\n\n        evaluator = MeanAveragePrecision(box_format=\"xyxy\", class_metrics=True)\n        evaluator.warn_on_many_detections = False\n        evaluator.update(post_processed_predictions, post_processed_targets)\n\n        metrics = evaluator.compute()\n\n        # Replace list of per class metrics with separate metric for each class\n        classes = metrics.pop(\"classes\")\n        map_per_class = metrics.pop(\"map_per_class\")\n        mar_100_per_class = metrics.pop(\"mar_100_per_class\")\n        for class_id, class_map, class_mar in zip(classes, map_per_class, mar_100_per_class):\n            class_name = id2label[class_id.item()] if id2label is not None else class_id.item()\n            metrics[f\"map_{class_name}\"] = class_map\n            metrics[f\"mar_100_{class_name}\"] = class_mar\n\n        metrics = {k: round(v.item(), 4) for k, v in metrics.items()}\n\n        return metrics\n\neval_compute_metrics_fn = MAPEvaluator(image_processor=processor, threshold=0.01, id2label=id2label)\n</code></pre>"},{"location":"integrations/roboflow/train-rt-detr-on-custom-dataset-with-transformers/#training-the-detection-model","title":"Training the detection model","text":"<p>You have done most of the heavy lifting in the previous sections, so now you are ready to train your model! The images in this dataset are still quite large, even after resizing. This means that finetuning this model will require at least one GPU.</p> <p>Training involves the following steps:</p> <ul> <li>Load the model with <code>AutoModelForObjectDetection</code> using the same checkpoint as in the preprocessing.</li> <li>Define your training hyperparameters in <code>TrainingArguments</code>.</li> <li>Pass the training arguments to <code>Trainer</code> along with the model, dataset, image processor, and data collator.</li> <li>Call <code>train()</code> to finetune your model.</li> </ul> <p>When loading the model from the same checkpoint that you used for the preprocessing, remember to pass the <code>label2id</code> and <code>id2label</code> maps that you created earlier from the dataset's metadata. Additionally, we specify <code>ignore_mismatched_sizes=True</code> to replace the existing classification head with a new one.</p> Python<pre><code>model = AutoModelForObjectDetection.from_pretrained(\n    CHECKPOINT,\n    id2label=id2label,\n    label2id=label2id,\n    anchor_image_size=None,\n    ignore_mismatched_sizes=True,\n)\n</code></pre> <pre><code>Some weights of RTDetrForObjectDetection were not initialized from the model checkpoint at PekingU/rtdetr_r50vd_coco_o365 and are newly initialized because the shapes did not match:\n- model.decoder.class_embed.0.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([53]) in the model instantiated\n- model.decoder.class_embed.0.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([53, 256]) in the model instantiated\n- model.decoder.class_embed.1.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([53]) in the model instantiated\n- model.decoder.class_embed.1.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([53, 256]) in the model instantiated\n- model.decoder.class_embed.2.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([53]) in the model instantiated\n- model.decoder.class_embed.2.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([53, 256]) in the model instantiated\n- model.decoder.class_embed.3.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([53]) in the model instantiated\n- model.decoder.class_embed.3.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([53, 256]) in the model instantiated\n- model.decoder.class_embed.4.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([53]) in the model instantiated\n- model.decoder.class_embed.4.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([53, 256]) in the model instantiated\n- model.decoder.class_embed.5.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([53]) in the model instantiated\n- model.decoder.class_embed.5.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([53, 256]) in the model instantiated\n- model.denoising_class_embed.weight: found shape torch.Size([81, 256]) in the checkpoint and torch.Size([54, 256]) in the model instantiated\n- model.enc_score_head.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([53]) in the model instantiated\n- model.enc_score_head.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([53, 256]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n</code></pre> <p>In the <code>TrainingArguments</code> use <code>output_dir</code> to specify where to save your model, then configure hyperparameters as you see fit. For <code>num_train_epochs=10</code> training will take about 15 minutes in Google Colab T4 GPU, increase the number of epoch to get better results.</p> <p>Important notes:</p> <ul> <li>Do not remove unused columns because this will drop the image column. Without the image column, you can't create <code>pixel_values</code>. For this reason, set <code>remove_unused_columns</code> to <code>False</code>.</li> <li>Set <code>eval_do_concat_batches=False</code> to get proper evaluation results. Images have different number of target boxes, if batches are concatenated we will not be able to determine which boxes belongs to particular image.</li> </ul> Python<pre><code>training_args = TrainingArguments(\n    output_dir=f\"{dataset.name.replace(' ', '-')}-finetune\",\n    num_train_epochs=20,\n    max_grad_norm=0.1,\n    learning_rate=5e-5,\n    warmup_steps=300,\n    per_device_train_batch_size=16,\n    dataloader_num_workers=2,\n    metric_for_best_model=\"eval_map\",\n    greater_is_better=True,\n    load_best_model_at_end=True,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    save_total_limit=2,\n    remove_unused_columns=False,\n    eval_do_concat_batches=False,\n)\n</code></pre> <p>Finally, bring everything together, and call <code>train()</code>:</p> Python<pre><code>trainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=pytorch_dataset_train,\n    eval_dataset=pytorch_dataset_valid,\n    tokenizer=processor,\n    data_collator=collate_fn,\n    compute_metrics=eval_compute_metrics_fn,\n)\n\ntrainer.train()\n</code></pre>"},{"location":"integrations/roboflow/train-rt-detr-on-custom-dataset-with-transformers/#evaluate","title":"Evaluate","text":"Python<pre><code># @title Collect predictions\n\ntargets = []\npredictions = []\n\nfor i in range(len(ds_test)):\n    path, sourece_image, annotations = ds_test[i]\n\n    image = Image.open(path)\n    inputs = processor(image, return_tensors=\"pt\").to(DEVICE)\n\n    with torch.no_grad():\n        outputs = model(**inputs)\n\n    w, h = image.size\n    results = processor.post_process_object_detection(\n        outputs, target_sizes=[(h, w)], threshold=0.3)\n\n    detections = sv.Detections.from_transformers(results[0])\n\n    targets.append(annotations)\n    predictions.append(detections)\n</code></pre> Python<pre><code># @title Calculate mAP\nmean_average_precision = sv.MeanAveragePrecision.from_detections(\n    predictions=predictions,\n    targets=targets,\n)\n\nprint(f\"map50_95: {mean_average_precision.map50_95:.2f}\")\nprint(f\"map50: {mean_average_precision.map50:.2f}\")\nprint(f\"map75: {mean_average_precision.map75:.2f}\")\n</code></pre> <pre><code>map50_95: 0.89\nmap50: 0.94\nmap75: 0.94\n</code></pre> Python<pre><code># @title Calculate Confusion Matrix\nconfusion_matrix = sv.ConfusionMatrix.from_detections(\n    predictions=predictions,\n    targets=targets,\n    classes=ds_test.classes\n)\n\n_ = confusion_matrix.plot()\n</code></pre>"},{"location":"integrations/roboflow/train-rt-detr-on-custom-dataset-with-transformers/#save-fine-tuned-model-on-hard-drive","title":"Save fine-tuned model on hard drive","text":"Python<pre><code>model.save_pretrained(\"/content/rt-detr/\")\nprocessor.save_pretrained(\"/content/rt-detr/\")\n</code></pre> <pre><code>['/content/rt-detr/preprocessor_config.json']\n</code></pre>"},{"location":"integrations/roboflow/train-rt-detr-on-custom-dataset-with-transformers/#inference-with-fine-tuned-rt-detr-model","title":"Inference with fine-tuned RT-DETR model","text":"Python<pre><code>IMAGE_COUNT = 5\n\nfor i in range(IMAGE_COUNT):\n    path, sourece_image, annotations = ds_test[i]\n\n    image = Image.open(path)\n    inputs = processor(image, return_tensors=\"pt\").to(DEVICE)\n\n    with torch.no_grad():\n        outputs = model(**inputs)\n\n    w, h = image.size\n    results = processor.post_process_object_detection(\n        outputs, target_sizes=[(h, w)], threshold=0.3)\n\n    detections = sv.Detections.from_transformers(results[0]).with_nms(threshold=0.1)\n\n    annotated_images = [\n        annotate(sourece_image, annotations, ds_train.classes),\n        annotate(sourece_image, detections, ds_train.classes)\n    ]\n    grid = sv.create_tiles(\n        annotated_images,\n        titles=['ground truth', 'prediction'],\n        titles_scale=0.5,\n        single_tile_size=(400, 400),\n        tile_padding_color=sv.Color.WHITE,\n        tile_margin_color=sv.Color.WHITE\n    )\n    sv.plot_image(grid, size=(6, 6))\n</code></pre>"},{"location":"introduction/image_augmentation/","title":"What is image augmentation and how it can improve the performance of deep neural networks","text":"<p>Deep neural networks require a lot of training data to obtain good results and prevent overfitting. However, it is often very difficult to get enough training samples. Multiple reasons could make it very hard or even impossible to gather enough data:</p> <ul> <li> <p>To make a training dataset, you need to obtain images and then label them. For example, you need to assign correct class labels if you have an image classification task. For an object detection task, you need to draw bounding boxes around objects.  For a semantic segmentation task, you need to assign a correct class to each input image pixel. This process requires manual labor, and sometimes it could be very costly to label the training data. For example, to correctly label medical images, you need expensive domain experts.</p> </li> <li> <p>Sometimes even collecting training images could be hard. There are many legal restrictions for working with healthcare data, and obtaining it requires a lot of effort. Sometimes getting the training images is more feasible, but it will cost a lot of money. For example, to get satellite images, you need to pay a satellite operator to take those photos. To get images for road scene recognition, you need an operator that will drive a car and collect the required data.</p> </li> </ul>"},{"location":"introduction/image_augmentation/#image-augmentation-to-the-rescue","title":"Image augmentation to the rescue","text":"<p>Image augmentation is a process of creating new training examples from the existing ones. To make a new sample, you slightly change the original image. For instance, you could make a new image a little brighter; you could cut a piece from the original image; you could make a new image by mirroring the original one, etc.</p> <p>Here are some examples of transformations of the original image that will create a new training sample.</p> <p></p> <p>By applying those transformations to the original training dataset, you could create an almost infinite amount of new training samples.</p>"},{"location":"introduction/image_augmentation/#how-much-does-image-augmentation-improves-the-quality-and-performance-of-deep-neural-networks","title":"How much does image augmentation improves the quality and performance of deep neural networks","text":"<p>Basic augmentations techniques were used almost in all papers that describe the state-of-the-art models for image recognition.</p> <p>AlexNet was the first model that demonstrated exceptional capabilities of using deep neural networks for image recognition. For training, the authors used a set of basic image augmentation techniques. They resized original images to the fixed size of 256 by 256 pixels, and then they cropped patches of size 224 by 224 pixels as well as their horizontal reflections from those resized images. Also, they altered the intensities of the RGB channels in images.</p> <p>Successive state-of-the-art models such as Inception, ResNet, and EfficientNet also used image augmentation techniques for training.</p> <p>In 2018 Google published a paper about AutoAugment - an algorithm that automatically discovers the best set of augmentations for the dataset. They showed that a custom set of augmentations improves the performance of the model.</p> <p>Here is a comparison between a model that used only the base set of augmentations and a model that used a specific set of augmentations discovered by AutoAugment. The table shows Top-1 accuracy (%) on the ImageNet validation set; higher is better.</p> Model Base augmentations AutoAugment augmentations ResNet-50 76.3 77.6 ResNet-200 78.5 80.0 AmoebaNet-B (6,190) 82.2 82.8 AmoebaNet-C (6,228) 83.1 83.5 <p>The table demonstrates that a diverse set of image augmentations improves the performance of neural networks compared to a base set with only a few most popular transformation techniques.</p> <p>Augmentations help to fight overfitting and improve the performance of deep neural networks for computer vision tasks such as classification, segmentation, and object detection. The best part is that image augmentations libraries such as Albumentations make it possible to add image augmentations to any computer vision pipeline with minimal effort.</p>"},{"location":"introduction/why_albumentations/","title":"Why Albumentations","text":""},{"location":"introduction/why_albumentations/#a-single-interface-to-work-with-images-masks-bounding-boxes-and-key-points","title":"A single interface to work with images, masks, bounding boxes, and key points.","text":"<p>Albumentations provides a single interface to work with different computer vision tasks such as classification, semantic segmentation, instance segmentation, object detection, pose estimation, etc.</p>"},{"location":"introduction/why_albumentations/#battle-tested","title":"Battle-tested","text":"<p>The library is widely used in industry, deep learning research, machine learning competitions, and open source projects.</p>"},{"location":"introduction/why_albumentations/#high-performance","title":"High performance","text":"<p>Albumentations optimized for maximum speed and performance. Under the hood, the library uses highly optimized functions from OpenCV and NumPy for data processing. We have a regularly updated benchmark that compares the speed of popular image augmentations libraries for the most common image transformations. Albumentations demonstrates the best performance in most cases.</p>"},{"location":"introduction/why_albumentations/#diverse-set-of-supported-augmentations","title":"Diverse set of supported augmentations","text":"<p>Albumentations supports more than 60 different image augmentations.</p>"},{"location":"introduction/why_albumentations/#extensibility","title":"Extensibility","text":"<p>Albumentations allows to easily add new augmentations and use them in computer vision pipelines through a single interface along with built-in transformations.</p>"},{"location":"introduction/why_albumentations/#rigorous-testing","title":"Rigorous testing","text":"<p>Bugs in the augmentation pipeline could silently corrupt the input data. They can easily go unnoticed, but the performance of the models trained with incorrect data will degrade. Albumentations has an extensive test suite that helps to discover bugs during development.</p>"},{"location":"introduction/why_albumentations/#it-is-open-source-and-mit-licensed","title":"It is open source and MIT licensed","text":"<p>You can find the source code on GitHub.</p>"},{"location":"introduction/why_you_need_a_dedicated_library_for_image_augmentation/","title":"Why you need a dedicated library for image augmentation","text":"<p>At first glance, image augmentations look very simple; you apply basic transformations to an image: mirroring, cropping, changing brightness and contrast, etc.</p> <p>There are a lot of libraries that could do such image transformations. Here is an example of how you could use Pillow, a popular image processing library for Python, to make simple augmentations.</p> Python<pre><code>from PIL import Image, ImageEnhance\n\nimage = Image.open(\"parrot.jpg\")\n\nmirrored_image = image.transpose(Image.FLIP_LEFT_RIGHT)\n\nrotated_image = image.rotate(45)\n\nbrightness_enhancer = ImageEnhance.Brightness(image)\nbrighter_image = brightness_enhancer.enhance(factor=1.5)\n</code></pre> <p></p> <p>However, this approach has many limitations, and it doesn't handle all cases with image augmentation. An image augmentation library such as Albumentations gives you a lot of advantages.</p> <p>Here is a list of few pitfalls that augmentation libraries can handle very well.</p>"},{"location":"introduction/why_you_need_a_dedicated_library_for_image_augmentation/#the-need-to-apply-the-same-transform-to-an-image-and-for-labels-for-segmentation-object-detection-and-keypoint-detection-tasks","title":"The need to apply the same transform to an image and for labels for segmentation, object detection, and keypoint detection tasks.","text":"<p>For image classification, you need to modify only an input image and keep output labels intact because output labels are invariant to image modifications.</p> <p></p> <p>Note</p> <p>There are some exceptions to this rule. For example, an image could contain a cat and have an assigned label <code>cat</code>. During image augmentation, if you crop a part of an image that doesn't have a cat on it, then the output label <code>cat</code> becomes wrong and misleading. Usually, you deal with those situations by deciding which augmentations you could apply to a dataset without risking to have problems with incorrect labels.</p> <p></p> <p>For segmentation, you need to apply some transformations both to an input image and an output mask. You also have to use the same parameters both for the image transformation and the mask transformation.</p> <p>Let's look at an example of a semantic segmentation task from Inria Aerial Image Labeling Dataset. The dataset contains aerial photos as well as masks for those photos. Each pixel of the mask is marked either as 1 if the pixel belongs to the class <code>building</code> and 0 otherwise.</p> <p>There are two types of image augmentations: pixel-level augmentations and spatial-level augmentations.</p> <p>Pixel-level augmentations change the values of pixels of the original image, but they don't change the output mask. Image transformations such as changing brightness or contrast of adjusting values of the RGB-palette of the image are pixel-level augmentations.</p> <p> We modify the input image by adjusting its brightness, but we keep the output mask intact.</p> <p>On the contrary, spatial-level augmentations change both the image and the mask. When you apply image transformations such as mirroring or rotation or cropping a part of the input image, you also need to apply the same transformation to the output label to preserve its correctness.</p> <p> We rotate both the input image and the output mask. We use the same set of transformations with the same parameters, both for the image and the mask.</p> <p>The same is true for object detection tasks. For pixel-level augmentations, you only need to change the input image. With spatial-level augmentations, you need to apply the same transformation not only to the image but for bounding boxes coordinates as well. After applying spatial-level augmentations, you need to update coordinates of bounding boxes to represent the correct locations of objects on the augmented image.</p> <p> Pixel-level augmentations such as brightness adjustment change only the input image but not the coordinates of bounding boxes. Spatial-level augmentations such as mirroring and cropping a part of the image change both the input image and the bounding boxes' coordinates.</p> <p>Albumentations knows how to correctly apply transformation both to the input data as well as the output labels.</p>"},{"location":"introduction/why_you_need_a_dedicated_library_for_image_augmentation/#working-with-probabilities","title":"Working with probabilities","text":"<p>During training, you usually want to apply augmentations with a probability of less than 100% since you also need to have the original images in your training pipeline. Also, it is beneficial to be able to control the magnitude of image augmentation, how much does the augmentation change the original image. If the original dataset is large, you could apply only the basic augmentations with probability around 10-30% and with a small magnitude of changes. If the dataset is small, you need to act more aggressively with augmentations to prevent overfitting of neural networks, so you usually need to increase the probability of applying each augmentation to 40-50% and increase the magnitude of changes the augmentation makes to the image.</p> <p>Image augmentation libraries allow you to set the required probabilities and the magnitude of values for each transformation.</p>"},{"location":"introduction/why_you_need_a_dedicated_library_for_image_augmentation/#declarative-definition-of-the-augmentation-pipeline-and-unified-interface","title":"Declarative definition of the augmentation pipeline and unified interface","text":"<p>Usually, you want to apply not a single augmentation, but a set of augmentations with specific parameters such as probability and magnitude of changes. Augmentation libraries allow you to declare such a pipeline in a single place and then use it for image transformation through a unified interface. Some libraries can store and load transformation parameters to formats such as JSON, YAML, etc.</p> <p>Here is an example definition of an augmentation pipeline. This pipeline will first crop a random 512px x 512px part of the input image. Then with probability 30%, it will randomly change brightness and contrast of that crop. Finally, with probability 50%, it will horizontally flip the resulting image.</p> Python<pre><code>import albumentations as A\n\ntransform = A.Compose([\n    A.RandomCrop(512, 512),\n    A.RandomBrightnessContrast(p=0.3),\n    A.HorizontalFlip(p=0.5),\n])\n</code></pre>"},{"location":"introduction/why_you_need_a_dedicated_library_for_image_augmentation/#rigorous-testing","title":"Rigorous testing","text":"<p>A bug in the augmentation pipeline could easily go unnoticed. A buggy pipeline could silently corrupt input data. There won't be any exceptions and code failures, but the performance of trained neural networks will degrade because they received a garbage input during training. Augmentation libraries usually have large test suites that capture regressions during development. Also large user base helps to find unnoticed bugs and report them to developers.</p>"}]}