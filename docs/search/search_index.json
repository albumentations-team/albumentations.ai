{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Albumentations documentation \u00b6 Albumentations is a fast and flexible image augmentation library. The library is widely used in industry , deep learning research , machine learning competitions , and open source projects . Albumentations is written in Python, and it is licensed under the MIT license. The source code is available at https://github.com/albumentations-team/albumentations . If you are new to image augmentation, start with articles in the \"Introduction to image augmentation\" section. They describe what image augmentation is, how it can boost deep neural networks' performance, and why you should use Albumentations. Articles in the \"Getting started with Albumentations\" section show how you can use the library for different computer vision tasks: image classification, semantic segmentation, instance segmentation, and object detection, keypoint detection. The \"Examples\" section contains Jupyter Notebooks that demonstrate how to use various features of Albumentations. Each notebook includes a link to Google Colab, where you can run the code by yourself. \"API Reference\" contains the description of Albumentations' methods and classes. Introduction to image augmentation \u00b6 What is image augmentation and how it can improve the performance of deep neural networks Why you need a dedicated library for image augmentation Why Albumentations Getting started with Albumentations \u00b6 Installation Image augmentation for classification Mask augmentation for segmentation Bounding boxes augmentation for object detection Keypoints augmentation Simultaneous augmentation of multiple targets: masks, bounding boxes, keypoints A list of transforms and their supported targets Setting probabilities for transforms in an augmentation pipeline Examples \u00b6 Defining a simple augmentation pipeline for image augmentation Working with non-8-bit images Using Albumentations to augment bounding boxes for object detection tasks How to use Albumentations for detection tasks if you need to keep all bounding boxes Using Albumentations for a semantic segmentation task Using Albumentations to augment keypoints Applying the same augmentation with the same parameters to multiple images, masks, bounding boxes, or keypoints Weather augmentations in Albumentations Migrating from torchvision to Albumentations Debugging an augmentation pipeline with ReplayCompose How to save and load parameters of an augmentation pipeline Showcase. Cool augmentation examples on diverse set of images from various real-world tasks. Examples of how to use Albumentations with different deep learning frameworks \u00b6 PyTorch PyTorch and Albumentations for image classification PyTorch and Albumentations for semantic segmentation TensorFlow 2 Using Albumentations with Tensorflow External resources \u00b6 Blog posts, podcasts, talks, and videos about Albumentations Books that mention Albumentations Other topics \u00b6 Frequently Asked Questions AutoAlbument - AutoML for image augmentation Albumentations Experimental - experimental features for Albumentations Release notes Contributing API Reference \u00b6 Index Core API (albumentations.core) Augmentations (albumentations.augmentations) ImgAug Helpers (albumentations.imgaug) PyTorch Helpers (albumentations.pytorch)","title":"Welcome to Albumentations documentation"},{"location":"#welcome-to-albumentations-documentation","text":"Albumentations is a fast and flexible image augmentation library. The library is widely used in industry , deep learning research , machine learning competitions , and open source projects . Albumentations is written in Python, and it is licensed under the MIT license. The source code is available at https://github.com/albumentations-team/albumentations . If you are new to image augmentation, start with articles in the \"Introduction to image augmentation\" section. They describe what image augmentation is, how it can boost deep neural networks' performance, and why you should use Albumentations. Articles in the \"Getting started with Albumentations\" section show how you can use the library for different computer vision tasks: image classification, semantic segmentation, instance segmentation, and object detection, keypoint detection. The \"Examples\" section contains Jupyter Notebooks that demonstrate how to use various features of Albumentations. Each notebook includes a link to Google Colab, where you can run the code by yourself. \"API Reference\" contains the description of Albumentations' methods and classes.","title":"Welcome to Albumentations documentation"},{"location":"#introduction-to-image-augmentation","text":"What is image augmentation and how it can improve the performance of deep neural networks Why you need a dedicated library for image augmentation Why Albumentations","title":"Introduction to image augmentation"},{"location":"#getting-started-with-albumentations","text":"Installation Image augmentation for classification Mask augmentation for segmentation Bounding boxes augmentation for object detection Keypoints augmentation Simultaneous augmentation of multiple targets: masks, bounding boxes, keypoints A list of transforms and their supported targets Setting probabilities for transforms in an augmentation pipeline","title":"Getting started with Albumentations"},{"location":"#examples","text":"Defining a simple augmentation pipeline for image augmentation Working with non-8-bit images Using Albumentations to augment bounding boxes for object detection tasks How to use Albumentations for detection tasks if you need to keep all bounding boxes Using Albumentations for a semantic segmentation task Using Albumentations to augment keypoints Applying the same augmentation with the same parameters to multiple images, masks, bounding boxes, or keypoints Weather augmentations in Albumentations Migrating from torchvision to Albumentations Debugging an augmentation pipeline with ReplayCompose How to save and load parameters of an augmentation pipeline Showcase. Cool augmentation examples on diverse set of images from various real-world tasks.","title":"Examples"},{"location":"#examples-of-how-to-use-albumentations-with-different-deep-learning-frameworks","text":"PyTorch PyTorch and Albumentations for image classification PyTorch and Albumentations for semantic segmentation TensorFlow 2 Using Albumentations with Tensorflow","title":"Examples of how to use Albumentations with different deep learning frameworks"},{"location":"#external-resources","text":"Blog posts, podcasts, talks, and videos about Albumentations Books that mention Albumentations","title":"External resources"},{"location":"#other-topics","text":"Frequently Asked Questions AutoAlbument - AutoML for image augmentation Albumentations Experimental - experimental features for Albumentations Release notes Contributing","title":"Other topics"},{"location":"#api-reference","text":"Index Core API (albumentations.core) Augmentations (albumentations.augmentations) ImgAug Helpers (albumentations.imgaug) PyTorch Helpers (albumentations.pytorch)","title":"API Reference"},{"location":"contributing/","text":"Contributing \u00b6 All development is done on GitHub: https://github.com/albumentations-team/albumentations If you find a bug or have a feature request file an issue at https://github.com/albumentations-team/albumentations/issues To create a pull request: Fork the repository. Clone the repository locally. Install pre-commit (a library for running pre-commit hooks), black (code formatter) and flake8 (code linter): pip install pre-commit black flake8 Initialize pre-commit : pre-commit install Install albumentations in development mode: pip install -e . [ tests ] Make changes to the code. Run tests: pytest Push the code to your forked repo. Create a pull request to https://github.com/albumentations-team/albumentations","title":"Contributing"},{"location":"contributing/#contributing","text":"All development is done on GitHub: https://github.com/albumentations-team/albumentations If you find a bug or have a feature request file an issue at https://github.com/albumentations-team/albumentations/issues To create a pull request: Fork the repository. Clone the repository locally. Install pre-commit (a library for running pre-commit hooks), black (code formatter) and flake8 (code linter): pip install pre-commit black flake8 Initialize pre-commit : pre-commit install Install albumentations in development mode: pip install -e . [ tests ] Make changes to the code. Run tests: pytest Push the code to your forked repo. Create a pull request to https://github.com/albumentations-team/albumentations","title":"Contributing"},{"location":"faq/","text":"Frequently Asked Questions \u00b6 Installation I am receiving an error message Failed building wheel for imagecodecs when I am trying to install Albumentations. How can I fix the problem? I successfully installed the library, but when I am trying to import it I receive an error ImportError: libXrender.so.1: cannot open shared object file: No such file or directory . Examples Why do you call cv2.cvtColor(image, cv2.COLOR_BGR2RGB) in your examples? Usage Which transformation should I use to convert a NumPy array with an image or a mask to a PyTorch tensor: ToTensor() or ToTensorV2() ? How can I find which augmentations were applied to the input data and which parameters they used? My computer vision pipeline works with a sequence of images. I want to apply the same augmentations with the same parameters to each image in the sequence. Can Albumentations do it? I want to augment 16-bit TIFF images. Can Albumentations work with them? Augmentations have a parameter named p that sets the probability of applying that augmentation, but they also have the always_apply parameter that can either be True or False . What is the difference between p and always_apply ? Is always_apply=True equals to p=1.0 ? When I use augmentations with the border_mode parameter (such as Rotate ) and set border_mode to cv2.BORDER_REFLECT or cv2.BORDER_REFLECT_101 Albumentations mirrors regions of images and masks but doesn't mirror bounding boxes and keypoints. Is it a bug? I created annotations for bounding boxes using labeling service or labeling software. How can I use those annotations in Albumentations? Installation \u00b6 I am receiving an error message Failed building wheel for imagecodecs when I am trying to install Albumentations. How can I fix the problem? \u00b6 Try to update pip by running the following command: python3 -m pip install --upgrade pip I successfully installed the library, but when I am trying to import it I receive an error ImportError: libXrender.so.1: cannot open shared object file: No such file or directory . \u00b6 Probably your system doesn't have libXrender . To install the libXrender package on Ubuntu or Debian run: sudo apt-get update sudo apt-get install libxrender1 To install the package on other operating systems, consult the documentation for the OS' package manager. Examples \u00b6 Why do you call cv2.cvtColor(image, cv2.COLOR_BGR2RGB) in your examples? \u00b6 For historical reasons , OpenCV reads an image in BGR format (so color channels of the image have the following order: Blue, Green, Red). Albumentations uses the most common and popular RGB image format. So when using OpenCV, we need to convert the image format to RGB explicitly. Usage \u00b6 Which transformation should I use to convert a NumPy array with an image or a mask to a PyTorch tensor: ToTensor() or ToTensorV2() ? \u00b6 Always use ToTensorV2() . ToTensor() is a legacy transformation that contains complex and unnecessary logic for mask transformation. We don't want to break the existing pipelines that use that transformation, so instead of changing the behavior of the original ToTensor() , we created a new, more simple transformation. ToTensor() is now deprecated and will be removed in future versions. For all new projects, you should always use ToTensorV2(). How can I find which augmentations were applied to the input data and which parameters they used? \u00b6 To save and inspect parameters of augmentations, you can replace Compose with ReplayCompose. ReplayCompose behaves just like regular Compose, but it also saves information about which augmentations were applied and which parameters were uses. Take a look at the example that shows how you can use ReplayCompose. My computer vision pipeline works with a sequence of images. I want to apply the same augmentations with the same parameters to each image in the sequence. Can Albumentations do it? \u00b6 Yes. You can define additional images, masks, bounding boxes, or keypoints through the additional_targets argument to Compose . You can then pass those additional targets to the augmentation pipeline, and Albumentations will augment them in the same way. See this example for more info. I want to augment 16-bit TIFF images. Can Albumentations work with them? \u00b6 Yes. Albumentations can also work with non-8-bit images. See this example for more info. Augmentations have a parameter named p that sets the probability of applying that augmentation, but they also have the always_apply parameter that can either be True or False . What is the difference between p and always_apply ? Is always_apply=True equals to p=1.0 ? \u00b6 When always_apply is set to True , Albumentations will always apply that transform, even if p is set to a value less than 1.0 . However, always_apply=True doesn't equal to p=1.0 because with always_apply=True , Albumentations will apply a transform even in a case when top-level containers are not applied. Let's look at an example when a container Compose contains one augmentation Resize : transform = A . Compose ([ A . Resize ( height = 256 , width = 256 , p = 1.0 ), ], p = 0.9 ) If you set p=1.0 for Resize and p=0.9 for Compose , then Resize has a 90% chance to be applied, because there is a 90% chance for Compose to be applied and if Compose is applied, there is a 100% chance for Resize to be applied. But if you set always_apply=True for Resize , Albumentations will apply it with 100% probability even if Albumentations decides not to apply the parent container ( Compose in the example): transform = A . Compose ([ A . Resize ( height = 256 , width = 256 , always_apply = True ), ], p = 0.9 ) When I use augmentations with the border_mode parameter (such as Rotate ) and set border_mode to cv2.BORDER_REFLECT or cv2.BORDER_REFLECT_101 Albumentations mirrors regions of images and masks but doesn't mirror bounding boxes and keypoints. Is it a bug? \u00b6 Unfortunately, adding extra bounding boxes or keypoints to reflected regions of the image is not supported. You can change border_mode mode to cv2.BORDER_CONSTANT if this causes a significant impact on the training of your model. I created annotations for bounding boxes using labeling service or labeling software. How can I use those annotations in Albumentations? \u00b6 You need to convert those annotations to one of the formats, supported by Albumentations. For the list of formats, please refer to this article . Consult the documentation of the labeling service to see how you can export annotations in those formats.","title":"Frequently Asked Questions"},{"location":"faq/#frequently-asked-questions","text":"Installation I am receiving an error message Failed building wheel for imagecodecs when I am trying to install Albumentations. How can I fix the problem? I successfully installed the library, but when I am trying to import it I receive an error ImportError: libXrender.so.1: cannot open shared object file: No such file or directory . Examples Why do you call cv2.cvtColor(image, cv2.COLOR_BGR2RGB) in your examples? Usage Which transformation should I use to convert a NumPy array with an image or a mask to a PyTorch tensor: ToTensor() or ToTensorV2() ? How can I find which augmentations were applied to the input data and which parameters they used? My computer vision pipeline works with a sequence of images. I want to apply the same augmentations with the same parameters to each image in the sequence. Can Albumentations do it? I want to augment 16-bit TIFF images. Can Albumentations work with them? Augmentations have a parameter named p that sets the probability of applying that augmentation, but they also have the always_apply parameter that can either be True or False . What is the difference between p and always_apply ? Is always_apply=True equals to p=1.0 ? When I use augmentations with the border_mode parameter (such as Rotate ) and set border_mode to cv2.BORDER_REFLECT or cv2.BORDER_REFLECT_101 Albumentations mirrors regions of images and masks but doesn't mirror bounding boxes and keypoints. Is it a bug? I created annotations for bounding boxes using labeling service or labeling software. How can I use those annotations in Albumentations?","title":"Frequently Asked Questions"},{"location":"faq/#installation","text":"","title":"Installation"},{"location":"faq/#i-am-receiving-an-error-message-failed-building-wheel-for-imagecodecs-when-i-am-trying-to-install-albumentations-how-can-i-fix-the-problem","text":"Try to update pip by running the following command: python3 -m pip install --upgrade pip","title":"I am receiving an error message Failed building wheel for imagecodecs when I am trying to install Albumentations. How can I fix the problem?"},{"location":"faq/#i-successfully-installed-the-library-but-when-i-am-trying-to-import-it-i-receive-an-error-importerror-libxrenderso1-cannot-open-shared-object-file-no-such-file-or-directory","text":"Probably your system doesn't have libXrender . To install the libXrender package on Ubuntu or Debian run: sudo apt-get update sudo apt-get install libxrender1 To install the package on other operating systems, consult the documentation for the OS' package manager.","title":"I successfully installed the library, but when I am trying to import it I receive an error ImportError: libXrender.so.1: cannot open shared object file: No such file or directory."},{"location":"faq/#examples","text":"","title":"Examples"},{"location":"faq/#why-do-you-call-cv2cvtcolorimage-cv2color_bgr2rgb-in-your-examples","text":"For historical reasons , OpenCV reads an image in BGR format (so color channels of the image have the following order: Blue, Green, Red). Albumentations uses the most common and popular RGB image format. So when using OpenCV, we need to convert the image format to RGB explicitly.","title":"Why do you call cv2.cvtColor(image, cv2.COLOR_BGR2RGB) in your examples?"},{"location":"faq/#usage","text":"","title":"Usage"},{"location":"faq/#which-transformation-should-i-use-to-convert-a-numpy-array-with-an-image-or-a-mask-to-a-pytorch-tensor-totensor-or-totensorv2","text":"Always use ToTensorV2() . ToTensor() is a legacy transformation that contains complex and unnecessary logic for mask transformation. We don't want to break the existing pipelines that use that transformation, so instead of changing the behavior of the original ToTensor() , we created a new, more simple transformation. ToTensor() is now deprecated and will be removed in future versions. For all new projects, you should always use ToTensorV2().","title":"Which transformation should I use to convert a NumPy array with an image or a mask to a PyTorch tensor: ToTensor() or ToTensorV2()?"},{"location":"faq/#how-can-i-find-which-augmentations-were-applied-to-the-input-data-and-which-parameters-they-used","text":"To save and inspect parameters of augmentations, you can replace Compose with ReplayCompose. ReplayCompose behaves just like regular Compose, but it also saves information about which augmentations were applied and which parameters were uses. Take a look at the example that shows how you can use ReplayCompose.","title":"How can I find which augmentations were applied to the input data and which parameters they used?"},{"location":"faq/#my-computer-vision-pipeline-works-with-a-sequence-of-images-i-want-to-apply-the-same-augmentations-with-the-same-parameters-to-each-image-in-the-sequence-can-albumentations-do-it","text":"Yes. You can define additional images, masks, bounding boxes, or keypoints through the additional_targets argument to Compose . You can then pass those additional targets to the augmentation pipeline, and Albumentations will augment them in the same way. See this example for more info.","title":"My computer vision pipeline works with a sequence of images. I want to apply the same augmentations with the same parameters to each image in the sequence. Can Albumentations do it?"},{"location":"faq/#i-want-to-augment-16-bit-tiff-images-can-albumentations-work-with-them","text":"Yes. Albumentations can also work with non-8-bit images. See this example for more info.","title":"I want to augment 16-bit TIFF images. Can Albumentations work with them?"},{"location":"faq/#augmentations-have-a-parameter-named-p-that-sets-the-probability-of-applying-that-augmentation-but-they-also-have-the-always_apply-parameter-that-can-either-be-true-or-false-what-is-the-difference-between-p-and-always_apply-is-always_applytrue-equals-to-p10","text":"When always_apply is set to True , Albumentations will always apply that transform, even if p is set to a value less than 1.0 . However, always_apply=True doesn't equal to p=1.0 because with always_apply=True , Albumentations will apply a transform even in a case when top-level containers are not applied. Let's look at an example when a container Compose contains one augmentation Resize : transform = A . Compose ([ A . Resize ( height = 256 , width = 256 , p = 1.0 ), ], p = 0.9 ) If you set p=1.0 for Resize and p=0.9 for Compose , then Resize has a 90% chance to be applied, because there is a 90% chance for Compose to be applied and if Compose is applied, there is a 100% chance for Resize to be applied. But if you set always_apply=True for Resize , Albumentations will apply it with 100% probability even if Albumentations decides not to apply the parent container ( Compose in the example): transform = A . Compose ([ A . Resize ( height = 256 , width = 256 , always_apply = True ), ], p = 0.9 )","title":"Augmentations have a parameter named p that sets the probability of applying that augmentation, but they also have the always_apply parameter that can either be True or False. What is the difference between p and always_apply? Is always_apply=True equals to p=1.0?"},{"location":"faq/#when-i-use-augmentations-with-the-border_mode-parameter-such-as-rotate-and-set-border_mode-to-cv2border_reflect-or-cv2border_reflect_101-albumentations-mirrors-regions-of-images-and-masks-but-doesnt-mirror-bounding-boxes-and-keypoints-is-it-a-bug","text":"Unfortunately, adding extra bounding boxes or keypoints to reflected regions of the image is not supported. You can change border_mode mode to cv2.BORDER_CONSTANT if this causes a significant impact on the training of your model.","title":"When I use augmentations with the border_mode parameter (such as Rotate) and set border_mode to cv2.BORDER_REFLECT or cv2.BORDER_REFLECT_101 Albumentations mirrors regions of images and masks but doesn't mirror bounding boxes and keypoints. Is it a bug?"},{"location":"faq/#i-created-annotations-for-bounding-boxes-using-labeling-service-or-labeling-software-how-can-i-use-those-annotations-in-albumentations","text":"You need to convert those annotations to one of the formats, supported by Albumentations. For the list of formats, please refer to this article . Consult the documentation of the labeling service to see how you can export annotations in those formats.","title":"I created annotations for bounding boxes using labeling service or labeling software. How can I use those annotations in Albumentations?"},{"location":"release_notes/","text":"Release notes \u00b6 0.5.2 (29 November 2020) \u00b6 Minor changes \u00b6 ToTensorV2 now automatically expands grayscale images with the shape [H, W] to the shape [H, W, 1] . PR #604 by @Ingwar . CropNonEmptyMaskIfExists now also works with multiple masks that are provided by the masks argument to the transform function. Previously this augmentation worked only with a single mask provided by the mask argument. PR #761 . 0.5.1 (2 November 2020) \u00b6 Breaking changes \u00b6 API for A.FDA is changed to resemble API of A.HistogramMatching . Now, both transformations expect to receive a list of reference images, a function to read those image, and additional augmentation parameters. (#734) A.HistogramMatching now uses read_rgb_image as a default read_fn . This function reads an image from the disk as an RGB NumPy array. Previously, the default read_fn was cv2.imread which read an image as a BGR NumPy array. (#734) New transformations \u00b6 A.Sequential transform that can apply augmentations in a sequence. This transform is not intended to be a replacement for A.Compose . Instead, it should be used inside A.Compose the same way A.OneOf or A.OneOrOther . For instance, you can combine A.OneOf with A.Sequential to create an augmentation pipeline containing multiple sequences of augmentations and apply one randomly chosen sequence to input data. (#735) Minor changes \u00b6 A.ShiftScaleRotate now has two additional optional parameters: shift_limit_x and shift_limit_y . If either of those parameters (or both of them) is set A.ShiftScaleRotate will use the set values to shift images on the respective axis. (#735) A.ToTensorV2 now supports an additional argument transpose_mask ( False by default). If the argument is set to True and an input mask has 3 dimensions, A.ToTensorV2 will transpose dimensions of a mask tensor in addition to transposing dimensions of an image tensor. (#735) Bugfixes \u00b6 A.FDA now correctly uses coordinates of the center of an image. (#730) Fixed problems with grayscale images for A.HistogramMatching . (#734) Fixed a bug that led to an exception when A.load() was called to deserialize a pipeline that contained A.ToTensor or A.ToTensorV2 , but those transforms were not imported in the code before the call. (#735) 0.5.0 (19 October 2020) \u00b6 Breaking changes \u00b6 Albumentations now explicitly checks that all inputs to augmentations are named arguments and raise an exception otherwise. So if an augmentation receives input like aug(image) instead of aug(image=image), Albumentations will raise an exception. ( #560 ) Dropped support of Python 3.5 ( #709 ) Keypoints and bboxes are checked for visibility after each transform ( #566 ) New transformations \u00b6 A.FDA transform for Fourier-based domain adaptation. ( #685 ) A.HistogramMatching transform that applies histogram matching. ( #708 ) A.ColorJitter transform that behaves similarly to ColorJitter from torchvision (though there are some minor differences due to different internal logic for working with HSV colorspace in Pillow, which is used in torchvision and OpenCV, which is used in Albumentations). ( #705 ) Minor changes \u00b6 A.PadIfNeeded now accepts additional pad_width_divisor , pad_height_divisor ( None by default) to ensure image has width & height that is dividable by given values. ( #700 ) Added support to apply A.CoarseDropout to masks via mask_fill_value . ( #699 ) A.GaussianBlur now supports the sigma parameter that sets standard deviation for Gaussian kernel. ( #674 , #673 ) . Bugfixes \u00b6 Fixed bugs in A.HueSaturationValue for float dtype. ( #696 , #710 ) Fixed incorrect rounding error on bboxes in YOLO format. ( #688 ) 0.4.6 (19 July 2020) \u00b6 Improvements \u00b6 Change the ImgAug dependency version from \u201cimgaug>=0.2.5,<0.2.7\u201d to \u201cimgaug>=0.4.0\". Now Albumentations won\u2019t downgrade your existing ImgAug installation to the old version. PR #658 . Do not try to resize an image if it already has the required height and width. That eliminates the redundant call to the OpenCV function that requires additional copying of the input data. PR #639 . ReplayCompose is now serializable. PR #623 by IlyaOvodov Documentation fixes and updates. Bug Fixes \u00b6 Fix a bug that causes some keypoints and bounding boxes to lie outside the visible part of the augmented image if an augmentation pipeline contained augmentations that increase the height and width of an image (such as PadIfNeeded ). That happened because Albumentations checked which bounding boxes and keypoints lie outside the image only after applying all augmentations. Now Albumentations will check and remove keypoints and bounding boxes that lie outside the image after each augmentation. If, for some reason, you need the old behavior, pass check_each_transform=False in your KeypointParams or BboxParams . Issue #565 and PR #566 . Fix a bug that causes an exception when Albumentations received images with the number of color channels that are even but are not multiples of 4 (such as 6, 10, etc.). PR #638 . Fix the off-by-one error in applying steps for GridDistortion. Commit 9c225a9 Fix bugs that prevent serialization of ImageCompression and GaussNoise . PR #569 Fix a bug that causes errors with some values for label_fields in BboxParams . PR #504 by IlyaOvodov Fix a bug that prevents HueSaturationValue for working with grayscale images. PR #500 .","title":"Release notes"},{"location":"release_notes/#release-notes","text":"","title":"Release notes"},{"location":"release_notes/#052-29-november-2020","text":"","title":"0.5.2 (29 November 2020)"},{"location":"release_notes/#minor-changes","text":"ToTensorV2 now automatically expands grayscale images with the shape [H, W] to the shape [H, W, 1] . PR #604 by @Ingwar . CropNonEmptyMaskIfExists now also works with multiple masks that are provided by the masks argument to the transform function. Previously this augmentation worked only with a single mask provided by the mask argument. PR #761 .","title":"Minor changes"},{"location":"release_notes/#051-2-november-2020","text":"","title":"0.5.1 (2 November 2020)"},{"location":"release_notes/#breaking-changes","text":"API for A.FDA is changed to resemble API of A.HistogramMatching . Now, both transformations expect to receive a list of reference images, a function to read those image, and additional augmentation parameters. (#734) A.HistogramMatching now uses read_rgb_image as a default read_fn . This function reads an image from the disk as an RGB NumPy array. Previously, the default read_fn was cv2.imread which read an image as a BGR NumPy array. (#734)","title":"Breaking changes"},{"location":"release_notes/#new-transformations","text":"A.Sequential transform that can apply augmentations in a sequence. This transform is not intended to be a replacement for A.Compose . Instead, it should be used inside A.Compose the same way A.OneOf or A.OneOrOther . For instance, you can combine A.OneOf with A.Sequential to create an augmentation pipeline containing multiple sequences of augmentations and apply one randomly chosen sequence to input data. (#735)","title":"New transformations"},{"location":"release_notes/#minor-changes_1","text":"A.ShiftScaleRotate now has two additional optional parameters: shift_limit_x and shift_limit_y . If either of those parameters (or both of them) is set A.ShiftScaleRotate will use the set values to shift images on the respective axis. (#735) A.ToTensorV2 now supports an additional argument transpose_mask ( False by default). If the argument is set to True and an input mask has 3 dimensions, A.ToTensorV2 will transpose dimensions of a mask tensor in addition to transposing dimensions of an image tensor. (#735)","title":"Minor changes"},{"location":"release_notes/#bugfixes","text":"A.FDA now correctly uses coordinates of the center of an image. (#730) Fixed problems with grayscale images for A.HistogramMatching . (#734) Fixed a bug that led to an exception when A.load() was called to deserialize a pipeline that contained A.ToTensor or A.ToTensorV2 , but those transforms were not imported in the code before the call. (#735)","title":"Bugfixes"},{"location":"release_notes/#050-19-october-2020","text":"","title":"0.5.0 (19 October 2020)"},{"location":"release_notes/#breaking-changes_1","text":"Albumentations now explicitly checks that all inputs to augmentations are named arguments and raise an exception otherwise. So if an augmentation receives input like aug(image) instead of aug(image=image), Albumentations will raise an exception. ( #560 ) Dropped support of Python 3.5 ( #709 ) Keypoints and bboxes are checked for visibility after each transform ( #566 )","title":"Breaking changes"},{"location":"release_notes/#new-transformations_1","text":"A.FDA transform for Fourier-based domain adaptation. ( #685 ) A.HistogramMatching transform that applies histogram matching. ( #708 ) A.ColorJitter transform that behaves similarly to ColorJitter from torchvision (though there are some minor differences due to different internal logic for working with HSV colorspace in Pillow, which is used in torchvision and OpenCV, which is used in Albumentations). ( #705 )","title":"New transformations"},{"location":"release_notes/#minor-changes_2","text":"A.PadIfNeeded now accepts additional pad_width_divisor , pad_height_divisor ( None by default) to ensure image has width & height that is dividable by given values. ( #700 ) Added support to apply A.CoarseDropout to masks via mask_fill_value . ( #699 ) A.GaussianBlur now supports the sigma parameter that sets standard deviation for Gaussian kernel. ( #674 , #673 ) .","title":"Minor changes"},{"location":"release_notes/#bugfixes_1","text":"Fixed bugs in A.HueSaturationValue for float dtype. ( #696 , #710 ) Fixed incorrect rounding error on bboxes in YOLO format. ( #688 )","title":"Bugfixes"},{"location":"release_notes/#046-19-july-2020","text":"","title":"0.4.6 (19 July 2020)"},{"location":"release_notes/#improvements","text":"Change the ImgAug dependency version from \u201cimgaug>=0.2.5,<0.2.7\u201d to \u201cimgaug>=0.4.0\". Now Albumentations won\u2019t downgrade your existing ImgAug installation to the old version. PR #658 . Do not try to resize an image if it already has the required height and width. That eliminates the redundant call to the OpenCV function that requires additional copying of the input data. PR #639 . ReplayCompose is now serializable. PR #623 by IlyaOvodov Documentation fixes and updates.","title":"Improvements"},{"location":"release_notes/#bug-fixes","text":"Fix a bug that causes some keypoints and bounding boxes to lie outside the visible part of the augmented image if an augmentation pipeline contained augmentations that increase the height and width of an image (such as PadIfNeeded ). That happened because Albumentations checked which bounding boxes and keypoints lie outside the image only after applying all augmentations. Now Albumentations will check and remove keypoints and bounding boxes that lie outside the image after each augmentation. If, for some reason, you need the old behavior, pass check_each_transform=False in your KeypointParams or BboxParams . Issue #565 and PR #566 . Fix a bug that causes an exception when Albumentations received images with the number of color channels that are even but are not multiples of 4 (such as 6, 10, etc.). PR #638 . Fix the off-by-one error in applying steps for GridDistortion. Commit 9c225a9 Fix bugs that prevent serialization of ImageCompression and GaussNoise . PR #569 Fix a bug that causes errors with some values for label_fields in BboxParams . PR #504 by IlyaOvodov Fix a bug that prevents HueSaturationValue for working with grayscale images. PR #500 .","title":"Bug Fixes"},{"location":"api_reference/","text":"Core API (albumentations.core) Composition API (albumentations.core.composition) Serialization API (albumentations.core.serialization) Transforms Interface (albumentations.core.transforms_interface) Augmentations (albumentations.augmentations) Transforms (albumentations.augmentations.transforms) Functional transforms (albumentations.augmentations.functional) Helper functions for working with bounding boxes (albumentations.augmentations.bbox_utils) Helper functions for working with keypoints (albumentations.augmentations.keypoints_utils) ImgAug Helpers (albumentations.imgaug) Transforms (albumentations.imgaug.transforms) PyTorch Helpers (albumentations.pytorch) Transforms (albumentations.pytorch.transforms)","title":"Index"},{"location":"api_reference/augmentations/","text":"Transforms (albumentations.augmentations.transforms) Crop transforms (albumentations.augmentations.crops) Geometric transforms (albumentations.augmentations.geometric) Domain adaptation transforms (albumentations.augmentations.domain_adaptation) Functional transforms (albumentations.augmentations.functional) Helper functions for working with bounding boxes (albumentations.augmentations.bbox_utils) Helper functions for working with keypoints (albumentations.augmentations.keypoints_utils)","title":"Index"},{"location":"api_reference/augmentations/bbox_utils/","text":"Helper functions for working with bounding boxes (augmentations.bbox_utils) \u00b6 \u00b6 def albumentations.augmentations.bbox_utils.calculate_bbox_area ( bbox , rows , cols ) [view source on GitHub] \u00b6 Calculate the area of a bounding box in pixels. Parameters: Name Type Description bbox tuple A bounding box (x_min, y_min, x_max, y_max) . rows int Image height. cols int Image width. Returns: Type Description int Area of a bounding box in pixels. def albumentations.augmentations.bbox_utils.check_bbox ( bbox ) [view source on GitHub] \u00b6 Check if bbox boundaries are in range 0, 1 and minimums are lesser then maximums def albumentations.augmentations.bbox_utils.check_bboxes ( bboxes ) [view source on GitHub] \u00b6 Check if bboxes boundaries are in range 0, 1 and minimums are lesser then maximums def albumentations.augmentations.bbox_utils.convert_bbox_from_albumentations ( bbox , target_format , rows , cols , check_validity = False ) [view source on GitHub] \u00b6 Convert a bounding box from the format used by albumentations to a format, specified in target_format . Parameters: Name Type Description bbox tuple An albumentation bounding box (x_min, y_min, x_max, y_max) . target_format str required format of the output bounding box. Should be 'coco', 'pascal_voc' or 'yolo'. rows int Image height. cols int Image width. check_validity bool Check if all boxes are valid boxes. Returns: Type Description tuple A bounding box. Note: The coco format of a bounding box looks like [x_min, y_min, width, height] , e.g. [97, 12, 150, 200]. The pascal_voc format of a bounding box looks like [x_min, y_min, x_max, y_max] , e.g. [97, 12, 247, 212]. The yolo format of a bounding box looks like [x, y, width, height] , e.g. [0.3, 0.1, 0.05, 0.07]. Exceptions: Type Description ValueError if target_format is not equal to coco , pascal_voc or yolo . def albumentations.augmentations.bbox_utils.convert_bbox_to_albumentations ( bbox , source_format , rows , cols , check_validity = False ) [view source on GitHub] \u00b6 Convert a bounding box from a format specified in source_format to the format used by albumentations: normalized coordinates of bottom-left and top-right corners of the bounding box in a form of (x_min, y_min, x_max, y_max) e.g. (0.15, 0.27, 0.67, 0.5) . Parameters: Name Type Description bbox tuple A bounding box tuple. source_format str format of the bounding box. Should be 'coco', 'pascal_voc', or 'yolo'. check_validity bool Check if all boxes are valid boxes. rows int Image height. cols int Image width. Returns: Type Description tuple A bounding box (x_min, y_min, x_max, y_max) . Note: The coco format of a bounding box looks like (x_min, y_min, width, height) , e.g. (97, 12, 150, 200). The pascal_voc format of a bounding box looks like (x_min, y_min, x_max, y_max) , e.g. (97, 12, 247, 212). The yolo format of a bounding box looks like (x, y, width, height) , e.g. (0.3, 0.1, 0.05, 0.07); where x , y coordinates of the center of the box, all values normalized to 1 by image height and width. Exceptions: Type Description ValueError if target_format is not equal to coco or pascal_voc , ot yolo . ValueError If in YOLO format all labels not in range (0, 1). def albumentations.augmentations.bbox_utils.convert_bboxes_from_albumentations ( bboxes , target_format , rows , cols , check_validity = False ) [view source on GitHub] \u00b6 Convert a list of bounding boxes from the format used by albumentations to a format, specified in target_format . Parameters: Name Type Description bboxes List[tuple] List of albumentation bounding box (x_min, y_min, x_max, y_max) . target_format str required format of the output bounding box. Should be 'coco', 'pascal_voc' or 'yolo'. rows int Image height. cols int Image width. check_validity bool Check if all boxes are valid boxes. Returns: Type Description list[tuple] List of bounding box. def albumentations.augmentations.bbox_utils.convert_bboxes_to_albumentations ( bboxes , source_format , rows , cols , check_validity = False ) [view source on GitHub] \u00b6 Convert a list bounding boxes from a format specified in source_format to the format used by albumentations def albumentations.augmentations.bbox_utils.denormalize_bbox ( bbox , rows , cols ) [view source on GitHub] \u00b6 Denormalize coordinates of a bounding box. Multiply x-coordinates by image width and y-coordinates by image height. This is an inverse operation for :func: ~albumentations.augmentations.bbox.normalize_bbox . Parameters: Name Type Description bbox tuple Normalized bounding box (x_min, y_min, x_max, y_max) . rows int Image height. cols int Image width. Returns: Type Description tuple Denormalized bounding box (x_min, y_min, x_max, y_max) . Exceptions: Type Description ValueError If rows or cols is less or equal zero def albumentations.augmentations.bbox_utils.denormalize_bboxes ( bboxes , rows , cols ) [view source on GitHub] \u00b6 Denormalize a list of bounding boxes. Parameters: Name Type Description bboxes List[tuple] Normalized bounding boxes [(x_min, y_min, x_max, y_max)] . rows int Image height. cols int Image width. Returns: Type Description List[tuple] Denormalized bounding boxes [(x_min, y_min, x_max, y_max)] . def albumentations.augmentations.bbox_utils.filter_bboxes ( bboxes , rows , cols , min_area = 0.0 , min_visibility = 0.0 ) [view source on GitHub] \u00b6 Remove bounding boxes that either lie outside of the visible area by more then min_visibility or whose area in pixels is under the threshold set by min_area . Also it crops boxes to final image size. Parameters: Name Type Description bboxes List[tuple] List of albumentation bounding box (x_min, y_min, x_max, y_max) . rows int Image height. cols int Image width. min_area float Minimum area of a bounding box. All bounding boxes whose visible area in pixels. is less than this value will be removed. Default: 0.0. min_visibility float Minimum fraction of area for a bounding box to remain this box in list. Default: 0.0. Returns: Type Description List[tuple] List of bounding box. def albumentations.augmentations.bbox_utils.filter_bboxes_by_visibility ( original_shape , bboxes , transformed_shape , transformed_bboxes , threshold = 0.0 , min_area = 0.0 ) [view source on GitHub] \u00b6 Filter bounding boxes and return only those boxes whose visibility after transformation is above the threshold and minimal area of bounding box in pixels is more then min_area. Parameters: Name Type Description original_shape tuple Original image shape (height, width) . bboxes List[tuple] Original bounding boxes [(x_min, y_min, x_max, y_max)] . transformed_shape tuple Transformed image shape (height, width) . transformed_bboxes List[tuple] Transformed bounding boxes [(x_min, y_min, x_max, y_max)] . threshold float visibility threshold. Should be a value in the range [0.0, 1.0]. min_area float Minimal area threshold. Returns: Type Description List[tuple] Filtered bounding boxes [(x_min, y_min, x_max, y_max)] . def albumentations.augmentations.bbox_utils.normalize_bbox ( bbox , rows , cols ) [view source on GitHub] \u00b6 Normalize coordinates of a bounding box. Divide x-coordinates by image width and y-coordinates by image height. Parameters: Name Type Description bbox tuple Denormalized bounding box (x_min, y_min, x_max, y_max) . rows int Image height. cols int Image width. Returns: Type Description tuple Normalized bounding box (x_min, y_min, x_max, y_max) . Exceptions: Type Description ValueError If rows or cols is less or equal zero def albumentations.augmentations.bbox_utils.normalize_bboxes ( bboxes , rows , cols ) [view source on GitHub] \u00b6 Normalize a list of bounding boxes. Parameters: Name Type Description bboxes List[tuple] Denormalized bounding boxes [(x_min, y_min, x_max, y_max)] . rows int Image height. cols int Image width. Returns: Type Description List[tuple] Normalized bounding boxes [(x_min, y_min, x_max, y_max)] . def albumentations.augmentations.bbox_utils.union_of_bboxes ( height , width , bboxes , erosion_rate = 0.0 ) [view source on GitHub] \u00b6 Calculate union of bounding boxes. Parameters: Name Type Description height float Height of image or space. width float Width of image or space. bboxes List[tuple] List like bounding boxes. Format is [(x_min, y_min, x_max, y_max)] . erosion_rate float How much each bounding box can be shrinked, useful for erosive cropping. Set this in range [0, 1]. 0 will not be erosive at all, 1.0 can make any bbox to lose its volume. Returns: Type Description tuple A bounding box (x_min, y_min, x_max, y_max) .","title":"Helper functions for working with bounding boxes (augmentations.bbox_utils)"},{"location":"api_reference/augmentations/bbox_utils/#helper-functions-for-working-with-bounding-boxes-augmentationsbbox_utils","text":"","title":"Helper functions for working with bounding boxes (augmentations.bbox_utils)"},{"location":"api_reference/augmentations/bbox_utils/#albumentations.augmentations.bbox_utils","text":"","title":"albumentations.augmentations.bbox_utils"},{"location":"api_reference/augmentations/bbox_utils/#albumentations.augmentations.bbox_utils.calculate_bbox_area","text":"Calculate the area of a bounding box in pixels. Parameters: Name Type Description bbox tuple A bounding box (x_min, y_min, x_max, y_max) . rows int Image height. cols int Image width. Returns: Type Description int Area of a bounding box in pixels.","title":"calculate_bbox_area()"},{"location":"api_reference/augmentations/bbox_utils/#albumentations.augmentations.bbox_utils.check_bbox","text":"Check if bbox boundaries are in range 0, 1 and minimums are lesser then maximums","title":"check_bbox()"},{"location":"api_reference/augmentations/bbox_utils/#albumentations.augmentations.bbox_utils.check_bboxes","text":"Check if bboxes boundaries are in range 0, 1 and minimums are lesser then maximums","title":"check_bboxes()"},{"location":"api_reference/augmentations/bbox_utils/#albumentations.augmentations.bbox_utils.convert_bbox_from_albumentations","text":"Convert a bounding box from the format used by albumentations to a format, specified in target_format . Parameters: Name Type Description bbox tuple An albumentation bounding box (x_min, y_min, x_max, y_max) . target_format str required format of the output bounding box. Should be 'coco', 'pascal_voc' or 'yolo'. rows int Image height. cols int Image width. check_validity bool Check if all boxes are valid boxes. Returns: Type Description tuple A bounding box. Note: The coco format of a bounding box looks like [x_min, y_min, width, height] , e.g. [97, 12, 150, 200]. The pascal_voc format of a bounding box looks like [x_min, y_min, x_max, y_max] , e.g. [97, 12, 247, 212]. The yolo format of a bounding box looks like [x, y, width, height] , e.g. [0.3, 0.1, 0.05, 0.07]. Exceptions: Type Description ValueError if target_format is not equal to coco , pascal_voc or yolo .","title":"convert_bbox_from_albumentations()"},{"location":"api_reference/augmentations/bbox_utils/#albumentations.augmentations.bbox_utils.convert_bbox_to_albumentations","text":"Convert a bounding box from a format specified in source_format to the format used by albumentations: normalized coordinates of bottom-left and top-right corners of the bounding box in a form of (x_min, y_min, x_max, y_max) e.g. (0.15, 0.27, 0.67, 0.5) . Parameters: Name Type Description bbox tuple A bounding box tuple. source_format str format of the bounding box. Should be 'coco', 'pascal_voc', or 'yolo'. check_validity bool Check if all boxes are valid boxes. rows int Image height. cols int Image width. Returns: Type Description tuple A bounding box (x_min, y_min, x_max, y_max) . Note: The coco format of a bounding box looks like (x_min, y_min, width, height) , e.g. (97, 12, 150, 200). The pascal_voc format of a bounding box looks like (x_min, y_min, x_max, y_max) , e.g. (97, 12, 247, 212). The yolo format of a bounding box looks like (x, y, width, height) , e.g. (0.3, 0.1, 0.05, 0.07); where x , y coordinates of the center of the box, all values normalized to 1 by image height and width. Exceptions: Type Description ValueError if target_format is not equal to coco or pascal_voc , ot yolo . ValueError If in YOLO format all labels not in range (0, 1).","title":"convert_bbox_to_albumentations()"},{"location":"api_reference/augmentations/bbox_utils/#albumentations.augmentations.bbox_utils.convert_bboxes_from_albumentations","text":"Convert a list of bounding boxes from the format used by albumentations to a format, specified in target_format . Parameters: Name Type Description bboxes List[tuple] List of albumentation bounding box (x_min, y_min, x_max, y_max) . target_format str required format of the output bounding box. Should be 'coco', 'pascal_voc' or 'yolo'. rows int Image height. cols int Image width. check_validity bool Check if all boxes are valid boxes. Returns: Type Description list[tuple] List of bounding box.","title":"convert_bboxes_from_albumentations()"},{"location":"api_reference/augmentations/bbox_utils/#albumentations.augmentations.bbox_utils.convert_bboxes_to_albumentations","text":"Convert a list bounding boxes from a format specified in source_format to the format used by albumentations","title":"convert_bboxes_to_albumentations()"},{"location":"api_reference/augmentations/bbox_utils/#albumentations.augmentations.bbox_utils.denormalize_bbox","text":"Denormalize coordinates of a bounding box. Multiply x-coordinates by image width and y-coordinates by image height. This is an inverse operation for :func: ~albumentations.augmentations.bbox.normalize_bbox . Parameters: Name Type Description bbox tuple Normalized bounding box (x_min, y_min, x_max, y_max) . rows int Image height. cols int Image width. Returns: Type Description tuple Denormalized bounding box (x_min, y_min, x_max, y_max) . Exceptions: Type Description ValueError If rows or cols is less or equal zero","title":"denormalize_bbox()"},{"location":"api_reference/augmentations/bbox_utils/#albumentations.augmentations.bbox_utils.denormalize_bboxes","text":"Denormalize a list of bounding boxes. Parameters: Name Type Description bboxes List[tuple] Normalized bounding boxes [(x_min, y_min, x_max, y_max)] . rows int Image height. cols int Image width. Returns: Type Description List[tuple] Denormalized bounding boxes [(x_min, y_min, x_max, y_max)] .","title":"denormalize_bboxes()"},{"location":"api_reference/augmentations/bbox_utils/#albumentations.augmentations.bbox_utils.filter_bboxes","text":"Remove bounding boxes that either lie outside of the visible area by more then min_visibility or whose area in pixels is under the threshold set by min_area . Also it crops boxes to final image size. Parameters: Name Type Description bboxes List[tuple] List of albumentation bounding box (x_min, y_min, x_max, y_max) . rows int Image height. cols int Image width. min_area float Minimum area of a bounding box. All bounding boxes whose visible area in pixels. is less than this value will be removed. Default: 0.0. min_visibility float Minimum fraction of area for a bounding box to remain this box in list. Default: 0.0. Returns: Type Description List[tuple] List of bounding box.","title":"filter_bboxes()"},{"location":"api_reference/augmentations/bbox_utils/#albumentations.augmentations.bbox_utils.filter_bboxes_by_visibility","text":"Filter bounding boxes and return only those boxes whose visibility after transformation is above the threshold and minimal area of bounding box in pixels is more then min_area. Parameters: Name Type Description original_shape tuple Original image shape (height, width) . bboxes List[tuple] Original bounding boxes [(x_min, y_min, x_max, y_max)] . transformed_shape tuple Transformed image shape (height, width) . transformed_bboxes List[tuple] Transformed bounding boxes [(x_min, y_min, x_max, y_max)] . threshold float visibility threshold. Should be a value in the range [0.0, 1.0]. min_area float Minimal area threshold. Returns: Type Description List[tuple] Filtered bounding boxes [(x_min, y_min, x_max, y_max)] .","title":"filter_bboxes_by_visibility()"},{"location":"api_reference/augmentations/bbox_utils/#albumentations.augmentations.bbox_utils.normalize_bbox","text":"Normalize coordinates of a bounding box. Divide x-coordinates by image width and y-coordinates by image height. Parameters: Name Type Description bbox tuple Denormalized bounding box (x_min, y_min, x_max, y_max) . rows int Image height. cols int Image width. Returns: Type Description tuple Normalized bounding box (x_min, y_min, x_max, y_max) . Exceptions: Type Description ValueError If rows or cols is less or equal zero","title":"normalize_bbox()"},{"location":"api_reference/augmentations/bbox_utils/#albumentations.augmentations.bbox_utils.normalize_bboxes","text":"Normalize a list of bounding boxes. Parameters: Name Type Description bboxes List[tuple] Denormalized bounding boxes [(x_min, y_min, x_max, y_max)] . rows int Image height. cols int Image width. Returns: Type Description List[tuple] Normalized bounding boxes [(x_min, y_min, x_max, y_max)] .","title":"normalize_bboxes()"},{"location":"api_reference/augmentations/bbox_utils/#albumentations.augmentations.bbox_utils.union_of_bboxes","text":"Calculate union of bounding boxes. Parameters: Name Type Description height float Height of image or space. width float Width of image or space. bboxes List[tuple] List like bounding boxes. Format is [(x_min, y_min, x_max, y_max)] . erosion_rate float How much each bounding box can be shrinked, useful for erosive cropping. Set this in range [0, 1]. 0 will not be erosive at all, 1.0 can make any bbox to lose its volume. Returns: Type Description tuple A bounding box (x_min, y_min, x_max, y_max) .","title":"union_of_bboxes()"},{"location":"api_reference/augmentations/domain_adaptation/","text":"Domain adaptation transforms (augmentations.domain_adaptation) \u00b6 \u00b6 class albumentations.augmentations.domain_adaptation.FDA ( reference_images , beta_limit = 0.1 , read_fn =< function read_rgb_image at 0x7f7d853e8af0 > , always_apply = False , p = 0.5 ) [view source on GitHub] \u00b6 Fourier Domain Adaptation from https://github.com/YanchaoYang/FDA Simple \"style transfer\". Parameters: Name Type Description reference_images List[str] or List(np.ndarray List of file paths for reference images or list of reference images. beta_limit float or tuple of float coefficient beta from paper. Recommended less 0.3. read_fn Callable Used-defined function to read image. Function should get image path and return numpy array of image pixels. Targets: image Image types: uint8, float32 Reference: https://github.com/YanchaoYang/FDA https://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_FDA_Fourier_Domain_Adaptation_for_Semantic_Segmentation_CVPR_2020_paper.pdf Examples: 1 2 3 4 5 6 >>> import numpy as np >>> import albumentations as A >>> image = np . random . randint ( 0 , 256 , [ 100 , 100 , 3 ], dtype = np . uint8 ) >>> target_image = np . random . randint ( 0 , 256 , [ 100 , 100 , 3 ], dtype = np . uint8 ) >>> aug = A . Compose ([ A . FDA ([ target_image ], p = 1 , read_fn = lambda x : x )]) >>> result = aug ( image = image ) class albumentations.augmentations.domain_adaptation.HistogramMatching ( reference_images , blend_ratio = ( 0.5 , 1.0 ), read_fn =< function read_rgb_image at 0x7f7d853e8af0 > , always_apply = False , p = 0.5 ) [view source on GitHub] \u00b6 Apply histogram matching. It manipulates the pixels of an input image so that its histogram matches the histogram of the reference image. If the images have multiple channels, the matching is done independently for each channel, as long as the number of channels is equal in the input image and the reference. Histogram matching can be used as a lightweight normalisation for image processing, such as feature matching, especially in circumstances where the images have been taken from different sources or in different conditions (i.e. lighting). See: https://scikit-image.org/docs/dev/auto_examples/color_exposure/plot_histogram_matching.html Parameters: Name Type Description reference_images List[str] or List(np.ndarray List of file paths for reference images or list of reference images. blend_ratio [float, float] Tuple of min and max blend ratio. Matched image will be blended with original with random blend factor for increased diversity of generated images. read_fn Callable Used-defined function to read image. Function should get image path and return numpy array of image pixels. p float probability of applying the transform. Default: 1.0. Targets: image Image types: uint8, uint16, float32 def albumentations.augmentations.domain_adaptation.fourier_domain_adaptation ( img , target_img , beta ) [view source on GitHub] \u00b6 Fourier Domain Adaptation from https://github.com/YanchaoYang/FDA Parameters: Name Type Description img ndarray source image target_img ndarray target image for domain adaptation beta float coefficient from source paper Returns: Type Description ndarray transformed image","title":"Domain adaptation transforms (augmentations.domain_adaptation)"},{"location":"api_reference/augmentations/domain_adaptation/#domain-adaptation-transforms-augmentationsdomain_adaptation","text":"","title":"Domain adaptation transforms (augmentations.domain_adaptation)"},{"location":"api_reference/augmentations/domain_adaptation/#albumentations.augmentations.domain_adaptation","text":"","title":"albumentations.augmentations.domain_adaptation"},{"location":"api_reference/augmentations/domain_adaptation/#albumentations.augmentations.domain_adaptation.FDA","text":"Fourier Domain Adaptation from https://github.com/YanchaoYang/FDA Simple \"style transfer\". Parameters: Name Type Description reference_images List[str] or List(np.ndarray List of file paths for reference images or list of reference images. beta_limit float or tuple of float coefficient beta from paper. Recommended less 0.3. read_fn Callable Used-defined function to read image. Function should get image path and return numpy array of image pixels. Targets: image Image types: uint8, float32 Reference: https://github.com/YanchaoYang/FDA https://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_FDA_Fourier_Domain_Adaptation_for_Semantic_Segmentation_CVPR_2020_paper.pdf Examples: 1 2 3 4 5 6 >>> import numpy as np >>> import albumentations as A >>> image = np . random . randint ( 0 , 256 , [ 100 , 100 , 3 ], dtype = np . uint8 ) >>> target_image = np . random . randint ( 0 , 256 , [ 100 , 100 , 3 ], dtype = np . uint8 ) >>> aug = A . Compose ([ A . FDA ([ target_image ], p = 1 , read_fn = lambda x : x )]) >>> result = aug ( image = image )","title":"FDA"},{"location":"api_reference/augmentations/domain_adaptation/#albumentations.augmentations.domain_adaptation.HistogramMatching","text":"Apply histogram matching. It manipulates the pixels of an input image so that its histogram matches the histogram of the reference image. If the images have multiple channels, the matching is done independently for each channel, as long as the number of channels is equal in the input image and the reference. Histogram matching can be used as a lightweight normalisation for image processing, such as feature matching, especially in circumstances where the images have been taken from different sources or in different conditions (i.e. lighting). See: https://scikit-image.org/docs/dev/auto_examples/color_exposure/plot_histogram_matching.html Parameters: Name Type Description reference_images List[str] or List(np.ndarray List of file paths for reference images or list of reference images. blend_ratio [float, float] Tuple of min and max blend ratio. Matched image will be blended with original with random blend factor for increased diversity of generated images. read_fn Callable Used-defined function to read image. Function should get image path and return numpy array of image pixels. p float probability of applying the transform. Default: 1.0. Targets: image Image types: uint8, uint16, float32","title":"HistogramMatching"},{"location":"api_reference/augmentations/domain_adaptation/#albumentations.augmentations.domain_adaptation.fourier_domain_adaptation","text":"Fourier Domain Adaptation from https://github.com/YanchaoYang/FDA Parameters: Name Type Description img ndarray source image target_img ndarray target image for domain adaptation beta float coefficient from source paper Returns: Type Description ndarray transformed image","title":"fourier_domain_adaptation()"},{"location":"api_reference/augmentations/functional/","text":"Functional transforms (augmentations.functional) \u00b6 \u00b6 def albumentations.augmentations.functional.add_fog ( img , fog_coef , alpha_coef , haze_list ) [view source on GitHub] \u00b6 Add fog to the image. From https://github.com/UjjwalSaxena/Automold--Road-Augmentation-Library Parameters: Name Type Description img numpy.ndarray Image. fog_coef float Fog coefficient. alpha_coef float Alpha coefficient. haze_list list Returns: Type Description numpy.ndarray Image. def albumentations.augmentations.functional.add_rain ( img , slant , drop_length , drop_width , drop_color , blur_value , brightness_coefficient , rain_drops ) [view source on GitHub] \u00b6 From https://github.com/UjjwalSaxena/Automold--Road-Augmentation-Library Parameters: Name Type Description img numpy.ndarray Image. slant int drop_length drop_width drop_color blur_value int Rainy view are blurry. brightness_coefficient float Rainy days are usually shady. rain_drops Returns: Type Description numpy.ndarray Image. def albumentations.augmentations.functional.add_shadow ( img , vertices_list ) [view source on GitHub] \u00b6 Add shadows to the image. From https://github.com/UjjwalSaxena/Automold--Road-Augmentation-Library Parameters: Name Type Description img numpy.ndarray vertices_list list Returns: Type Description numpy.ndarray def albumentations.augmentations.functional.add_snow ( img , snow_point , brightness_coeff ) [view source on GitHub] \u00b6 Bleaches out pixels, imitation snow. From https://github.com/UjjwalSaxena/Automold--Road-Augmentation-Library Parameters: Name Type Description img numpy.ndarray Image. snow_point Number of show points. brightness_coeff Brightness coefficient. Returns: Type Description numpy.ndarray Image. def albumentations.augmentations.functional.add_sun_flare ( img , flare_center_x , flare_center_y , src_radius , src_color , circles ) [view source on GitHub] \u00b6 Add sun flare. From https://github.com/UjjwalSaxena/Automold--Road-Augmentation-Library Parameters: Name Type Description img numpy.ndarray flare_center_x float flare_center_y float src_radius src_color int, int, int circles list Returns: Type Description numpy.ndarray def albumentations.augmentations.functional.bbox_flip ( bbox , d , rows , cols ) [view source on GitHub] \u00b6 Flip a bounding box either vertically, horizontally or both depending on the value of d . Parameters: Name Type Description bbox tuple A bounding box (x_min, y_min, x_max, y_max) . d int rows int Image rows. cols int Image cols. Returns: Type Description tuple A bounding box (x_min, y_min, x_max, y_max) . Exceptions: Type Description ValueError if value of d is not -1, 0 or 1. def albumentations.augmentations.functional.bbox_hflip ( bbox , rows , cols ) [view source on GitHub] \u00b6 Flip a bounding box horizontally around the y-axis. Parameters: Name Type Description bbox tuple A bounding box (x_min, y_min, x_max, y_max) . rows int Image rows. cols int Image cols. Returns: Type Description tuple A bounding box (x_min, y_min, x_max, y_max) . def albumentations.augmentations.functional.bbox_transpose ( bbox , axis , rows , cols ) [view source on GitHub] \u00b6 Transposes a bounding box along given axis. Parameters: Name Type Description bbox tuple A bounding box (x_min, y_min, x_max, y_max) . axis int 0 - main axis, 1 - secondary axis. rows int Image rows. cols int Image cols. Returns: Type Description tuple A bounding box tuple (x_min, y_min, x_max, y_max) . Exceptions: Type Description ValueError If axis not equal to 0 or 1. def albumentations.augmentations.functional.bbox_vflip ( bbox , rows , cols ) [view source on GitHub] \u00b6 Flip a bounding box vertically around the x-axis. Parameters: Name Type Description bbox tuple A bounding box (x_min, y_min, x_max, y_max) . rows int Image rows. cols int Image cols. Returns: Type Description tuple A bounding box (x_min, y_min, x_max, y_max) . def albumentations.augmentations.functional.elastic_transform_approx ( img , alpha , sigma , alpha_affine , interpolation = 1 , border_mode = 4 , value = None , random_state = None ) [view source on GitHub] \u00b6 Elastic deformation of images as described in [Simard2003]_ (with modifications for speed). Based on https://gist.github.com/erniejunior/601cdf56d2b424757de5 .. [Simard2003] Simard, Steinkraus and Platt, \"Best Practices for Convolutional Neural Networks applied to Visual Document Analysis\", in Proc. of the International Conference on Document Analysis and Recognition, 2003. def albumentations.augmentations.functional.equalize ( img , mask = None , mode = 'cv' , by_channels = True ) [view source on GitHub] \u00b6 Equalize the image histogram. Parameters: Name Type Description img numpy.ndarray RGB or grayscale image. mask numpy.ndarray An optional mask. If given, only the pixels selected by the mask are included in the analysis. Maybe 1 channel or 3 channel array. mode str {'cv', 'pil'}. Use OpenCV or Pillow equalization method. by_channels bool If True, use equalization by channels separately, else convert image to YCbCr representation and use equalization by Y channel. Returns: Type Description numpy.ndarray Equalized image. def albumentations.augmentations.functional.fancy_pca ( img , alpha = 0.1 ) [view source on GitHub] \u00b6 Perform 'Fancy PCA' augmentation from: http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf Parameters: Name Type Description img numpy array with (h, w, rgb) shape, as ints between 0-255) alpha how much to perturb/scale the eigen vecs and vals the paper used std=0.1 Returns: Type Description numpy image-like array as float range(0, 1) def albumentations.augmentations.functional.grid_distortion ( img , num_steps = 10 , xsteps = (), ysteps = (), interpolation = 1 , border_mode = 4 , value = None ) [view source on GitHub] \u00b6 Perform a grid distortion of an input image. Reference: http://pythology.blogspot.sg/2014/03/interpolation-on-regular-distorted-grid.html def albumentations.augmentations.functional.iso_noise ( image , color_shift = 0.05 , intensity = 0.5 , random_state = None , ** kwargs ) [view source on GitHub] \u00b6 Apply poisson noise to image to simulate camera sensor noise. Parameters: Name Type Description image numpy.ndarray Input image, currently, only RGB, uint8 images are supported. color_shift float intensity float Multiplication factor for noise values. Values of ~0.5 are produce noticeable, yet acceptable level of noise. random_state **kwargs Returns: Type Description numpy.ndarray Noised image def albumentations.augmentations.functional.keypoint_flip ( keypoint , d , rows , cols ) [view source on GitHub] \u00b6 Flip a keypoint either vertically, horizontally or both depending on the value of d . Parameters: Name Type Description keypoint tuple A keypoint (x, y, angle, scale) . d int Number of flip. Must be -1, 0 or 1: * 0 - vertical flip, * 1 - horizontal flip, * -1 - vertical and horizontal flip. rows int Image height. cols int Image width. Returns: Type Description tuple A keypoint (x, y, angle, scale) . Exceptions: Type Description ValueError if value of d is not -1, 0 or 1. def albumentations.augmentations.functional.keypoint_hflip ( keypoint , rows , cols ) [view source on GitHub] \u00b6 Flip a keypoint horizontally around the y-axis. Parameters: Name Type Description keypoint tuple A keypoint (x, y, angle, scale) . rows int Image height. cols int Image width. Returns: Type Description tuple A keypoint (x, y, angle, scale) . def albumentations.augmentations.functional.keypoint_transpose ( keypoint ) [view source on GitHub] \u00b6 Rotate a keypoint by angle. Parameters: Name Type Description keypoint tuple A keypoint (x, y, angle, scale) . Returns: Type Description tuple A keypoint (x, y, angle, scale) . def albumentations.augmentations.functional.keypoint_vflip ( keypoint , rows , cols ) [view source on GitHub] \u00b6 Flip a keypoint vertically around the x-axis. Parameters: Name Type Description keypoint tuple A keypoint (x, y, angle, scale) . rows int Image height. cols( int Image width. Returns: Type Description tuple A keypoint (x, y, angle, scale) . def albumentations.augmentations.functional.move_tone_curve ( img , low_y , high_y ) [view source on GitHub] \u00b6 Rescales the relationship between bright and dark areas of the image by manipulating its tone curve. Parameters: Name Type Description img numpy.ndarray RGB or grayscale image. low_y float y-position of a Bezier control point used to adjust the tone curve, must be in range [0, 1] high_y float y-position of a Bezier control point used to adjust image tone curve, must be in range [0, 1] def albumentations.augmentations.functional.multiply ( img , multiplier ) [view source on GitHub] \u00b6 Parameters: Name Type Description img numpy.ndarray Image. multiplier numpy.ndarray Multiplier coefficient. Returns: Type Description numpy.ndarray Image multiplied by multiplier coefficient. def albumentations.augmentations.functional.optical_distortion ( img , k = 0 , dx = 0 , dy = 0 , interpolation = 1 , border_mode = 4 , value = None ) [view source on GitHub] \u00b6 Barrel / pincushion distortion. Unconventional augment. Reference: | https://stackoverflow.com/questions/6199636/formulas-for-barrel-pincushion-distortion | https://stackoverflow.com/questions/10364201/image-transformation-in-opencv | https://stackoverflow.com/questions/2477774/correcting-fisheye-distortion-programmatically | http://www.coldvision.io/2017/03/02/advanced-lane-finding-using-opencv/ def albumentations.augmentations.functional.posterize ( img , bits ) [view source on GitHub] \u00b6 Reduce the number of bits for each color channel. Parameters: Name Type Description img numpy.ndarray image to posterize. bits int number of high bits. Must be in range [0, 8] Returns: Type Description numpy.ndarray Image with reduced color channels. def albumentations.augmentations.functional.preserve_channel_dim ( func ) [view source on GitHub] \u00b6 Preserve dummy channel dim. def albumentations.augmentations.functional.preserve_shape ( func ) [view source on GitHub] \u00b6 Preserve shape of the image def albumentations.augmentations.functional.solarize ( img , threshold = 128 ) [view source on GitHub] \u00b6 Invert all pixel values above a threshold. Parameters: Name Type Description img numpy.ndarray The image to solarize. threshold int All pixels above this greyscale level are inverted. Returns: Type Description numpy.ndarray Solarized image. def albumentations.augmentations.functional.swap_tiles_on_image ( image , tiles ) [view source on GitHub] \u00b6 Swap tiles on image. Parameters: Name Type Description image np.ndarray Input image. tiles np.ndarray array of tuples( current_left_up_corner_row, current_left_up_corner_col, old_left_up_corner_row, old_left_up_corner_col, height_tile, width_tile) Returns: Type Description np.ndarray Output image.","title":"Functional transforms (augmentations.functional)"},{"location":"api_reference/augmentations/functional/#functional-transforms-augmentationsfunctional","text":"","title":"Functional transforms (augmentations.functional)"},{"location":"api_reference/augmentations/functional/#albumentations.augmentations.functional","text":"","title":"albumentations.augmentations.functional"},{"location":"api_reference/augmentations/functional/#albumentations.augmentations.functional.add_fog","text":"Add fog to the image. From https://github.com/UjjwalSaxena/Automold--Road-Augmentation-Library Parameters: Name Type Description img numpy.ndarray Image. fog_coef float Fog coefficient. alpha_coef float Alpha coefficient. haze_list list Returns: Type Description numpy.ndarray Image.","title":"add_fog()"},{"location":"api_reference/augmentations/functional/#albumentations.augmentations.functional.add_rain","text":"From https://github.com/UjjwalSaxena/Automold--Road-Augmentation-Library Parameters: Name Type Description img numpy.ndarray Image. slant int drop_length drop_width drop_color blur_value int Rainy view are blurry. brightness_coefficient float Rainy days are usually shady. rain_drops Returns: Type Description numpy.ndarray Image.","title":"add_rain()"},{"location":"api_reference/augmentations/functional/#albumentations.augmentations.functional.add_shadow","text":"Add shadows to the image. From https://github.com/UjjwalSaxena/Automold--Road-Augmentation-Library Parameters: Name Type Description img numpy.ndarray vertices_list list Returns: Type Description numpy.ndarray","title":"add_shadow()"},{"location":"api_reference/augmentations/functional/#albumentations.augmentations.functional.add_snow","text":"Bleaches out pixels, imitation snow. From https://github.com/UjjwalSaxena/Automold--Road-Augmentation-Library Parameters: Name Type Description img numpy.ndarray Image. snow_point Number of show points. brightness_coeff Brightness coefficient. Returns: Type Description numpy.ndarray Image.","title":"add_snow()"},{"location":"api_reference/augmentations/functional/#albumentations.augmentations.functional.add_sun_flare","text":"Add sun flare. From https://github.com/UjjwalSaxena/Automold--Road-Augmentation-Library Parameters: Name Type Description img numpy.ndarray flare_center_x float flare_center_y float src_radius src_color int, int, int circles list Returns: Type Description numpy.ndarray","title":"add_sun_flare()"},{"location":"api_reference/augmentations/functional/#albumentations.augmentations.functional.bbox_flip","text":"Flip a bounding box either vertically, horizontally or both depending on the value of d . Parameters: Name Type Description bbox tuple A bounding box (x_min, y_min, x_max, y_max) . d int rows int Image rows. cols int Image cols. Returns: Type Description tuple A bounding box (x_min, y_min, x_max, y_max) . Exceptions: Type Description ValueError if value of d is not -1, 0 or 1.","title":"bbox_flip()"},{"location":"api_reference/augmentations/functional/#albumentations.augmentations.functional.bbox_hflip","text":"Flip a bounding box horizontally around the y-axis. Parameters: Name Type Description bbox tuple A bounding box (x_min, y_min, x_max, y_max) . rows int Image rows. cols int Image cols. Returns: Type Description tuple A bounding box (x_min, y_min, x_max, y_max) .","title":"bbox_hflip()"},{"location":"api_reference/augmentations/functional/#albumentations.augmentations.functional.bbox_transpose","text":"Transposes a bounding box along given axis. Parameters: Name Type Description bbox tuple A bounding box (x_min, y_min, x_max, y_max) . axis int 0 - main axis, 1 - secondary axis. rows int Image rows. cols int Image cols. Returns: Type Description tuple A bounding box tuple (x_min, y_min, x_max, y_max) . Exceptions: Type Description ValueError If axis not equal to 0 or 1.","title":"bbox_transpose()"},{"location":"api_reference/augmentations/functional/#albumentations.augmentations.functional.bbox_vflip","text":"Flip a bounding box vertically around the x-axis. Parameters: Name Type Description bbox tuple A bounding box (x_min, y_min, x_max, y_max) . rows int Image rows. cols int Image cols. Returns: Type Description tuple A bounding box (x_min, y_min, x_max, y_max) .","title":"bbox_vflip()"},{"location":"api_reference/augmentations/functional/#albumentations.augmentations.functional.elastic_transform_approx","text":"Elastic deformation of images as described in [Simard2003]_ (with modifications for speed). Based on https://gist.github.com/erniejunior/601cdf56d2b424757de5 .. [Simard2003] Simard, Steinkraus and Platt, \"Best Practices for Convolutional Neural Networks applied to Visual Document Analysis\", in Proc. of the International Conference on Document Analysis and Recognition, 2003.","title":"elastic_transform_approx()"},{"location":"api_reference/augmentations/functional/#albumentations.augmentations.functional.equalize","text":"Equalize the image histogram. Parameters: Name Type Description img numpy.ndarray RGB or grayscale image. mask numpy.ndarray An optional mask. If given, only the pixels selected by the mask are included in the analysis. Maybe 1 channel or 3 channel array. mode str {'cv', 'pil'}. Use OpenCV or Pillow equalization method. by_channels bool If True, use equalization by channels separately, else convert image to YCbCr representation and use equalization by Y channel. Returns: Type Description numpy.ndarray Equalized image.","title":"equalize()"},{"location":"api_reference/augmentations/functional/#albumentations.augmentations.functional.fancy_pca","text":"Perform 'Fancy PCA' augmentation from: http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf Parameters: Name Type Description img numpy array with (h, w, rgb) shape, as ints between 0-255) alpha how much to perturb/scale the eigen vecs and vals the paper used std=0.1 Returns: Type Description numpy image-like array as float range(0, 1)","title":"fancy_pca()"},{"location":"api_reference/augmentations/functional/#albumentations.augmentations.functional.grid_distortion","text":"Perform a grid distortion of an input image. Reference: http://pythology.blogspot.sg/2014/03/interpolation-on-regular-distorted-grid.html","title":"grid_distortion()"},{"location":"api_reference/augmentations/functional/#albumentations.augmentations.functional.iso_noise","text":"Apply poisson noise to image to simulate camera sensor noise. Parameters: Name Type Description image numpy.ndarray Input image, currently, only RGB, uint8 images are supported. color_shift float intensity float Multiplication factor for noise values. Values of ~0.5 are produce noticeable, yet acceptable level of noise. random_state **kwargs Returns: Type Description numpy.ndarray Noised image","title":"iso_noise()"},{"location":"api_reference/augmentations/functional/#albumentations.augmentations.functional.keypoint_flip","text":"Flip a keypoint either vertically, horizontally or both depending on the value of d . Parameters: Name Type Description keypoint tuple A keypoint (x, y, angle, scale) . d int Number of flip. Must be -1, 0 or 1: * 0 - vertical flip, * 1 - horizontal flip, * -1 - vertical and horizontal flip. rows int Image height. cols int Image width. Returns: Type Description tuple A keypoint (x, y, angle, scale) . Exceptions: Type Description ValueError if value of d is not -1, 0 or 1.","title":"keypoint_flip()"},{"location":"api_reference/augmentations/functional/#albumentations.augmentations.functional.keypoint_hflip","text":"Flip a keypoint horizontally around the y-axis. Parameters: Name Type Description keypoint tuple A keypoint (x, y, angle, scale) . rows int Image height. cols int Image width. Returns: Type Description tuple A keypoint (x, y, angle, scale) .","title":"keypoint_hflip()"},{"location":"api_reference/augmentations/functional/#albumentations.augmentations.functional.keypoint_transpose","text":"Rotate a keypoint by angle. Parameters: Name Type Description keypoint tuple A keypoint (x, y, angle, scale) . Returns: Type Description tuple A keypoint (x, y, angle, scale) .","title":"keypoint_transpose()"},{"location":"api_reference/augmentations/functional/#albumentations.augmentations.functional.keypoint_vflip","text":"Flip a keypoint vertically around the x-axis. Parameters: Name Type Description keypoint tuple A keypoint (x, y, angle, scale) . rows int Image height. cols( int Image width. Returns: Type Description tuple A keypoint (x, y, angle, scale) .","title":"keypoint_vflip()"},{"location":"api_reference/augmentations/functional/#albumentations.augmentations.functional.move_tone_curve","text":"Rescales the relationship between bright and dark areas of the image by manipulating its tone curve. Parameters: Name Type Description img numpy.ndarray RGB or grayscale image. low_y float y-position of a Bezier control point used to adjust the tone curve, must be in range [0, 1] high_y float y-position of a Bezier control point used to adjust image tone curve, must be in range [0, 1]","title":"move_tone_curve()"},{"location":"api_reference/augmentations/functional/#albumentations.augmentations.functional.multiply","text":"Parameters: Name Type Description img numpy.ndarray Image. multiplier numpy.ndarray Multiplier coefficient. Returns: Type Description numpy.ndarray Image multiplied by multiplier coefficient.","title":"multiply()"},{"location":"api_reference/augmentations/functional/#albumentations.augmentations.functional.optical_distortion","text":"Barrel / pincushion distortion. Unconventional augment. Reference: | https://stackoverflow.com/questions/6199636/formulas-for-barrel-pincushion-distortion | https://stackoverflow.com/questions/10364201/image-transformation-in-opencv | https://stackoverflow.com/questions/2477774/correcting-fisheye-distortion-programmatically | http://www.coldvision.io/2017/03/02/advanced-lane-finding-using-opencv/","title":"optical_distortion()"},{"location":"api_reference/augmentations/functional/#albumentations.augmentations.functional.posterize","text":"Reduce the number of bits for each color channel. Parameters: Name Type Description img numpy.ndarray image to posterize. bits int number of high bits. Must be in range [0, 8] Returns: Type Description numpy.ndarray Image with reduced color channels.","title":"posterize()"},{"location":"api_reference/augmentations/functional/#albumentations.augmentations.functional.preserve_channel_dim","text":"Preserve dummy channel dim.","title":"preserve_channel_dim()"},{"location":"api_reference/augmentations/functional/#albumentations.augmentations.functional.preserve_shape","text":"Preserve shape of the image","title":"preserve_shape()"},{"location":"api_reference/augmentations/functional/#albumentations.augmentations.functional.solarize","text":"Invert all pixel values above a threshold. Parameters: Name Type Description img numpy.ndarray The image to solarize. threshold int All pixels above this greyscale level are inverted. Returns: Type Description numpy.ndarray Solarized image.","title":"solarize()"},{"location":"api_reference/augmentations/functional/#albumentations.augmentations.functional.swap_tiles_on_image","text":"Swap tiles on image. Parameters: Name Type Description image np.ndarray Input image. tiles np.ndarray array of tuples( current_left_up_corner_row, current_left_up_corner_col, old_left_up_corner_row, old_left_up_corner_col, height_tile, width_tile) Returns: Type Description np.ndarray Output image.","title":"swap_tiles_on_image()"},{"location":"api_reference/augmentations/geometric/","text":"Geometric augmentations (augmentations.geometric) \u00b6 \u00b6 albumentations.augmentations.geometric.functional \u00b6 def albumentations.augmentations.geometric.functional.bbox_rot90 ( bbox , factor , rows , cols ) [view source on GitHub] \u00b6 Rotates a bounding box by 90 degrees CCW (see np.rot90) Parameters: Name Type Description bbox tuple A bounding box tuple (x_min, y_min, x_max, y_max). factor int Number of CCW rotations. Must be in set {0, 1, 2, 3} See np.rot90. rows int Image rows. cols int Image cols. Returns: Type Description tuple A bounding box tuple (x_min, y_min, x_max, y_max). def albumentations.augmentations.geometric.functional.bbox_rotate ( bbox , angle , rows , cols ) [view source on GitHub] \u00b6 Rotates a bounding box by angle degrees. Parameters: Name Type Description bbox tuple A bounding box (x_min, y_min, x_max, y_max) . angle int Angle of rotation in degrees. rows int Image rows. cols int Image cols. Returns: Type Description A bounding box (x_min, y_min, x_max, y_max) . def albumentations.augmentations.geometric.functional.elastic_transform ( img , alpha , sigma , alpha_affine , interpolation = 1 , border_mode = 4 , value = None , random_state = None , approximate = False ) [view source on GitHub] \u00b6 Elastic deformation of images as described in [Simard2003]_ (with modifications). Based on https://gist.github.com/erniejunior/601cdf56d2b424757de5 .. [Simard2003] Simard, Steinkraus and Platt, \"Best Practices for Convolutional Neural Networks applied to Visual Document Analysis\", in Proc. of the International Conference on Document Analysis and Recognition, 2003. def albumentations.augmentations.geometric.functional.keypoint_rot90 ( keypoint , factor , rows , cols , ** params ) [view source on GitHub] \u00b6 Rotates a keypoint by 90 degrees CCW (see np.rot90) Parameters: Name Type Description keypoint tuple A keypoint (x, y, angle, scale) . factor int Number of CCW rotations. Must be in range [0;3] See np.rot90. rows int Image height. cols int Image width. Returns: Type Description tuple A keypoint (x, y, angle, scale) . Exceptions: Type Description ValueError if factor not in set {0, 1, 2, 3} def albumentations.augmentations.geometric.functional.keypoint_rotate ( keypoint , angle , rows , cols , ** params ) [view source on GitHub] \u00b6 Rotate a keypoint by angle. Parameters: Name Type Description keypoint tuple A keypoint (x, y, angle, scale) . angle float Rotation angle. rows int Image height. cols int Image width. Returns: Type Description tuple A keypoint (x, y, angle, scale) . def albumentations.augmentations.geometric.functional.keypoint_scale ( keypoint , scale_x , scale_y ) [view source on GitHub] \u00b6 Scales a keypoint by scale_x and scale_y. Parameters: Name Type Description keypoint Sequence[float] A keypoint (x, y, angle, scale) . scale_x float Scale coefficient x-axis. scale_y float Scale coefficient y-axis. Returns: Type Description A keypoint (x, y, angle, scale) . def albumentations.augmentations.geometric.functional.py3round ( number ) [view source on GitHub] \u00b6 Unified rounding in all python versions. albumentations.augmentations.geometric.resize \u00b6 class albumentations.augmentations.geometric.resize.LongestMaxSize ( max_size = 1024 , interpolation = 1 , always_apply = False , p = 1 ) [view source on GitHub] \u00b6 Rescale an image so that maximum side is equal to max_size, keeping the aspect ratio of the initial image. Parameters: Name Type Description max_size int maximum size of the image after the transformation. interpolation OpenCV flag interpolation method. Default: cv2.INTER_LINEAR. p float probability of applying the transform. Default: 1. Targets: image, mask, bboxes, keypoints Image types: uint8, float32 class albumentations.augmentations.geometric.resize.RandomScale ( scale_limit = 0.1 , interpolation = 1 , always_apply = False , p = 0.5 ) [view source on GitHub] \u00b6 Randomly resize the input. Output image size is different from the input image size. Parameters: Name Type Description scale_limit [float, float] or float scaling factor range. If scale_limit is a single float value, the range will be (1 - scale_limit, 1 + scale_limit). Default: (0.9, 1.1). interpolation OpenCV flag flag that is used to specify the interpolation algorithm. Should be one of: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4. Default: cv2.INTER_LINEAR. p float probability of applying the transform. Default: 0.5. Targets: image, mask, bboxes, keypoints Image types: uint8, float32 class albumentations.augmentations.geometric.resize.Resize ( height , width , interpolation = 1 , always_apply = False , p = 1 ) [view source on GitHub] \u00b6 Resize the input to the given height and width. Parameters: Name Type Description height int desired height of the output. width int desired width of the output. interpolation OpenCV flag flag that is used to specify the interpolation algorithm. Should be one of: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4. Default: cv2.INTER_LINEAR. p float probability of applying the transform. Default: 1. Targets: image, mask, bboxes, keypoints Image types: uint8, float32 class albumentations.augmentations.geometric.resize.SmallestMaxSize ( max_size = 1024 , interpolation = 1 , always_apply = False , p = 1 ) [view source on GitHub] \u00b6 Rescale an image so that minimum side is equal to max_size, keeping the aspect ratio of the initial image. Parameters: Name Type Description max_size int maximum size of smallest side of the image after the transformation. interpolation OpenCV flag interpolation method. Default: cv2.INTER_LINEAR. p float probability of applying the transform. Default: 1. Targets: image, mask, bboxes, keypoints Image types: uint8, float32 albumentations.augmentations.geometric.rotate \u00b6 class albumentations.augmentations.geometric.rotate.RandomRotate90 [view source on GitHub] \u00b6 Randomly rotate the input by 90 degrees zero or more times. Parameters: Name Type Description p float probability of applying the transform. Default: 0.5. Targets: image, mask, bboxes, keypoints Image types: uint8, float32 albumentations.augmentations.geometric.rotate.RandomRotate90.apply ( self , img , factor = 0 , ** params ) \u00b6 Parameters: Name Type Description factor int number of times the input will be rotated by 90 degrees. class albumentations.augmentations.geometric.rotate.Rotate ( limit = 90 , interpolation = 1 , border_mode = 4 , value = None , mask_value = None , always_apply = False , p = 0.5 ) [view source on GitHub] \u00b6 Rotate the input by an angle selected randomly from the uniform distribution. Parameters: Name Type Description limit [int, int] or int range from which a random angle is picked. If limit is a single int an angle is picked from (-limit, limit). Default: (-90, 90) interpolation OpenCV flag flag that is used to specify the interpolation algorithm. Should be one of: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4. Default: cv2.INTER_LINEAR. border_mode OpenCV flag flag that is used to specify the pixel extrapolation method. Should be one of: cv2.BORDER_CONSTANT, cv2.BORDER_REPLICATE, cv2.BORDER_REFLECT, cv2.BORDER_WRAP, cv2.BORDER_REFLECT_101. Default: cv2.BORDER_REFLECT_101 value int, float, list of ints, list of float padding value if border_mode is cv2.BORDER_CONSTANT. mask_value int, float, list of ints, list of float padding value if border_mode is cv2.BORDER_CONSTANT applied for masks. p float probability of applying the transform. Default: 0.5. Targets: image, mask, bboxes, keypoints Image types: uint8, float32 albumentations.augmentations.geometric.transforms \u00b6 class albumentations.augmentations.geometric.transforms.ElasticTransform ( alpha = 1 , sigma = 50 , alpha_affine = 50 , interpolation = 1 , border_mode = 4 , value = None , mask_value = None , always_apply = False , approximate = False , p = 0.5 ) [view source on GitHub] \u00b6 Elastic deformation of images as described in [Simard2003]_ (with modifications). Based on https://gist.github.com/erniejunior/601cdf56d2b424757de5 .. [Simard2003] Simard, Steinkraus and Platt, \"Best Practices for Convolutional Neural Networks applied to Visual Document Analysis\", in Proc. of the International Conference on Document Analysis and Recognition, 2003. Parameters: Name Type Description alpha float sigma float Gaussian filter parameter. alpha_affine float The range will be (-alpha_affine, alpha_affine) interpolation OpenCV flag flag that is used to specify the interpolation algorithm. Should be one of: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4. Default: cv2.INTER_LINEAR. border_mode OpenCV flag flag that is used to specify the pixel extrapolation method. Should be one of: cv2.BORDER_CONSTANT, cv2.BORDER_REPLICATE, cv2.BORDER_REFLECT, cv2.BORDER_WRAP, cv2.BORDER_REFLECT_101. Default: cv2.BORDER_REFLECT_101 value int, float, list of ints, list of float padding value if border_mode is cv2.BORDER_CONSTANT. mask_value int, float, list of ints, list of float padding value if border_mode is cv2.BORDER_CONSTANT applied for masks. approximate boolean Whether to smooth displacement map with fixed kernel size. Enabling this option gives ~2X speedup on large images. Targets: image, mask Image types: uint8, float32 class albumentations.augmentations.geometric.transforms.Perspective ( scale = ( 0.05 , 0.1 ), keep_size = True , pad_mode = 0 , pad_val = 0 , mask_pad_val = 0 , fit_output = False , interpolation = 1 , always_apply = False , p = 0.5 ) [view source on GitHub] \u00b6 Perform a random four point perspective transform of the input. Parameters: Name Type Description scale float or [float, float] standard deviation of the normal distributions. These are used to sample the random distances of the subimage's corners from the full image's corners. If scale is a single float value, the range will be (0, scale). Default: (0.05, 0.1). keep_size bool Whether to resize image\u2019s back to their original size after applying the perspective transform. If set to False, the resulting images may end up having different shapes and will always be a list, never an array. Default: True pad_mode OpenCV flag OpenCV border mode. pad_val int, float, list of int, list of float padding value if border_mode is cv2.BORDER_CONSTANT. Default: 0 mask_pad_val int, float, list of int, list of float padding value for mask if border_mode is cv2.BORDER_CONSTANT. Default: 0 fit_output bool If True, the image plane size and position will be adjusted to still capture the whole image after perspective transformation. (Followed by image resizing if keep_size is set to True.) Otherwise, parts of the transformed image may be outside of the image plane. This setting should not be set to True when using large scale values as it could lead to very large images. Default: False p float probability of applying the transform. Default: 0.5. Targets: image, mask, keypoints, bboxes Image types: uint8, float32 class albumentations.augmentations.geometric.transforms.ShiftScaleRotate ( shift_limit = 0.0625 , scale_limit = 0.1 , rotate_limit = 45 , interpolation = 1 , border_mode = 4 , value = None , mask_value = None , shift_limit_x = None , shift_limit_y = None , always_apply = False , p = 0.5 ) [view source on GitHub] \u00b6 Randomly apply affine transforms: translate, scale and rotate the input. Parameters: Name Type Description shift_limit [float, float] or float shift factor range for both height and width. If shift_limit is a single float value, the range will be (-shift_limit, shift_limit). Absolute values for lower and upper bounds should lie in range [0, 1]. Default: (-0.0625, 0.0625). scale_limit [float, float] or float scaling factor range. If scale_limit is a single float value, the range will be (-scale_limit, scale_limit). Default: (-0.1, 0.1). rotate_limit [int, int] or int rotation range. If rotate_limit is a single int value, the range will be (-rotate_limit, rotate_limit). Default: (-45, 45). interpolation OpenCV flag flag that is used to specify the interpolation algorithm. Should be one of: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4. Default: cv2.INTER_LINEAR. border_mode OpenCV flag flag that is used to specify the pixel extrapolation method. Should be one of: cv2.BORDER_CONSTANT, cv2.BORDER_REPLICATE, cv2.BORDER_REFLECT, cv2.BORDER_WRAP, cv2.BORDER_REFLECT_101. Default: cv2.BORDER_REFLECT_101 value int, float, list of int, list of float padding value if border_mode is cv2.BORDER_CONSTANT. mask_value int, float, list of int, list of float padding value if border_mode is cv2.BORDER_CONSTANT applied for masks. shift_limit_x [float, float] or float shift factor range for width. If it is set then this value instead of shift_limit will be used for shifting width. If shift_limit_x is a single float value, the range will be (-shift_limit_x, shift_limit_x). Absolute values for lower and upper bounds should lie in the range [0, 1]. Default: None. shift_limit_y [float, float] or float shift factor range for height. If it is set then this value instead of shift_limit will be used for shifting height. If shift_limit_y is a single float value, the range will be (-shift_limit_y, shift_limit_y). Absolute values for lower and upper bounds should lie in the range [0, 1]. Default: None. p float probability of applying the transform. Default: 0.5. Targets: image, mask, keypoints Image types: uint8, float32","title":"Geometric augmentations (augmentations.geometric)"},{"location":"api_reference/augmentations/geometric/#geometric-augmentations-augmentationsgeometric","text":"","title":"Geometric augmentations (augmentations.geometric)"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric","text":"","title":"albumentations.augmentations.geometric"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.functional","text":"","title":"functional"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.functional.bbox_rot90","text":"Rotates a bounding box by 90 degrees CCW (see np.rot90) Parameters: Name Type Description bbox tuple A bounding box tuple (x_min, y_min, x_max, y_max). factor int Number of CCW rotations. Must be in set {0, 1, 2, 3} See np.rot90. rows int Image rows. cols int Image cols. Returns: Type Description tuple A bounding box tuple (x_min, y_min, x_max, y_max).","title":"bbox_rot90()"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.functional.bbox_rotate","text":"Rotates a bounding box by angle degrees. Parameters: Name Type Description bbox tuple A bounding box (x_min, y_min, x_max, y_max) . angle int Angle of rotation in degrees. rows int Image rows. cols int Image cols. Returns: Type Description A bounding box (x_min, y_min, x_max, y_max) .","title":"bbox_rotate()"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.functional.elastic_transform","text":"Elastic deformation of images as described in [Simard2003]_ (with modifications). Based on https://gist.github.com/erniejunior/601cdf56d2b424757de5 .. [Simard2003] Simard, Steinkraus and Platt, \"Best Practices for Convolutional Neural Networks applied to Visual Document Analysis\", in Proc. of the International Conference on Document Analysis and Recognition, 2003.","title":"elastic_transform()"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.functional.keypoint_rot90","text":"Rotates a keypoint by 90 degrees CCW (see np.rot90) Parameters: Name Type Description keypoint tuple A keypoint (x, y, angle, scale) . factor int Number of CCW rotations. Must be in range [0;3] See np.rot90. rows int Image height. cols int Image width. Returns: Type Description tuple A keypoint (x, y, angle, scale) . Exceptions: Type Description ValueError if factor not in set {0, 1, 2, 3}","title":"keypoint_rot90()"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.functional.keypoint_rotate","text":"Rotate a keypoint by angle. Parameters: Name Type Description keypoint tuple A keypoint (x, y, angle, scale) . angle float Rotation angle. rows int Image height. cols int Image width. Returns: Type Description tuple A keypoint (x, y, angle, scale) .","title":"keypoint_rotate()"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.functional.keypoint_scale","text":"Scales a keypoint by scale_x and scale_y. Parameters: Name Type Description keypoint Sequence[float] A keypoint (x, y, angle, scale) . scale_x float Scale coefficient x-axis. scale_y float Scale coefficient y-axis. Returns: Type Description A keypoint (x, y, angle, scale) .","title":"keypoint_scale()"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.functional.py3round","text":"Unified rounding in all python versions.","title":"py3round()"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.resize","text":"","title":"resize"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.resize.LongestMaxSize","text":"Rescale an image so that maximum side is equal to max_size, keeping the aspect ratio of the initial image. Parameters: Name Type Description max_size int maximum size of the image after the transformation. interpolation OpenCV flag interpolation method. Default: cv2.INTER_LINEAR. p float probability of applying the transform. Default: 1. Targets: image, mask, bboxes, keypoints Image types: uint8, float32","title":"LongestMaxSize"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.resize.RandomScale","text":"Randomly resize the input. Output image size is different from the input image size. Parameters: Name Type Description scale_limit [float, float] or float scaling factor range. If scale_limit is a single float value, the range will be (1 - scale_limit, 1 + scale_limit). Default: (0.9, 1.1). interpolation OpenCV flag flag that is used to specify the interpolation algorithm. Should be one of: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4. Default: cv2.INTER_LINEAR. p float probability of applying the transform. Default: 0.5. Targets: image, mask, bboxes, keypoints Image types: uint8, float32","title":"RandomScale"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.resize.Resize","text":"Resize the input to the given height and width. Parameters: Name Type Description height int desired height of the output. width int desired width of the output. interpolation OpenCV flag flag that is used to specify the interpolation algorithm. Should be one of: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4. Default: cv2.INTER_LINEAR. p float probability of applying the transform. Default: 1. Targets: image, mask, bboxes, keypoints Image types: uint8, float32","title":"Resize"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.resize.SmallestMaxSize","text":"Rescale an image so that minimum side is equal to max_size, keeping the aspect ratio of the initial image. Parameters: Name Type Description max_size int maximum size of smallest side of the image after the transformation. interpolation OpenCV flag interpolation method. Default: cv2.INTER_LINEAR. p float probability of applying the transform. Default: 1. Targets: image, mask, bboxes, keypoints Image types: uint8, float32","title":"SmallestMaxSize"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.rotate","text":"","title":"rotate"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.rotate.RandomRotate90","text":"Randomly rotate the input by 90 degrees zero or more times. Parameters: Name Type Description p float probability of applying the transform. Default: 0.5. Targets: image, mask, bboxes, keypoints Image types: uint8, float32","title":"RandomRotate90"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.rotate.RandomRotate90.apply","text":"Parameters: Name Type Description factor int number of times the input will be rotated by 90 degrees.","title":"apply()"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.rotate.Rotate","text":"Rotate the input by an angle selected randomly from the uniform distribution. Parameters: Name Type Description limit [int, int] or int range from which a random angle is picked. If limit is a single int an angle is picked from (-limit, limit). Default: (-90, 90) interpolation OpenCV flag flag that is used to specify the interpolation algorithm. Should be one of: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4. Default: cv2.INTER_LINEAR. border_mode OpenCV flag flag that is used to specify the pixel extrapolation method. Should be one of: cv2.BORDER_CONSTANT, cv2.BORDER_REPLICATE, cv2.BORDER_REFLECT, cv2.BORDER_WRAP, cv2.BORDER_REFLECT_101. Default: cv2.BORDER_REFLECT_101 value int, float, list of ints, list of float padding value if border_mode is cv2.BORDER_CONSTANT. mask_value int, float, list of ints, list of float padding value if border_mode is cv2.BORDER_CONSTANT applied for masks. p float probability of applying the transform. Default: 0.5. Targets: image, mask, bboxes, keypoints Image types: uint8, float32","title":"Rotate"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.transforms","text":"","title":"transforms"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.transforms.ElasticTransform","text":"Elastic deformation of images as described in [Simard2003]_ (with modifications). Based on https://gist.github.com/erniejunior/601cdf56d2b424757de5 .. [Simard2003] Simard, Steinkraus and Platt, \"Best Practices for Convolutional Neural Networks applied to Visual Document Analysis\", in Proc. of the International Conference on Document Analysis and Recognition, 2003. Parameters: Name Type Description alpha float sigma float Gaussian filter parameter. alpha_affine float The range will be (-alpha_affine, alpha_affine) interpolation OpenCV flag flag that is used to specify the interpolation algorithm. Should be one of: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4. Default: cv2.INTER_LINEAR. border_mode OpenCV flag flag that is used to specify the pixel extrapolation method. Should be one of: cv2.BORDER_CONSTANT, cv2.BORDER_REPLICATE, cv2.BORDER_REFLECT, cv2.BORDER_WRAP, cv2.BORDER_REFLECT_101. Default: cv2.BORDER_REFLECT_101 value int, float, list of ints, list of float padding value if border_mode is cv2.BORDER_CONSTANT. mask_value int, float, list of ints, list of float padding value if border_mode is cv2.BORDER_CONSTANT applied for masks. approximate boolean Whether to smooth displacement map with fixed kernel size. Enabling this option gives ~2X speedup on large images. Targets: image, mask Image types: uint8, float32","title":"ElasticTransform"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.transforms.Perspective","text":"Perform a random four point perspective transform of the input. Parameters: Name Type Description scale float or [float, float] standard deviation of the normal distributions. These are used to sample the random distances of the subimage's corners from the full image's corners. If scale is a single float value, the range will be (0, scale). Default: (0.05, 0.1). keep_size bool Whether to resize image\u2019s back to their original size after applying the perspective transform. If set to False, the resulting images may end up having different shapes and will always be a list, never an array. Default: True pad_mode OpenCV flag OpenCV border mode. pad_val int, float, list of int, list of float padding value if border_mode is cv2.BORDER_CONSTANT. Default: 0 mask_pad_val int, float, list of int, list of float padding value for mask if border_mode is cv2.BORDER_CONSTANT. Default: 0 fit_output bool If True, the image plane size and position will be adjusted to still capture the whole image after perspective transformation. (Followed by image resizing if keep_size is set to True.) Otherwise, parts of the transformed image may be outside of the image plane. This setting should not be set to True when using large scale values as it could lead to very large images. Default: False p float probability of applying the transform. Default: 0.5. Targets: image, mask, keypoints, bboxes Image types: uint8, float32","title":"Perspective"},{"location":"api_reference/augmentations/geometric/#albumentations.augmentations.geometric.transforms.ShiftScaleRotate","text":"Randomly apply affine transforms: translate, scale and rotate the input. Parameters: Name Type Description shift_limit [float, float] or float shift factor range for both height and width. If shift_limit is a single float value, the range will be (-shift_limit, shift_limit). Absolute values for lower and upper bounds should lie in range [0, 1]. Default: (-0.0625, 0.0625). scale_limit [float, float] or float scaling factor range. If scale_limit is a single float value, the range will be (-scale_limit, scale_limit). Default: (-0.1, 0.1). rotate_limit [int, int] or int rotation range. If rotate_limit is a single int value, the range will be (-rotate_limit, rotate_limit). Default: (-45, 45). interpolation OpenCV flag flag that is used to specify the interpolation algorithm. Should be one of: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4. Default: cv2.INTER_LINEAR. border_mode OpenCV flag flag that is used to specify the pixel extrapolation method. Should be one of: cv2.BORDER_CONSTANT, cv2.BORDER_REPLICATE, cv2.BORDER_REFLECT, cv2.BORDER_WRAP, cv2.BORDER_REFLECT_101. Default: cv2.BORDER_REFLECT_101 value int, float, list of int, list of float padding value if border_mode is cv2.BORDER_CONSTANT. mask_value int, float, list of int, list of float padding value if border_mode is cv2.BORDER_CONSTANT applied for masks. shift_limit_x [float, float] or float shift factor range for width. If it is set then this value instead of shift_limit will be used for shifting width. If shift_limit_x is a single float value, the range will be (-shift_limit_x, shift_limit_x). Absolute values for lower and upper bounds should lie in the range [0, 1]. Default: None. shift_limit_y [float, float] or float shift factor range for height. If it is set then this value instead of shift_limit will be used for shifting height. If shift_limit_y is a single float value, the range will be (-shift_limit_y, shift_limit_y). Absolute values for lower and upper bounds should lie in the range [0, 1]. Default: None. p float probability of applying the transform. Default: 0.5. Targets: image, mask, keypoints Image types: uint8, float32","title":"ShiftScaleRotate"},{"location":"api_reference/augmentations/keypoints_utils/","text":"Helper functions for working with keypoints (augmentations.keypoints_utils) \u00b6 \u00b6 def albumentations.augmentations.keypoints_utils.check_keypoint ( kp , rows , cols ) [view source on GitHub] \u00b6 Check if keypoint coordinates are less than image shapes def albumentations.augmentations.keypoints_utils.check_keypoints ( keypoints , rows , cols ) [view source on GitHub] \u00b6 Check if keypoints boundaries are less than image shapes","title":"Helper functions for working with keypoints (augmentations.keypoints_utils)"},{"location":"api_reference/augmentations/keypoints_utils/#helper-functions-for-working-with-keypoints-augmentationskeypoints_utils","text":"","title":"Helper functions for working with keypoints (augmentations.keypoints_utils)"},{"location":"api_reference/augmentations/keypoints_utils/#albumentations.augmentations.keypoints_utils","text":"","title":"albumentations.augmentations.keypoints_utils"},{"location":"api_reference/augmentations/keypoints_utils/#albumentations.augmentations.keypoints_utils.check_keypoint","text":"Check if keypoint coordinates are less than image shapes","title":"check_keypoint()"},{"location":"api_reference/augmentations/keypoints_utils/#albumentations.augmentations.keypoints_utils.check_keypoints","text":"Check if keypoints boundaries are less than image shapes","title":"check_keypoints()"},{"location":"api_reference/augmentations/transforms/","text":"Transforms (augmentations.transforms) \u00b6 \u00b6 class albumentations.augmentations.transforms.Blur ( blur_limit = 7 , always_apply = False , p = 0.5 ) [view source on GitHub] \u00b6 Blur the input image using a random-sized kernel. Parameters: Name Type Description blur_limit int, [int, int] maximum kernel size for blurring the input image. Should be in range [3, inf). Default: (3, 7). p float probability of applying the transform. Default: 0.5. Targets: image Image types: uint8, float32 class albumentations.augmentations.transforms.ChannelDropout ( channel_drop_range = ( 1 , 1 ), fill_value = 0 , always_apply = False , p = 0.5 ) [view source on GitHub] \u00b6 Randomly Drop Channels in the input Image. Parameters: Name Type Description channel_drop_range [int, int] range from which we choose the number of channels to drop. fill_value int, float pixel value for the dropped channel. p float probability of applying the transform. Default: 0.5. Targets: image Image types: uint8, uint16, unit32, float32 class albumentations.augmentations.transforms.ChannelShuffle [view source on GitHub] \u00b6 Randomly rearrange channels of the input RGB image. Parameters: Name Type Description p float probability of applying the transform. Default: 0.5. Targets: image Image types: uint8, float32 class albumentations.augmentations.transforms.CLAHE ( clip_limit = 4.0 , tile_grid_size = ( 8 , 8 ), always_apply = False , p = 0.5 ) [view source on GitHub] \u00b6 Apply Contrast Limited Adaptive Histogram Equalization to the input image. Parameters: Name Type Description clip_limit float or [float, float] upper threshold value for contrast limiting. If clip_limit is a single float value, the range will be (1, clip_limit). Default: (1, 4). tile_grid_size [int, int] size of grid for histogram equalization. Default: (8, 8). p float probability of applying the transform. Default: 0.5. Targets: image Image types: uint8 class albumentations.augmentations.transforms.CoarseDropout ( max_holes = 8 , max_height = 8 , max_width = 8 , min_holes = None , min_height = None , min_width = None , fill_value = 0 , mask_fill_value = None , always_apply = False , p = 0.5 ) [view source on GitHub] \u00b6 CoarseDropout of the rectangular regions in the image. Parameters: Name Type Description max_holes int Maximum number of regions to zero out. max_height int Maximum height of the hole. max_width int Maximum width of the hole. min_holes int Minimum number of regions to zero out. If None , min_holes is be set to max_holes . Default: None . min_height int Minimum height of the hole. Default: None. If None , min_height is set to max_height . Default: None . min_width int Minimum width of the hole. If None , min_height is set to max_width . Default: None . fill_value int, float, list of int, list of float value for dropped pixels. mask_fill_value int, float, list of int, list of float fill value for dropped pixels in mask. If None - mask is not affected. Default: None . Targets: image, mask Image types: uint8, float32 Reference: | https://arxiv.org/abs/1708.04552 | https://github.com/uoguelph-mlrg/Cutout/blob/master/util/cutout.py | https://github.com/aleju/imgaug/blob/master/imgaug/augmenters/arithmetic.py class albumentations.augmentations.transforms.ColorJitter ( brightness = 0.2 , contrast = 0.2 , saturation = 0.2 , hue = 0.2 , always_apply = False , p = 0.5 ) [view source on GitHub] \u00b6 Randomly changes the brightness, contrast, and saturation of an image. Compared to ColorJitter from torchvision, this transform gives a little bit different results because Pillow (used in torchvision) and OpenCV (used in Albumentations) transform an image to HSV format by different formulas. Another difference - Pillow uses uint8 overflow, but we use value saturation. Parameters: Name Type Description brightness float or tuple of float (min, max How much to jitter brightness. brightness_factor is chosen uniformly from [max(0, 1 - brightness), 1 + brightness] or the given [min, max]. Should be non negative numbers. contrast float or tuple of float (min, max How much to jitter contrast. contrast_factor is chosen uniformly from [max(0, 1 - contrast), 1 + contrast] or the given [min, max]. Should be non negative numbers. saturation float or tuple of float (min, max How much to jitter saturation. saturation_factor is chosen uniformly from [max(0, 1 - saturation), 1 + saturation] or the given [min, max]. Should be non negative numbers. hue float or tuple of float (min, max How much to jitter hue. hue_factor is chosen uniformly from [-hue, hue] or the given [min, max]. Should have 0 <= hue <= 0.5 or -0.5 <= min <= max <= 0.5. class albumentations.augmentations.transforms.Cutout ( num_holes = 8 , max_h_size = 8 , max_w_size = 8 , fill_value = 0 , always_apply = False , p = 0.5 ) [view source on GitHub] \u00b6 CoarseDropout of the square regions in the image. Parameters: Name Type Description num_holes int number of regions to zero out max_h_size int maximum height of the hole max_w_size int maximum width of the hole fill_value int, float, list of int, list of float value for dropped pixels. Targets: image Image types: uint8, float32 Reference: | https://arxiv.org/abs/1708.04552 | https://github.com/uoguelph-mlrg/Cutout/blob/master/util/cutout.py | https://github.com/aleju/imgaug/blob/master/imgaug/augmenters/arithmetic.py class albumentations.augmentations.transforms.Downscale ( scale_min = 0.25 , scale_max = 0.25 , interpolation = 0 , always_apply = False , p = 0.5 ) [view source on GitHub] \u00b6 Decreases image quality by downscaling and upscaling back. Parameters: Name Type Description scale_min float lower bound on the image scale. Should be < 1. scale_max float lower bound on the image scale. Should be . interpolation cv2 interpolation method. cv2.INTER_NEAREST by default Targets: image Image types: uint8, float32 class albumentations.augmentations.transforms.Emboss ( alpha = ( 0.2 , 0.5 ), strength = ( 0.2 , 0.7 ), always_apply = False , p = 0.5 ) [view source on GitHub] \u00b6 Emboss the input image and overlays the result with the original image. Parameters: Name Type Description alpha [float, float] range to choose the visibility of the embossed image. At 0, only the original image is visible,at 1.0 only its embossed version is visible. Default: (0.2, 0.5). strength [float, float] strength range of the embossing. Default: (0.2, 0.7). p float probability of applying the transform. Default: 0.5. Targets: image class albumentations.augmentations.transforms.Equalize ( mode = 'cv' , by_channels = True , mask = None , mask_params = (), always_apply = False , p = 0.5 ) [view source on GitHub] \u00b6 Equalize the image histogram. Parameters: Name Type Description mode str {'cv', 'pil'}. Use OpenCV or Pillow equalization method. by_channels bool If True, use equalization by channels separately, else convert image to YCbCr representation and use equalization by Y channel. mask np.ndarray, callable If given, only the pixels selected by the mask are included in the analysis. Maybe 1 channel or 3 channel array or callable. Function signature must include image argument. mask_params list of str Params for mask function. Targets: image Image types: uint8 class albumentations.augmentations.transforms.FancyPCA ( alpha = 0.1 , always_apply = False , p = 0.5 ) [view source on GitHub] \u00b6 Augment RGB image using FancyPCA from Krizhevsky's paper \"ImageNet Classification with Deep Convolutional Neural Networks\" Parameters: Name Type Description alpha float how much to perturb/scale the eigen vecs and vals. scale is samples from gaussian distribution (mu=0, sigma=alpha) Targets: image Image types: 3-channel uint8 images only Credit: http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf https://deshanadesai.github.io/notes/Fancy-PCA-with-Scikit-Image https://pixelatedbrian.github.io/2018-04-29-fancy_pca/ class albumentations.augmentations.transforms.Flip [view source on GitHub] \u00b6 Flip the input either horizontally, vertically or both horizontally and vertically. Parameters: Name Type Description p float probability of applying the transform. Default: 0.5. Targets: image, mask, bboxes, keypoints Image types: uint8, float32 albumentations.augmentations.transforms.Flip.apply ( self , img , d = 0 , ** params ) \u00b6 d (int): code that specifies how to flip the input. 0 for vertical flipping, 1 for horizontal flipping, -1 for both vertical and horizontal flipping (which is also could be seen as rotating the input by 180 degrees). class albumentations.augmentations.transforms.FromFloat ( dtype = 'uint16' , max_value = None , always_apply = False , p = 1.0 ) [view source on GitHub] \u00b6 Take an input array where all values should lie in the range [0, 1.0], multiply them by max_value and then cast the resulted value to a type specified by dtype . If max_value is None the transform will try to infer the maximum value for the data type from the dtype argument. This is the inverse transform for :class: ~albumentations.augmentations.transforms.ToFloat . Parameters: Name Type Description max_value float maximum possible input value. Default: None. dtype string or numpy data type data type of the output. See the 'Data types' page from the NumPy docs _. Default: 'uint16'. p float probability of applying the transform. Default: 1.0. Targets: image Image types: float32 .. _'Data types' page from the NumPy docs: https://docs.scipy.org/doc/numpy/user/basics.types.html class albumentations.augmentations.transforms.GaussianBlur ( blur_limit = ( 3 , 7 ), sigma_limit = 0 , always_apply = False , p = 0.5 ) [view source on GitHub] \u00b6 Blur the input image using a Gaussian filter with a random kernel size. Parameters: Name Type Description blur_limit int, [int, int] maximum Gaussian kernel size for blurring the input image. Must be zero or odd and in range [0, inf). If set to 0 it will be computed from sigma as round(sigma * (3 if img.dtype == np.uint8 else 4) * 2 + 1) + 1 . If set single value blur_limit will be in range (0, blur_limit). Default: (3, 7). sigma_limit float, [float, float] Gaussian kernel standard deviation. Must be greater in range [0, inf). If set single value sigma_limit will be in range (0, sigma_limit). If set to 0 sigma will be computed as sigma = 0.3*((ksize-1)*0.5 - 1) + 0.8 . Default: 0. p float probability of applying the transform. Default: 0.5. Targets: image Image types: uint8, float32 class albumentations.augmentations.transforms.GaussNoise ( var_limit = ( 10.0 , 50.0 ), mean = 0 , per_channel = True , always_apply = False , p = 0.5 ) [view source on GitHub] \u00b6 Apply gaussian noise to the input image. Parameters: Name Type Description var_limit [float, float] or float variance range for noise. If var_limit is a single float, the range will be (0, var_limit). Default: (10.0, 50.0). mean float mean of the noise. Default: 0 per_channel bool if set to True, noise will be sampled for each channel independently. Otherwise, the noise will be sampled once for all channels. Default: True p float probability of applying the transform. Default: 0.5. Targets: image Image types: uint8, float32 class albumentations.augmentations.transforms.GlassBlur ( sigma = 0.7 , max_delta = 4 , iterations = 2 , always_apply = False , mode = 'fast' , p = 0.5 ) [view source on GitHub] \u00b6 Apply glass noise to the input image. Parameters: Name Type Description sigma float standard deviation for Gaussian kernel. max_delta int max distance between pixels which are swapped. iterations int number of repeats. Should be in range [1, inf). Default: (2). mode str mode of computation: fast or exact. Default: \"fast\". p float probability of applying the transform. Default: 0.5. Targets: image Image types: uint8, float32 Reference: | https://arxiv.org/abs/1903.12261 | https://github.com/hendrycks/robustness/blob/master/ImageNet-C/create_c/make_imagenet_c.py class albumentations.augmentations.transforms.GridDistortion ( num_steps = 5 , distort_limit = 0.3 , interpolation = 1 , border_mode = 4 , value = None , mask_value = None , always_apply = False , p = 0.5 ) [view source on GitHub] \u00b6 Parameters: Name Type Description num_steps int count of grid cells on each side. distort_limit float, [float, float] If distort_limit is a single float, the range will be (-distort_limit, distort_limit). Default: (-0.03, 0.03). interpolation OpenCV flag flag that is used to specify the interpolation algorithm. Should be one of: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4. Default: cv2.INTER_LINEAR. border_mode OpenCV flag flag that is used to specify the pixel extrapolation method. Should be one of: cv2.BORDER_CONSTANT, cv2.BORDER_REPLICATE, cv2.BORDER_REFLECT, cv2.BORDER_WRAP, cv2.BORDER_REFLECT_101. Default: cv2.BORDER_REFLECT_101 value int, float, list of ints, list of float padding value if border_mode is cv2.BORDER_CONSTANT. mask_value int, float, list of ints, list of float padding value if border_mode is cv2.BORDER_CONSTANT applied for masks. Targets: image, mask Image types: uint8, float32 class albumentations.augmentations.transforms.GridDropout ( ratio = 0.5 , unit_size_min = None , unit_size_max = None , holes_number_x = None , holes_number_y = None , shift_x = 0 , shift_y = 0 , random_offset = False , fill_value = 0 , mask_fill_value = None , always_apply = False , p = 0.5 ) [view source on GitHub] \u00b6 GridDropout, drops out rectangular regions of an image and the corresponding mask in a grid fashion. Parameters: Name Type Description ratio float the ratio of the mask holes to the unit_size (same for horizontal and vertical directions). Must be between 0 and 1. Default: 0.5. unit_size_min int minimum size of the grid unit. Must be between 2 and the image shorter edge. If 'None', holes_number_x and holes_number_y are used to setup the grid. Default: None . unit_size_max int maximum size of the grid unit. Must be between 2 and the image shorter edge. If 'None', holes_number_x and holes_number_y are used to setup the grid. Default: None . holes_number_x int the number of grid units in x direction. Must be between 1 and image width//2. If 'None', grid unit width is set as image_width//10. Default: None . holes_number_y int the number of grid units in y direction. Must be between 1 and image height//2. If None , grid unit height is set equal to the grid unit width or image height, whatever is smaller. shift_x int offsets of the grid start in x direction from (0,0) coordinate. Clipped between 0 and grid unit_width - hole_width. Default: 0. shift_y int offsets of the grid start in y direction from (0,0) coordinate. Clipped between 0 and grid unit height - hole_height. Default: 0. random_offset boolean weather to offset the grid randomly between 0 and grid unit size - hole size If 'True', entered shift_x, shift_y are ignored and set randomly. Default: False . fill_value int value for the dropped pixels. Default = 0 mask_fill_value int value for the dropped pixels in mask. If None , transformation is not applied to the mask. Default: None . Targets: image, mask Image types: uint8, float32 References: https://arxiv.org/abs/2001.04086 class albumentations.augmentations.transforms.HorizontalFlip [view source on GitHub] \u00b6 Flip the input horizontally around the y-axis. Parameters: Name Type Description p float probability of applying the transform. Default: 0.5. Targets: image, mask, bboxes, keypoints Image types: uint8, float32 class albumentations.augmentations.transforms.HueSaturationValue ( hue_shift_limit = 20 , sat_shift_limit = 30 , val_shift_limit = 20 , always_apply = False , p = 0.5 ) [view source on GitHub] \u00b6 Randomly change hue, saturation and value of the input image. Parameters: Name Type Description hue_shift_limit [int, int] or int range for changing hue. If hue_shift_limit is a single int, the range will be (-hue_shift_limit, hue_shift_limit). Default: (-20, 20). sat_shift_limit [int, int] or int range for changing saturation. If sat_shift_limit is a single int, the range will be (-sat_shift_limit, sat_shift_limit). Default: (-30, 30). val_shift_limit [int, int] or int range for changing value. If val_shift_limit is a single int, the range will be (-val_shift_limit, val_shift_limit). Default: (-20, 20). p float probability of applying the transform. Default: 0.5. Targets: image Image types: uint8, float32 class albumentations.augmentations.transforms.ImageCompression ( quality_lower = 99 , quality_upper = 100 , compression_type =< ImageCompressionType . JPEG : 0 > , always_apply = False , p = 0.5 ) [view source on GitHub] \u00b6 Decrease Jpeg, WebP compression of an image. Parameters: Name Type Description quality_lower float lower bound on the image quality. Should be in [0, 100] range for jpeg and [1, 100] for webp. quality_upper float upper bound on the image quality. Should be in [0, 100] range for jpeg and [1, 100] for webp. compression_type ImageCompressionType should be ImageCompressionType.JPEG or ImageCompressionType.WEBP. Default: ImageCompressionType.JPEG Targets: image Image types: uint8, float32 class albumentations.augmentations.transforms.ImageCompression.ImageCompressionType \u00b6 An enumeration. class albumentations.augmentations.transforms.InvertImg [view source on GitHub] \u00b6 Invert the input image by subtracting pixel values from 255. Parameters: Name Type Description p float probability of applying the transform. Default: 0.5. Targets: image Image types: uint8 class albumentations.augmentations.transforms.ISONoise ( color_shift = ( 0.01 , 0.05 ), intensity = ( 0.1 , 0.5 ), always_apply = False , p = 0.5 ) [view source on GitHub] \u00b6 Apply camera sensor noise. Parameters: Name Type Description color_shift [float, float] variance range for color hue change. Measured as a fraction of 360 degree Hue angle in HLS colorspace. intensity [float, float] Multiplicative factor that control strength of color and luminace noise. p float probability of applying the transform. Default: 0.5. Targets: image Image types: uint8 class albumentations.augmentations.transforms.JpegCompression ( quality_lower = 99 , quality_upper = 100 , always_apply = False , p = 0.5 ) [view source on GitHub] \u00b6 Decrease Jpeg compression of an image. Parameters: Name Type Description quality_lower float lower bound on the jpeg quality. Should be in [0, 100] range quality_upper float upper bound on the jpeg quality. Should be in [0, 100] range Targets: image Image types: uint8, float32 class albumentations.augmentations.transforms.Lambda ( image = None , mask = None , keypoint = None , bbox = None , name = None , always_apply = False , p = 1.0 ) [view source on GitHub] \u00b6 A flexible transformation class for using user-defined transformation functions per targets. Function signature must include **kwargs to accept optinal arguments like interpolation method, image size, etc: Parameters: Name Type Description image callable Image transformation function. mask callable Mask transformation function. keypoint callable Keypoint transformation function. bbox callable BBox transformation function. always_apply bool Indicates whether this transformation should be always applied. p float probability of applying the transform. Default: 1.0. Targets: image, mask, bboxes, keypoints Image types: Any class albumentations.augmentations.transforms.MaskDropout ( max_objects = 1 , image_fill_value = 0 , mask_fill_value = 0 , always_apply = False , p = 0.5 ) [view source on GitHub] \u00b6 Image & mask augmentation that zero out mask and image regions corresponding to randomly chosen object instance from mask. Mask must be single-channel image, zero values treated as background. Image can be any number of channels. Inspired by https://www.kaggle.com/c/severstal-steel-defect-detection/discussion/114254 albumentations.augmentations.transforms.MaskDropout.__init__ ( self , max_objects = 1 , image_fill_value = 0 , mask_fill_value = 0 , always_apply = False , p = 0.5 ) special \u00b6 Parameters: Name Type Description max_objects Maximum number of labels that can be zeroed out. Can be tuple, in this case it's [min, max] image_fill_value Fill value to use when filling image. Can be 'inpaint' to apply inpaining (works only for 3-chahnel images) mask_fill_value Fill value to use when filling mask. Targets: image, mask Image types: uint8, float32 class albumentations.augmentations.transforms.MedianBlur ( blur_limit = 7 , always_apply = False , p = 0.5 ) [view source on GitHub] \u00b6 Blur the input image using a median filter with a random aperture linear size. Parameters: Name Type Description blur_limit int maximum aperture linear size for blurring the input image. Must be odd and in range [3, inf). Default: (3, 7). p float probability of applying the transform. Default: 0.5. Targets: image Image types: uint8, float32 class albumentations.augmentations.transforms.MotionBlur [view source on GitHub] \u00b6 Apply motion blur to the input image using a random-sized kernel. Parameters: Name Type Description blur_limit int maximum kernel size for blurring the input image. Should be in range [3, inf). Default: (3, 7). p float probability of applying the transform. Default: 0.5. Targets: image Image types: uint8, float32 class albumentations.augmentations.transforms.MultiplicativeNoise ( multiplier = ( 0.9 , 1.1 ), per_channel = False , elementwise = False , always_apply = False , p = 0.5 ) [view source on GitHub] \u00b6 Multiply image to random number or array of numbers. Parameters: Name Type Description multiplier float or tuple of floats If single float image will be multiplied to this number. If tuple of float multiplier will be in range [multiplier[0], multiplier[1]) . Default: (0.9, 1.1). per_channel bool If False , same values for all channels will be used. If True use sample values for each channels. Default False. elementwise bool If False multiply multiply all pixels in an image with a random value sampled once. If True Multiply image pixels with values that are pixelwise randomly sampled. Defaule: False. Targets: image Image types: Any class albumentations.augmentations.transforms.Normalize ( mean = ( 0.485 , 0.456 , 0.406 ), std = ( 0.229 , 0.224 , 0.225 ), max_pixel_value = 255.0 , always_apply = False , p = 1.0 ) [view source on GitHub] \u00b6 Divide pixel values by 255 = 2**8 - 1, subtract mean per channel and divide by std per channel. Parameters: Name Type Description mean float, list of float mean values std (float, list of float std values max_pixel_value float maximum possible pixel value Targets: image Image types: uint8, float32 class albumentations.augmentations.transforms.OpticalDistortion ( distort_limit = 0.05 , shift_limit = 0.05 , interpolation = 1 , border_mode = 4 , value = None , mask_value = None , always_apply = False , p = 0.5 ) [view source on GitHub] \u00b6 Parameters: Name Type Description distort_limit float, [float, float] If distort_limit is a single float, the range will be (-distort_limit, distort_limit). Default: (-0.05, 0.05). shift_limit float, [float, float] If shift_limit is a single float, the range will be (-shift_limit, shift_limit). Default: (-0.05, 0.05). interpolation OpenCV flag flag that is used to specify the interpolation algorithm. Should be one of: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4. Default: cv2.INTER_LINEAR. border_mode OpenCV flag flag that is used to specify the pixel extrapolation method. Should be one of: cv2.BORDER_CONSTANT, cv2.BORDER_REPLICATE, cv2.BORDER_REFLECT, cv2.BORDER_WRAP, cv2.BORDER_REFLECT_101. Default: cv2.BORDER_REFLECT_101 value int, float, list of ints, list of float padding value if border_mode is cv2.BORDER_CONSTANT. mask_value int, float, list of ints, list of float padding value if border_mode is cv2.BORDER_CONSTANT applied for masks. Targets: image, mask Image types: uint8, float32 class albumentations.augmentations.transforms.PadIfNeeded ( min_height = 1024 , min_width = 1024 , pad_height_divisor = None , pad_width_divisor = None , border_mode = 4 , value = None , mask_value = None , always_apply = False , p = 1.0 ) [view source on GitHub] \u00b6 Pad side of the image / max if side is less than desired number. Parameters: Name Type Description min_height int minimal result image height. min_width int minimal result image width. pad_height_divisor int if not None, ensures image height is dividable by value of this argument. pad_width_divisor int if not None, ensures image width is dividable by value of this argument. border_mode OpenCV flag OpenCV border mode. value int, float, list of int, list of float padding value if border_mode is cv2.BORDER_CONSTANT. mask_value int, float, list of int, list of float padding value for mask if border_mode is cv2.BORDER_CONSTANT. p float probability of applying the transform. Default: 1.0. Targets: image, mask, bbox, keypoints Image types: uint8, float32 class albumentations.augmentations.transforms.Posterize ( num_bits = 4 , always_apply = False , p = 0.5 ) [view source on GitHub] \u00b6 Reduce the number of bits for each color channel. Parameters: Name Type Description num_bits [int, int] or int, or list of ints [r, g, b], or list of ints [[r1, r1], [g1, g2], [b1, b2]] number of high bits. If num_bits is a single value, the range will be [num_bits, num_bits]. Must be in range [0, 8]. Default: 4. p float probability of applying the transform. Default: 0.5. Targets: image Image types: uint8 class albumentations.augmentations.transforms.RandomBrightness ( limit = 0.2 , always_apply = False , p = 0.5 ) [view source on GitHub] \u00b6 Randomly change brightness of the input image. Parameters: Name Type Description limit [float, float] or float factor range for changing brightness. If limit is a single float, the range will be (-limit, limit). Default: (-0.2, 0.2). p float probability of applying the transform. Default: 0.5. Targets: image Image types: uint8, float32 class albumentations.augmentations.transforms.RandomBrightnessContrast ( brightness_limit = 0.2 , contrast_limit = 0.2 , brightness_by_max = True , always_apply = False , p = 0.5 ) [view source on GitHub] \u00b6 Randomly change brightness and contrast of the input image. Parameters: Name Type Description brightness_limit [float, float] or float factor range for changing brightness. If limit is a single float, the range will be (-limit, limit). Default: (-0.2, 0.2). contrast_limit [float, float] or float factor range for changing contrast. If limit is a single float, the range will be (-limit, limit). Default: (-0.2, 0.2). brightness_by_max Boolean If True adjust contrast by image dtype maximum, else adjust contrast by image mean. p float probability of applying the transform. Default: 0.5. Targets: image Image types: uint8, float32 class albumentations.augmentations.transforms.RandomContrast ( limit = 0.2 , always_apply = False , p = 0.5 ) [view source on GitHub] \u00b6 Randomly change contrast of the input image. Parameters: Name Type Description limit [float, float] or float factor range for changing contrast. If limit is a single float, the range will be (-limit, limit). Default: (-0.2, 0.2). p float probability of applying the transform. Default: 0.5. Targets: image Image types: uint8, float32 class albumentations.augmentations.transforms.RandomFog ( fog_coef_lower = 0.3 , fog_coef_upper = 1 , alpha_coef = 0.08 , always_apply = False , p = 0.5 ) [view source on GitHub] \u00b6 Simulates fog for the image From https://github.com/UjjwalSaxena/Automold--Road-Augmentation-Library Parameters: Name Type Description fog_coef_lower float lower limit for fog intensity coefficient. Should be in [0, 1] range. fog_coef_upper float upper limit for fog intensity coefficient. Should be in [0, 1] range. alpha_coef float transparency of the fog circles. Should be in [0, 1] range. Targets: image Image types: uint8, float32 class albumentations.augmentations.transforms.RandomGamma ( gamma_limit = ( 80 , 120 ), eps = None , always_apply = False , p = 0.5 ) [view source on GitHub] \u00b6 Parameters: Name Type Description gamma_limit float or [float, float] If gamma_limit is a single float value, the range will be (-gamma_limit, gamma_limit). Default: (80, 120). eps Deprecated. Targets: image Image types: uint8, float32 class albumentations.augmentations.transforms.RandomGridShuffle ( grid = ( 3 , 3 ), always_apply = False , p = 0.5 ) [view source on GitHub] \u00b6 Random shuffle grid's cells on image. Parameters: Name Type Description grid [int, int] size of grid for splitting image. Targets: image, mask Image types: uint8, float32 class albumentations.augmentations.transforms.RandomRain ( slant_lower =- 10 , slant_upper = 10 , drop_length = 20 , drop_width = 1 , drop_color = ( 200 , 200 , 200 ), blur_value = 7 , brightness_coefficient = 0.7 , rain_type = None , always_apply = False , p = 0.5 ) [view source on GitHub] \u00b6 Adds rain effects. From https://github.com/UjjwalSaxena/Automold--Road-Augmentation-Library Parameters: Name Type Description slant_lower should be in range [-20, 20]. slant_upper should be in range [-20, 20]. drop_length should be in range [0, 100]. drop_width should be in range [1, 5]. drop_color list of (r, g, b rain lines color. blur_value int rainy view are blurry brightness_coefficient float rainy days are usually shady. Should be in range [0, 1]. rain_type One of [None, \"drizzle\", \"heavy\", \"torrestial\"] Targets: image Image types: uint8, float32 class albumentations.augmentations.transforms.RandomShadow ( shadow_roi = ( 0 , 0.5 , 1 , 1 ), num_shadows_lower = 1 , num_shadows_upper = 2 , shadow_dimension = 5 , always_apply = False , p = 0.5 ) [view source on GitHub] \u00b6 Simulates shadows for the image From https://github.com/UjjwalSaxena/Automold--Road-Augmentation-Library Parameters: Name Type Description shadow_roi float, float, float, float region of the image where shadows will appear (x_min, y_min, x_max, y_max). All values should be in range [0, 1]. num_shadows_lower int Lower limit for the possible number of shadows. Should be in range [0, num_shadows_upper ]. num_shadows_upper int Lower limit for the possible number of shadows. Should be in range [ num_shadows_lower , inf]. shadow_dimension int number of edges in the shadow polygons Targets: image Image types: uint8, float32 class albumentations.augmentations.transforms.RandomSnow ( snow_point_lower = 0.1 , snow_point_upper = 0.3 , brightness_coeff = 2.5 , always_apply = False , p = 0.5 ) [view source on GitHub] \u00b6 Bleach out some pixel values simulating snow. From https://github.com/UjjwalSaxena/Automold--Road-Augmentation-Library Parameters: Name Type Description snow_point_lower float lower_bond of the amount of snow. Should be in [0, 1] range snow_point_upper float upper_bond of the amount of snow. Should be in [0, 1] range brightness_coeff float larger number will lead to a more snow on the image. Should be >= 0 Targets: image Image types: uint8, float32 class albumentations.augmentations.transforms.RandomSunFlare ( flare_roi = ( 0 , 0 , 1 , 0.5 ), angle_lower = 0 , angle_upper = 1 , num_flare_circles_lower = 6 , num_flare_circles_upper = 10 , src_radius = 400 , src_color = ( 255 , 255 , 255 ), always_apply = False , p = 0.5 ) [view source on GitHub] \u00b6 Simulates Sun Flare for the image From https://github.com/UjjwalSaxena/Automold--Road-Augmentation-Library Parameters: Name Type Description flare_roi float, float, float, float region of the image where flare will appear (x_min, y_min, x_max, y_max). All values should be in range [0, 1]. angle_lower float should be in range [0, angle_upper ]. angle_upper float should be in range [ angle_lower , 1]. num_flare_circles_lower int lower limit for the number of flare circles. Should be in range [0, num_flare_circles_upper ]. num_flare_circles_upper int upper limit for the number of flare circles. Should be in range [ num_flare_circles_lower , inf]. src_radius int src_color int, int, int color of the flare Targets: image Image types: uint8, float32 class albumentations.augmentations.transforms.RandomToneCurve ( scale = 0.1 , always_apply = False , p = 0.5 ) [view source on GitHub] \u00b6 Randomly change the relationship between bright and dark areas of the image by manipulating its tone curve. Parameters: Name Type Description scale float standard deviation of the normal distribution. Used to sample random distances to move two control points that modify the image's curve. Values should be in range [0, 1]. Default: 0.1 Targets: image Image types: uint8 class albumentations.augmentations.transforms.RGBShift ( r_shift_limit = 20 , g_shift_limit = 20 , b_shift_limit = 20 , always_apply = False , p = 0.5 ) [view source on GitHub] \u00b6 Randomly shift values for each channel of the input RGB image. Parameters: Name Type Description r_shift_limit [int, int] or int range for changing values for the red channel. If r_shift_limit is a single int, the range will be (-r_shift_limit, r_shift_limit). Default: (-20, 20). g_shift_limit [int, int] or int range for changing values for the green channel. If g_shift_limit is a single int, the range will be (-g_shift_limit, g_shift_limit). Default: (-20, 20). b_shift_limit [int, int] or int range for changing values for the blue channel. If b_shift_limit is a single int, the range will be (-b_shift_limit, b_shift_limit). Default: (-20, 20). p float probability of applying the transform. Default: 0.5. Targets: image Image types: uint8, float32 class albumentations.augmentations.transforms.Sharpen ( alpha = ( 0.2 , 0.5 ), lightness = ( 0.5 , 1.0 ), always_apply = False , p = 0.5 ) [view source on GitHub] \u00b6 Sharpen the input image and overlays the result with the original image. Parameters: Name Type Description alpha [float, float] range to choose the visibility of the sharpened image. At 0, only the original image is visible, at 1.0 only its sharpened version is visible. Default: (0.2, 0.5). lightness [float, float] range to choose the lightness of the sharpened image. Default: (0.5, 1.0). p float probability of applying the transform. Default: 0.5. Targets: image class albumentations.augmentations.transforms.Solarize ( threshold = 128 , always_apply = False , p = 0.5 ) [view source on GitHub] \u00b6 Invert all pixel values above a threshold. Parameters: Name Type Description threshold [int, int] or int, or [float, float] or float range for solarizing threshold. If threshold is a single value, the range will be [threshold, threshold]. Default 128. p float probability of applying the transform. Default: 0.5. Targets: image Image types: any class albumentations.augmentations.transforms.Superpixels ( p_replace = 0.1 , n_segments = 100 , max_size = 128 , interpolation = 1 , always_apply = False , p = 0.5 ) [view source on GitHub] \u00b6 Transform images parially/completely to their superpixel representation. This implementation uses skimage's version of the SLIC algorithm. Parameters: Name Type Description p_replace float or tuple of float Defines for any segment the probability that the pixels within that segment are replaced by their average color (otherwise, the pixels are not changed). Examples: * A probability of 0.0 would mean, that the pixels in no segment are replaced by their average color (image is not changed at all). * A probability of 0.5 would mean, that around half of all segments are replaced by their average color. * A probability of 1.0 would mean, that all segments are replaced by their average color (resulting in a voronoi image). Behaviour based on chosen data types for this parameter: * If a float , then that flat will always be used. * If tuple (a, b) , then a random probability will be sampled from the interval [a, b] per image. n_segments int, or tuple of int Rough target number of how many superpixels to generate (the algorithm may deviate from this number). Lower value will lead to coarser superpixels. Higher values are computationally more intensive and will hence lead to a slowdown * If a single int , then that value will always be used as the number of segments. * If a tuple (a, b) , then a value from the discrete interval [a..b] will be sampled per image. max_size int or None Maximum image size at which the augmentation is performed. If the width or height of an image exceeds this value, it will be downscaled before the augmentation so that the longest side matches max_size . This is done to speed up the process. The final output image has the same size as the input image. Note that in case p_replace is below 1.0 , the down-/upscaling will affect the not-replaced pixels too. Use None to apply no down-/upscaling. interpolation OpenCV flag flag that is used to specify the interpolation algorithm. Should be one of: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4. Default: cv2.INTER_LINEAR. p float probability of applying the transform. Default: 0.5. Targets: image class albumentations.augmentations.transforms.ToFloat ( max_value = None , always_apply = False , p = 1.0 ) [view source on GitHub] \u00b6 Divide pixel values by max_value to get a float32 output array where all values lie in the range [0, 1.0]. If max_value is None the transform will try to infer the maximum value by inspecting the data type of the input image. See Also: :class: ~albumentations.augmentations.transforms.FromFloat Parameters: Name Type Description max_value float maximum possible input value. Default: None. p float probability of applying the transform. Default: 1.0. Targets: image Image types: any type class albumentations.augmentations.transforms.ToGray [view source on GitHub] \u00b6 Convert the input RGB image to grayscale. If the mean pixel value for the resulting image is greater than 127, invert the resulting grayscale image. Parameters: Name Type Description p float probability of applying the transform. Default: 0.5. Targets: image Image types: uint8, float32 class albumentations.augmentations.transforms.ToSepia ( always_apply = False , p = 0.5 ) [view source on GitHub] \u00b6 Applies sepia filter to the input RGB image Parameters: Name Type Description p float probability of applying the transform. Default: 0.5. Targets: image Image types: uint8, float32 class albumentations.augmentations.transforms.Transpose [view source on GitHub] \u00b6 Transpose the input by swapping rows and columns. Parameters: Name Type Description p float probability of applying the transform. Default: 0.5. Targets: image, mask, bboxes, keypoints Image types: uint8, float32 class albumentations.augmentations.transforms.VerticalFlip [view source on GitHub] \u00b6 Flip the input vertically around the x-axis. Parameters: Name Type Description p float probability of applying the transform. Default: 0.5. Targets: image, mask, bboxes, keypoints Image types: uint8, float32","title":"Transforms (augmentations.transforms)"},{"location":"api_reference/augmentations/transforms/#transforms-augmentationstransforms","text":"","title":"Transforms (augmentations.transforms)"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms","text":"","title":"albumentations.augmentations.transforms"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.Blur","text":"Blur the input image using a random-sized kernel. Parameters: Name Type Description blur_limit int, [int, int] maximum kernel size for blurring the input image. Should be in range [3, inf). Default: (3, 7). p float probability of applying the transform. Default: 0.5. Targets: image Image types: uint8, float32","title":"Blur"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.ChannelDropout","text":"Randomly Drop Channels in the input Image. Parameters: Name Type Description channel_drop_range [int, int] range from which we choose the number of channels to drop. fill_value int, float pixel value for the dropped channel. p float probability of applying the transform. Default: 0.5. Targets: image Image types: uint8, uint16, unit32, float32","title":"ChannelDropout"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.ChannelShuffle","text":"Randomly rearrange channels of the input RGB image. Parameters: Name Type Description p float probability of applying the transform. Default: 0.5. Targets: image Image types: uint8, float32","title":"ChannelShuffle"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.CLAHE","text":"Apply Contrast Limited Adaptive Histogram Equalization to the input image. Parameters: Name Type Description clip_limit float or [float, float] upper threshold value for contrast limiting. If clip_limit is a single float value, the range will be (1, clip_limit). Default: (1, 4). tile_grid_size [int, int] size of grid for histogram equalization. Default: (8, 8). p float probability of applying the transform. Default: 0.5. Targets: image Image types: uint8","title":"CLAHE"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.CoarseDropout","text":"CoarseDropout of the rectangular regions in the image. Parameters: Name Type Description max_holes int Maximum number of regions to zero out. max_height int Maximum height of the hole. max_width int Maximum width of the hole. min_holes int Minimum number of regions to zero out. If None , min_holes is be set to max_holes . Default: None . min_height int Minimum height of the hole. Default: None. If None , min_height is set to max_height . Default: None . min_width int Minimum width of the hole. If None , min_height is set to max_width . Default: None . fill_value int, float, list of int, list of float value for dropped pixels. mask_fill_value int, float, list of int, list of float fill value for dropped pixels in mask. If None - mask is not affected. Default: None . Targets: image, mask Image types: uint8, float32 Reference: | https://arxiv.org/abs/1708.04552 | https://github.com/uoguelph-mlrg/Cutout/blob/master/util/cutout.py | https://github.com/aleju/imgaug/blob/master/imgaug/augmenters/arithmetic.py","title":"CoarseDropout"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.ColorJitter","text":"Randomly changes the brightness, contrast, and saturation of an image. Compared to ColorJitter from torchvision, this transform gives a little bit different results because Pillow (used in torchvision) and OpenCV (used in Albumentations) transform an image to HSV format by different formulas. Another difference - Pillow uses uint8 overflow, but we use value saturation. Parameters: Name Type Description brightness float or tuple of float (min, max How much to jitter brightness. brightness_factor is chosen uniformly from [max(0, 1 - brightness), 1 + brightness] or the given [min, max]. Should be non negative numbers. contrast float or tuple of float (min, max How much to jitter contrast. contrast_factor is chosen uniformly from [max(0, 1 - contrast), 1 + contrast] or the given [min, max]. Should be non negative numbers. saturation float or tuple of float (min, max How much to jitter saturation. saturation_factor is chosen uniformly from [max(0, 1 - saturation), 1 + saturation] or the given [min, max]. Should be non negative numbers. hue float or tuple of float (min, max How much to jitter hue. hue_factor is chosen uniformly from [-hue, hue] or the given [min, max]. Should have 0 <= hue <= 0.5 or -0.5 <= min <= max <= 0.5.","title":"ColorJitter"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.Cutout","text":"CoarseDropout of the square regions in the image. Parameters: Name Type Description num_holes int number of regions to zero out max_h_size int maximum height of the hole max_w_size int maximum width of the hole fill_value int, float, list of int, list of float value for dropped pixels. Targets: image Image types: uint8, float32 Reference: | https://arxiv.org/abs/1708.04552 | https://github.com/uoguelph-mlrg/Cutout/blob/master/util/cutout.py | https://github.com/aleju/imgaug/blob/master/imgaug/augmenters/arithmetic.py","title":"Cutout"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.Downscale","text":"Decreases image quality by downscaling and upscaling back. Parameters: Name Type Description scale_min float lower bound on the image scale. Should be < 1. scale_max float lower bound on the image scale. Should be . interpolation cv2 interpolation method. cv2.INTER_NEAREST by default Targets: image Image types: uint8, float32","title":"Downscale"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.Emboss","text":"Emboss the input image and overlays the result with the original image. Parameters: Name Type Description alpha [float, float] range to choose the visibility of the embossed image. At 0, only the original image is visible,at 1.0 only its embossed version is visible. Default: (0.2, 0.5). strength [float, float] strength range of the embossing. Default: (0.2, 0.7). p float probability of applying the transform. Default: 0.5. Targets: image","title":"Emboss"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.Equalize","text":"Equalize the image histogram. Parameters: Name Type Description mode str {'cv', 'pil'}. Use OpenCV or Pillow equalization method. by_channels bool If True, use equalization by channels separately, else convert image to YCbCr representation and use equalization by Y channel. mask np.ndarray, callable If given, only the pixels selected by the mask are included in the analysis. Maybe 1 channel or 3 channel array or callable. Function signature must include image argument. mask_params list of str Params for mask function. Targets: image Image types: uint8","title":"Equalize"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.FancyPCA","text":"Augment RGB image using FancyPCA from Krizhevsky's paper \"ImageNet Classification with Deep Convolutional Neural Networks\" Parameters: Name Type Description alpha float how much to perturb/scale the eigen vecs and vals. scale is samples from gaussian distribution (mu=0, sigma=alpha) Targets: image Image types: 3-channel uint8 images only Credit: http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf https://deshanadesai.github.io/notes/Fancy-PCA-with-Scikit-Image https://pixelatedbrian.github.io/2018-04-29-fancy_pca/","title":"FancyPCA"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.Flip","text":"Flip the input either horizontally, vertically or both horizontally and vertically. Parameters: Name Type Description p float probability of applying the transform. Default: 0.5. Targets: image, mask, bboxes, keypoints Image types: uint8, float32","title":"Flip"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.Flip.apply","text":"d (int): code that specifies how to flip the input. 0 for vertical flipping, 1 for horizontal flipping, -1 for both vertical and horizontal flipping (which is also could be seen as rotating the input by 180 degrees).","title":"apply()"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.FromFloat","text":"Take an input array where all values should lie in the range [0, 1.0], multiply them by max_value and then cast the resulted value to a type specified by dtype . If max_value is None the transform will try to infer the maximum value for the data type from the dtype argument. This is the inverse transform for :class: ~albumentations.augmentations.transforms.ToFloat . Parameters: Name Type Description max_value float maximum possible input value. Default: None. dtype string or numpy data type data type of the output. See the 'Data types' page from the NumPy docs _. Default: 'uint16'. p float probability of applying the transform. Default: 1.0. Targets: image Image types: float32 .. _'Data types' page from the NumPy docs: https://docs.scipy.org/doc/numpy/user/basics.types.html","title":"FromFloat"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.GaussianBlur","text":"Blur the input image using a Gaussian filter with a random kernel size. Parameters: Name Type Description blur_limit int, [int, int] maximum Gaussian kernel size for blurring the input image. Must be zero or odd and in range [0, inf). If set to 0 it will be computed from sigma as round(sigma * (3 if img.dtype == np.uint8 else 4) * 2 + 1) + 1 . If set single value blur_limit will be in range (0, blur_limit). Default: (3, 7). sigma_limit float, [float, float] Gaussian kernel standard deviation. Must be greater in range [0, inf). If set single value sigma_limit will be in range (0, sigma_limit). If set to 0 sigma will be computed as sigma = 0.3*((ksize-1)*0.5 - 1) + 0.8 . Default: 0. p float probability of applying the transform. Default: 0.5. Targets: image Image types: uint8, float32","title":"GaussianBlur"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.GaussNoise","text":"Apply gaussian noise to the input image. Parameters: Name Type Description var_limit [float, float] or float variance range for noise. If var_limit is a single float, the range will be (0, var_limit). Default: (10.0, 50.0). mean float mean of the noise. Default: 0 per_channel bool if set to True, noise will be sampled for each channel independently. Otherwise, the noise will be sampled once for all channels. Default: True p float probability of applying the transform. Default: 0.5. Targets: image Image types: uint8, float32","title":"GaussNoise"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.GlassBlur","text":"Apply glass noise to the input image. Parameters: Name Type Description sigma float standard deviation for Gaussian kernel. max_delta int max distance between pixels which are swapped. iterations int number of repeats. Should be in range [1, inf). Default: (2). mode str mode of computation: fast or exact. Default: \"fast\". p float probability of applying the transform. Default: 0.5. Targets: image Image types: uint8, float32 Reference: | https://arxiv.org/abs/1903.12261 | https://github.com/hendrycks/robustness/blob/master/ImageNet-C/create_c/make_imagenet_c.py","title":"GlassBlur"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.GridDistortion","text":"Parameters: Name Type Description num_steps int count of grid cells on each side. distort_limit float, [float, float] If distort_limit is a single float, the range will be (-distort_limit, distort_limit). Default: (-0.03, 0.03). interpolation OpenCV flag flag that is used to specify the interpolation algorithm. Should be one of: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4. Default: cv2.INTER_LINEAR. border_mode OpenCV flag flag that is used to specify the pixel extrapolation method. Should be one of: cv2.BORDER_CONSTANT, cv2.BORDER_REPLICATE, cv2.BORDER_REFLECT, cv2.BORDER_WRAP, cv2.BORDER_REFLECT_101. Default: cv2.BORDER_REFLECT_101 value int, float, list of ints, list of float padding value if border_mode is cv2.BORDER_CONSTANT. mask_value int, float, list of ints, list of float padding value if border_mode is cv2.BORDER_CONSTANT applied for masks. Targets: image, mask Image types: uint8, float32","title":"GridDistortion"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.GridDropout","text":"GridDropout, drops out rectangular regions of an image and the corresponding mask in a grid fashion. Parameters: Name Type Description ratio float the ratio of the mask holes to the unit_size (same for horizontal and vertical directions). Must be between 0 and 1. Default: 0.5. unit_size_min int minimum size of the grid unit. Must be between 2 and the image shorter edge. If 'None', holes_number_x and holes_number_y are used to setup the grid. Default: None . unit_size_max int maximum size of the grid unit. Must be between 2 and the image shorter edge. If 'None', holes_number_x and holes_number_y are used to setup the grid. Default: None . holes_number_x int the number of grid units in x direction. Must be between 1 and image width//2. If 'None', grid unit width is set as image_width//10. Default: None . holes_number_y int the number of grid units in y direction. Must be between 1 and image height//2. If None , grid unit height is set equal to the grid unit width or image height, whatever is smaller. shift_x int offsets of the grid start in x direction from (0,0) coordinate. Clipped between 0 and grid unit_width - hole_width. Default: 0. shift_y int offsets of the grid start in y direction from (0,0) coordinate. Clipped between 0 and grid unit height - hole_height. Default: 0. random_offset boolean weather to offset the grid randomly between 0 and grid unit size - hole size If 'True', entered shift_x, shift_y are ignored and set randomly. Default: False . fill_value int value for the dropped pixels. Default = 0 mask_fill_value int value for the dropped pixels in mask. If None , transformation is not applied to the mask. Default: None . Targets: image, mask Image types: uint8, float32 References: https://arxiv.org/abs/2001.04086","title":"GridDropout"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.HorizontalFlip","text":"Flip the input horizontally around the y-axis. Parameters: Name Type Description p float probability of applying the transform. Default: 0.5. Targets: image, mask, bboxes, keypoints Image types: uint8, float32","title":"HorizontalFlip"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.HueSaturationValue","text":"Randomly change hue, saturation and value of the input image. Parameters: Name Type Description hue_shift_limit [int, int] or int range for changing hue. If hue_shift_limit is a single int, the range will be (-hue_shift_limit, hue_shift_limit). Default: (-20, 20). sat_shift_limit [int, int] or int range for changing saturation. If sat_shift_limit is a single int, the range will be (-sat_shift_limit, sat_shift_limit). Default: (-30, 30). val_shift_limit [int, int] or int range for changing value. If val_shift_limit is a single int, the range will be (-val_shift_limit, val_shift_limit). Default: (-20, 20). p float probability of applying the transform. Default: 0.5. Targets: image Image types: uint8, float32","title":"HueSaturationValue"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.ImageCompression","text":"Decrease Jpeg, WebP compression of an image. Parameters: Name Type Description quality_lower float lower bound on the image quality. Should be in [0, 100] range for jpeg and [1, 100] for webp. quality_upper float upper bound on the image quality. Should be in [0, 100] range for jpeg and [1, 100] for webp. compression_type ImageCompressionType should be ImageCompressionType.JPEG or ImageCompressionType.WEBP. Default: ImageCompressionType.JPEG Targets: image Image types: uint8, float32","title":"ImageCompression"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.ImageCompression.ImageCompressionType","text":"An enumeration.","title":"ImageCompressionType"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.InvertImg","text":"Invert the input image by subtracting pixel values from 255. Parameters: Name Type Description p float probability of applying the transform. Default: 0.5. Targets: image Image types: uint8","title":"InvertImg"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.ISONoise","text":"Apply camera sensor noise. Parameters: Name Type Description color_shift [float, float] variance range for color hue change. Measured as a fraction of 360 degree Hue angle in HLS colorspace. intensity [float, float] Multiplicative factor that control strength of color and luminace noise. p float probability of applying the transform. Default: 0.5. Targets: image Image types: uint8","title":"ISONoise"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.JpegCompression","text":"Decrease Jpeg compression of an image. Parameters: Name Type Description quality_lower float lower bound on the jpeg quality. Should be in [0, 100] range quality_upper float upper bound on the jpeg quality. Should be in [0, 100] range Targets: image Image types: uint8, float32","title":"JpegCompression"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.Lambda","text":"A flexible transformation class for using user-defined transformation functions per targets. Function signature must include **kwargs to accept optinal arguments like interpolation method, image size, etc: Parameters: Name Type Description image callable Image transformation function. mask callable Mask transformation function. keypoint callable Keypoint transformation function. bbox callable BBox transformation function. always_apply bool Indicates whether this transformation should be always applied. p float probability of applying the transform. Default: 1.0. Targets: image, mask, bboxes, keypoints Image types: Any","title":"Lambda"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.MaskDropout","text":"Image & mask augmentation that zero out mask and image regions corresponding to randomly chosen object instance from mask. Mask must be single-channel image, zero values treated as background. Image can be any number of channels. Inspired by https://www.kaggle.com/c/severstal-steel-defect-detection/discussion/114254","title":"MaskDropout"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.MaskDropout.__init__","text":"Parameters: Name Type Description max_objects Maximum number of labels that can be zeroed out. Can be tuple, in this case it's [min, max] image_fill_value Fill value to use when filling image. Can be 'inpaint' to apply inpaining (works only for 3-chahnel images) mask_fill_value Fill value to use when filling mask. Targets: image, mask Image types: uint8, float32","title":"__init__()"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.MedianBlur","text":"Blur the input image using a median filter with a random aperture linear size. Parameters: Name Type Description blur_limit int maximum aperture linear size for blurring the input image. Must be odd and in range [3, inf). Default: (3, 7). p float probability of applying the transform. Default: 0.5. Targets: image Image types: uint8, float32","title":"MedianBlur"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.MotionBlur","text":"Apply motion blur to the input image using a random-sized kernel. Parameters: Name Type Description blur_limit int maximum kernel size for blurring the input image. Should be in range [3, inf). Default: (3, 7). p float probability of applying the transform. Default: 0.5. Targets: image Image types: uint8, float32","title":"MotionBlur"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.MultiplicativeNoise","text":"Multiply image to random number or array of numbers. Parameters: Name Type Description multiplier float or tuple of floats If single float image will be multiplied to this number. If tuple of float multiplier will be in range [multiplier[0], multiplier[1]) . Default: (0.9, 1.1). per_channel bool If False , same values for all channels will be used. If True use sample values for each channels. Default False. elementwise bool If False multiply multiply all pixels in an image with a random value sampled once. If True Multiply image pixels with values that are pixelwise randomly sampled. Defaule: False. Targets: image Image types: Any","title":"MultiplicativeNoise"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.Normalize","text":"Divide pixel values by 255 = 2**8 - 1, subtract mean per channel and divide by std per channel. Parameters: Name Type Description mean float, list of float mean values std (float, list of float std values max_pixel_value float maximum possible pixel value Targets: image Image types: uint8, float32","title":"Normalize"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.OpticalDistortion","text":"Parameters: Name Type Description distort_limit float, [float, float] If distort_limit is a single float, the range will be (-distort_limit, distort_limit). Default: (-0.05, 0.05). shift_limit float, [float, float] If shift_limit is a single float, the range will be (-shift_limit, shift_limit). Default: (-0.05, 0.05). interpolation OpenCV flag flag that is used to specify the interpolation algorithm. Should be one of: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4. Default: cv2.INTER_LINEAR. border_mode OpenCV flag flag that is used to specify the pixel extrapolation method. Should be one of: cv2.BORDER_CONSTANT, cv2.BORDER_REPLICATE, cv2.BORDER_REFLECT, cv2.BORDER_WRAP, cv2.BORDER_REFLECT_101. Default: cv2.BORDER_REFLECT_101 value int, float, list of ints, list of float padding value if border_mode is cv2.BORDER_CONSTANT. mask_value int, float, list of ints, list of float padding value if border_mode is cv2.BORDER_CONSTANT applied for masks. Targets: image, mask Image types: uint8, float32","title":"OpticalDistortion"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.PadIfNeeded","text":"Pad side of the image / max if side is less than desired number. Parameters: Name Type Description min_height int minimal result image height. min_width int minimal result image width. pad_height_divisor int if not None, ensures image height is dividable by value of this argument. pad_width_divisor int if not None, ensures image width is dividable by value of this argument. border_mode OpenCV flag OpenCV border mode. value int, float, list of int, list of float padding value if border_mode is cv2.BORDER_CONSTANT. mask_value int, float, list of int, list of float padding value for mask if border_mode is cv2.BORDER_CONSTANT. p float probability of applying the transform. Default: 1.0. Targets: image, mask, bbox, keypoints Image types: uint8, float32","title":"PadIfNeeded"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.Posterize","text":"Reduce the number of bits for each color channel. Parameters: Name Type Description num_bits [int, int] or int, or list of ints [r, g, b], or list of ints [[r1, r1], [g1, g2], [b1, b2]] number of high bits. If num_bits is a single value, the range will be [num_bits, num_bits]. Must be in range [0, 8]. Default: 4. p float probability of applying the transform. Default: 0.5. Targets: image Image types: uint8","title":"Posterize"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.RandomBrightness","text":"Randomly change brightness of the input image. Parameters: Name Type Description limit [float, float] or float factor range for changing brightness. If limit is a single float, the range will be (-limit, limit). Default: (-0.2, 0.2). p float probability of applying the transform. Default: 0.5. Targets: image Image types: uint8, float32","title":"RandomBrightness"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.RandomBrightnessContrast","text":"Randomly change brightness and contrast of the input image. Parameters: Name Type Description brightness_limit [float, float] or float factor range for changing brightness. If limit is a single float, the range will be (-limit, limit). Default: (-0.2, 0.2). contrast_limit [float, float] or float factor range for changing contrast. If limit is a single float, the range will be (-limit, limit). Default: (-0.2, 0.2). brightness_by_max Boolean If True adjust contrast by image dtype maximum, else adjust contrast by image mean. p float probability of applying the transform. Default: 0.5. Targets: image Image types: uint8, float32","title":"RandomBrightnessContrast"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.RandomContrast","text":"Randomly change contrast of the input image. Parameters: Name Type Description limit [float, float] or float factor range for changing contrast. If limit is a single float, the range will be (-limit, limit). Default: (-0.2, 0.2). p float probability of applying the transform. Default: 0.5. Targets: image Image types: uint8, float32","title":"RandomContrast"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.RandomFog","text":"Simulates fog for the image From https://github.com/UjjwalSaxena/Automold--Road-Augmentation-Library Parameters: Name Type Description fog_coef_lower float lower limit for fog intensity coefficient. Should be in [0, 1] range. fog_coef_upper float upper limit for fog intensity coefficient. Should be in [0, 1] range. alpha_coef float transparency of the fog circles. Should be in [0, 1] range. Targets: image Image types: uint8, float32","title":"RandomFog"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.RandomGamma","text":"Parameters: Name Type Description gamma_limit float or [float, float] If gamma_limit is a single float value, the range will be (-gamma_limit, gamma_limit). Default: (80, 120). eps Deprecated. Targets: image Image types: uint8, float32","title":"RandomGamma"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.RandomGridShuffle","text":"Random shuffle grid's cells on image. Parameters: Name Type Description grid [int, int] size of grid for splitting image. Targets: image, mask Image types: uint8, float32","title":"RandomGridShuffle"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.RandomRain","text":"Adds rain effects. From https://github.com/UjjwalSaxena/Automold--Road-Augmentation-Library Parameters: Name Type Description slant_lower should be in range [-20, 20]. slant_upper should be in range [-20, 20]. drop_length should be in range [0, 100]. drop_width should be in range [1, 5]. drop_color list of (r, g, b rain lines color. blur_value int rainy view are blurry brightness_coefficient float rainy days are usually shady. Should be in range [0, 1]. rain_type One of [None, \"drizzle\", \"heavy\", \"torrestial\"] Targets: image Image types: uint8, float32","title":"RandomRain"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.RandomShadow","text":"Simulates shadows for the image From https://github.com/UjjwalSaxena/Automold--Road-Augmentation-Library Parameters: Name Type Description shadow_roi float, float, float, float region of the image where shadows will appear (x_min, y_min, x_max, y_max). All values should be in range [0, 1]. num_shadows_lower int Lower limit for the possible number of shadows. Should be in range [0, num_shadows_upper ]. num_shadows_upper int Lower limit for the possible number of shadows. Should be in range [ num_shadows_lower , inf]. shadow_dimension int number of edges in the shadow polygons Targets: image Image types: uint8, float32","title":"RandomShadow"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.RandomSnow","text":"Bleach out some pixel values simulating snow. From https://github.com/UjjwalSaxena/Automold--Road-Augmentation-Library Parameters: Name Type Description snow_point_lower float lower_bond of the amount of snow. Should be in [0, 1] range snow_point_upper float upper_bond of the amount of snow. Should be in [0, 1] range brightness_coeff float larger number will lead to a more snow on the image. Should be >= 0 Targets: image Image types: uint8, float32","title":"RandomSnow"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.RandomSunFlare","text":"Simulates Sun Flare for the image From https://github.com/UjjwalSaxena/Automold--Road-Augmentation-Library Parameters: Name Type Description flare_roi float, float, float, float region of the image where flare will appear (x_min, y_min, x_max, y_max). All values should be in range [0, 1]. angle_lower float should be in range [0, angle_upper ]. angle_upper float should be in range [ angle_lower , 1]. num_flare_circles_lower int lower limit for the number of flare circles. Should be in range [0, num_flare_circles_upper ]. num_flare_circles_upper int upper limit for the number of flare circles. Should be in range [ num_flare_circles_lower , inf]. src_radius int src_color int, int, int color of the flare Targets: image Image types: uint8, float32","title":"RandomSunFlare"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.RandomToneCurve","text":"Randomly change the relationship between bright and dark areas of the image by manipulating its tone curve. Parameters: Name Type Description scale float standard deviation of the normal distribution. Used to sample random distances to move two control points that modify the image's curve. Values should be in range [0, 1]. Default: 0.1 Targets: image Image types: uint8","title":"RandomToneCurve"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.RGBShift","text":"Randomly shift values for each channel of the input RGB image. Parameters: Name Type Description r_shift_limit [int, int] or int range for changing values for the red channel. If r_shift_limit is a single int, the range will be (-r_shift_limit, r_shift_limit). Default: (-20, 20). g_shift_limit [int, int] or int range for changing values for the green channel. If g_shift_limit is a single int, the range will be (-g_shift_limit, g_shift_limit). Default: (-20, 20). b_shift_limit [int, int] or int range for changing values for the blue channel. If b_shift_limit is a single int, the range will be (-b_shift_limit, b_shift_limit). Default: (-20, 20). p float probability of applying the transform. Default: 0.5. Targets: image Image types: uint8, float32","title":"RGBShift"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.Sharpen","text":"Sharpen the input image and overlays the result with the original image. Parameters: Name Type Description alpha [float, float] range to choose the visibility of the sharpened image. At 0, only the original image is visible, at 1.0 only its sharpened version is visible. Default: (0.2, 0.5). lightness [float, float] range to choose the lightness of the sharpened image. Default: (0.5, 1.0). p float probability of applying the transform. Default: 0.5. Targets: image","title":"Sharpen"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.Solarize","text":"Invert all pixel values above a threshold. Parameters: Name Type Description threshold [int, int] or int, or [float, float] or float range for solarizing threshold. If threshold is a single value, the range will be [threshold, threshold]. Default 128. p float probability of applying the transform. Default: 0.5. Targets: image Image types: any","title":"Solarize"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.Superpixels","text":"Transform images parially/completely to their superpixel representation. This implementation uses skimage's version of the SLIC algorithm. Parameters: Name Type Description p_replace float or tuple of float Defines for any segment the probability that the pixels within that segment are replaced by their average color (otherwise, the pixels are not changed). Examples: * A probability of 0.0 would mean, that the pixels in no segment are replaced by their average color (image is not changed at all). * A probability of 0.5 would mean, that around half of all segments are replaced by their average color. * A probability of 1.0 would mean, that all segments are replaced by their average color (resulting in a voronoi image). Behaviour based on chosen data types for this parameter: * If a float , then that flat will always be used. * If tuple (a, b) , then a random probability will be sampled from the interval [a, b] per image. n_segments int, or tuple of int Rough target number of how many superpixels to generate (the algorithm may deviate from this number). Lower value will lead to coarser superpixels. Higher values are computationally more intensive and will hence lead to a slowdown * If a single int , then that value will always be used as the number of segments. * If a tuple (a, b) , then a value from the discrete interval [a..b] will be sampled per image. max_size int or None Maximum image size at which the augmentation is performed. If the width or height of an image exceeds this value, it will be downscaled before the augmentation so that the longest side matches max_size . This is done to speed up the process. The final output image has the same size as the input image. Note that in case p_replace is below 1.0 , the down-/upscaling will affect the not-replaced pixels too. Use None to apply no down-/upscaling. interpolation OpenCV flag flag that is used to specify the interpolation algorithm. Should be one of: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4. Default: cv2.INTER_LINEAR. p float probability of applying the transform. Default: 0.5. Targets: image","title":"Superpixels"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.ToFloat","text":"Divide pixel values by max_value to get a float32 output array where all values lie in the range [0, 1.0]. If max_value is None the transform will try to infer the maximum value by inspecting the data type of the input image. See Also: :class: ~albumentations.augmentations.transforms.FromFloat Parameters: Name Type Description max_value float maximum possible input value. Default: None. p float probability of applying the transform. Default: 1.0. Targets: image Image types: any type","title":"ToFloat"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.ToGray","text":"Convert the input RGB image to grayscale. If the mean pixel value for the resulting image is greater than 127, invert the resulting grayscale image. Parameters: Name Type Description p float probability of applying the transform. Default: 0.5. Targets: image Image types: uint8, float32","title":"ToGray"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.ToSepia","text":"Applies sepia filter to the input RGB image Parameters: Name Type Description p float probability of applying the transform. Default: 0.5. Targets: image Image types: uint8, float32","title":"ToSepia"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.Transpose","text":"Transpose the input by swapping rows and columns. Parameters: Name Type Description p float probability of applying the transform. Default: 0.5. Targets: image, mask, bboxes, keypoints Image types: uint8, float32","title":"Transpose"},{"location":"api_reference/augmentations/transforms/#albumentations.augmentations.transforms.VerticalFlip","text":"Flip the input vertically around the x-axis. Parameters: Name Type Description p float probability of applying the transform. Default: 0.5. Targets: image, mask, bboxes, keypoints Image types: uint8, float32","title":"VerticalFlip"},{"location":"api_reference/augmentations/crops/","text":"Crop functional transforms (albumentations.augmentations.crops.functional) Crop transforms (albumentations.augmentations.crops.transforms)","title":"Index"},{"location":"api_reference/augmentations/crops/functional/","text":"Crop functional transforms (augmentations.crops.functional) \u00b6 \u00b6 def albumentations.augmentations.crops.functional.bbox_crop ( bbox , x_min , y_min , x_max , y_max , rows , cols ) [view source on GitHub] \u00b6 Crop a bounding box. Parameters: Name Type Description bbox Union[List[int], List[float], Tuple[int, ...], Tuple[float, ...], numpy.ndarray] A bounding box (x_min, y_min, x_max, y_max) . x_min int y_min int x_max int y_max int rows int Image rows. cols int Image cols. Returns: Type Description tuple A cropped bounding box (x_min, y_min, x_max, y_max) . def albumentations.augmentations.crops.functional.crop_bbox_by_coords ( bbox , crop_coords , crop_height , crop_width , rows , cols ) [view source on GitHub] \u00b6 Crop a bounding box using the provided coordinates of bottom-left and top-right corners in pixels and the required height and width of the crop. Parameters: Name Type Description bbox Union[List[int], List[float], Tuple[int, ...], Tuple[float, ...], numpy.ndarray] A cropped box (x_min, y_min, x_max, y_max) . crop_coords Tuple[int, int, int, int] Crop coordinates (x1, y1, x2, y2) . crop_height int crop_width int rows int Image rows. cols int Image cols. Returns: Type Description tuple A cropped bounding box (x_min, y_min, x_max, y_max) . def albumentations.augmentations.crops.functional.crop_keypoint_by_coords ( keypoint , crop_coords ) [view source on GitHub] \u00b6 Crop a keypoint using the provided coordinates of bottom-left and top-right corners in pixels and the required height and width of the crop. Parameters: Name Type Description keypoint Union[List[int], List[float], Tuple[int, ...], Tuple[float, ...], numpy.ndarray] A keypoint (x, y, angle, scale) . crop_coords Tuple[int, int, int, int] Crop box coords (x1, x2, y1, y2) . Returns: Type Description A keypoint (x, y, angle, scale) . def albumentations.augmentations.crops.functional.keypoint_center_crop ( keypoint , crop_height , crop_width , rows , cols ) [view source on GitHub] \u00b6 Keypoint center crop. Parameters: Name Type Description keypoint Union[List[int], List[float], Tuple[int, ...], Tuple[float, ...], numpy.ndarray] A keypoint (x, y, angle, scale) . crop_height int Crop height. crop_width int Crop width. rows int Image height. cols int Image width. Returns: Type Description tuple A keypoint (x, y, angle, scale) . def albumentations.augmentations.crops.functional.keypoint_random_crop ( keypoint , crop_height , crop_width , h_start , w_start , rows , cols ) [view source on GitHub] \u00b6 Keypoint random crop. Parameters: Name Type Description keypoint Union[List[int], List[float], Tuple[int, ...], Tuple[float, ...], numpy.ndarray] (tuple): A keypoint (x, y, angle, scale) . crop_height int Crop height. crop_width int Crop width. h_start float Crop height start. w_start float Crop width start. rows int Image height. cols int Image width. Returns: Type Description A keypoint (x, y, angle, scale) .","title":"Crop functional transforms (augmentations.crops.functional)"},{"location":"api_reference/augmentations/crops/functional/#crop-functional-transforms-augmentationscropsfunctional","text":"","title":"Crop functional transforms (augmentations.crops.functional)"},{"location":"api_reference/augmentations/crops/functional/#albumentations.augmentations.crops.functional","text":"","title":"albumentations.augmentations.crops.functional"},{"location":"api_reference/augmentations/crops/functional/#albumentations.augmentations.crops.functional.bbox_crop","text":"Crop a bounding box. Parameters: Name Type Description bbox Union[List[int], List[float], Tuple[int, ...], Tuple[float, ...], numpy.ndarray] A bounding box (x_min, y_min, x_max, y_max) . x_min int y_min int x_max int y_max int rows int Image rows. cols int Image cols. Returns: Type Description tuple A cropped bounding box (x_min, y_min, x_max, y_max) .","title":"bbox_crop()"},{"location":"api_reference/augmentations/crops/functional/#albumentations.augmentations.crops.functional.crop_bbox_by_coords","text":"Crop a bounding box using the provided coordinates of bottom-left and top-right corners in pixels and the required height and width of the crop. Parameters: Name Type Description bbox Union[List[int], List[float], Tuple[int, ...], Tuple[float, ...], numpy.ndarray] A cropped box (x_min, y_min, x_max, y_max) . crop_coords Tuple[int, int, int, int] Crop coordinates (x1, y1, x2, y2) . crop_height int crop_width int rows int Image rows. cols int Image cols. Returns: Type Description tuple A cropped bounding box (x_min, y_min, x_max, y_max) .","title":"crop_bbox_by_coords()"},{"location":"api_reference/augmentations/crops/functional/#albumentations.augmentations.crops.functional.crop_keypoint_by_coords","text":"Crop a keypoint using the provided coordinates of bottom-left and top-right corners in pixels and the required height and width of the crop. Parameters: Name Type Description keypoint Union[List[int], List[float], Tuple[int, ...], Tuple[float, ...], numpy.ndarray] A keypoint (x, y, angle, scale) . crop_coords Tuple[int, int, int, int] Crop box coords (x1, x2, y1, y2) . Returns: Type Description A keypoint (x, y, angle, scale) .","title":"crop_keypoint_by_coords()"},{"location":"api_reference/augmentations/crops/functional/#albumentations.augmentations.crops.functional.keypoint_center_crop","text":"Keypoint center crop. Parameters: Name Type Description keypoint Union[List[int], List[float], Tuple[int, ...], Tuple[float, ...], numpy.ndarray] A keypoint (x, y, angle, scale) . crop_height int Crop height. crop_width int Crop width. rows int Image height. cols int Image width. Returns: Type Description tuple A keypoint (x, y, angle, scale) .","title":"keypoint_center_crop()"},{"location":"api_reference/augmentations/crops/functional/#albumentations.augmentations.crops.functional.keypoint_random_crop","text":"Keypoint random crop. Parameters: Name Type Description keypoint Union[List[int], List[float], Tuple[int, ...], Tuple[float, ...], numpy.ndarray] (tuple): A keypoint (x, y, angle, scale) . crop_height int Crop height. crop_width int Crop width. h_start float Crop height start. w_start float Crop width start. rows int Image height. cols int Image width. Returns: Type Description A keypoint (x, y, angle, scale) .","title":"keypoint_random_crop()"},{"location":"api_reference/augmentations/crops/transforms/","text":"Crop transforms (augmentations.crops.transforms) \u00b6 \u00b6 class albumentations.augmentations.crops.transforms.CenterCrop ( height , width , always_apply = False , p = 1.0 ) [view source on GitHub] \u00b6 Crop the central part of the input. Parameters: Name Type Description height int height of the crop. width int width of the crop. p float probability of applying the transform. Default: 1. Targets: image, mask, bboxes, keypoints Image types: uint8, float32 Note: It is recommended to use uint8 images as input. Otherwise the operation will require internal conversion float32 -> uint8 -> float32 that causes worse performance. class albumentations.augmentations.crops.transforms.Crop ( x_min = 0 , y_min = 0 , x_max = 1024 , y_max = 1024 , always_apply = False , p = 1.0 ) [view source on GitHub] \u00b6 Crop region from image. Parameters: Name Type Description x_min int Minimum upper left x coordinate. y_min int Minimum upper left y coordinate. x_max int Maximum lower right x coordinate. y_max int Maximum lower right y coordinate. Targets: image, mask, bboxes, keypoints Image types: uint8, float32 class albumentations.augmentations.crops.transforms.CropAndPad ( px = None , percent = None , pad_mode = 0 , pad_cval = 0 , pad_cval_mask = 0 , keep_size = True , sample_independently = True , interpolation = 1 , always_apply = False , p = 1.0 ) [view source on GitHub] \u00b6 Crop and pad images by pixel amounts or fractions of image sizes. Cropping removes pixels at the sides (i.e. extracts a subimage from a given full image). Padding adds pixels to the sides (e.g. black pixels). This transformation will never crop images below a height or width of 1 . Note: This transformation automatically resizes images back to their original size. To deactivate this, add the parameter keep_size=False . Parameters: Name Type Description px int or tuple The number of pixels to crop (negative values) or pad (positive values) on each side of the image. Either this or the parameter percent may be set, not both at the same time. * If None , then pixel-based cropping/padding will not be used. * If int , then that exact number of pixels will always be cropped/padded. * If a tuple of two int s with values a and b , then each side will be cropped/padded by a random amount sampled uniformly per image and side from the interval [a, b] . If however sample_independently is set to False , only one value will be sampled per image and used for all sides. * If a tuple of four entries, then the entries represent top, right, bottom, left. Each entry may be a single int (always crop/pad by exactly that value), a tuple of two int s a and b (crop/pad by an amount within [a, b] ), a list of int s (crop/pad by a random value that is contained in the list ). percent float or tuple The number of pixels to crop (negative values) or pad (positive values) on each side of the image given as a fraction of the image height/width. E.g. if this is set to -0.1 , the transformation will always crop away 10% of the image's height at both the top and the bottom (both 10% each), as well as 10% of the width at the right and left. Expected value range is (-1.0, inf) . Either this or the parameter px may be set, not both at the same time. * If None , then fraction-based cropping/padding will not be used. * If float , then that fraction will always be cropped/padded. * If a tuple of two float s with values a and b , then each side will be cropped/padded by a random fraction sampled uniformly per image and side from the interval [a, b] . If however sample_independently is set to False , only one value will be sampled per image and used for all sides. * If a tuple of four entries, then the entries represent top, right, bottom, left. Each entry may be a single float (always crop/pad by exactly that percent value), a tuple of two float s a and b (crop/pad by a fraction from [a, b] ), a list of float s (crop/pad by a random value that is contained in the list). pad_mode int OpenCV border mode. pad_cval number, Sequence[number] The constant value to use if the pad mode is BORDER_CONSTANT . * If number , then that value will be used. * If a tuple of two number s and at least one of them is a float , then a random number will be uniformly sampled per image from the continuous interval [a, b] and used as the value. If both number s are int s, the interval is discrete. * If a list of number , then a random value will be chosen from the elements of the list and used as the value. pad_cval_mask number, Sequence[number] Same as pad_cval but only for masks. keep_size bool After cropping and padding, the result image will usually have a different height/width compared to the original input image. If this parameter is set to True , then the cropped/padded image will be resized to the input image's size, i.e. the output shape is always identical to the input shape. sample_independently bool If False and the values for px / percent result in exactly one probability distribution for all image sides, only one single value will be sampled from that probability distribution and used for all sides. I.e. the crop/pad amount then is the same for all sides. If True , four values will be sampled independently, one per side. interpolation OpenCV flag flag that is used to specify the interpolation algorithm. Should be one of: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4. Default: cv2.INTER_LINEAR. Targets: image, mask, bboxes, keypoints Image types: any class albumentations.augmentations.crops.transforms.CropNonEmptyMaskIfExists ( height , width , ignore_values = None , ignore_channels = None , always_apply = False , p = 1.0 ) [view source on GitHub] \u00b6 Crop area with mask if mask is non-empty, else make random crop. Parameters: Name Type Description height int vertical size of crop in pixels width int horizontal size of crop in pixels ignore_values list of int values to ignore in mask, 0 values are always ignored (e.g. if background value is 5 set ignore_values=[5] to ignore) ignore_channels list of int channels to ignore in mask (e.g. if background is a first channel set ignore_channels=[0] to ignore) p float probability of applying the transform. Default: 1.0. Targets: image, mask, bboxes, keypoints Image types: uint8, float32 class albumentations.augmentations.crops.transforms.RandomCrop ( height , width , always_apply = False , p = 1.0 ) [view source on GitHub] \u00b6 Crop a random part of the input. Parameters: Name Type Description height int height of the crop. width int width of the crop. p float probability of applying the transform. Default: 1. Targets: image, mask, bboxes, keypoints Image types: uint8, float32 class albumentations.augmentations.crops.transforms.RandomCropNearBBox ( max_part_shift = 0.3 , always_apply = False , p = 1.0 ) [view source on GitHub] \u00b6 Crop bbox from image with random shift by x,y coordinates Parameters: Name Type Description max_part_shift float float value in (0.0, 1.0) range. Default 0.3 p float probability of applying the transform. Default: 1. Targets: image, mask, bboxes, keypoints Image types: uint8, float32 class albumentations.augmentations.crops.transforms.RandomResizedCrop ( height , width , scale = ( 0.08 , 1.0 ), ratio = ( 0.75 , 1.3333333333333333 ), interpolation = 1 , always_apply = False , p = 1.0 ) [view source on GitHub] \u00b6 Torchvision's variant of crop a random part of the input and rescale it to some size. Parameters: Name Type Description height int height after crop and resize. width int width after crop and resize. scale [float, float] range of size of the origin size cropped ratio [float, float] range of aspect ratio of the origin aspect ratio cropped interpolation OpenCV flag flag that is used to specify the interpolation algorithm. Should be one of: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4. Default: cv2.INTER_LINEAR. p float probability of applying the transform. Default: 1. Targets: image, mask, bboxes, keypoints Image types: uint8, float32 class albumentations.augmentations.crops.transforms.RandomSizedBBoxSafeCrop ( height , width , erosion_rate = 0.0 , interpolation = 1 , always_apply = False , p = 1.0 ) [view source on GitHub] \u00b6 Crop a random part of the input and rescale it to some size without loss of bboxes. Parameters: Name Type Description height int height after crop and resize. width int width after crop and resize. erosion_rate float erosion rate applied on input image height before crop. interpolation OpenCV flag flag that is used to specify the interpolation algorithm. Should be one of: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4. Default: cv2.INTER_LINEAR. p float probability of applying the transform. Default: 1. Targets: image, mask, bboxes Image types: uint8, float32 class albumentations.augmentations.crops.transforms.RandomSizedCrop ( min_max_height , height , width , w2h_ratio = 1.0 , interpolation = 1 , always_apply = False , p = 1.0 ) [view source on GitHub] \u00b6 Crop a random part of the input and rescale it to some size. Parameters: Name Type Description min_max_height [int, int] crop size limits. height int height after crop and resize. width int width after crop and resize. w2h_ratio float aspect ratio of crop. interpolation OpenCV flag flag that is used to specify the interpolation algorithm. Should be one of: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4. Default: cv2.INTER_LINEAR. p float probability of applying the transform. Default: 1. Targets: image, mask, bboxes, keypoints Image types: uint8, float32","title":"Crop transforms (augmentations.crops.transforms)"},{"location":"api_reference/augmentations/crops/transforms/#crop-transforms-augmentationscropstransforms","text":"","title":"Crop transforms (augmentations.crops.transforms)"},{"location":"api_reference/augmentations/crops/transforms/#albumentations.augmentations.crops.transforms","text":"","title":"albumentations.augmentations.crops.transforms"},{"location":"api_reference/augmentations/crops/transforms/#albumentations.augmentations.crops.transforms.CenterCrop","text":"Crop the central part of the input. Parameters: Name Type Description height int height of the crop. width int width of the crop. p float probability of applying the transform. Default: 1. Targets: image, mask, bboxes, keypoints Image types: uint8, float32 Note: It is recommended to use uint8 images as input. Otherwise the operation will require internal conversion float32 -> uint8 -> float32 that causes worse performance.","title":"CenterCrop"},{"location":"api_reference/augmentations/crops/transforms/#albumentations.augmentations.crops.transforms.Crop","text":"Crop region from image. Parameters: Name Type Description x_min int Minimum upper left x coordinate. y_min int Minimum upper left y coordinate. x_max int Maximum lower right x coordinate. y_max int Maximum lower right y coordinate. Targets: image, mask, bboxes, keypoints Image types: uint8, float32","title":"Crop"},{"location":"api_reference/augmentations/crops/transforms/#albumentations.augmentations.crops.transforms.CropAndPad","text":"Crop and pad images by pixel amounts or fractions of image sizes. Cropping removes pixels at the sides (i.e. extracts a subimage from a given full image). Padding adds pixels to the sides (e.g. black pixels). This transformation will never crop images below a height or width of 1 . Note: This transformation automatically resizes images back to their original size. To deactivate this, add the parameter keep_size=False . Parameters: Name Type Description px int or tuple The number of pixels to crop (negative values) or pad (positive values) on each side of the image. Either this or the parameter percent may be set, not both at the same time. * If None , then pixel-based cropping/padding will not be used. * If int , then that exact number of pixels will always be cropped/padded. * If a tuple of two int s with values a and b , then each side will be cropped/padded by a random amount sampled uniformly per image and side from the interval [a, b] . If however sample_independently is set to False , only one value will be sampled per image and used for all sides. * If a tuple of four entries, then the entries represent top, right, bottom, left. Each entry may be a single int (always crop/pad by exactly that value), a tuple of two int s a and b (crop/pad by an amount within [a, b] ), a list of int s (crop/pad by a random value that is contained in the list ). percent float or tuple The number of pixels to crop (negative values) or pad (positive values) on each side of the image given as a fraction of the image height/width. E.g. if this is set to -0.1 , the transformation will always crop away 10% of the image's height at both the top and the bottom (both 10% each), as well as 10% of the width at the right and left. Expected value range is (-1.0, inf) . Either this or the parameter px may be set, not both at the same time. * If None , then fraction-based cropping/padding will not be used. * If float , then that fraction will always be cropped/padded. * If a tuple of two float s with values a and b , then each side will be cropped/padded by a random fraction sampled uniformly per image and side from the interval [a, b] . If however sample_independently is set to False , only one value will be sampled per image and used for all sides. * If a tuple of four entries, then the entries represent top, right, bottom, left. Each entry may be a single float (always crop/pad by exactly that percent value), a tuple of two float s a and b (crop/pad by a fraction from [a, b] ), a list of float s (crop/pad by a random value that is contained in the list). pad_mode int OpenCV border mode. pad_cval number, Sequence[number] The constant value to use if the pad mode is BORDER_CONSTANT . * If number , then that value will be used. * If a tuple of two number s and at least one of them is a float , then a random number will be uniformly sampled per image from the continuous interval [a, b] and used as the value. If both number s are int s, the interval is discrete. * If a list of number , then a random value will be chosen from the elements of the list and used as the value. pad_cval_mask number, Sequence[number] Same as pad_cval but only for masks. keep_size bool After cropping and padding, the result image will usually have a different height/width compared to the original input image. If this parameter is set to True , then the cropped/padded image will be resized to the input image's size, i.e. the output shape is always identical to the input shape. sample_independently bool If False and the values for px / percent result in exactly one probability distribution for all image sides, only one single value will be sampled from that probability distribution and used for all sides. I.e. the crop/pad amount then is the same for all sides. If True , four values will be sampled independently, one per side. interpolation OpenCV flag flag that is used to specify the interpolation algorithm. Should be one of: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4. Default: cv2.INTER_LINEAR. Targets: image, mask, bboxes, keypoints Image types: any","title":"CropAndPad"},{"location":"api_reference/augmentations/crops/transforms/#albumentations.augmentations.crops.transforms.CropNonEmptyMaskIfExists","text":"Crop area with mask if mask is non-empty, else make random crop. Parameters: Name Type Description height int vertical size of crop in pixels width int horizontal size of crop in pixels ignore_values list of int values to ignore in mask, 0 values are always ignored (e.g. if background value is 5 set ignore_values=[5] to ignore) ignore_channels list of int channels to ignore in mask (e.g. if background is a first channel set ignore_channels=[0] to ignore) p float probability of applying the transform. Default: 1.0. Targets: image, mask, bboxes, keypoints Image types: uint8, float32","title":"CropNonEmptyMaskIfExists"},{"location":"api_reference/augmentations/crops/transforms/#albumentations.augmentations.crops.transforms.RandomCrop","text":"Crop a random part of the input. Parameters: Name Type Description height int height of the crop. width int width of the crop. p float probability of applying the transform. Default: 1. Targets: image, mask, bboxes, keypoints Image types: uint8, float32","title":"RandomCrop"},{"location":"api_reference/augmentations/crops/transforms/#albumentations.augmentations.crops.transforms.RandomCropNearBBox","text":"Crop bbox from image with random shift by x,y coordinates Parameters: Name Type Description max_part_shift float float value in (0.0, 1.0) range. Default 0.3 p float probability of applying the transform. Default: 1. Targets: image, mask, bboxes, keypoints Image types: uint8, float32","title":"RandomCropNearBBox"},{"location":"api_reference/augmentations/crops/transforms/#albumentations.augmentations.crops.transforms.RandomResizedCrop","text":"Torchvision's variant of crop a random part of the input and rescale it to some size. Parameters: Name Type Description height int height after crop and resize. width int width after crop and resize. scale [float, float] range of size of the origin size cropped ratio [float, float] range of aspect ratio of the origin aspect ratio cropped interpolation OpenCV flag flag that is used to specify the interpolation algorithm. Should be one of: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4. Default: cv2.INTER_LINEAR. p float probability of applying the transform. Default: 1. Targets: image, mask, bboxes, keypoints Image types: uint8, float32","title":"RandomResizedCrop"},{"location":"api_reference/augmentations/crops/transforms/#albumentations.augmentations.crops.transforms.RandomSizedBBoxSafeCrop","text":"Crop a random part of the input and rescale it to some size without loss of bboxes. Parameters: Name Type Description height int height after crop and resize. width int width after crop and resize. erosion_rate float erosion rate applied on input image height before crop. interpolation OpenCV flag flag that is used to specify the interpolation algorithm. Should be one of: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4. Default: cv2.INTER_LINEAR. p float probability of applying the transform. Default: 1. Targets: image, mask, bboxes Image types: uint8, float32","title":"RandomSizedBBoxSafeCrop"},{"location":"api_reference/augmentations/crops/transforms/#albumentations.augmentations.crops.transforms.RandomSizedCrop","text":"Crop a random part of the input and rescale it to some size. Parameters: Name Type Description min_max_height [int, int] crop size limits. height int height after crop and resize. width int width after crop and resize. w2h_ratio float aspect ratio of crop. interpolation OpenCV flag flag that is used to specify the interpolation algorithm. Should be one of: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4. Default: cv2.INTER_LINEAR. p float probability of applying the transform. Default: 1. Targets: image, mask, bboxes, keypoints Image types: uint8, float32","title":"RandomSizedCrop"},{"location":"api_reference/augmentations/geometric/","text":"Geometric functional transforms (albumentations.augmentations.geometric.functional) Resizing transforms (augmentations.geometric.resize) Rotation transforms (augmentations.geometric.functional) Geometric transforms (augmentations.geometric.transforms)","title":"Index"},{"location":"api_reference/augmentations/geometric/functional/","text":"Geometric functional transforms (augmentations.geometric.functional) \u00b6 \u00b6 def albumentations.augmentations.geometric.functional.bbox_rot90 ( bbox , factor , rows , cols ) [view source on GitHub] \u00b6 Rotates a bounding box by 90 degrees CCW (see np.rot90) Parameters: Name Type Description bbox tuple A bounding box tuple (x_min, y_min, x_max, y_max). factor int Number of CCW rotations. Must be in set {0, 1, 2, 3} See np.rot90. rows int Image rows. cols int Image cols. Returns: Type Description tuple A bounding box tuple (x_min, y_min, x_max, y_max). def albumentations.augmentations.geometric.functional.bbox_rotate ( bbox , angle , rows , cols ) [view source on GitHub] \u00b6 Rotates a bounding box by angle degrees. Parameters: Name Type Description bbox tuple A bounding box (x_min, y_min, x_max, y_max) . angle int Angle of rotation in degrees. rows int Image rows. cols int Image cols. Returns: Type Description A bounding box (x_min, y_min, x_max, y_max) . def albumentations.augmentations.geometric.functional.elastic_transform ( img , alpha , sigma , alpha_affine , interpolation = 1 , border_mode = 4 , value = None , random_state = None , approximate = False ) [view source on GitHub] \u00b6 Elastic deformation of images as described in [Simard2003]_ (with modifications). Based on https://gist.github.com/erniejunior/601cdf56d2b424757de5 .. [Simard2003] Simard, Steinkraus and Platt, \"Best Practices for Convolutional Neural Networks applied to Visual Document Analysis\", in Proc. of the International Conference on Document Analysis and Recognition, 2003. def albumentations.augmentations.geometric.functional.keypoint_rot90 ( keypoint , factor , rows , cols , ** params ) [view source on GitHub] \u00b6 Rotates a keypoint by 90 degrees CCW (see np.rot90) Parameters: Name Type Description keypoint tuple A keypoint (x, y, angle, scale) . factor int Number of CCW rotations. Must be in range [0;3] See np.rot90. rows int Image height. cols int Image width. Returns: Type Description tuple A keypoint (x, y, angle, scale) . Exceptions: Type Description ValueError if factor not in set {0, 1, 2, 3} def albumentations.augmentations.geometric.functional.keypoint_rotate ( keypoint , angle , rows , cols , ** params ) [view source on GitHub] \u00b6 Rotate a keypoint by angle. Parameters: Name Type Description keypoint tuple A keypoint (x, y, angle, scale) . angle float Rotation angle. rows int Image height. cols int Image width. Returns: Type Description tuple A keypoint (x, y, angle, scale) . def albumentations.augmentations.geometric.functional.keypoint_scale ( keypoint , scale_x , scale_y ) [view source on GitHub] \u00b6 Scales a keypoint by scale_x and scale_y. Parameters: Name Type Description keypoint Sequence[float] A keypoint (x, y, angle, scale) . scale_x float Scale coefficient x-axis. scale_y float Scale coefficient y-axis. Returns: Type Description A keypoint (x, y, angle, scale) . def albumentations.augmentations.geometric.functional.py3round ( number ) [view source on GitHub] \u00b6 Unified rounding in all python versions.","title":"Geometric functional transforms (augmentations.geometric.functional)"},{"location":"api_reference/augmentations/geometric/functional/#geometric-functional-transforms-augmentationsgeometricfunctional","text":"","title":"Geometric functional transforms (augmentations.geometric.functional)"},{"location":"api_reference/augmentations/geometric/functional/#albumentations.augmentations.geometric.functional","text":"","title":"albumentations.augmentations.geometric.functional"},{"location":"api_reference/augmentations/geometric/functional/#albumentations.augmentations.geometric.functional.bbox_rot90","text":"Rotates a bounding box by 90 degrees CCW (see np.rot90) Parameters: Name Type Description bbox tuple A bounding box tuple (x_min, y_min, x_max, y_max). factor int Number of CCW rotations. Must be in set {0, 1, 2, 3} See np.rot90. rows int Image rows. cols int Image cols. Returns: Type Description tuple A bounding box tuple (x_min, y_min, x_max, y_max).","title":"bbox_rot90()"},{"location":"api_reference/augmentations/geometric/functional/#albumentations.augmentations.geometric.functional.bbox_rotate","text":"Rotates a bounding box by angle degrees. Parameters: Name Type Description bbox tuple A bounding box (x_min, y_min, x_max, y_max) . angle int Angle of rotation in degrees. rows int Image rows. cols int Image cols. Returns: Type Description A bounding box (x_min, y_min, x_max, y_max) .","title":"bbox_rotate()"},{"location":"api_reference/augmentations/geometric/functional/#albumentations.augmentations.geometric.functional.elastic_transform","text":"Elastic deformation of images as described in [Simard2003]_ (with modifications). Based on https://gist.github.com/erniejunior/601cdf56d2b424757de5 .. [Simard2003] Simard, Steinkraus and Platt, \"Best Practices for Convolutional Neural Networks applied to Visual Document Analysis\", in Proc. of the International Conference on Document Analysis and Recognition, 2003.","title":"elastic_transform()"},{"location":"api_reference/augmentations/geometric/functional/#albumentations.augmentations.geometric.functional.keypoint_rot90","text":"Rotates a keypoint by 90 degrees CCW (see np.rot90) Parameters: Name Type Description keypoint tuple A keypoint (x, y, angle, scale) . factor int Number of CCW rotations. Must be in range [0;3] See np.rot90. rows int Image height. cols int Image width. Returns: Type Description tuple A keypoint (x, y, angle, scale) . Exceptions: Type Description ValueError if factor not in set {0, 1, 2, 3}","title":"keypoint_rot90()"},{"location":"api_reference/augmentations/geometric/functional/#albumentations.augmentations.geometric.functional.keypoint_rotate","text":"Rotate a keypoint by angle. Parameters: Name Type Description keypoint tuple A keypoint (x, y, angle, scale) . angle float Rotation angle. rows int Image height. cols int Image width. Returns: Type Description tuple A keypoint (x, y, angle, scale) .","title":"keypoint_rotate()"},{"location":"api_reference/augmentations/geometric/functional/#albumentations.augmentations.geometric.functional.keypoint_scale","text":"Scales a keypoint by scale_x and scale_y. Parameters: Name Type Description keypoint Sequence[float] A keypoint (x, y, angle, scale) . scale_x float Scale coefficient x-axis. scale_y float Scale coefficient y-axis. Returns: Type Description A keypoint (x, y, angle, scale) .","title":"keypoint_scale()"},{"location":"api_reference/augmentations/geometric/functional/#albumentations.augmentations.geometric.functional.py3round","text":"Unified rounding in all python versions.","title":"py3round()"},{"location":"api_reference/augmentations/geometric/resize/","text":"Resizing transforms (augmentations.geometric.resize) \u00b6 \u00b6 class albumentations.augmentations.geometric.resize.LongestMaxSize ( max_size = 1024 , interpolation = 1 , always_apply = False , p = 1 ) [view source on GitHub] \u00b6 Rescale an image so that maximum side is equal to max_size, keeping the aspect ratio of the initial image. Parameters: Name Type Description max_size int maximum size of the image after the transformation. interpolation OpenCV flag interpolation method. Default: cv2.INTER_LINEAR. p float probability of applying the transform. Default: 1. Targets: image, mask, bboxes, keypoints Image types: uint8, float32 class albumentations.augmentations.geometric.resize.RandomScale ( scale_limit = 0.1 , interpolation = 1 , always_apply = False , p = 0.5 ) [view source on GitHub] \u00b6 Randomly resize the input. Output image size is different from the input image size. Parameters: Name Type Description scale_limit [float, float] or float scaling factor range. If scale_limit is a single float value, the range will be (1 - scale_limit, 1 + scale_limit). Default: (0.9, 1.1). interpolation OpenCV flag flag that is used to specify the interpolation algorithm. Should be one of: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4. Default: cv2.INTER_LINEAR. p float probability of applying the transform. Default: 0.5. Targets: image, mask, bboxes, keypoints Image types: uint8, float32 class albumentations.augmentations.geometric.resize.Resize ( height , width , interpolation = 1 , always_apply = False , p = 1 ) [view source on GitHub] \u00b6 Resize the input to the given height and width. Parameters: Name Type Description height int desired height of the output. width int desired width of the output. interpolation OpenCV flag flag that is used to specify the interpolation algorithm. Should be one of: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4. Default: cv2.INTER_LINEAR. p float probability of applying the transform. Default: 1. Targets: image, mask, bboxes, keypoints Image types: uint8, float32 class albumentations.augmentations.geometric.resize.SmallestMaxSize ( max_size = 1024 , interpolation = 1 , always_apply = False , p = 1 ) [view source on GitHub] \u00b6 Rescale an image so that minimum side is equal to max_size, keeping the aspect ratio of the initial image. Parameters: Name Type Description max_size int maximum size of smallest side of the image after the transformation. interpolation OpenCV flag interpolation method. Default: cv2.INTER_LINEAR. p float probability of applying the transform. Default: 1. Targets: image, mask, bboxes, keypoints Image types: uint8, float32","title":"Resizing transforms (augmentations.geometric.resize)"},{"location":"api_reference/augmentations/geometric/resize/#resizing-transforms-augmentationsgeometricresize","text":"","title":"Resizing transforms (augmentations.geometric.resize)"},{"location":"api_reference/augmentations/geometric/resize/#albumentations.augmentations.geometric.resize","text":"","title":"albumentations.augmentations.geometric.resize"},{"location":"api_reference/augmentations/geometric/resize/#albumentations.augmentations.geometric.resize.LongestMaxSize","text":"Rescale an image so that maximum side is equal to max_size, keeping the aspect ratio of the initial image. Parameters: Name Type Description max_size int maximum size of the image after the transformation. interpolation OpenCV flag interpolation method. Default: cv2.INTER_LINEAR. p float probability of applying the transform. Default: 1. Targets: image, mask, bboxes, keypoints Image types: uint8, float32","title":"LongestMaxSize"},{"location":"api_reference/augmentations/geometric/resize/#albumentations.augmentations.geometric.resize.RandomScale","text":"Randomly resize the input. Output image size is different from the input image size. Parameters: Name Type Description scale_limit [float, float] or float scaling factor range. If scale_limit is a single float value, the range will be (1 - scale_limit, 1 + scale_limit). Default: (0.9, 1.1). interpolation OpenCV flag flag that is used to specify the interpolation algorithm. Should be one of: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4. Default: cv2.INTER_LINEAR. p float probability of applying the transform. Default: 0.5. Targets: image, mask, bboxes, keypoints Image types: uint8, float32","title":"RandomScale"},{"location":"api_reference/augmentations/geometric/resize/#albumentations.augmentations.geometric.resize.Resize","text":"Resize the input to the given height and width. Parameters: Name Type Description height int desired height of the output. width int desired width of the output. interpolation OpenCV flag flag that is used to specify the interpolation algorithm. Should be one of: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4. Default: cv2.INTER_LINEAR. p float probability of applying the transform. Default: 1. Targets: image, mask, bboxes, keypoints Image types: uint8, float32","title":"Resize"},{"location":"api_reference/augmentations/geometric/resize/#albumentations.augmentations.geometric.resize.SmallestMaxSize","text":"Rescale an image so that minimum side is equal to max_size, keeping the aspect ratio of the initial image. Parameters: Name Type Description max_size int maximum size of smallest side of the image after the transformation. interpolation OpenCV flag interpolation method. Default: cv2.INTER_LINEAR. p float probability of applying the transform. Default: 1. Targets: image, mask, bboxes, keypoints Image types: uint8, float32","title":"SmallestMaxSize"},{"location":"api_reference/augmentations/geometric/rotate/","text":"Rotation transforms (augmentations.geometric.functional) \u00b6 \u00b6 class albumentations.augmentations.geometric.rotate.RandomRotate90 [view source on GitHub] \u00b6 Randomly rotate the input by 90 degrees zero or more times. Parameters: Name Type Description p float probability of applying the transform. Default: 0.5. Targets: image, mask, bboxes, keypoints Image types: uint8, float32 albumentations.augmentations.geometric.rotate.RandomRotate90.apply ( self , img , factor = 0 , ** params ) \u00b6 Parameters: Name Type Description factor int number of times the input will be rotated by 90 degrees. class albumentations.augmentations.geometric.rotate.Rotate ( limit = 90 , interpolation = 1 , border_mode = 4 , value = None , mask_value = None , always_apply = False , p = 0.5 ) [view source on GitHub] \u00b6 Rotate the input by an angle selected randomly from the uniform distribution. Parameters: Name Type Description limit [int, int] or int range from which a random angle is picked. If limit is a single int an angle is picked from (-limit, limit). Default: (-90, 90) interpolation OpenCV flag flag that is used to specify the interpolation algorithm. Should be one of: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4. Default: cv2.INTER_LINEAR. border_mode OpenCV flag flag that is used to specify the pixel extrapolation method. Should be one of: cv2.BORDER_CONSTANT, cv2.BORDER_REPLICATE, cv2.BORDER_REFLECT, cv2.BORDER_WRAP, cv2.BORDER_REFLECT_101. Default: cv2.BORDER_REFLECT_101 value int, float, list of ints, list of float padding value if border_mode is cv2.BORDER_CONSTANT. mask_value int, float, list of ints, list of float padding value if border_mode is cv2.BORDER_CONSTANT applied for masks. p float probability of applying the transform. Default: 0.5. Targets: image, mask, bboxes, keypoints Image types: uint8, float32","title":"Rotation transforms (augmentations.geometric.functional)"},{"location":"api_reference/augmentations/geometric/rotate/#rotation-transforms-augmentationsgeometricfunctional","text":"","title":"Rotation transforms (augmentations.geometric.functional)"},{"location":"api_reference/augmentations/geometric/rotate/#albumentations.augmentations.geometric.rotate","text":"","title":"albumentations.augmentations.geometric.rotate"},{"location":"api_reference/augmentations/geometric/rotate/#albumentations.augmentations.geometric.rotate.RandomRotate90","text":"Randomly rotate the input by 90 degrees zero or more times. Parameters: Name Type Description p float probability of applying the transform. Default: 0.5. Targets: image, mask, bboxes, keypoints Image types: uint8, float32","title":"RandomRotate90"},{"location":"api_reference/augmentations/geometric/rotate/#albumentations.augmentations.geometric.rotate.RandomRotate90.apply","text":"Parameters: Name Type Description factor int number of times the input will be rotated by 90 degrees.","title":"apply()"},{"location":"api_reference/augmentations/geometric/rotate/#albumentations.augmentations.geometric.rotate.Rotate","text":"Rotate the input by an angle selected randomly from the uniform distribution. Parameters: Name Type Description limit [int, int] or int range from which a random angle is picked. If limit is a single int an angle is picked from (-limit, limit). Default: (-90, 90) interpolation OpenCV flag flag that is used to specify the interpolation algorithm. Should be one of: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4. Default: cv2.INTER_LINEAR. border_mode OpenCV flag flag that is used to specify the pixel extrapolation method. Should be one of: cv2.BORDER_CONSTANT, cv2.BORDER_REPLICATE, cv2.BORDER_REFLECT, cv2.BORDER_WRAP, cv2.BORDER_REFLECT_101. Default: cv2.BORDER_REFLECT_101 value int, float, list of ints, list of float padding value if border_mode is cv2.BORDER_CONSTANT. mask_value int, float, list of ints, list of float padding value if border_mode is cv2.BORDER_CONSTANT applied for masks. p float probability of applying the transform. Default: 0.5. Targets: image, mask, bboxes, keypoints Image types: uint8, float32","title":"Rotate"},{"location":"api_reference/augmentations/geometric/transforms/","text":"Geometric transforms (augmentations.geometric.transforms) \u00b6 \u00b6 class albumentations.augmentations.geometric.transforms.ElasticTransform ( alpha = 1 , sigma = 50 , alpha_affine = 50 , interpolation = 1 , border_mode = 4 , value = None , mask_value = None , always_apply = False , approximate = False , p = 0.5 ) [view source on GitHub] \u00b6 Elastic deformation of images as described in [Simard2003]_ (with modifications). Based on https://gist.github.com/erniejunior/601cdf56d2b424757de5 .. [Simard2003] Simard, Steinkraus and Platt, \"Best Practices for Convolutional Neural Networks applied to Visual Document Analysis\", in Proc. of the International Conference on Document Analysis and Recognition, 2003. Parameters: Name Type Description alpha float sigma float Gaussian filter parameter. alpha_affine float The range will be (-alpha_affine, alpha_affine) interpolation OpenCV flag flag that is used to specify the interpolation algorithm. Should be one of: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4. Default: cv2.INTER_LINEAR. border_mode OpenCV flag flag that is used to specify the pixel extrapolation method. Should be one of: cv2.BORDER_CONSTANT, cv2.BORDER_REPLICATE, cv2.BORDER_REFLECT, cv2.BORDER_WRAP, cv2.BORDER_REFLECT_101. Default: cv2.BORDER_REFLECT_101 value int, float, list of ints, list of float padding value if border_mode is cv2.BORDER_CONSTANT. mask_value int, float, list of ints, list of float padding value if border_mode is cv2.BORDER_CONSTANT applied for masks. approximate boolean Whether to smooth displacement map with fixed kernel size. Enabling this option gives ~2X speedup on large images. Targets: image, mask Image types: uint8, float32 class albumentations.augmentations.geometric.transforms.Perspective ( scale = ( 0.05 , 0.1 ), keep_size = True , pad_mode = 0 , pad_val = 0 , mask_pad_val = 0 , fit_output = False , interpolation = 1 , always_apply = False , p = 0.5 ) [view source on GitHub] \u00b6 Perform a random four point perspective transform of the input. Parameters: Name Type Description scale float or [float, float] standard deviation of the normal distributions. These are used to sample the random distances of the subimage's corners from the full image's corners. If scale is a single float value, the range will be (0, scale). Default: (0.05, 0.1). keep_size bool Whether to resize image\u2019s back to their original size after applying the perspective transform. If set to False, the resulting images may end up having different shapes and will always be a list, never an array. Default: True pad_mode OpenCV flag OpenCV border mode. pad_val int, float, list of int, list of float padding value if border_mode is cv2.BORDER_CONSTANT. Default: 0 mask_pad_val int, float, list of int, list of float padding value for mask if border_mode is cv2.BORDER_CONSTANT. Default: 0 fit_output bool If True, the image plane size and position will be adjusted to still capture the whole image after perspective transformation. (Followed by image resizing if keep_size is set to True.) Otherwise, parts of the transformed image may be outside of the image plane. This setting should not be set to True when using large scale values as it could lead to very large images. Default: False p float probability of applying the transform. Default: 0.5. Targets: image, mask, keypoints, bboxes Image types: uint8, float32 class albumentations.augmentations.geometric.transforms.ShiftScaleRotate ( shift_limit = 0.0625 , scale_limit = 0.1 , rotate_limit = 45 , interpolation = 1 , border_mode = 4 , value = None , mask_value = None , shift_limit_x = None , shift_limit_y = None , always_apply = False , p = 0.5 ) [view source on GitHub] \u00b6 Randomly apply affine transforms: translate, scale and rotate the input. Parameters: Name Type Description shift_limit [float, float] or float shift factor range for both height and width. If shift_limit is a single float value, the range will be (-shift_limit, shift_limit). Absolute values for lower and upper bounds should lie in range [0, 1]. Default: (-0.0625, 0.0625). scale_limit [float, float] or float scaling factor range. If scale_limit is a single float value, the range will be (-scale_limit, scale_limit). Default: (-0.1, 0.1). rotate_limit [int, int] or int rotation range. If rotate_limit is a single int value, the range will be (-rotate_limit, rotate_limit). Default: (-45, 45). interpolation OpenCV flag flag that is used to specify the interpolation algorithm. Should be one of: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4. Default: cv2.INTER_LINEAR. border_mode OpenCV flag flag that is used to specify the pixel extrapolation method. Should be one of: cv2.BORDER_CONSTANT, cv2.BORDER_REPLICATE, cv2.BORDER_REFLECT, cv2.BORDER_WRAP, cv2.BORDER_REFLECT_101. Default: cv2.BORDER_REFLECT_101 value int, float, list of int, list of float padding value if border_mode is cv2.BORDER_CONSTANT. mask_value int, float, list of int, list of float padding value if border_mode is cv2.BORDER_CONSTANT applied for masks. shift_limit_x [float, float] or float shift factor range for width. If it is set then this value instead of shift_limit will be used for shifting width. If shift_limit_x is a single float value, the range will be (-shift_limit_x, shift_limit_x). Absolute values for lower and upper bounds should lie in the range [0, 1]. Default: None. shift_limit_y [float, float] or float shift factor range for height. If it is set then this value instead of shift_limit will be used for shifting height. If shift_limit_y is a single float value, the range will be (-shift_limit_y, shift_limit_y). Absolute values for lower and upper bounds should lie in the range [0, 1]. Default: None. p float probability of applying the transform. Default: 0.5. Targets: image, mask, keypoints Image types: uint8, float32","title":"Geometric transforms (augmentations.geometric.transforms)"},{"location":"api_reference/augmentations/geometric/transforms/#geometric-transforms-augmentationsgeometrictransforms","text":"","title":"Geometric transforms (augmentations.geometric.transforms)"},{"location":"api_reference/augmentations/geometric/transforms/#albumentations.augmentations.geometric.transforms","text":"","title":"albumentations.augmentations.geometric.transforms"},{"location":"api_reference/augmentations/geometric/transforms/#albumentations.augmentations.geometric.transforms.ElasticTransform","text":"Elastic deformation of images as described in [Simard2003]_ (with modifications). Based on https://gist.github.com/erniejunior/601cdf56d2b424757de5 .. [Simard2003] Simard, Steinkraus and Platt, \"Best Practices for Convolutional Neural Networks applied to Visual Document Analysis\", in Proc. of the International Conference on Document Analysis and Recognition, 2003. Parameters: Name Type Description alpha float sigma float Gaussian filter parameter. alpha_affine float The range will be (-alpha_affine, alpha_affine) interpolation OpenCV flag flag that is used to specify the interpolation algorithm. Should be one of: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4. Default: cv2.INTER_LINEAR. border_mode OpenCV flag flag that is used to specify the pixel extrapolation method. Should be one of: cv2.BORDER_CONSTANT, cv2.BORDER_REPLICATE, cv2.BORDER_REFLECT, cv2.BORDER_WRAP, cv2.BORDER_REFLECT_101. Default: cv2.BORDER_REFLECT_101 value int, float, list of ints, list of float padding value if border_mode is cv2.BORDER_CONSTANT. mask_value int, float, list of ints, list of float padding value if border_mode is cv2.BORDER_CONSTANT applied for masks. approximate boolean Whether to smooth displacement map with fixed kernel size. Enabling this option gives ~2X speedup on large images. Targets: image, mask Image types: uint8, float32","title":"ElasticTransform"},{"location":"api_reference/augmentations/geometric/transforms/#albumentations.augmentations.geometric.transforms.Perspective","text":"Perform a random four point perspective transform of the input. Parameters: Name Type Description scale float or [float, float] standard deviation of the normal distributions. These are used to sample the random distances of the subimage's corners from the full image's corners. If scale is a single float value, the range will be (0, scale). Default: (0.05, 0.1). keep_size bool Whether to resize image\u2019s back to their original size after applying the perspective transform. If set to False, the resulting images may end up having different shapes and will always be a list, never an array. Default: True pad_mode OpenCV flag OpenCV border mode. pad_val int, float, list of int, list of float padding value if border_mode is cv2.BORDER_CONSTANT. Default: 0 mask_pad_val int, float, list of int, list of float padding value for mask if border_mode is cv2.BORDER_CONSTANT. Default: 0 fit_output bool If True, the image plane size and position will be adjusted to still capture the whole image after perspective transformation. (Followed by image resizing if keep_size is set to True.) Otherwise, parts of the transformed image may be outside of the image plane. This setting should not be set to True when using large scale values as it could lead to very large images. Default: False p float probability of applying the transform. Default: 0.5. Targets: image, mask, keypoints, bboxes Image types: uint8, float32","title":"Perspective"},{"location":"api_reference/augmentations/geometric/transforms/#albumentations.augmentations.geometric.transforms.ShiftScaleRotate","text":"Randomly apply affine transforms: translate, scale and rotate the input. Parameters: Name Type Description shift_limit [float, float] or float shift factor range for both height and width. If shift_limit is a single float value, the range will be (-shift_limit, shift_limit). Absolute values for lower and upper bounds should lie in range [0, 1]. Default: (-0.0625, 0.0625). scale_limit [float, float] or float scaling factor range. If scale_limit is a single float value, the range will be (-scale_limit, scale_limit). Default: (-0.1, 0.1). rotate_limit [int, int] or int rotation range. If rotate_limit is a single int value, the range will be (-rotate_limit, rotate_limit). Default: (-45, 45). interpolation OpenCV flag flag that is used to specify the interpolation algorithm. Should be one of: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4. Default: cv2.INTER_LINEAR. border_mode OpenCV flag flag that is used to specify the pixel extrapolation method. Should be one of: cv2.BORDER_CONSTANT, cv2.BORDER_REPLICATE, cv2.BORDER_REFLECT, cv2.BORDER_WRAP, cv2.BORDER_REFLECT_101. Default: cv2.BORDER_REFLECT_101 value int, float, list of int, list of float padding value if border_mode is cv2.BORDER_CONSTANT. mask_value int, float, list of int, list of float padding value if border_mode is cv2.BORDER_CONSTANT applied for masks. shift_limit_x [float, float] or float shift factor range for width. If it is set then this value instead of shift_limit will be used for shifting width. If shift_limit_x is a single float value, the range will be (-shift_limit_x, shift_limit_x). Absolute values for lower and upper bounds should lie in the range [0, 1]. Default: None. shift_limit_y [float, float] or float shift factor range for height. If it is set then this value instead of shift_limit will be used for shifting height. If shift_limit_y is a single float value, the range will be (-shift_limit_y, shift_limit_y). Absolute values for lower and upper bounds should lie in the range [0, 1]. Default: None. p float probability of applying the transform. Default: 0.5. Targets: image, mask, keypoints Image types: uint8, float32","title":"ShiftScaleRotate"},{"location":"api_reference/core/","text":"Composition API (albumentations.core.composition) Serialization API (albumentations.core.serialization) Transforms Interface (albumentations.core.transforms_interface)","title":"Index"},{"location":"api_reference/core/composition/","text":"Composition API (core.composition) \u00b6 \u00b6 class albumentations.core.composition.BboxParams ( format , label_fields = None , min_area = 0.0 , min_visibility = 0.0 , check_each_transform = True ) [view source on GitHub] \u00b6 Parameters of bounding boxes Parameters: Name Type Description format str format of bounding boxes. Should be 'coco', 'pascal_voc', 'albumentations' or 'yolo'. The coco format [x_min, y_min, width, height] , e.g. [97, 12, 150, 200]. The pascal_voc format [x_min, y_min, x_max, y_max] , e.g. [97, 12, 247, 212]. The albumentations format is like pascal_voc , but normalized, in other words: [x_min, y_min, x_max, y_max] , e.g. [0.2, 0.3, 0.4, 0.5]. The yolo format [x, y, width, height] , e.g. [0.1, 0.2, 0.3, 0.4]; x , y - normalized bbox center; width , height` - normalized bbox width and height. label_fields list list of fields that are joined with boxes, e.g labels. Should be same type as boxes. min_area float minimum area of a bounding box. All bounding boxes whose visible area in pixels is less than this value will be removed. Default: 0.0. min_visibility float minimum fraction of area for a bounding box to remain this box in list. Default: 0.0. check_each_transform bool if True , then bboxes will be checked after each dual transform. Default: True class albumentations.core.composition.Compose ( transforms , bbox_params = None , keypoint_params = None , additional_targets = None , p = 1.0 ) [view source on GitHub] \u00b6 Compose transforms and handle all transformations regarding bounding boxes Parameters: Name Type Description transforms list list of transformations to compose. bbox_params BboxParams Parameters for bounding boxes transforms keypoint_params KeypointParams Parameters for keypoints transforms additional_targets dict Dict with keys - new target name, values - old target name. ex: {'image2': 'image'} p float probability of applying all list of transforms. Default: 1.0. class albumentations.core.composition.KeypointParams ( format , label_fields = None , remove_invisible = True , angle_in_degrees = True , check_each_transform = True ) [view source on GitHub] \u00b6 Parameters of keypoints Parameters: Name Type Description format str format of keypoints. Should be 'xy', 'yx', 'xya', 'xys', 'xyas', 'xysa'. x - X coordinate, y - Y coordinate s - Keypoint scale a - Keypoint orientation in radians or degrees (depending on KeypointParams.angle_in_degrees) label_fields list list of fields that are joined with keypoints, e.g labels. Should be same type as keypoints. remove_invisible bool to remove invisible points after transform or not angle_in_degrees bool angle in degrees or radians in 'xya', 'xyas', 'xysa' keypoints check_each_transform bool if True , then keypoints will be checked after each dual transform. Default: True class albumentations.core.composition.OneOf ( transforms , p = 0.5 ) [view source on GitHub] \u00b6 Select one of transforms to apply. Selected transform will be called with force_apply=True . Transforms probabilities will be normalized to one 1, so in this case transforms probabilities works as weights. Parameters: Name Type Description transforms list list of transformations to compose. p float probability of applying selected transform. Default: 0.5. class albumentations.core.composition.OneOrOther ( first = None , second = None , transforms = None , p = 0.5 ) [view source on GitHub] \u00b6 Select one or another transform to apply. Selected transform will be called with force_apply=True . class albumentations.core.composition.PerChannel ( transforms , channels = None , p = 0.5 ) [view source on GitHub] \u00b6 Apply transformations per-channel Parameters: Name Type Description transforms list list of transformations to compose. channels list channels to apply the transform to. Pass None to apply to all. Default: None (apply to all) p float probability of applying the transform. Default: 0.5. class albumentations.core.composition.Sequential ( transforms , p = 0.5 ) [view source on GitHub] \u00b6 Sequentially applies all transforms to targets. Note: This transform is not intended to be a replacement for Compose . Instead, it should be used inside Compose the same way OneOf or OneOrOther are used. For instance, you can combine OneOf with Sequential to create an augmentation pipeline that contains multiple sequences of augmentations and applies one randomly chose sequence to input data (see the Example section for an example definition of such pipeline). Examples: 1 2 3 4 5 6 7 8 9 10 11 12 13 >>> import albumentations as A >>> transform = A . Compose ([ >>> A . OneOf ([ >>> A . Sequential ([ >>> A . HorizontalFlip ( p = 0.5 ), >>> A . ShiftScaleRotate ( p = 0.5 ), >>> ]), >>> A . Sequential ([ >>> A . VerticalFlip ( p = 0.5 ), >>> A . RandomBrightnessContrast ( p = 0.5 ), >>> ]), >>> ], p = 1 ) >>> ])","title":"Composition API (core.composition)"},{"location":"api_reference/core/composition/#composition-api-corecomposition","text":"","title":"Composition API (core.composition)"},{"location":"api_reference/core/composition/#albumentations.core.composition","text":"","title":"albumentations.core.composition"},{"location":"api_reference/core/composition/#albumentations.core.composition.BboxParams","text":"Parameters of bounding boxes Parameters: Name Type Description format str format of bounding boxes. Should be 'coco', 'pascal_voc', 'albumentations' or 'yolo'. The coco format [x_min, y_min, width, height] , e.g. [97, 12, 150, 200]. The pascal_voc format [x_min, y_min, x_max, y_max] , e.g. [97, 12, 247, 212]. The albumentations format is like pascal_voc , but normalized, in other words: [x_min, y_min, x_max, y_max] , e.g. [0.2, 0.3, 0.4, 0.5]. The yolo format [x, y, width, height] , e.g. [0.1, 0.2, 0.3, 0.4]; x , y - normalized bbox center; width , height` - normalized bbox width and height. label_fields list list of fields that are joined with boxes, e.g labels. Should be same type as boxes. min_area float minimum area of a bounding box. All bounding boxes whose visible area in pixels is less than this value will be removed. Default: 0.0. min_visibility float minimum fraction of area for a bounding box to remain this box in list. Default: 0.0. check_each_transform bool if True , then bboxes will be checked after each dual transform. Default: True","title":"BboxParams"},{"location":"api_reference/core/composition/#albumentations.core.composition.Compose","text":"Compose transforms and handle all transformations regarding bounding boxes Parameters: Name Type Description transforms list list of transformations to compose. bbox_params BboxParams Parameters for bounding boxes transforms keypoint_params KeypointParams Parameters for keypoints transforms additional_targets dict Dict with keys - new target name, values - old target name. ex: {'image2': 'image'} p float probability of applying all list of transforms. Default: 1.0.","title":"Compose"},{"location":"api_reference/core/composition/#albumentations.core.composition.KeypointParams","text":"Parameters of keypoints Parameters: Name Type Description format str format of keypoints. Should be 'xy', 'yx', 'xya', 'xys', 'xyas', 'xysa'. x - X coordinate, y - Y coordinate s - Keypoint scale a - Keypoint orientation in radians or degrees (depending on KeypointParams.angle_in_degrees) label_fields list list of fields that are joined with keypoints, e.g labels. Should be same type as keypoints. remove_invisible bool to remove invisible points after transform or not angle_in_degrees bool angle in degrees or radians in 'xya', 'xyas', 'xysa' keypoints check_each_transform bool if True , then keypoints will be checked after each dual transform. Default: True","title":"KeypointParams"},{"location":"api_reference/core/composition/#albumentations.core.composition.OneOf","text":"Select one of transforms to apply. Selected transform will be called with force_apply=True . Transforms probabilities will be normalized to one 1, so in this case transforms probabilities works as weights. Parameters: Name Type Description transforms list list of transformations to compose. p float probability of applying selected transform. Default: 0.5.","title":"OneOf"},{"location":"api_reference/core/composition/#albumentations.core.composition.OneOrOther","text":"Select one or another transform to apply. Selected transform will be called with force_apply=True .","title":"OneOrOther"},{"location":"api_reference/core/composition/#albumentations.core.composition.PerChannel","text":"Apply transformations per-channel Parameters: Name Type Description transforms list list of transformations to compose. channels list channels to apply the transform to. Pass None to apply to all. Default: None (apply to all) p float probability of applying the transform. Default: 0.5.","title":"PerChannel"},{"location":"api_reference/core/composition/#albumentations.core.composition.Sequential","text":"Sequentially applies all transforms to targets. Note: This transform is not intended to be a replacement for Compose . Instead, it should be used inside Compose the same way OneOf or OneOrOther are used. For instance, you can combine OneOf with Sequential to create an augmentation pipeline that contains multiple sequences of augmentations and applies one randomly chose sequence to input data (see the Example section for an example definition of such pipeline). Examples: 1 2 3 4 5 6 7 8 9 10 11 12 13 >>> import albumentations as A >>> transform = A . Compose ([ >>> A . OneOf ([ >>> A . Sequential ([ >>> A . HorizontalFlip ( p = 0.5 ), >>> A . ShiftScaleRotate ( p = 0.5 ), >>> ]), >>> A . Sequential ([ >>> A . VerticalFlip ( p = 0.5 ), >>> A . RandomBrightnessContrast ( p = 0.5 ), >>> ]), >>> ], p = 1 ) >>> ])","title":"Sequential"},{"location":"api_reference/core/serialization/","text":"Serialization API (core.serialization) \u00b6 \u00b6 class albumentations.core.serialization.SerializableMeta [view source on GitHub] \u00b6 A metaclass that is used to register classes in SERIALIZABLE_REGISTRY so they can be found later while deserializing transformation pipeline using classes full names. albumentations.core.serialization.SerializableMeta.__new__ ( cls , name , bases , class_dict ) special staticmethod \u00b6 Create and return a new object. See help(type) for accurate signature. def albumentations.core.serialization.from_dict ( transform_dict , lambda_transforms = None ) [view source on GitHub] \u00b6 Parameters: Name Type Description transform dict A dictionary with serialized transform pipeline. lambda_transforms dict A dictionary that contains lambda transforms, that is instances of the Lambda class. This dictionary is required when you are restoring a pipeline that contains lambda transforms. Keys in that dictionary should be named same as name arguments in respective lambda transforms from a serialized pipeline. def albumentations.core.serialization.load ( filepath , data_format = 'json' , lambda_transforms = None ) [view source on GitHub] \u00b6 Load a serialized pipeline from a json or yaml file and construct a transform pipeline. Parameters: Name Type Description transform obj Transform to serialize. filepath str Filepath to read from. data_format str Serialization format. Should be either json or 'yaml'. lambda_transforms dict A dictionary that contains lambda transforms, that is instances of the Lambda class. This dictionary is required when you are restoring a pipeline that contains lambda transforms. Keys in that dictionary should be named same as name arguments in respective lambda transforms from a serialized pipeline. def albumentations.core.serialization.register_additional_transforms () [view source on GitHub] \u00b6 Register transforms that are not imported directly into the albumentations module. def albumentations.core.serialization.save ( transform , filepath , data_format = 'json' , on_not_implemented_error = 'raise' ) [view source on GitHub] \u00b6 Take a transform pipeline, serialize it and save a serialized version to a file using either json or yaml format. Parameters: Name Type Description transform obj Transform to serialize. filepath str Filepath to write to. data_format str Serialization format. Should be either json or 'yaml'. on_not_implemented_error str Parameter that describes what to do if a transform doesn't implement the to_dict method. If 'raise' then NotImplementedError is raised, if warn then the exception will be ignored and no transform arguments will be saved. def albumentations.core.serialization.to_dict ( transform , on_not_implemented_error = 'raise' ) [view source on GitHub] \u00b6 Take a transform pipeline and convert it to a serializable representation that uses only standard python data types: dictionaries, lists, strings, integers, and floats. Parameters: Name Type Description transform object A transform that should be serialized. If the transform doesn't implement the to_dict method and on_not_implemented_error equals to 'raise' then NotImplementedError is raised. If on_not_implemented_error equals to 'warn' then NotImplementedError will be ignored but no transform parameters will be serialized.","title":"Serialization API (core.serialization)"},{"location":"api_reference/core/serialization/#serialization-api-coreserialization","text":"","title":"Serialization API (core.serialization)"},{"location":"api_reference/core/serialization/#albumentations.core.serialization","text":"","title":"albumentations.core.serialization"},{"location":"api_reference/core/serialization/#albumentations.core.serialization.SerializableMeta","text":"A metaclass that is used to register classes in SERIALIZABLE_REGISTRY so they can be found later while deserializing transformation pipeline using classes full names.","title":"SerializableMeta"},{"location":"api_reference/core/serialization/#albumentations.core.serialization.SerializableMeta.__new__","text":"Create and return a new object. See help(type) for accurate signature.","title":"__new__()"},{"location":"api_reference/core/serialization/#albumentations.core.serialization.from_dict","text":"Parameters: Name Type Description transform dict A dictionary with serialized transform pipeline. lambda_transforms dict A dictionary that contains lambda transforms, that is instances of the Lambda class. This dictionary is required when you are restoring a pipeline that contains lambda transforms. Keys in that dictionary should be named same as name arguments in respective lambda transforms from a serialized pipeline.","title":"from_dict()"},{"location":"api_reference/core/serialization/#albumentations.core.serialization.load","text":"Load a serialized pipeline from a json or yaml file and construct a transform pipeline. Parameters: Name Type Description transform obj Transform to serialize. filepath str Filepath to read from. data_format str Serialization format. Should be either json or 'yaml'. lambda_transforms dict A dictionary that contains lambda transforms, that is instances of the Lambda class. This dictionary is required when you are restoring a pipeline that contains lambda transforms. Keys in that dictionary should be named same as name arguments in respective lambda transforms from a serialized pipeline.","title":"load()"},{"location":"api_reference/core/serialization/#albumentations.core.serialization.register_additional_transforms","text":"Register transforms that are not imported directly into the albumentations module.","title":"register_additional_transforms()"},{"location":"api_reference/core/serialization/#albumentations.core.serialization.save","text":"Take a transform pipeline, serialize it and save a serialized version to a file using either json or yaml format. Parameters: Name Type Description transform obj Transform to serialize. filepath str Filepath to write to. data_format str Serialization format. Should be either json or 'yaml'. on_not_implemented_error str Parameter that describes what to do if a transform doesn't implement the to_dict method. If 'raise' then NotImplementedError is raised, if warn then the exception will be ignored and no transform arguments will be saved.","title":"save()"},{"location":"api_reference/core/serialization/#albumentations.core.serialization.to_dict","text":"Take a transform pipeline and convert it to a serializable representation that uses only standard python data types: dictionaries, lists, strings, integers, and floats. Parameters: Name Type Description transform object A transform that should be serialized. If the transform doesn't implement the to_dict method and on_not_implemented_error equals to 'raise' then NotImplementedError is raised. If on_not_implemented_error equals to 'warn' then NotImplementedError will be ignored but no transform parameters will be serialized.","title":"to_dict()"},{"location":"api_reference/core/transforms_interface/","text":"Transforms Interface (core.transforms_interface) \u00b6 \u00b6 class albumentations.core.transforms_interface.BasicTransform ( always_apply = False , p = 0.5 ) [view source on GitHub] \u00b6 albumentations.core.transforms_interface.BasicTransform.add_targets ( self , additional_targets ) \u00b6 Add targets to transform them the same way as one of existing targets ex: {'target_image': 'image'} ex: {'obj1_mask': 'mask', 'obj2_mask': 'mask'} by the way you must have at least one object with key 'image' Parameters: Name Type Description additional_targets dict keys - new target name, values - old target name. ex: {'image2': 'image'} class albumentations.core.transforms_interface.DualTransform [view source on GitHub] \u00b6 Transform for segmentation task. class albumentations.core.transforms_interface.ImageOnlyTransform [view source on GitHub] \u00b6 Transform applied to image only. class albumentations.core.transforms_interface.NoOp [view source on GitHub] \u00b6 Does nothing def albumentations.core.transforms_interface.to_tuple ( param , low = None , bias = None ) [view source on GitHub] \u00b6 Convert input argument to min-max tuple Parameters: Name Type Description param scalar, tuple or list of 2+ elements Input value. If value is scalar, return value would be (offset - value, offset + value). If value is tuple, return value would be value + offset (broadcasted). low Second element of tuple can be passed as optional argument bias An offset factor added to each element","title":"Transforms Interface (core.transforms_interface)"},{"location":"api_reference/core/transforms_interface/#transforms-interface-coretransforms_interface","text":"","title":"Transforms Interface (core.transforms_interface)"},{"location":"api_reference/core/transforms_interface/#albumentations.core.transforms_interface","text":"","title":"albumentations.core.transforms_interface"},{"location":"api_reference/core/transforms_interface/#albumentations.core.transforms_interface.BasicTransform","text":"","title":"BasicTransform"},{"location":"api_reference/core/transforms_interface/#albumentations.core.transforms_interface.BasicTransform.add_targets","text":"Add targets to transform them the same way as one of existing targets ex: {'target_image': 'image'} ex: {'obj1_mask': 'mask', 'obj2_mask': 'mask'} by the way you must have at least one object with key 'image' Parameters: Name Type Description additional_targets dict keys - new target name, values - old target name. ex: {'image2': 'image'}","title":"add_targets()"},{"location":"api_reference/core/transforms_interface/#albumentations.core.transforms_interface.DualTransform","text":"Transform for segmentation task.","title":"DualTransform"},{"location":"api_reference/core/transforms_interface/#albumentations.core.transforms_interface.ImageOnlyTransform","text":"Transform applied to image only.","title":"ImageOnlyTransform"},{"location":"api_reference/core/transforms_interface/#albumentations.core.transforms_interface.NoOp","text":"Does nothing","title":"NoOp"},{"location":"api_reference/core/transforms_interface/#albumentations.core.transforms_interface.to_tuple","text":"Convert input argument to min-max tuple Parameters: Name Type Description param scalar, tuple or list of 2+ elements Input value. If value is scalar, return value would be (offset - value, offset + value). If value is tuple, return value would be value + offset (broadcasted). low Second element of tuple can be passed as optional argument bias An offset factor added to each element","title":"to_tuple()"},{"location":"api_reference/imgaug/","text":"Transforms (albumentations.imgaug.transforms)","title":"Index"},{"location":"api_reference/imgaug/transforms/","text":"Transforms (imgaug.transforms) \u00b6 \u00b6 class albumentations.imgaug.transforms.IAAAdditiveGaussianNoise ( loc = 0 , scale = ( 2.5500000000000003 , 12.75 ), per_channel = False , always_apply = False , p = 0.5 ) [view source on GitHub] \u00b6 Add gaussian noise to the input image. Parameters: Name Type Description loc int mean of the normal distribution that generates the noise. Default: 0. scale [float, float] standard deviation of the normal distribution that generates the noise. Default: (0.01 * 255, 0.05 * 255). p float probability of applying the transform. Default: 0.5. Targets: image class albumentations.imgaug.transforms.IAAAffine ( scale = 1.0 , translate_percent = None , translate_px = None , rotate = 0.0 , shear = 0.0 , order = 1 , cval = 0 , mode = 'reflect' , always_apply = False , p = 0.5 ) [view source on GitHub] \u00b6 Place a regular grid of points on the input and randomly move the neighbourhood of these point around via affine transformations. Note: This class introduce interpolation artifacts to mask if it has values other than {0;1} Parameters: Name Type Description p float probability of applying the transform. Default: 0.5. Targets: image, mask class albumentations.imgaug.transforms.IAACropAndPad ( px = None , percent = None , pad_mode = 'constant' , pad_cval = 0 , keep_size = True , always_apply = False , p = 1 ) [view source on GitHub] \u00b6 This augmentation is deprecated. Please use CropAndPad instead. class albumentations.imgaug.transforms.IAAEmboss ( alpha = ( 0.2 , 0.5 ), strength = ( 0.2 , 0.7 ), always_apply = False , p = 0.5 ) [view source on GitHub] \u00b6 Emboss the input image and overlays the result with the original image. This augmentation is deprecated. Please use Emboss instead. Parameters: Name Type Description alpha [float, float] range to choose the visibility of the embossed image. At 0, only the original image is visible,at 1.0 only its embossed version is visible. Default: (0.2, 0.5). strength [float, float] strength range of the embossing. Default: (0.2, 0.7). p float probability of applying the transform. Default: 0.5. Targets: image class albumentations.imgaug.transforms.IAAPerspective ( scale = ( 0.05 , 0.1 ), keep_size = True , always_apply = False , p = 0.5 , ** kwargs ) [view source on GitHub] \u00b6 Perform a random four point perspective transform of the input. This augmentation is deprecated. Please use Perspective instead. Note: This class introduce interpolation artifacts to mask if it has values other than {0;1} Parameters: Name Type Description scale [float, float] standard deviation of the normal distributions. These are used to sample the random distances of the subimage's corners from the full image's corners. Default: (0.05, 0.1). p float probability of applying the transform. Default: 0.5. Targets: image, mask class albumentations.imgaug.transforms.IAAPiecewiseAffine ( scale = ( 0.03 , 0.05 ), nb_rows = 4 , nb_cols = 4 , order = 1 , cval = 0 , mode = 'constant' , always_apply = False , p = 0.5 ) [view source on GitHub] \u00b6 Place a regular grid of points on the input and randomly move the neighbourhood of these point around via affine transformations. Note: This class introduce interpolation artifacts to mask if it has values other than {0;1} Parameters: Name Type Description scale [float, float] factor range that determines how far each point is moved. Default: (0.03, 0.05). nb_rows int number of rows of points that the regular grid should have. Default: 4. nb_cols int number of columns of points that the regular grid should have. Default: 4. p float probability of applying the transform. Default: 0.5. Targets: image, mask class albumentations.imgaug.transforms.IAASharpen ( alpha = ( 0.2 , 0.5 ), lightness = ( 0.5 , 1.0 ), always_apply = False , p = 0.5 ) [view source on GitHub] \u00b6 Sharpen the input image and overlays the result with the original image. This augmentation is deprecated. Please use Sharpen instead Parameters: Name Type Description alpha [float, float] range to choose the visibility of the sharpened image. At 0, only the original image is visible, at 1.0 only its sharpened version is visible. Default: (0.2, 0.5). lightness [float, float] range to choose the lightness of the sharpened image. Default: (0.5, 1.0). p float probability of applying the transform. Default: 0.5. Targets: image class albumentations.imgaug.transforms.IAASuperpixels ( p_replace = 0.1 , n_segments = 100 , always_apply = False , p = 0.5 ) [view source on GitHub] \u00b6 Completely or partially transform the input image to its superpixel representation. Uses skimage's version of the SLIC algorithm. May be slow. Parameters: Name Type Description p_replace float defines the probability of any superpixel area being replaced by the superpixel, i.e. by the average pixel color within its area. Default: 0.1. n_segments int target number of superpixels to generate. Default: 100. p float probability of applying the transform. Default: 0.5. Targets: image","title":"Transforms (imgaug.transforms)"},{"location":"api_reference/imgaug/transforms/#transforms-imgaugtransforms","text":"","title":"Transforms (imgaug.transforms)"},{"location":"api_reference/imgaug/transforms/#albumentations.imgaug.transforms","text":"","title":"albumentations.imgaug.transforms"},{"location":"api_reference/imgaug/transforms/#albumentations.imgaug.transforms.IAAAdditiveGaussianNoise","text":"Add gaussian noise to the input image. Parameters: Name Type Description loc int mean of the normal distribution that generates the noise. Default: 0. scale [float, float] standard deviation of the normal distribution that generates the noise. Default: (0.01 * 255, 0.05 * 255). p float probability of applying the transform. Default: 0.5. Targets: image","title":"IAAAdditiveGaussianNoise"},{"location":"api_reference/imgaug/transforms/#albumentations.imgaug.transforms.IAAAffine","text":"Place a regular grid of points on the input and randomly move the neighbourhood of these point around via affine transformations. Note: This class introduce interpolation artifacts to mask if it has values other than {0;1} Parameters: Name Type Description p float probability of applying the transform. Default: 0.5. Targets: image, mask","title":"IAAAffine"},{"location":"api_reference/imgaug/transforms/#albumentations.imgaug.transforms.IAACropAndPad","text":"This augmentation is deprecated. Please use CropAndPad instead.","title":"IAACropAndPad"},{"location":"api_reference/imgaug/transforms/#albumentations.imgaug.transforms.IAAEmboss","text":"Emboss the input image and overlays the result with the original image. This augmentation is deprecated. Please use Emboss instead. Parameters: Name Type Description alpha [float, float] range to choose the visibility of the embossed image. At 0, only the original image is visible,at 1.0 only its embossed version is visible. Default: (0.2, 0.5). strength [float, float] strength range of the embossing. Default: (0.2, 0.7). p float probability of applying the transform. Default: 0.5. Targets: image","title":"IAAEmboss"},{"location":"api_reference/imgaug/transforms/#albumentations.imgaug.transforms.IAAPerspective","text":"Perform a random four point perspective transform of the input. This augmentation is deprecated. Please use Perspective instead. Note: This class introduce interpolation artifacts to mask if it has values other than {0;1} Parameters: Name Type Description scale [float, float] standard deviation of the normal distributions. These are used to sample the random distances of the subimage's corners from the full image's corners. Default: (0.05, 0.1). p float probability of applying the transform. Default: 0.5. Targets: image, mask","title":"IAAPerspective"},{"location":"api_reference/imgaug/transforms/#albumentations.imgaug.transforms.IAAPiecewiseAffine","text":"Place a regular grid of points on the input and randomly move the neighbourhood of these point around via affine transformations. Note: This class introduce interpolation artifacts to mask if it has values other than {0;1} Parameters: Name Type Description scale [float, float] factor range that determines how far each point is moved. Default: (0.03, 0.05). nb_rows int number of rows of points that the regular grid should have. Default: 4. nb_cols int number of columns of points that the regular grid should have. Default: 4. p float probability of applying the transform. Default: 0.5. Targets: image, mask","title":"IAAPiecewiseAffine"},{"location":"api_reference/imgaug/transforms/#albumentations.imgaug.transforms.IAASharpen","text":"Sharpen the input image and overlays the result with the original image. This augmentation is deprecated. Please use Sharpen instead Parameters: Name Type Description alpha [float, float] range to choose the visibility of the sharpened image. At 0, only the original image is visible, at 1.0 only its sharpened version is visible. Default: (0.2, 0.5). lightness [float, float] range to choose the lightness of the sharpened image. Default: (0.5, 1.0). p float probability of applying the transform. Default: 0.5. Targets: image","title":"IAASharpen"},{"location":"api_reference/imgaug/transforms/#albumentations.imgaug.transforms.IAASuperpixels","text":"Completely or partially transform the input image to its superpixel representation. Uses skimage's version of the SLIC algorithm. May be slow. Parameters: Name Type Description p_replace float defines the probability of any superpixel area being replaced by the superpixel, i.e. by the average pixel color within its area. Default: 0.1. n_segments int target number of superpixels to generate. Default: 100. p float probability of applying the transform. Default: 0.5. Targets: image","title":"IAASuperpixels"},{"location":"api_reference/pytorch/","text":"Transforms (albumentations.pytorch.transforms)","title":"Index"},{"location":"api_reference/pytorch/transforms/","text":"Transforms (pytorch.transforms) \u00b6 \u00b6 class albumentations.pytorch.transforms.ToTensor ( num_classes = 1 , sigmoid = True , normalize = None ) [view source on GitHub] \u00b6 Convert image and mask to torch.Tensor and divide by 255 if image or mask are uint8 type. WARNING! Please use this with care and look into sources before usage. Parameters: Name Type Description num_classes int only for segmentation sigmoid bool only for segmentation, transform mask to LongTensor or not. normalize dict dict with keys [mean, std] to pass it into torchvision.normalize class albumentations.pytorch.transforms.ToTensorV2 ( transpose_mask = False , always_apply = True , p = 1.0 ) [view source on GitHub] \u00b6 Convert image and mask to torch.Tensor . Parameters: Name Type Description transpose_mask bool if True and an input mask has three dimensions, this transform will transpose dimensions standard format for PyTorch Tensors. Default False.","title":"Transforms (pytorch.transforms)"},{"location":"api_reference/pytorch/transforms/#transforms-pytorchtransforms","text":"","title":"Transforms (pytorch.transforms)"},{"location":"api_reference/pytorch/transforms/#albumentations.pytorch.transforms","text":"","title":"albumentations.pytorch.transforms"},{"location":"api_reference/pytorch/transforms/#albumentations.pytorch.transforms.ToTensor","text":"Convert image and mask to torch.Tensor and divide by 255 if image or mask are uint8 type. WARNING! Please use this with care and look into sources before usage. Parameters: Name Type Description num_classes int only for segmentation sigmoid bool only for segmentation, transform mask to LongTensor or not. normalize dict dict with keys [mean, std] to pass it into torchvision.normalize","title":"ToTensor"},{"location":"api_reference/pytorch/transforms/#albumentations.pytorch.transforms.ToTensorV2","text":"Convert image and mask to torch.Tensor . Parameters: Name Type Description transpose_mask bool if True and an input mask has three dimensions, this transform will transpose dimensions standard format for PyTorch Tensors. Default False.","title":"ToTensorV2"},{"location":"autoalbument/","text":"AutoAlbument Overview \u00b6 AutoAlbument is an AutoML tool that learns image augmentation policies from data using the Faster AutoAugment algorithm . It relieves the user from manually selecting augmentations and tuning their parameters. AutoAlbument provides a complete ready-to-use configuration for an augmentation pipeline. AutoAlbument supports image classification and semantic segmentation tasks. The library requires Python 3.6 or higher. The source code and issue tracker are available at https://github.com/albumentations-team/autoalbument Table of contents: AutoAlbument introduction and core concepts Installation Benchmarks and a comparison with baseline augmentation strategies How to use AutoAlbument How to use an AutoAlbument Docker image How to use a custom classification or semantic segmentation model Metrics and their meaning Tuning parameters Examples Search algorithms FAQ","title":"AutoAlbument Overview"},{"location":"autoalbument/#autoalbument-overview","text":"AutoAlbument is an AutoML tool that learns image augmentation policies from data using the Faster AutoAugment algorithm . It relieves the user from manually selecting augmentations and tuning their parameters. AutoAlbument provides a complete ready-to-use configuration for an augmentation pipeline. AutoAlbument supports image classification and semantic segmentation tasks. The library requires Python 3.6 or higher. The source code and issue tracker are available at https://github.com/albumentations-team/autoalbument Table of contents: AutoAlbument introduction and core concepts Installation Benchmarks and a comparison with baseline augmentation strategies How to use AutoAlbument How to use an AutoAlbument Docker image How to use a custom classification or semantic segmentation model Metrics and their meaning Tuning parameters Examples Search algorithms FAQ","title":"AutoAlbument Overview"},{"location":"autoalbument/benchmarks/","text":"Benchmarks and a comparison with baseline augmentation strategies \u00b6 Here is a comparison between a baseline augmentation strategy and an augmentation policy discovered by AutoAlbument for different classification and semantic segmentation tasks. You can read more about these benchmarks in the autoalbument-benchmarks repository. Classification \u00b6 Dataset Baseline Top-1 Accuracy AutoAlbument Top-1 Accuracy CIFAR10 91.79 96.02 SVHN 98.31 98.48 ImageNet 73.27 75.17 Semantic segmentation \u00b6 Dataset Baseline mIOU AutoAlbument mIOU Pascal VOC 73.34 75.55 Cityscapes 79.47 79.92","title":"Benchmarks and a comparison with baseline augmentation strategies"},{"location":"autoalbument/benchmarks/#benchmarks-and-a-comparison-with-baseline-augmentation-strategies","text":"Here is a comparison between a baseline augmentation strategy and an augmentation policy discovered by AutoAlbument for different classification and semantic segmentation tasks. You can read more about these benchmarks in the autoalbument-benchmarks repository.","title":"Benchmarks and a comparison with baseline augmentation strategies"},{"location":"autoalbument/benchmarks/#classification","text":"Dataset Baseline Top-1 Accuracy AutoAlbument Top-1 Accuracy CIFAR10 91.79 96.02 SVHN 98.31 98.48 ImageNet 73.27 75.17","title":"Classification"},{"location":"autoalbument/benchmarks/#semantic-segmentation","text":"Dataset Baseline mIOU AutoAlbument mIOU Pascal VOC 73.34 75.55 Cityscapes 79.47 79.92","title":"Semantic segmentation"},{"location":"autoalbument/custom_model/","text":"How to use a custom classification or semantic segmentation model \u00b6 By default AutoAlbument uses pytorch-image-models for classification and segmentation_models.pytorch for semantic segmentation. You can use any model from these packages by providing an appropriate model name. However, you can also use a custom model with AutoAlbument. To do so, you need to define a Discriminator model. This Discriminator model should have two outputs. The first output should provide a prediction for a classification or semantic segmentation task. For classification, it should output a tensor with a shape [batch_size, num_classes] with logits. For semantic segmentation, it should output a tensor with the shape [batch_size, num_classes, height, width] with logits. The second (auxiliary) output should return a tensor with the shape [batch_size] that contains logits for Discriminator's predictions (whether Discriminator thinks that an image wasn't or was augmented). To create such a model, you need to subclass the autoalbument.faster_autoaugment.models.BaseDiscriminator class and implement the forward method. This method should take a batch of images, that is, a tensor with the shape [batch_size, num_channels, height, width] . It should return a tuple that contains tensors from the two outputs described above. As an example, take a look at how default classification and semantic segmentation models are defined in AutoAlbument - https://github.com/albumentations-team/autoalbument/blob/master/autoalbument/faster_autoaugment/models.py or explore an example of a custom model for the CIFAR10 dataset. Next, you need to specify this custom model in config.yaml , an AutoAlbument config file. AutoAlbument uses the instantiate function from Hydra to instantiate an object. You need to set the _target_ config variable in the classification_model or semantic_segmentation_model section, depending on the task. In this config variable, you need to provide a path to a class with the model. This path should be located inside PYTHONPATH, so Hydra could correctly use it. The simplest way is to define your model in a file such as model.py and place this file in the same directory with dataset.py and search.yaml because this directory is automatically added to PYTHONPATH. Next, you could define _target_ such as _target_: model.MyClassificationModel . Take a look at the CIFAR10 example config that uses a custom model defined in model.py as a starting point for defining a custom model.","title":"How to use a custom classification or semantic segmentation model"},{"location":"autoalbument/custom_model/#how-to-use-a-custom-classification-or-semantic-segmentation-model","text":"By default AutoAlbument uses pytorch-image-models for classification and segmentation_models.pytorch for semantic segmentation. You can use any model from these packages by providing an appropriate model name. However, you can also use a custom model with AutoAlbument. To do so, you need to define a Discriminator model. This Discriminator model should have two outputs. The first output should provide a prediction for a classification or semantic segmentation task. For classification, it should output a tensor with a shape [batch_size, num_classes] with logits. For semantic segmentation, it should output a tensor with the shape [batch_size, num_classes, height, width] with logits. The second (auxiliary) output should return a tensor with the shape [batch_size] that contains logits for Discriminator's predictions (whether Discriminator thinks that an image wasn't or was augmented). To create such a model, you need to subclass the autoalbument.faster_autoaugment.models.BaseDiscriminator class and implement the forward method. This method should take a batch of images, that is, a tensor with the shape [batch_size, num_channels, height, width] . It should return a tuple that contains tensors from the two outputs described above. As an example, take a look at how default classification and semantic segmentation models are defined in AutoAlbument - https://github.com/albumentations-team/autoalbument/blob/master/autoalbument/faster_autoaugment/models.py or explore an example of a custom model for the CIFAR10 dataset. Next, you need to specify this custom model in config.yaml , an AutoAlbument config file. AutoAlbument uses the instantiate function from Hydra to instantiate an object. You need to set the _target_ config variable in the classification_model or semantic_segmentation_model section, depending on the task. In this config variable, you need to provide a path to a class with the model. This path should be located inside PYTHONPATH, so Hydra could correctly use it. The simplest way is to define your model in a file such as model.py and place this file in the same directory with dataset.py and search.yaml because this directory is automatically added to PYTHONPATH. Next, you could define _target_ such as _target_: model.MyClassificationModel . Take a look at the CIFAR10 example config that uses a custom model defined in model.py as a starting point for defining a custom model.","title":"How to use a custom classification or semantic segmentation model"},{"location":"autoalbument/docker/","text":"How to use an AutoAlbument Docker image \u00b6 You can run AutoAlbument from a Docker image. The ghcr.io/albumentations-team/autoalbument:latest Docker image contains the latest release version of AutoAlbument. You can also use an image that contains a specific version of AutoAlbument. In that case, you need to use the AutoAlbument version as a tag for a Docker image, e.g., the ghcr.io/albumentations-team/autoalbument:0.3.0 image contains AutoAlbument 0.3.0. The latest AutoAlbument image is based on the pytorch/pytorch:1.7.0-cuda11.0-cudnn8-runtime image. When you run a Docker container with AutoAlbument, you need to mount a config directory (a directory containing dataset.py and search.yaml files) and other required directories, such as a directory that contains training data. Here is an example command that runs a Docker container that will search for CIFAR10 augmentation policies. docker run -it --rm --gpus all --ipc=host -v ~/projects/autoalbument/examples/cifar10:/config -v ~/data:/home/autoalbument/data -u $(id -u ${USER}):$(id -g ${USER}) ghcr.io/albumentations-team/autoalbument:latest Let's take a look at the arguments: --it . Tell Docker that you run an interactive process. Read more in the Docker documentation. --rm . Automatically clean up a container when it exits. Read more in the Docker documentation. --gpus all . Specify GPUs to use. Read more in the Docker documentation. --ipc=host . Increase shared memory size for PyTorch DataLoader. Read more in the PyTorch documentation. -v ~/projects/autoalbument/examples/cifar10:/config . Mounts the ~/projects/autoalbument/examples/cifar10 directory from the host to the /config directory into the container. This example assumes that you have the AutoAlbument repository in the ~/projects/autoalbument/ directory. Generally speaking, you need to mount a directory containing dataset.py and search.yaml into the /config directory in a container. -v ~/data:/home/autoalbument/data . Mounts the directory ~/data that contains the CIFAR10 dataset into the /home/autoalbument/data directory. You can mount a host directory with a dataset into any container directory, but you need to specify config parameters accordingly. In this example, we mount the directory into /home/autoalbument/data because we set this directory ( ~/data/cifar10 ) in the config as a root directory for the dataset . Note that Docker doesn't support tilde expansion for the HOME directory, so we explicitly name HOME directory as /home/autoalbument because autoalbument is a default user inside the container. -u $(id -u ${USER}):$(id -g ${USER}) . We use that command to tell Docker to use the host's user ID to run code inside a container. We need this command because AutoAlbument will produce artifacts in the config directory (such as augmentation configs and logs). We need that the host user owns those files (and not root , for example) so you can access them afterward. ghcr.io/albumentations-team/autoalbument:latest is the Docker image's name. latest is a tag for the latest stable release. Alternatively, you can use a tag that specifies an AutoAlbument version, e.g., ghcr.io/albumentations-team/autoalbument:0.3.0 .","title":"How to use an AutoAlbument Docker image"},{"location":"autoalbument/docker/#how-to-use-an-autoalbument-docker-image","text":"You can run AutoAlbument from a Docker image. The ghcr.io/albumentations-team/autoalbument:latest Docker image contains the latest release version of AutoAlbument. You can also use an image that contains a specific version of AutoAlbument. In that case, you need to use the AutoAlbument version as a tag for a Docker image, e.g., the ghcr.io/albumentations-team/autoalbument:0.3.0 image contains AutoAlbument 0.3.0. The latest AutoAlbument image is based on the pytorch/pytorch:1.7.0-cuda11.0-cudnn8-runtime image. When you run a Docker container with AutoAlbument, you need to mount a config directory (a directory containing dataset.py and search.yaml files) and other required directories, such as a directory that contains training data. Here is an example command that runs a Docker container that will search for CIFAR10 augmentation policies. docker run -it --rm --gpus all --ipc=host -v ~/projects/autoalbument/examples/cifar10:/config -v ~/data:/home/autoalbument/data -u $(id -u ${USER}):$(id -g ${USER}) ghcr.io/albumentations-team/autoalbument:latest Let's take a look at the arguments: --it . Tell Docker that you run an interactive process. Read more in the Docker documentation. --rm . Automatically clean up a container when it exits. Read more in the Docker documentation. --gpus all . Specify GPUs to use. Read more in the Docker documentation. --ipc=host . Increase shared memory size for PyTorch DataLoader. Read more in the PyTorch documentation. -v ~/projects/autoalbument/examples/cifar10:/config . Mounts the ~/projects/autoalbument/examples/cifar10 directory from the host to the /config directory into the container. This example assumes that you have the AutoAlbument repository in the ~/projects/autoalbument/ directory. Generally speaking, you need to mount a directory containing dataset.py and search.yaml into the /config directory in a container. -v ~/data:/home/autoalbument/data . Mounts the directory ~/data that contains the CIFAR10 dataset into the /home/autoalbument/data directory. You can mount a host directory with a dataset into any container directory, but you need to specify config parameters accordingly. In this example, we mount the directory into /home/autoalbument/data because we set this directory ( ~/data/cifar10 ) in the config as a root directory for the dataset . Note that Docker doesn't support tilde expansion for the HOME directory, so we explicitly name HOME directory as /home/autoalbument because autoalbument is a default user inside the container. -u $(id -u ${USER}):$(id -g ${USER}) . We use that command to tell Docker to use the host's user ID to run code inside a container. We need this command because AutoAlbument will produce artifacts in the config directory (such as augmentation configs and logs). We need that the host user owns those files (and not root , for example) so you can access them afterward. ghcr.io/albumentations-team/autoalbument:latest is the Docker image's name. latest is a tag for the latest stable release. Alternatively, you can use a tag that specifies an AutoAlbument version, e.g., ghcr.io/albumentations-team/autoalbument:0.3.0 .","title":"How to use an AutoAlbument Docker image"},{"location":"autoalbument/faq/","text":"FAQ \u00b6 Search takes a lot of time. How can I speed it up? \u00b6 Instead of a full training dataset, you can use a reduced version to search for augmentation policies. For example, the authors of Faster AutoAugment used 6000 images from the 120 selected classes to find augmentation policies for ImageNet (while the full dataset for ILSVRC contains 1.2 million images and 1000 classes).","title":"FAQ"},{"location":"autoalbument/faq/#faq","text":"","title":"FAQ"},{"location":"autoalbument/faq/#search-takes-a-lot-of-time-how-can-i-speed-it-up","text":"Instead of a full training dataset, you can use a reduced version to search for augmentation policies. For example, the authors of Faster AutoAugment used 6000 images from the 120 selected classes to find augmentation policies for ImageNet (while the full dataset for ILSVRC contains 1.2 million images and 1000 classes).","title":"Search takes a lot of time. How can I speed it up?"},{"location":"autoalbument/how_to_use/","text":"How to use AutoAlbument \u00b6 You need to create a configuration file with AutoAlbument parameters and a Python file that implements a custom PyTorch Dataset for your data. Next, you need to pass those files to AutoAlbument. AutoAlbument will use Generative Adversarial Network to discover augmentation policies and then create a file containing those policies. Finally, you can use Albumentations to load augmentation policies from the file and utilize them in your computer vision pipeline. Step 1. Create a configuration file and a custom PyTorch Dataset for your data. \u00b6 a. Create a directory with configuration files. \u00b6 Run autoalbument-create --config-dir </path/to/directory> --task <deep_learning_task> --num-classes <num_classes> , e.g. autoalbument-create --config-dir ~/experiments/autoalbument-search-cifar10 --task classification --num-classes 10 . - A value for the --config-dir option should contain a path to the directory. AutoAlbument will create this directory and put two files into it: dataset.py and search.yaml (more on them later). - A value for the --task option should contain the name of a deep learning task. Supported values are classification and semantic_segmentation . - A value for the --num-classes option should contain the number of distinct classes in the classification or segmentation dataset. By default, AutoAlbument creates a search.yaml file that contains only most important configuration parameters. To explore all avaiable parameters you can create a config file that contans them all by providing the --generate-full-config argument, e.g. autoalbument-create --config-dir ~/experiments/autoalbument-search-cifar10 --task classification --num-classes 10 --generate-full-config b. Add implementation for __len__ and __getitem__ methods in dataset.py . \u00b6 The dataset.py file created at step 1 by autoalbument-create contains stubs for implementing a PyTorch dataset (you can read more about creating custom PyTorch datasets here ). You need to add implementation for __len__ and __getitem__ methods (and optionally add the initialization logic if required). A dataset for a classification task should return an image and a class label. A dataset for a segmentation task should return an image and an associated mask. c. [Optional] Adjust search parameters in search.yaml . \u00b6 You may want to change the parameters that AutoAlbument will use to search for augmentation policies. To do this, you need to edit the search.yaml file created by autoalbument-create at step 1. Each configuration parameter contains a comment that describes the meaning of the setting. Please refer to the \"Tuning the search parameters\" section that includes a description of the most critical parameters. search.yaml is a Hydra config file. You can use all Hydra features inside it. Step 2. Use AutoAlbument to search for augmentation policies. \u00b6 To search for augmentation policies, run autoalbument-search --config-dir </path/to/directory> , e.g. autoalbument-search --config-dir ~/experiments/autoalbument-search-cifar10 . The value of --config-dir should be the same value that was passed to autoalbument-create at step 1. autoalbument-search will create a directory with output files (by default the path of the directory will be <config_dir>/outputs/<current_date>/<current_time> , but you can customize it in search.yaml). The policy subdirectory will contain JSON files with policies found at each search phase's epoch. autoalbument-search is a command wrapped with the @hydra.main decorator from Hydra . You can use all Hydra features when calling this command. AutoAlbument uses PyTorch to search for augmentation policies. You can speed up the search by using a CUDA-capable GPU. Step 3. Use Albumentations to load augmentation policies and utilize them in your training pipeline. \u00b6 AutoAlbument produces a JSON file that contains a configuration for an augmentation pipeline. You can load that JSON file with Albumentations : import albumentations as A transform = A.load(\"/path/to/policy.json\") Then you can use the created augmentation pipeline to augment the input data. For example, to augment an image for a classification task: transformed = transform(image=image) transformed_image = transformed[\"image\"] To augment an image and a mask for a semantic segmentation task: transformed = transform(image=image, mask=mask) transformed_image = transformed[\"image\"] transformed_mask = transformed[\"mask\"] Additional resources \u00b6 You can read more about the most important configuration parameters for AutoAlbument in Tuning the search parameters . To see examples of configuration files and custom PyTorch Datasets, please refer to Examples You can read more about using Albumentations for augmentation in those articles Image augmentation for classification , Mask augmentation for segmentation . Refer to this section of the documentation to get examples of how to use Albumentations with PyTorch and TensorFlow 2.","title":"How to use AutoAlbument"},{"location":"autoalbument/how_to_use/#how-to-use-autoalbument","text":"You need to create a configuration file with AutoAlbument parameters and a Python file that implements a custom PyTorch Dataset for your data. Next, you need to pass those files to AutoAlbument. AutoAlbument will use Generative Adversarial Network to discover augmentation policies and then create a file containing those policies. Finally, you can use Albumentations to load augmentation policies from the file and utilize them in your computer vision pipeline.","title":"How to use AutoAlbument"},{"location":"autoalbument/how_to_use/#step-1-create-a-configuration-file-and-a-custom-pytorch-dataset-for-your-data","text":"","title":"Step 1. Create a configuration file and a custom PyTorch Dataset for your data."},{"location":"autoalbument/how_to_use/#a-create-a-directory-with-configuration-files","text":"Run autoalbument-create --config-dir </path/to/directory> --task <deep_learning_task> --num-classes <num_classes> , e.g. autoalbument-create --config-dir ~/experiments/autoalbument-search-cifar10 --task classification --num-classes 10 . - A value for the --config-dir option should contain a path to the directory. AutoAlbument will create this directory and put two files into it: dataset.py and search.yaml (more on them later). - A value for the --task option should contain the name of a deep learning task. Supported values are classification and semantic_segmentation . - A value for the --num-classes option should contain the number of distinct classes in the classification or segmentation dataset. By default, AutoAlbument creates a search.yaml file that contains only most important configuration parameters. To explore all avaiable parameters you can create a config file that contans them all by providing the --generate-full-config argument, e.g. autoalbument-create --config-dir ~/experiments/autoalbument-search-cifar10 --task classification --num-classes 10 --generate-full-config","title":"a. Create a directory with configuration files."},{"location":"autoalbument/how_to_use/#b-add-implementation-for-__len__-and-__getitem__-methods-in-datasetpy","text":"The dataset.py file created at step 1 by autoalbument-create contains stubs for implementing a PyTorch dataset (you can read more about creating custom PyTorch datasets here ). You need to add implementation for __len__ and __getitem__ methods (and optionally add the initialization logic if required). A dataset for a classification task should return an image and a class label. A dataset for a segmentation task should return an image and an associated mask.","title":"b. Add implementation for __len__ and __getitem__ methods in dataset.py."},{"location":"autoalbument/how_to_use/#c-optional-adjust-search-parameters-in-searchyaml","text":"You may want to change the parameters that AutoAlbument will use to search for augmentation policies. To do this, you need to edit the search.yaml file created by autoalbument-create at step 1. Each configuration parameter contains a comment that describes the meaning of the setting. Please refer to the \"Tuning the search parameters\" section that includes a description of the most critical parameters. search.yaml is a Hydra config file. You can use all Hydra features inside it.","title":"c. [Optional] Adjust search parameters in search.yaml."},{"location":"autoalbument/how_to_use/#step-2-use-autoalbument-to-search-for-augmentation-policies","text":"To search for augmentation policies, run autoalbument-search --config-dir </path/to/directory> , e.g. autoalbument-search --config-dir ~/experiments/autoalbument-search-cifar10 . The value of --config-dir should be the same value that was passed to autoalbument-create at step 1. autoalbument-search will create a directory with output files (by default the path of the directory will be <config_dir>/outputs/<current_date>/<current_time> , but you can customize it in search.yaml). The policy subdirectory will contain JSON files with policies found at each search phase's epoch. autoalbument-search is a command wrapped with the @hydra.main decorator from Hydra . You can use all Hydra features when calling this command. AutoAlbument uses PyTorch to search for augmentation policies. You can speed up the search by using a CUDA-capable GPU.","title":"Step 2. Use AutoAlbument to search for augmentation policies."},{"location":"autoalbument/how_to_use/#step-3-use-albumentations-to-load-augmentation-policies-and-utilize-them-in-your-training-pipeline","text":"AutoAlbument produces a JSON file that contains a configuration for an augmentation pipeline. You can load that JSON file with Albumentations : import albumentations as A transform = A.load(\"/path/to/policy.json\") Then you can use the created augmentation pipeline to augment the input data. For example, to augment an image for a classification task: transformed = transform(image=image) transformed_image = transformed[\"image\"] To augment an image and a mask for a semantic segmentation task: transformed = transform(image=image, mask=mask) transformed_image = transformed[\"image\"] transformed_mask = transformed[\"mask\"]","title":"Step 3. Use Albumentations to load augmentation policies and utilize them in your training pipeline."},{"location":"autoalbument/how_to_use/#additional-resources","text":"You can read more about the most important configuration parameters for AutoAlbument in Tuning the search parameters . To see examples of configuration files and custom PyTorch Datasets, please refer to Examples You can read more about using Albumentations for augmentation in those articles Image augmentation for classification , Mask augmentation for segmentation . Refer to this section of the documentation to get examples of how to use Albumentations with PyTorch and TensorFlow 2.","title":"Additional resources"},{"location":"autoalbument/installation/","text":"Installation \u00b6 AutoAlbument requires Python 3.6 or higher. PyPI \u00b6 To install the latest stable version from PyPI: pip install -U autoalbument GitHub \u00b6 To install the latest version from GitHub: pip install -U git+https://github.com/albumentations-team/autoalbument","title":"Installation"},{"location":"autoalbument/installation/#installation","text":"AutoAlbument requires Python 3.6 or higher.","title":"Installation"},{"location":"autoalbument/installation/#pypi","text":"To install the latest stable version from PyPI: pip install -U autoalbument","title":"PyPI"},{"location":"autoalbument/installation/#github","text":"To install the latest version from GitHub: pip install -U git+https://github.com/albumentations-team/autoalbument","title":"GitHub"},{"location":"autoalbument/introduction/","text":"AutoAlbument introduction and core concepts \u00b6 What is AutoAlbument \u00b6 AutoAlbument is a tool that automatically searches for the best augmentation policies for your data. Under the hood, it uses the Faster AutoAugment algorithm . Briefly speaking, the idea is to use a GAN-like architecture in which Generator applies augmentation to some input images, and Discriminator must determine whether an image was or wasn't augmented. This process helps to find augmentation policies that will produce images similar to the original images. How to use AutoAlbument \u00b6 To use AutoAlbument, you need to define two things: a PyTorch Dataset for your data and configuration parameters for AutoAlbument. You can read the detailed instruction in the How to use AutoAlbument article. Internally AutoAlbument uses PyTorch Lightning for training a GAN and Hydra for handling configuration parameters. Here are a few things about AutoAlbument and Hydra. Hydra \u00b6 The main internal configuration file is located at autoalbument/cli/conf/config.yaml Here is its content: defaults: - _version - task - policy_model: default - classification_model: default - semantic_segmentation_model: default - data: default - searcher: default - trainer: default - optim: default - callbacks: default - logger: default - hydra: default - seed - search Basically, it includes a bunch of config files with default values. Those config files are split into sets of closely related parameters such as model parameters or optimizer parameters. All default config files are located in their respective directories inside autoalbument/cli/conf The main config file also includes the search.yaml file, which you will use for overriding default parameters for your specific dataset and task (you can read more about creating the search.yaml file with autoalbument-create in How to use AutoAlbument ) To allow great flexibility, AutoAlbument relies heavily on the instantiate function from Hydra. This function allows to define a path to a Python class in a YAML config (using the _target_ parameter) along with arguments to that class, and Hydra will create an instance of this class with the provided arguments. As a practice example, if a config contains a definition like this: _target_: autoalbument.faster_autoaugment.models.ClassificationModel num_classes: 10 architecture: resnet18 pretrained: False AutoAlbument will translate it approximately to the following call: from autoalbument.faster_autoaugment.models import ClassificationModel model = ClassificationModel(num_classes=10, architecture='resnet18', pretrained=False) By relying on this feature, AutoAlbument allows customizing its behavior without changing the library's internal code. PyTorch Lightning \u00b6 AutoAlbument relies on PyTorch Lightning to train a GAN. In AutoAlbument configs, you can configure PyTorch Lightning by passing the appropriate arguments to Trainer through the trainer config or defining a list of Callbacks through the callbacks config .","title":"AutoAlbument introduction and core concepts"},{"location":"autoalbument/introduction/#autoalbument-introduction-and-core-concepts","text":"","title":"AutoAlbument introduction and core concepts"},{"location":"autoalbument/introduction/#what-is-autoalbument","text":"AutoAlbument is a tool that automatically searches for the best augmentation policies for your data. Under the hood, it uses the Faster AutoAugment algorithm . Briefly speaking, the idea is to use a GAN-like architecture in which Generator applies augmentation to some input images, and Discriminator must determine whether an image was or wasn't augmented. This process helps to find augmentation policies that will produce images similar to the original images.","title":"What is AutoAlbument"},{"location":"autoalbument/introduction/#how-to-use-autoalbument","text":"To use AutoAlbument, you need to define two things: a PyTorch Dataset for your data and configuration parameters for AutoAlbument. You can read the detailed instruction in the How to use AutoAlbument article. Internally AutoAlbument uses PyTorch Lightning for training a GAN and Hydra for handling configuration parameters. Here are a few things about AutoAlbument and Hydra.","title":"How to use AutoAlbument"},{"location":"autoalbument/introduction/#hydra","text":"The main internal configuration file is located at autoalbument/cli/conf/config.yaml Here is its content: defaults: - _version - task - policy_model: default - classification_model: default - semantic_segmentation_model: default - data: default - searcher: default - trainer: default - optim: default - callbacks: default - logger: default - hydra: default - seed - search Basically, it includes a bunch of config files with default values. Those config files are split into sets of closely related parameters such as model parameters or optimizer parameters. All default config files are located in their respective directories inside autoalbument/cli/conf The main config file also includes the search.yaml file, which you will use for overriding default parameters for your specific dataset and task (you can read more about creating the search.yaml file with autoalbument-create in How to use AutoAlbument ) To allow great flexibility, AutoAlbument relies heavily on the instantiate function from Hydra. This function allows to define a path to a Python class in a YAML config (using the _target_ parameter) along with arguments to that class, and Hydra will create an instance of this class with the provided arguments. As a practice example, if a config contains a definition like this: _target_: autoalbument.faster_autoaugment.models.ClassificationModel num_classes: 10 architecture: resnet18 pretrained: False AutoAlbument will translate it approximately to the following call: from autoalbument.faster_autoaugment.models import ClassificationModel model = ClassificationModel(num_classes=10, architecture='resnet18', pretrained=False) By relying on this feature, AutoAlbument allows customizing its behavior without changing the library's internal code.","title":"Hydra"},{"location":"autoalbument/introduction/#pytorch-lightning","text":"AutoAlbument relies on PyTorch Lightning to train a GAN. In AutoAlbument configs, you can configure PyTorch Lightning by passing the appropriate arguments to Trainer through the trainer config or defining a list of Callbacks through the callbacks config .","title":"PyTorch Lightning"},{"location":"autoalbument/metrics/","text":"Metrics and their meaning \u00b6 During the search phase, AutoAlbument outputs four metrics: loss , d_loss , a_loss , and Average Parameter Change (at the end of an epoch). a_loss \u00b6 a_loss is a loss for the policy network (or Generator in terms of GAN), which applies augmentations to input images. d_loss \u00b6 d_loss is a loss for the Discriminator, the network that tries to guess whether the input image is an augmented or non-augmented one. loss \u00b6 loss is a task-specific loss ( CrossEntropyLoss for classification, BCEWithLogitsLoss for semantic segmentation) that acts as a regularizer and prevents the policy network from applying such augmentations that will make an object with class A looks like an object with class B. Average Parameter Change \u00b6 Average Parameter Change is a difference between magnitudes of augmentation parameters multiplied by their probabilities at the end of an epoch and the same parameters at the beginning of the epoch. The metric is calculated using the following formula: m' and m are magnitude values for the i-th augmentation at the end and the beginning of the epoch, respectively. p' and p are probability values for the i-th augmentation at the end and the beginning of the epoch, respectively. The intuition behind this metric is that at the beginning, augmentation parameters are initialized at random, so they are now optimal and prone to heavy change at each epoch. After some time, these parameters should begin to converge, and they should change less at each epoch. Examples for metric values \u00b6 Below are TensorBoard logs for AutoAlbument on different datasets. The search was performed using AutoAlbument configs from the examples directory. CIFAR10 SVHN ImageNet Pascal VOC Cityscapes As you see, in all these charts, loss is slightly decreasing at each epoch, and a_loss or d_loss could either decrease or increase. Average Parameter Change is usually large at first epochs, but then it starts to decrease. As a rule of thumb, to decide whether you should stop AutoAlbument search and use the resulting policy, you should check that Average Parameter Change is stopped decreasing and started to oscillate, wait for a few more epochs, and use the found policy from that epoch. In autoalbument-benchmaks , we use AutoAlbument policies produced by the last epoch on these charts.","title":"Metrics and their meaning"},{"location":"autoalbument/metrics/#metrics-and-their-meaning","text":"During the search phase, AutoAlbument outputs four metrics: loss , d_loss , a_loss , and Average Parameter Change (at the end of an epoch).","title":"Metrics and their meaning"},{"location":"autoalbument/metrics/#a_loss","text":"a_loss is a loss for the policy network (or Generator in terms of GAN), which applies augmentations to input images.","title":"a_loss"},{"location":"autoalbument/metrics/#d_loss","text":"d_loss is a loss for the Discriminator, the network that tries to guess whether the input image is an augmented or non-augmented one.","title":"d_loss"},{"location":"autoalbument/metrics/#loss","text":"loss is a task-specific loss ( CrossEntropyLoss for classification, BCEWithLogitsLoss for semantic segmentation) that acts as a regularizer and prevents the policy network from applying such augmentations that will make an object with class A looks like an object with class B.","title":"loss"},{"location":"autoalbument/metrics/#average-parameter-change","text":"Average Parameter Change is a difference between magnitudes of augmentation parameters multiplied by their probabilities at the end of an epoch and the same parameters at the beginning of the epoch. The metric is calculated using the following formula: m' and m are magnitude values for the i-th augmentation at the end and the beginning of the epoch, respectively. p' and p are probability values for the i-th augmentation at the end and the beginning of the epoch, respectively. The intuition behind this metric is that at the beginning, augmentation parameters are initialized at random, so they are now optimal and prone to heavy change at each epoch. After some time, these parameters should begin to converge, and they should change less at each epoch.","title":"Average Parameter Change"},{"location":"autoalbument/metrics/#examples-for-metric-values","text":"Below are TensorBoard logs for AutoAlbument on different datasets. The search was performed using AutoAlbument configs from the examples directory. CIFAR10 SVHN ImageNet Pascal VOC Cityscapes As you see, in all these charts, loss is slightly decreasing at each epoch, and a_loss or d_loss could either decrease or increase. Average Parameter Change is usually large at first epochs, but then it starts to decrease. As a rule of thumb, to decide whether you should stop AutoAlbument search and use the resulting policy, you should check that Average Parameter Change is stopped decreasing and started to oscillate, wait for a few more epochs, and use the found policy from that epoch. In autoalbument-benchmaks , we use AutoAlbument policies produced by the last epoch on these charts.","title":"Examples for metric values"},{"location":"autoalbument/search_algorithms/","text":"Search algorithms \u00b6 AutoAlbument uses the following algorithms to search for augmentation policies. Faster AutoAugment \u00b6 \"Faster AutoAugment: Learning Augmentation Strategies using Backpropagation\" by Ryuichiro Hataya, Jan Zdenek, Kazuki Yoshizoe, and Hideki Nakayama. Paper | Original implementation","title":"Search algorithms"},{"location":"autoalbument/search_algorithms/#search-algorithms","text":"AutoAlbument uses the following algorithms to search for augmentation policies.","title":"Search algorithms"},{"location":"autoalbument/search_algorithms/#faster-autoaugment","text":"\"Faster AutoAugment: Learning Augmentation Strategies using Backpropagation\" by Ryuichiro Hataya, Jan Zdenek, Kazuki Yoshizoe, and Hideki Nakayama. Paper | Original implementation","title":"Faster AutoAugment"},{"location":"autoalbument/tuning_parameters/","text":"Tuning the search parameters \u00b6 The search.yaml file contains parameters for the search of augmentation policies. Here is an example search.yaml for image classification on the CIFAR-10 dataset, and here is an example search.yaml for semantic segmentation on the Pascal VOC dataset. Task-specific model \u00b6 A task-specific model is a model that classifies images for a classification task or outputs masks for a semantic segmentation task. Settings for a task-specific model are defined by either classification_model or semantic_segmentation_model depending on a selected task. Ideally, you should select the same model (the same architecture and the same pretrained weights) that you will use in an actual task. AutoAlbument uses models from PyTorch Image Models and Segmentation models packages for classification and semantic segmentation respectively. Base PyTorch parameters. \u00b6 You may want to adjust the following parameters for a PyTorch pipeline: data.dataloader parameters such as batch_size and num_workers Number of epochs to search for best augmentation policies in optim.epochs . Learning rate for optimizers in optim.main.lr and optim.policy.lr . Parameters for the augmentations search. \u00b6 Those parameters are defined in policy_model . You may want to tune the following ones: num_sub_policies - number of distinct augmentation sub-policies. A random sub-policy is selected in each iteration, and that sub-policy is applied to input data. The larger number of sub-policies will produce a more diverse set of augmentations. On the other side, the more sub-policies you have, the more time and data you need to tune those sub-policies correctly. num_chunks controls the balance between speed and diversity of augmentations in a search phase. Each batch is split-up into num_chunks chunks, and then a random sub-policy is applied to each chunk separately. The larger the value of num_chunks helps to learn augmentation policies better but simultaneously increases the searching time. Authors of FasterAutoAugment used such values for num_chunks that each chunk consisted of 8 to 16 images. operation_count - the number of augmentation operations that will be applied to each input data instance. For example, operation_count: 1 means that only one operation will be applied to an input image/mask, and operation_count: 4 means that four sequential operations will be applied to each input image/mask. The larger number of operations produces a more diverse set of augmentations but simultaneously increases the searching time. Preprocessing transforms \u00b6 If images have different sizes or you want to train a model on image patches, you could define preprocessing transforms (such as Resizing, Cropping, and Padding) in data.preprocessing . Those transforms will always be applied to all input data. Found augmentation policies will also contain those preprocessing transforms. Note that it is crucial for Policy Model (a model that searches for augmentation parameters) to receive images of the same size that will be used during the training of an actual model. For some augmentations, parameters depend on input data's height and width (for example, hole sizes for the Cutout augmentation).","title":"Tuning the search parameters"},{"location":"autoalbument/tuning_parameters/#tuning-the-search-parameters","text":"The search.yaml file contains parameters for the search of augmentation policies. Here is an example search.yaml for image classification on the CIFAR-10 dataset, and here is an example search.yaml for semantic segmentation on the Pascal VOC dataset.","title":"Tuning the search parameters"},{"location":"autoalbument/tuning_parameters/#task-specific-model","text":"A task-specific model is a model that classifies images for a classification task or outputs masks for a semantic segmentation task. Settings for a task-specific model are defined by either classification_model or semantic_segmentation_model depending on a selected task. Ideally, you should select the same model (the same architecture and the same pretrained weights) that you will use in an actual task. AutoAlbument uses models from PyTorch Image Models and Segmentation models packages for classification and semantic segmentation respectively.","title":"Task-specific model"},{"location":"autoalbument/tuning_parameters/#base-pytorch-parameters","text":"You may want to adjust the following parameters for a PyTorch pipeline: data.dataloader parameters such as batch_size and num_workers Number of epochs to search for best augmentation policies in optim.epochs . Learning rate for optimizers in optim.main.lr and optim.policy.lr .","title":"Base PyTorch parameters."},{"location":"autoalbument/tuning_parameters/#parameters-for-the-augmentations-search","text":"Those parameters are defined in policy_model . You may want to tune the following ones: num_sub_policies - number of distinct augmentation sub-policies. A random sub-policy is selected in each iteration, and that sub-policy is applied to input data. The larger number of sub-policies will produce a more diverse set of augmentations. On the other side, the more sub-policies you have, the more time and data you need to tune those sub-policies correctly. num_chunks controls the balance between speed and diversity of augmentations in a search phase. Each batch is split-up into num_chunks chunks, and then a random sub-policy is applied to each chunk separately. The larger the value of num_chunks helps to learn augmentation policies better but simultaneously increases the searching time. Authors of FasterAutoAugment used such values for num_chunks that each chunk consisted of 8 to 16 images. operation_count - the number of augmentation operations that will be applied to each input data instance. For example, operation_count: 1 means that only one operation will be applied to an input image/mask, and operation_count: 4 means that four sequential operations will be applied to each input image/mask. The larger number of operations produces a more diverse set of augmentations but simultaneously increases the searching time.","title":"Parameters for the augmentations search."},{"location":"autoalbument/tuning_parameters/#preprocessing-transforms","text":"If images have different sizes or you want to train a model on image patches, you could define preprocessing transforms (such as Resizing, Cropping, and Padding) in data.preprocessing . Those transforms will always be applied to all input data. Found augmentation policies will also contain those preprocessing transforms. Note that it is crucial for Policy Model (a model that searches for augmentation parameters) to receive images of the same size that will be used during the training of an actual model. For some augmentations, parameters depend on input data's height and width (for example, hole sizes for the Cutout augmentation).","title":"Preprocessing transforms"},{"location":"autoalbument/examples/cifar10/","text":"Image classification on the CIFAR10 dataset \u00b6 The following files are also available on GitHub - https://github.com/albumentations-team/autoalbument/tree/master/examples/cifar10 dataset.py \u00b6 import cv2 import torchvision cv2 . setNumThreads ( 0 ) cv2 . ocl . setUseOpenCL ( False ) class Cifar10SearchDataset ( torchvision . datasets . CIFAR10 ): def __init__ ( self , root = \"~/data/cifar10\" , train = True , download = True , transform = None ): super () . __init__ ( root = root , train = train , download = download , transform = transform ) def __getitem__ ( self , index ): image , label = self . data [ index ], self . targets [ index ] if self . transform is not None : transformed = self . transform ( image = image ) image = transformed [ \"image\" ] return image , label search.yaml \u00b6 # @package _global_ _version : 2 # An internal value that indicates a version of the config schema. This value is used by # `autoalbument-search` and `autoalbument-migrate` to upgrade the config to the latest version if necessary. # Please do not change it manually. task : classification # Deep learning task. Should either be `classification` or `semantic_segmentation`. policy_model : # Settings for Policy Model that searches augmentation policies. task_factor : 0.1 # Multiplier for classification loss of a model. Faster AutoAugment uses classification loss to prevent augmentations # from transforming images of a particular class to another class. The authors of Faster AutoAugment use 0.1 as # default value. gp_factor : 10 # Multiplier for the gradient penalty for WGAN-GP training. 10 is the default value that was proposed in # `Improved Training of Wasserstein GANs`. temperature : 0.05 # Temperature for Relaxed Bernoulli distribution. The probability of applying a certain augmentation is sampled from # Relaxed Bernoulli distribution (because Bernoulli distribution is not differentiable). With lower values of # `temperature` Relaxed Bernoulli distribution behaves like Bernoulli distribution. In the paper, the authors # of Faster AutoAugment used 0.05 as a default value for `temperature`. num_sub_policies : 100 # Number of augmentation sub-policies. When an image passes through an augmentation pipeline, Faster AutoAugment # randomly chooses one sub-policy and uses augmentations from that sub-policy to transform an input image. A larger # number of sub-policies leads to a more diverse set of augmentations and better performance of a model trained on # augmented images. However, an increase in the number of sub-policies leads to the exponential growth of a search # space of augmentations, so you need more training data for Policy Model to find good augmentation policies. num_chunks : 8 # Number of chunks in a batch. Faster AutoAugment splits each batch of images into `num_chunks` chunks. Then it # applies the same sub-policy with the same parameters to each image in a chunk. This parameter controls the tradeoff # between the speed of augmentation search and diversity of augmentations. Larger `num_chunks` values will lead to # faster searching but less diverse set of augmentations. Note that this parameter is used only in the searching # phase. When you train a model with found sub-policies, Albumentations will apply a distinct set of transformations # to each image separately. operation_count : 4 # Number of consecutive augmentations in each sub-policy. Faster AutoAugment will sequentially apply `operation_count` # augmentations from a sub-policy to an image. Larger values of `operation_count` lead to better performance of # a model trained on augmented images. Simultaneously, larger values of `operation_count` affect the speed of search # and increase the searching time. classification_model : # Settings for Classification Model that is used for two purposes: # 1. As a model that performs classification of input images. # 2. As a Discriminator for Policy Model. _target_ : model.Cifar10ClassificationModel # A custom classification model is used. This model is defined inside the `model.py` file which is located # in the same directory with `search.yaml` and `dataset.py`. # # As an alternative, you could use a built-in AutoAlbument model using the following config: # # _target_: autoalbument.faster_autoaugment.models.ClassificationModel # # # Number of classes in the dataset. The dataset implementation should return an integer in the range # # [0, num_classes - 1] as a class label of an image. # num_classes: 10 # # # The architecture of Classification Model. AutoAlbument uses models from # # https://github.com/rwightman/pytorch-image-models/. Please refer to its documentation to get a list of available # # models - https://rwightman.github.io/pytorch-image-models/#list-models-with-pretrained-weights. # architecture: resnet18 # # # Boolean flag that indicates whether the selected model architecture should load pretrained weights or use randomly # # initialized weights. # pretrained: False data : dataset : _target_ : dataset.Cifar10SearchDataset root : ~/data/cifar10 train : true download : true # Class for the PyTorch Dataset and arguments to it. AutoAlbument will create an object of this class using # the `instantiate` method from Hydra - https://hydra.cc/docs/next/patterns/instantiate_objects/overview/. # # Note that the target class value in the `_target_` argument should be located inside PYTHONPATH so Hydra could # find it. The directory with the config file is automatically added to PYTHONPATH, so the default value # `dataset.SearchDataset` points to the class `SearchDataset` from the `dataset.py` file. This `dataset.py` file is # located along with the `search.yaml` file in the same directory provided by `--config-dir`. # # As an alternative, you could provide a path to a Python file with the dataset using the `dataset_file` parameter # instead of the `dataset` parameter. The Python file should contain the implementation of a PyTorch dataset for # augmentation search. The dataset class should have named `SearchDataset`. The value in `dataset_file` could either # be a relative or an absolute path ; in the case of a relative path, the path should be relative to this config # file's location. # # - Example of a relative path: # dataset_file: dataset.py # # - Example of an absolute path: # dataset_file: /projects/pytorch/dataset.py # input_dtype : uint8 # The data type of input images. Two values are supported: # - uint8. In that case, all input images should be NumPy arrays with the np.uint8 data type and values in the range # [0, 255]. # - float32. In that case, all input images should be NumPy arrays with the np.float32 data type and values in the # range [0.0, 1.0]. preprocessing : null # A list of preprocessing augmentations that will be applied to each image before applying augmentations from # a policy. A preprocessing augmentation should be defined as `key`: `value`, where `key` is the name of augmentation # from Albumentations, and `value` is a dictionary with augmentation parameters. The found policy will also apply # those preprocessing augmentations before applying the main augmentations. # # Here is an example of an augmentation pipeline that first pads an image to the size 512x512 pixels, then resizes # the resulting image to the size 256x256 pixels and finally crops a random patch with the size 224x224 pixels. # # preprocessing: # - PadIfNeeded: # min_height: 512 # min_width: 512 # - Resize: # height: 256 # width: 256 # - RandomCrop: # height: 224 # width: 224 # normalization : mean : [ 0.4914 , 0.4822 , 0.4465 ] std : [ 0.247 , 0.243 , 0.261 ] # Normalization values for images. For each image, the search pipeline will subtract `mean` and divide by `std`. # Normalization is applied after transforms defined in `preprocessing`. Note that regardless of `input_dtype`, # the normalization function will always receive a `float32` input with values in the range [0.0, 1.0], so you should # define `mean` and `std` values accordingly. dataloader : _target_ : torch.utils.data.DataLoader batch_size : 128 shuffle : true num_workers : 8 pin_memory : true drop_last : true # Parameters for the PyTorch DataLoader. Please refer to the PyTorch documentation for the description of parameters - # https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader. optim : main : _target_ : torch.optim.Adam lr : 1e-3 betas : [ 0 , 0.999 ] # Optimizer configuration for the main (either Classification or Semantic Segmentation) Model policy : _target_ : torch.optim.Adam lr : 1e-3 betas : [ 0 , 0.999 ] # Optimizer configuration for Policy Model seed : 42 # Random seed. If the value is not null, it will be passed to `seed_everything` - # https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.utilities.seed.html?highlight=seed_everything hydra : run : dir : ${config_dir:}/outputs/${now:%Y-%m-%d}/${now:%H-%M-%S} # Path to the directory that will contain all outputs produced by the search algorithm. `${config_dir:}` contains # path to the directory with the `search.yaml` config file. Please refer to the Hydra documentation for more # information - https://hydra.cc/docs/configure_hydra/workdir. trainer : # Configuration for PyTorch Lightning Trainer. You can read more about Trainer and its arguments at # https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html. max_epochs : 40 # Number of epochs to search for augmentation parameters. # More detailed description - https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html#max-epochs benchmark : true # If true enables cudnn.benchmark. # More detailed description - https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html#benchmark gpus : 1 # Number of GPUs to train on. Set to `0` or None` to use CPU for training. # More detailed description - https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html#gpus model.py \u00b6 \"\"\"WideResNet code from https://github.com/xternalz/WideResNet-pytorch\"\"\" import torch import torch.nn as nn import torch.nn.functional as F from autoalbument.faster_autoaugment.models import BaseDiscriminator class BasicBlock ( nn . Module ): def __init__ ( self , in_planes , out_planes , stride ): super ( BasicBlock , self ) . __init__ () self . bn1 = nn . BatchNorm2d ( in_planes ) self . relu1 = nn . ReLU ( inplace = True ) self . conv1 = nn . Conv2d ( in_planes , out_planes , kernel_size = 3 , stride = stride , padding = 1 , bias = False ) self . bn2 = nn . BatchNorm2d ( out_planes ) self . relu2 = nn . ReLU ( inplace = True ) self . conv2 = nn . Conv2d ( out_planes , out_planes , kernel_size = 3 , stride = 1 , padding = 1 , bias = False ) self . equal_in_out = in_planes == out_planes self . conv_shortcut = ( ( not self . equal_in_out ) and nn . Conv2d ( in_planes , out_planes , kernel_size = 1 , stride = stride , padding = 0 , bias = False ) or None ) def forward ( self , x ): if not self . equal_in_out : x = self . relu1 ( self . bn1 ( x )) else : out = self . relu1 ( self . bn1 ( x )) out = self . relu2 ( self . bn2 ( self . conv1 ( out if self . equal_in_out else x ))) out = self . conv2 ( out ) return torch . add ( x if self . equal_in_out else self . conv_shortcut ( x ), out ) class NetworkBlock ( nn . Module ): def __init__ ( self , nb_layers , in_planes , out_planes , block , stride ): super ( NetworkBlock , self ) . __init__ () self . layer = self . _make_layer ( block , in_planes , out_planes , nb_layers , stride ) def _make_layer ( self , block , in_planes , out_planes , nb_layers , stride ): layers = [] for i in range ( int ( nb_layers )): layers . append ( block ( i == 0 and in_planes or out_planes , out_planes , i == 0 and stride or 1 )) return nn . Sequential ( * layers ) def forward ( self , x ): return self . layer ( x ) class WideResNet ( nn . Module ): def __init__ ( self , depth , num_classes , widen_factor = 1 ): super ( WideResNet , self ) . __init__ () n_channels = [ 16 , 16 * widen_factor , 32 * widen_factor , 64 * widen_factor ] assert ( depth - 4 ) % 6 == 0 n = ( depth - 4 ) / 6 block = BasicBlock # 1st conv before any network block self . conv1 = nn . Conv2d ( 3 , n_channels [ 0 ], kernel_size = 3 , stride = 1 , padding = 1 , bias = False ) # 1st block self . block1 = NetworkBlock ( n , n_channels [ 0 ], n_channels [ 1 ], block , 1 ) # 2nd block self . block2 = NetworkBlock ( n , n_channels [ 1 ], n_channels [ 2 ], block , 2 ) # 3rd block self . block3 = NetworkBlock ( n , n_channels [ 2 ], n_channels [ 3 ], block , 2 ) # global average pooling and classifier self . bn1 = nn . BatchNorm2d ( n_channels [ 3 ]) self . relu = nn . ReLU ( inplace = True ) self . fc = nn . Linear ( n_channels [ 3 ], num_classes ) self . n_channels = n_channels [ 3 ] for m in self . modules (): if isinstance ( m , nn . Conv2d ): nn . init . kaiming_normal_ ( m . weight , mode = \"fan_out\" , nonlinearity = \"relu\" ) elif isinstance ( m , nn . BatchNorm2d ): m . weight . data . fill_ ( 1 ) m . bias . data . zero_ () elif isinstance ( m , nn . Linear ): m . bias . data . zero_ () def forward_features ( self , x ): x = self . conv1 ( x ) x = self . block1 ( x ) x = self . block2 ( x ) x = self . block3 ( x ) x = self . relu ( self . bn1 ( x )) x = F . avg_pool2d ( x , 8 , 1 , 0 ) x = x . view ( - 1 , self . n_channels ) return x def forward_classifier ( self , x ): return self . fc ( x ) def forward ( self , x ): x = self . forward_features ( x ) x = self . forward_classifier ( x ) return x def wide_resnet_28x10 ( num_classes ): return WideResNet ( depth = 28 , widen_factor = 10 , num_classes = num_classes ) class Cifar10ClassificationModel ( BaseDiscriminator ): def __init__ ( self , * args , ** kwargs ): super () . __init__ () self . base_model = wide_resnet_28x10 ( num_classes = 10 ) num_features = self . base_model . fc . in_features self . discriminator = nn . Sequential ( nn . Linear ( num_features , num_features ), nn . ReLU (), nn . Linear ( num_features , 1 ) ) def forward ( self , input ): x = self . base_model . forward_features ( input ) return self . base_model . forward_classifier ( x ), self . discriminator ( x ) . view ( - 1 )","title":"Image classification on the CIFAR10 dataset"},{"location":"autoalbument/examples/cifar10/#image-classification-on-the-cifar10-dataset","text":"The following files are also available on GitHub - https://github.com/albumentations-team/autoalbument/tree/master/examples/cifar10","title":"Image classification on the CIFAR10 dataset"},{"location":"autoalbument/examples/cifar10/#datasetpy","text":"import cv2 import torchvision cv2 . setNumThreads ( 0 ) cv2 . ocl . setUseOpenCL ( False ) class Cifar10SearchDataset ( torchvision . datasets . CIFAR10 ): def __init__ ( self , root = \"~/data/cifar10\" , train = True , download = True , transform = None ): super () . __init__ ( root = root , train = train , download = download , transform = transform ) def __getitem__ ( self , index ): image , label = self . data [ index ], self . targets [ index ] if self . transform is not None : transformed = self . transform ( image = image ) image = transformed [ \"image\" ] return image , label","title":"dataset.py"},{"location":"autoalbument/examples/cifar10/#searchyaml","text":"# @package _global_ _version : 2 # An internal value that indicates a version of the config schema. This value is used by # `autoalbument-search` and `autoalbument-migrate` to upgrade the config to the latest version if necessary. # Please do not change it manually. task : classification # Deep learning task. Should either be `classification` or `semantic_segmentation`. policy_model : # Settings for Policy Model that searches augmentation policies. task_factor : 0.1 # Multiplier for classification loss of a model. Faster AutoAugment uses classification loss to prevent augmentations # from transforming images of a particular class to another class. The authors of Faster AutoAugment use 0.1 as # default value. gp_factor : 10 # Multiplier for the gradient penalty for WGAN-GP training. 10 is the default value that was proposed in # `Improved Training of Wasserstein GANs`. temperature : 0.05 # Temperature for Relaxed Bernoulli distribution. The probability of applying a certain augmentation is sampled from # Relaxed Bernoulli distribution (because Bernoulli distribution is not differentiable). With lower values of # `temperature` Relaxed Bernoulli distribution behaves like Bernoulli distribution. In the paper, the authors # of Faster AutoAugment used 0.05 as a default value for `temperature`. num_sub_policies : 100 # Number of augmentation sub-policies. When an image passes through an augmentation pipeline, Faster AutoAugment # randomly chooses one sub-policy and uses augmentations from that sub-policy to transform an input image. A larger # number of sub-policies leads to a more diverse set of augmentations and better performance of a model trained on # augmented images. However, an increase in the number of sub-policies leads to the exponential growth of a search # space of augmentations, so you need more training data for Policy Model to find good augmentation policies. num_chunks : 8 # Number of chunks in a batch. Faster AutoAugment splits each batch of images into `num_chunks` chunks. Then it # applies the same sub-policy with the same parameters to each image in a chunk. This parameter controls the tradeoff # between the speed of augmentation search and diversity of augmentations. Larger `num_chunks` values will lead to # faster searching but less diverse set of augmentations. Note that this parameter is used only in the searching # phase. When you train a model with found sub-policies, Albumentations will apply a distinct set of transformations # to each image separately. operation_count : 4 # Number of consecutive augmentations in each sub-policy. Faster AutoAugment will sequentially apply `operation_count` # augmentations from a sub-policy to an image. Larger values of `operation_count` lead to better performance of # a model trained on augmented images. Simultaneously, larger values of `operation_count` affect the speed of search # and increase the searching time. classification_model : # Settings for Classification Model that is used for two purposes: # 1. As a model that performs classification of input images. # 2. As a Discriminator for Policy Model. _target_ : model.Cifar10ClassificationModel # A custom classification model is used. This model is defined inside the `model.py` file which is located # in the same directory with `search.yaml` and `dataset.py`. # # As an alternative, you could use a built-in AutoAlbument model using the following config: # # _target_: autoalbument.faster_autoaugment.models.ClassificationModel # # # Number of classes in the dataset. The dataset implementation should return an integer in the range # # [0, num_classes - 1] as a class label of an image. # num_classes: 10 # # # The architecture of Classification Model. AutoAlbument uses models from # # https://github.com/rwightman/pytorch-image-models/. Please refer to its documentation to get a list of available # # models - https://rwightman.github.io/pytorch-image-models/#list-models-with-pretrained-weights. # architecture: resnet18 # # # Boolean flag that indicates whether the selected model architecture should load pretrained weights or use randomly # # initialized weights. # pretrained: False data : dataset : _target_ : dataset.Cifar10SearchDataset root : ~/data/cifar10 train : true download : true # Class for the PyTorch Dataset and arguments to it. AutoAlbument will create an object of this class using # the `instantiate` method from Hydra - https://hydra.cc/docs/next/patterns/instantiate_objects/overview/. # # Note that the target class value in the `_target_` argument should be located inside PYTHONPATH so Hydra could # find it. The directory with the config file is automatically added to PYTHONPATH, so the default value # `dataset.SearchDataset` points to the class `SearchDataset` from the `dataset.py` file. This `dataset.py` file is # located along with the `search.yaml` file in the same directory provided by `--config-dir`. # # As an alternative, you could provide a path to a Python file with the dataset using the `dataset_file` parameter # instead of the `dataset` parameter. The Python file should contain the implementation of a PyTorch dataset for # augmentation search. The dataset class should have named `SearchDataset`. The value in `dataset_file` could either # be a relative or an absolute path ; in the case of a relative path, the path should be relative to this config # file's location. # # - Example of a relative path: # dataset_file: dataset.py # # - Example of an absolute path: # dataset_file: /projects/pytorch/dataset.py # input_dtype : uint8 # The data type of input images. Two values are supported: # - uint8. In that case, all input images should be NumPy arrays with the np.uint8 data type and values in the range # [0, 255]. # - float32. In that case, all input images should be NumPy arrays with the np.float32 data type and values in the # range [0.0, 1.0]. preprocessing : null # A list of preprocessing augmentations that will be applied to each image before applying augmentations from # a policy. A preprocessing augmentation should be defined as `key`: `value`, where `key` is the name of augmentation # from Albumentations, and `value` is a dictionary with augmentation parameters. The found policy will also apply # those preprocessing augmentations before applying the main augmentations. # # Here is an example of an augmentation pipeline that first pads an image to the size 512x512 pixels, then resizes # the resulting image to the size 256x256 pixels and finally crops a random patch with the size 224x224 pixels. # # preprocessing: # - PadIfNeeded: # min_height: 512 # min_width: 512 # - Resize: # height: 256 # width: 256 # - RandomCrop: # height: 224 # width: 224 # normalization : mean : [ 0.4914 , 0.4822 , 0.4465 ] std : [ 0.247 , 0.243 , 0.261 ] # Normalization values for images. For each image, the search pipeline will subtract `mean` and divide by `std`. # Normalization is applied after transforms defined in `preprocessing`. Note that regardless of `input_dtype`, # the normalization function will always receive a `float32` input with values in the range [0.0, 1.0], so you should # define `mean` and `std` values accordingly. dataloader : _target_ : torch.utils.data.DataLoader batch_size : 128 shuffle : true num_workers : 8 pin_memory : true drop_last : true # Parameters for the PyTorch DataLoader. Please refer to the PyTorch documentation for the description of parameters - # https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader. optim : main : _target_ : torch.optim.Adam lr : 1e-3 betas : [ 0 , 0.999 ] # Optimizer configuration for the main (either Classification or Semantic Segmentation) Model policy : _target_ : torch.optim.Adam lr : 1e-3 betas : [ 0 , 0.999 ] # Optimizer configuration for Policy Model seed : 42 # Random seed. If the value is not null, it will be passed to `seed_everything` - # https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.utilities.seed.html?highlight=seed_everything hydra : run : dir : ${config_dir:}/outputs/${now:%Y-%m-%d}/${now:%H-%M-%S} # Path to the directory that will contain all outputs produced by the search algorithm. `${config_dir:}` contains # path to the directory with the `search.yaml` config file. Please refer to the Hydra documentation for more # information - https://hydra.cc/docs/configure_hydra/workdir. trainer : # Configuration for PyTorch Lightning Trainer. You can read more about Trainer and its arguments at # https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html. max_epochs : 40 # Number of epochs to search for augmentation parameters. # More detailed description - https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html#max-epochs benchmark : true # If true enables cudnn.benchmark. # More detailed description - https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html#benchmark gpus : 1 # Number of GPUs to train on. Set to `0` or None` to use CPU for training. # More detailed description - https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html#gpus","title":"search.yaml"},{"location":"autoalbument/examples/cifar10/#modelpy","text":"\"\"\"WideResNet code from https://github.com/xternalz/WideResNet-pytorch\"\"\" import torch import torch.nn as nn import torch.nn.functional as F from autoalbument.faster_autoaugment.models import BaseDiscriminator class BasicBlock ( nn . Module ): def __init__ ( self , in_planes , out_planes , stride ): super ( BasicBlock , self ) . __init__ () self . bn1 = nn . BatchNorm2d ( in_planes ) self . relu1 = nn . ReLU ( inplace = True ) self . conv1 = nn . Conv2d ( in_planes , out_planes , kernel_size = 3 , stride = stride , padding = 1 , bias = False ) self . bn2 = nn . BatchNorm2d ( out_planes ) self . relu2 = nn . ReLU ( inplace = True ) self . conv2 = nn . Conv2d ( out_planes , out_planes , kernel_size = 3 , stride = 1 , padding = 1 , bias = False ) self . equal_in_out = in_planes == out_planes self . conv_shortcut = ( ( not self . equal_in_out ) and nn . Conv2d ( in_planes , out_planes , kernel_size = 1 , stride = stride , padding = 0 , bias = False ) or None ) def forward ( self , x ): if not self . equal_in_out : x = self . relu1 ( self . bn1 ( x )) else : out = self . relu1 ( self . bn1 ( x )) out = self . relu2 ( self . bn2 ( self . conv1 ( out if self . equal_in_out else x ))) out = self . conv2 ( out ) return torch . add ( x if self . equal_in_out else self . conv_shortcut ( x ), out ) class NetworkBlock ( nn . Module ): def __init__ ( self , nb_layers , in_planes , out_planes , block , stride ): super ( NetworkBlock , self ) . __init__ () self . layer = self . _make_layer ( block , in_planes , out_planes , nb_layers , stride ) def _make_layer ( self , block , in_planes , out_planes , nb_layers , stride ): layers = [] for i in range ( int ( nb_layers )): layers . append ( block ( i == 0 and in_planes or out_planes , out_planes , i == 0 and stride or 1 )) return nn . Sequential ( * layers ) def forward ( self , x ): return self . layer ( x ) class WideResNet ( nn . Module ): def __init__ ( self , depth , num_classes , widen_factor = 1 ): super ( WideResNet , self ) . __init__ () n_channels = [ 16 , 16 * widen_factor , 32 * widen_factor , 64 * widen_factor ] assert ( depth - 4 ) % 6 == 0 n = ( depth - 4 ) / 6 block = BasicBlock # 1st conv before any network block self . conv1 = nn . Conv2d ( 3 , n_channels [ 0 ], kernel_size = 3 , stride = 1 , padding = 1 , bias = False ) # 1st block self . block1 = NetworkBlock ( n , n_channels [ 0 ], n_channels [ 1 ], block , 1 ) # 2nd block self . block2 = NetworkBlock ( n , n_channels [ 1 ], n_channels [ 2 ], block , 2 ) # 3rd block self . block3 = NetworkBlock ( n , n_channels [ 2 ], n_channels [ 3 ], block , 2 ) # global average pooling and classifier self . bn1 = nn . BatchNorm2d ( n_channels [ 3 ]) self . relu = nn . ReLU ( inplace = True ) self . fc = nn . Linear ( n_channels [ 3 ], num_classes ) self . n_channels = n_channels [ 3 ] for m in self . modules (): if isinstance ( m , nn . Conv2d ): nn . init . kaiming_normal_ ( m . weight , mode = \"fan_out\" , nonlinearity = \"relu\" ) elif isinstance ( m , nn . BatchNorm2d ): m . weight . data . fill_ ( 1 ) m . bias . data . zero_ () elif isinstance ( m , nn . Linear ): m . bias . data . zero_ () def forward_features ( self , x ): x = self . conv1 ( x ) x = self . block1 ( x ) x = self . block2 ( x ) x = self . block3 ( x ) x = self . relu ( self . bn1 ( x )) x = F . avg_pool2d ( x , 8 , 1 , 0 ) x = x . view ( - 1 , self . n_channels ) return x def forward_classifier ( self , x ): return self . fc ( x ) def forward ( self , x ): x = self . forward_features ( x ) x = self . forward_classifier ( x ) return x def wide_resnet_28x10 ( num_classes ): return WideResNet ( depth = 28 , widen_factor = 10 , num_classes = num_classes ) class Cifar10ClassificationModel ( BaseDiscriminator ): def __init__ ( self , * args , ** kwargs ): super () . __init__ () self . base_model = wide_resnet_28x10 ( num_classes = 10 ) num_features = self . base_model . fc . in_features self . discriminator = nn . Sequential ( nn . Linear ( num_features , num_features ), nn . ReLU (), nn . Linear ( num_features , 1 ) ) def forward ( self , input ): x = self . base_model . forward_features ( input ) return self . base_model . forward_classifier ( x ), self . discriminator ( x ) . view ( - 1 )","title":"model.py"},{"location":"autoalbument/examples/cityscapes/","text":"Semantic segmentation on the Pascal VOC dataset \u00b6 The following files are also available on GitHub - https://github.com/albumentations-team/autoalbument/tree/master/examples/cityscapes dataset.py \u00b6 import cv2 import numpy as np import torchvision cv2 . setNumThreads ( 0 ) cv2 . ocl . setUseOpenCL ( False ) class CityscapesSearchDataset ( torchvision . datasets . Cityscapes ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs , target_type = \"semantic\" ) self . semantic_target_type_index = [ i for i , t in enumerate ( self . target_type ) if t == \"semantic\" ][ 0 ] self . colormap = self . _generate_colormap () def _generate_colormap ( self ): colormap = {} for class_ in self . classes : if class_ . train_id in ( - 1 , 255 ): continue colormap [ class_ . train_id ] = class_ . id return colormap def _convert_to_segmentation_mask ( self , mask ): height , width = mask . shape [: 2 ] segmentation_mask = np . zeros (( height , width , len ( self . colormap )), dtype = np . float32 ) for label_index , label in self . colormap . items (): segmentation_mask [:, :, label_index ] = ( mask == label ) . astype ( float ) return segmentation_mask def __getitem__ ( self , index ): image = cv2 . imread ( self . images [ index ]) image = cv2 . cvtColor ( image , cv2 . COLOR_BGR2RGB ) mask = cv2 . imread ( self . targets [ index ][ self . semantic_target_type_index ], cv2 . IMREAD_UNCHANGED ) mask = self . _convert_to_segmentation_mask ( mask ) if self . transform is not None : transformed = self . transform ( image = image , mask = mask ) image = transformed [ \"image\" ] mask = transformed [ \"mask\" ] return image , mask search.yaml \u00b6 # @package _global_ _version : 2 # An internal value that indicates a version of the config schema. This value is used by # `autoalbument-search` and `autoalbument-migrate` to upgrade the config to the latest version if necessary. # Please do not change it manually. task : semantic_segmentation # Deep learning task. Should either be `classification` or `semantic_segmentation`. policy_model : # Settings for Policy Model that searches augmentation policies. task_factor : 0.1 # Multiplier for segmentation loss of a model. Faster AutoAugment uses segmentation loss to prevent augmentations # from transforming images of a particular class to another class. gp_factor : 10 # Multiplier for the gradient penalty for WGAN-GP training. 10 is the default value that was proposed in # `Improved Training of Wasserstein GANs`. temperature : 0.05 # Temperature for Relaxed Bernoulli distribution. The probability of applying a certain augmentation is sampled from # Relaxed Bernoulli distribution (because Bernoulli distribution is not differentiable). With lower values of # `temperature` Relaxed Bernoulli distribution behaves like Bernoulli distribution. In the paper, the authors # of Faster AutoAugment used 0.05 as a default value for `temperature`. num_sub_policies : 25 # Number of augmentation sub-policies. When an image passes through an augmentation pipeline, Faster AutoAugment # randomly chooses one sub-policy and uses augmentations from that sub-policy to transform an input image. A larger # number of sub-policies leads to a more diverse set of augmentations and better performance of a model trained on # augmented images. However, an increase in the number of sub-policies leads to the exponential growth of a search # space of augmentations, so you need more training data for Policy Model to find good augmentation policies. num_chunks : 4 # Number of chunks in a batch. Faster AutoAugment splits each batch of images into `num_chunks` chunks. Then it # applies the same sub-policy with the same parameters to each image in a chunk. This parameter controls the tradeoff # between the speed of augmentation search and diversity of augmentations. Larger `num_chunks` values will lead to # faster searching but less diverse set of augmentations. Note that this parameter is used only in the searching # phase. When you train a model with found sub-policies, Albumentations will apply a distinct set of transformations # to each image separately. operation_count : 4 # Number of consecutive augmentations in each sub-policy. Faster AutoAugment will sequentially apply `operation_count` # augmentations from a sub-policy to an image. Larger values of `operation_count` lead to better performance of # a model trained on augmented images. Simultaneously, larger values of `operation_count` affect the speed of search # and increase the searching time. semantic_segmentation_model : # Settings for Semantic Segmentation Model that is used for two purposes: # 1. As a model that performs semantic segmentation of input images. # 2. As a Discriminator for Policy Model. _target_ : autoalbument.faster_autoaugment.models.SemanticSegmentationModel # By default, AutoAlbument uses an instance of `autoalbument.faster_autoaugment.models.SemanticSegmentationModel` as # a semantic segmentation model. # This model takes four parameters: `num_classes`, `architecture`, `encoder_architecture` and `pretrained`. num_classes : 19 # The number of classes in the dataset. The dataset implementation should return a mask as a NumPy array with # the shape [height, width, num_classes]. In a case of binary segmentation you can set `num_classes` to 1. architecture : DeepLabV3Plus # The architecture of Semantic Segmentation Model. AutoAlbument uses models from # https://github.com/qubvel/segmentation_models.pytorch. Please refer to its documentation to get a list of available # models - https://github.com/qubvel/segmentation_models.pytorch#models-. encoder_architecture : resnet50 # The architecture of encoder in Semantic Segmentation Model. Please refer to Segmentation Models' documentation to # get a list of available encoders - https://github.com/qubvel/segmentation_models.pytorch#encoders- pretrained : true # Either boolean flag or string with that indicates whether the selected encoder architecture should load pretrained # weights or use randomly initialized weights. # - In the case of boolean flag `true` means using pretrained weights from ImageNet and `false` means using randomly # initialized weights. # - In the case of string the value should specify the name of the weights. For the list of available weights please # refer to https://github.com/qubvel/segmentation_models.pytorch#encoders- data : dataset : _target_ : dataset.CityscapesSearchDataset root : ~/data/cityscapes/data split : train # Class for the PyTorch Dataset and arguments to it. AutoAlbument will create an object of this class using # the `instantiate` method from Hydra - https://hydra.cc/docs/next/patterns/instantiate_objects/overview/. # # Note that the target class value in the `_target_` argument should be located inside PYTHONPATH so Hydra could # find it. The directory with the config file is automatically added to PYTHONPATH, so the default value # `dataset.SearchDataset` points to the class `SearchDataset` from the `dataset.py` file. This `dataset.py` file is # located along with the `search.yaml` file in the same directory provided by `--config-dir`. # # As an alternative, you could provide a path to a Python file with the dataset using the `dataset_file` parameter # instead of the `dataset` parameter. The Python file should contain the implementation of a PyTorch dataset for # augmentation search. The dataset class should have named `SearchDataset`. The value in `dataset_file` could either # be a relative or an absolute path ; in the case of a relative path, the path should be relative to this config # file's location. # # - Example of a relative path: # dataset_file: dataset.py # # - Example of an absolute path: # dataset_file: /projects/pytorch/dataset.py # input_dtype : uint8 # The data type of input images. Two values are supported: # - uint8. In that case, all input images should be NumPy arrays with the np.uint8 data type and values in the range # [0, 255]. # - float32. In that case, all input images should be NumPy arrays with the np.float32 data type and values in the # range [0.0, 1.0]. preprocessing : - LongestMaxSize : max_size : 256 - PadIfNeeded : min_height : 256 min_width : 256 border_mode : 0 value : [ 0 , 0 , 0 ] # A list of preprocessing augmentations that will be applied to each image before applying augmentations from # a policy. A preprocessing augmentation should be defined as `key`: `value`, where `key` is the name of augmentation # from Albumentations, and `value` is a dictionary with augmentation parameters. The found policy will also apply # those preprocessing augmentations before applying the main augmentations. # # Here is an example of an augmentation pipeline that first pads an image to the size 512x512 pixels, then resizes # the resulting image to the size 256x256 pixels and finally crops a random patch with the size 224x224 pixels. # # preprocessing: # - PadIfNeeded: # min_height: 512 # min_width: 512 # - Resize: # height: 256 # width: 256 # - RandomCrop: # height: 224 # width: 224 # normalization : mean : [ 0.485 , 0.456 , 0.406 ] std : [ 0.229 , 0.224 , 0.225 ] # Normalization values for images. For each image, the search pipeline will subtract `mean` and divide by `std`. # Normalization is applied after transforms defined in `preprocessing`. Note that regardless of `input_dtype`, # the normalization function will always receive a `float32` input with values in the range [0.0, 1.0], so you should # define `mean` and `std` values accordingly. ImageNet normalization is used by default. dataloader : _target_ : torch.utils.data.DataLoader batch_size : 32 shuffle : true num_workers : 8 pin_memory : true drop_last : true # Parameters for the PyTorch DataLoader. Please refer to the PyTorch documentation for the description of parameters - # https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader. optim : main : _target_ : torch.optim.Adam lr : 1e-3 betas : [ 0 , 0.999 ] # Optimizer configuration for the main (either Classification or Semantic Segmentation) Model policy : _target_ : torch.optim.Adam lr : 1e-3 betas : [ 0 , 0.999 ] # Optimizer configuration for Policy Model seed : 42 # Random seed. If the value is not null, it will be passed to `seed_everything` - # https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.utilities.seed.html?highlight=seed_everything hydra : run : dir : ${config_dir:}/outputs/${now:%Y-%m-%d}/${now:%H-%M-%S} # Path to the directory that will contain all outputs produced by the search algorithm. `${config_dir:}` contains # path to the directory with the `search.yaml` config file. Please refer to the Hydra documentation for more # information - https://hydra.cc/docs/configure_hydra/workdir. trainer : # Configuration for PyTorch Lightning Trainer. You can read more about Trainer and its arguments at # https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html. max_epochs : 50 # Number of epochs to search for augmentation parameters. # More detailed description - https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html#max-epochs benchmark : true # If true enables cudnn.benchmark. # More detailed description - https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html#benchmark gpus : 1 # Number of GPUs to train on. Set to `0` or None` to use CPU for training. # More detailed description - https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html#gpus","title":"Semantic segmentation on the Pascal VOC dataset"},{"location":"autoalbument/examples/cityscapes/#semantic-segmentation-on-the-pascal-voc-dataset","text":"The following files are also available on GitHub - https://github.com/albumentations-team/autoalbument/tree/master/examples/cityscapes","title":"Semantic segmentation on the Pascal VOC dataset"},{"location":"autoalbument/examples/cityscapes/#datasetpy","text":"import cv2 import numpy as np import torchvision cv2 . setNumThreads ( 0 ) cv2 . ocl . setUseOpenCL ( False ) class CityscapesSearchDataset ( torchvision . datasets . Cityscapes ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs , target_type = \"semantic\" ) self . semantic_target_type_index = [ i for i , t in enumerate ( self . target_type ) if t == \"semantic\" ][ 0 ] self . colormap = self . _generate_colormap () def _generate_colormap ( self ): colormap = {} for class_ in self . classes : if class_ . train_id in ( - 1 , 255 ): continue colormap [ class_ . train_id ] = class_ . id return colormap def _convert_to_segmentation_mask ( self , mask ): height , width = mask . shape [: 2 ] segmentation_mask = np . zeros (( height , width , len ( self . colormap )), dtype = np . float32 ) for label_index , label in self . colormap . items (): segmentation_mask [:, :, label_index ] = ( mask == label ) . astype ( float ) return segmentation_mask def __getitem__ ( self , index ): image = cv2 . imread ( self . images [ index ]) image = cv2 . cvtColor ( image , cv2 . COLOR_BGR2RGB ) mask = cv2 . imread ( self . targets [ index ][ self . semantic_target_type_index ], cv2 . IMREAD_UNCHANGED ) mask = self . _convert_to_segmentation_mask ( mask ) if self . transform is not None : transformed = self . transform ( image = image , mask = mask ) image = transformed [ \"image\" ] mask = transformed [ \"mask\" ] return image , mask","title":"dataset.py"},{"location":"autoalbument/examples/cityscapes/#searchyaml","text":"# @package _global_ _version : 2 # An internal value that indicates a version of the config schema. This value is used by # `autoalbument-search` and `autoalbument-migrate` to upgrade the config to the latest version if necessary. # Please do not change it manually. task : semantic_segmentation # Deep learning task. Should either be `classification` or `semantic_segmentation`. policy_model : # Settings for Policy Model that searches augmentation policies. task_factor : 0.1 # Multiplier for segmentation loss of a model. Faster AutoAugment uses segmentation loss to prevent augmentations # from transforming images of a particular class to another class. gp_factor : 10 # Multiplier for the gradient penalty for WGAN-GP training. 10 is the default value that was proposed in # `Improved Training of Wasserstein GANs`. temperature : 0.05 # Temperature for Relaxed Bernoulli distribution. The probability of applying a certain augmentation is sampled from # Relaxed Bernoulli distribution (because Bernoulli distribution is not differentiable). With lower values of # `temperature` Relaxed Bernoulli distribution behaves like Bernoulli distribution. In the paper, the authors # of Faster AutoAugment used 0.05 as a default value for `temperature`. num_sub_policies : 25 # Number of augmentation sub-policies. When an image passes through an augmentation pipeline, Faster AutoAugment # randomly chooses one sub-policy and uses augmentations from that sub-policy to transform an input image. A larger # number of sub-policies leads to a more diverse set of augmentations and better performance of a model trained on # augmented images. However, an increase in the number of sub-policies leads to the exponential growth of a search # space of augmentations, so you need more training data for Policy Model to find good augmentation policies. num_chunks : 4 # Number of chunks in a batch. Faster AutoAugment splits each batch of images into `num_chunks` chunks. Then it # applies the same sub-policy with the same parameters to each image in a chunk. This parameter controls the tradeoff # between the speed of augmentation search and diversity of augmentations. Larger `num_chunks` values will lead to # faster searching but less diverse set of augmentations. Note that this parameter is used only in the searching # phase. When you train a model with found sub-policies, Albumentations will apply a distinct set of transformations # to each image separately. operation_count : 4 # Number of consecutive augmentations in each sub-policy. Faster AutoAugment will sequentially apply `operation_count` # augmentations from a sub-policy to an image. Larger values of `operation_count` lead to better performance of # a model trained on augmented images. Simultaneously, larger values of `operation_count` affect the speed of search # and increase the searching time. semantic_segmentation_model : # Settings for Semantic Segmentation Model that is used for two purposes: # 1. As a model that performs semantic segmentation of input images. # 2. As a Discriminator for Policy Model. _target_ : autoalbument.faster_autoaugment.models.SemanticSegmentationModel # By default, AutoAlbument uses an instance of `autoalbument.faster_autoaugment.models.SemanticSegmentationModel` as # a semantic segmentation model. # This model takes four parameters: `num_classes`, `architecture`, `encoder_architecture` and `pretrained`. num_classes : 19 # The number of classes in the dataset. The dataset implementation should return a mask as a NumPy array with # the shape [height, width, num_classes]. In a case of binary segmentation you can set `num_classes` to 1. architecture : DeepLabV3Plus # The architecture of Semantic Segmentation Model. AutoAlbument uses models from # https://github.com/qubvel/segmentation_models.pytorch. Please refer to its documentation to get a list of available # models - https://github.com/qubvel/segmentation_models.pytorch#models-. encoder_architecture : resnet50 # The architecture of encoder in Semantic Segmentation Model. Please refer to Segmentation Models' documentation to # get a list of available encoders - https://github.com/qubvel/segmentation_models.pytorch#encoders- pretrained : true # Either boolean flag or string with that indicates whether the selected encoder architecture should load pretrained # weights or use randomly initialized weights. # - In the case of boolean flag `true` means using pretrained weights from ImageNet and `false` means using randomly # initialized weights. # - In the case of string the value should specify the name of the weights. For the list of available weights please # refer to https://github.com/qubvel/segmentation_models.pytorch#encoders- data : dataset : _target_ : dataset.CityscapesSearchDataset root : ~/data/cityscapes/data split : train # Class for the PyTorch Dataset and arguments to it. AutoAlbument will create an object of this class using # the `instantiate` method from Hydra - https://hydra.cc/docs/next/patterns/instantiate_objects/overview/. # # Note that the target class value in the `_target_` argument should be located inside PYTHONPATH so Hydra could # find it. The directory with the config file is automatically added to PYTHONPATH, so the default value # `dataset.SearchDataset` points to the class `SearchDataset` from the `dataset.py` file. This `dataset.py` file is # located along with the `search.yaml` file in the same directory provided by `--config-dir`. # # As an alternative, you could provide a path to a Python file with the dataset using the `dataset_file` parameter # instead of the `dataset` parameter. The Python file should contain the implementation of a PyTorch dataset for # augmentation search. The dataset class should have named `SearchDataset`. The value in `dataset_file` could either # be a relative or an absolute path ; in the case of a relative path, the path should be relative to this config # file's location. # # - Example of a relative path: # dataset_file: dataset.py # # - Example of an absolute path: # dataset_file: /projects/pytorch/dataset.py # input_dtype : uint8 # The data type of input images. Two values are supported: # - uint8. In that case, all input images should be NumPy arrays with the np.uint8 data type and values in the range # [0, 255]. # - float32. In that case, all input images should be NumPy arrays with the np.float32 data type and values in the # range [0.0, 1.0]. preprocessing : - LongestMaxSize : max_size : 256 - PadIfNeeded : min_height : 256 min_width : 256 border_mode : 0 value : [ 0 , 0 , 0 ] # A list of preprocessing augmentations that will be applied to each image before applying augmentations from # a policy. A preprocessing augmentation should be defined as `key`: `value`, where `key` is the name of augmentation # from Albumentations, and `value` is a dictionary with augmentation parameters. The found policy will also apply # those preprocessing augmentations before applying the main augmentations. # # Here is an example of an augmentation pipeline that first pads an image to the size 512x512 pixels, then resizes # the resulting image to the size 256x256 pixels and finally crops a random patch with the size 224x224 pixels. # # preprocessing: # - PadIfNeeded: # min_height: 512 # min_width: 512 # - Resize: # height: 256 # width: 256 # - RandomCrop: # height: 224 # width: 224 # normalization : mean : [ 0.485 , 0.456 , 0.406 ] std : [ 0.229 , 0.224 , 0.225 ] # Normalization values for images. For each image, the search pipeline will subtract `mean` and divide by `std`. # Normalization is applied after transforms defined in `preprocessing`. Note that regardless of `input_dtype`, # the normalization function will always receive a `float32` input with values in the range [0.0, 1.0], so you should # define `mean` and `std` values accordingly. ImageNet normalization is used by default. dataloader : _target_ : torch.utils.data.DataLoader batch_size : 32 shuffle : true num_workers : 8 pin_memory : true drop_last : true # Parameters for the PyTorch DataLoader. Please refer to the PyTorch documentation for the description of parameters - # https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader. optim : main : _target_ : torch.optim.Adam lr : 1e-3 betas : [ 0 , 0.999 ] # Optimizer configuration for the main (either Classification or Semantic Segmentation) Model policy : _target_ : torch.optim.Adam lr : 1e-3 betas : [ 0 , 0.999 ] # Optimizer configuration for Policy Model seed : 42 # Random seed. If the value is not null, it will be passed to `seed_everything` - # https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.utilities.seed.html?highlight=seed_everything hydra : run : dir : ${config_dir:}/outputs/${now:%Y-%m-%d}/${now:%H-%M-%S} # Path to the directory that will contain all outputs produced by the search algorithm. `${config_dir:}` contains # path to the directory with the `search.yaml` config file. Please refer to the Hydra documentation for more # information - https://hydra.cc/docs/configure_hydra/workdir. trainer : # Configuration for PyTorch Lightning Trainer. You can read more about Trainer and its arguments at # https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html. max_epochs : 50 # Number of epochs to search for augmentation parameters. # More detailed description - https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html#max-epochs benchmark : true # If true enables cudnn.benchmark. # More detailed description - https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html#benchmark gpus : 1 # Number of GPUs to train on. Set to `0` or None` to use CPU for training. # More detailed description - https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html#gpus","title":"search.yaml"},{"location":"autoalbument/examples/imagenet/","text":"Image classification on the ImageNet dataset \u00b6 The following files are also available on GitHub - https://github.com/albumentations-team/autoalbument/tree/master/examples/imagenet dataset.py \u00b6 import cv2 import torchvision cv2 . setNumThreads ( 0 ) cv2 . ocl . setUseOpenCL ( False ) class ImageNetSearchDataset ( torchvision . datasets . ImageNet ): def __getitem__ ( self , index ): path , label = self . samples [ index ] image = cv2 . imread ( path ) image = cv2 . cvtColor ( image , cv2 . COLOR_BGR2RGB ) if self . transform is not None : transformed = self . transform ( image = image ) image = transformed [ \"image\" ] return image , label search.yaml \u00b6 # @package _global_ _version : 2 # An internal value that indicates a version of the config schema. This value is used by # `autoalbument-search` and `autoalbument-migrate` to upgrade the config to the latest version if necessary. # Please do not change it manually. task : classification # Deep learning task. Should either be `classification` or `semantic_segmentation`. policy_model : # Settings for Policy Model that searches augmentation policies. task_factor : 0.1 # Multiplier for classification loss of a model. Faster AutoAugment uses classification loss to prevent augmentations # from transforming images of a particular class to another class. The authors of Faster AutoAugment use 0.1 as # default value. gp_factor : 10 # Multiplier for the gradient penalty for WGAN-GP training. 10 is the default value that was proposed in # `Improved Training of Wasserstein GANs`. temperature : 0.05 # Temperature for Relaxed Bernoulli distribution. The probability of applying a certain augmentation is sampled from # Relaxed Bernoulli distribution (because Bernoulli distribution is not differentiable). With lower values of # `temperature` Relaxed Bernoulli distribution behaves like Bernoulli distribution. In the paper, the authors # of Faster AutoAugment used 0.05 as a default value for `temperature`. num_sub_policies : 100 # Number of augmentation sub-policies. When an image passes through an augmentation pipeline, Faster AutoAugment # randomly chooses one sub-policy and uses augmentations from that sub-policy to transform an input image. A larger # number of sub-policies leads to a more diverse set of augmentations and better performance of a model trained on # augmented images. However, an increase in the number of sub-policies leads to the exponential growth of a search # space of augmentations, so you need more training data for Policy Model to find good augmentation policies. num_chunks : 6 # Number of chunks in a batch. Faster AutoAugment splits each batch of images into `num_chunks` chunks. Then it # applies the same sub-policy with the same parameters to each image in a chunk. This parameter controls the tradeoff # between the speed of augmentation search and diversity of augmentations. Larger `num_chunks` values will lead to # faster searching but less diverse set of augmentations. Note that this parameter is used only in the searching # phase. When you train a model with found sub-policies, Albumentations will apply a distinct set of transformations # to each image separately. operation_count : 4 # Number of consecutive augmentations in each sub-policy. Faster AutoAugment will sequentially apply `operation_count` # augmentations from a sub-policy to an image. Larger values of `operation_count` lead to better performance of # a model trained on augmented images. Simultaneously, larger values of `operation_count` affect the speed of search # and increase the searching time. classification_model : # Settings for Classification Model that is used for two purposes: # 1. As a model that performs classification of input images. # 2. As a Discriminator for Policy Model. num_classes : 1000 # Number of classes in the dataset. The dataset implementation should return an integer in the range # [0, num_classes - 1] as a class label of an image. architecture : resnet50 # The architecture of Classification Model. AutoAlbument uses models from # https://github.com/rwightman/pytorch-image-models/. Please refer to its documentation to get a list of available # models - https://rwightman.github.io/pytorch-image-models/#list-models-with-pretrained-weights. pretrained : false # Boolean flag that indicates whether the selected model architecture should load pretrained weights or use randomly # initialized weights. data : dataset : _target_ : dataset.ImageNetSearchDataset root : ~/data/imagenet split : train # Class for the PyTorch Dataset and arguments to it. AutoAlbument will create an object of this class using # the `instantiate` method from Hydra - https://hydra.cc/docs/next/patterns/instantiate_objects/overview/. # # Note that the target class value in the `_target_` argument should be located inside PYTHONPATH so Hydra could # find it. The directory with the config file is automatically added to PYTHONPATH, so the default value # `dataset.SearchDataset` points to the class `SearchDataset` from the `dataset.py` file. This `dataset.py` file is # located along with the `search.yaml` file in the same directory provided by `--config-dir`. # # As an alternative, you could provide a path to a Python file with the dataset using the `dataset_file` parameter # instead of the `dataset` parameter. The Python file should contain the implementation of a PyTorch dataset for # augmentation search. The dataset class should have named `SearchDataset`. The value in `dataset_file` could either # be a relative or an absolute path ; in the case of a relative path, the path should be relative to this config # file's location. # # - Example of a relative path: # dataset_file: dataset.py # # - Example of an absolute path: # dataset_file: /projects/pytorch/dataset.py # input_dtype : uint8 # The data type of input images. Two values are supported: # - uint8. In that case, all input images should be NumPy arrays with the np.uint8 data type and values in the range # [0, 255]. # - float32. In that case, all input images should be NumPy arrays with the np.float32 data type and values in the # range [0.0, 1.0]. preprocessing : - Resize : height : 256 width : 256 - RandomCrop : height : 224 width : 224 # A list of preprocessing augmentations that will be applied to each image before applying augmentations from # a policy. A preprocessing augmentation should be defined as `key`: `value`, where `key` is the name of augmentation # from Albumentations, and `value` is a dictionary with augmentation parameters. The found policy will also apply # those preprocessing augmentations before applying the main augmentations. # # Here is an example of an augmentation pipeline that first pads an image to the size 512x512 pixels, then resizes # the resulting image to the size 256x256 pixels and finally crops a random patch with the size 224x224 pixels. # # preprocessing: # - PadIfNeeded: # min_height: 512 # min_width: 512 # - Resize: # height: 256 # width: 256 # - RandomCrop: # height: 224 # width: 224 # normalization : mean : [ 0.485 , 0.456 , 0.406 ] std : [ 0.229 , 0.224 , 0.225 ] # Normalization values for images. For each image, the search pipeline will subtract `mean` and divide by `std`. # Normalization is applied after transforms defined in `preprocessing`. Note that regardless of `input_dtype`, # the normalization function will always receive a `float32` input with values in the range [0.0, 1.0], so you should # define `mean` and `std` values accordingly. dataloader : _target_ : torch.utils.data.DataLoader batch_size : 96 shuffle : true num_workers : 8 pin_memory : true drop_last : true # Parameters for the PyTorch DataLoader. Please refer to the PyTorch documentation for the description of parameters - # https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader. optim : main : _target_ : torch.optim.Adam lr : 1e-3 betas : [ 0 , 0.999 ] # Optimizer configuration for the main (either Classification or Semantic Segmentation) Model policy : _target_ : torch.optim.Adam lr : 1e-3 betas : [ 0 , 0.999 ] # Optimizer configuration for Policy Model seed : 42 # Random seed. If the value is not null, it will be passed to `seed_everything` - # https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.utilities.seed.html?highlight=seed_everything hydra : run : dir : ${config_dir:}/outputs/${now:%Y-%m-%d}/${now:%H-%M-%S} # Path to the directory that will contain all outputs produced by the search algorithm. `${config_dir:}` contains # path to the directory with the `search.yaml` config file. Please refer to the Hydra documentation for more # information - https://hydra.cc/docs/configure_hydra/workdir. trainer : # Configuration for PyTorch Lightning Trainer. You can read more about Trainer and its arguments at # https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html. max_epochs : 1 # Number of epochs to search for augmentation parameters. # More detailed description - https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html#max-epochs benchmark : true # If true enables cudnn.benchmark. # More detailed description - https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html#benchmark gpus : 1 # Number of GPUs to train on. Set to `0` or None` to use CPU for training. # More detailed description - https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html#gpus","title":"Image classification on the ImageNet dataset"},{"location":"autoalbument/examples/imagenet/#image-classification-on-the-imagenet-dataset","text":"The following files are also available on GitHub - https://github.com/albumentations-team/autoalbument/tree/master/examples/imagenet","title":"Image classification on the ImageNet dataset"},{"location":"autoalbument/examples/imagenet/#datasetpy","text":"import cv2 import torchvision cv2 . setNumThreads ( 0 ) cv2 . ocl . setUseOpenCL ( False ) class ImageNetSearchDataset ( torchvision . datasets . ImageNet ): def __getitem__ ( self , index ): path , label = self . samples [ index ] image = cv2 . imread ( path ) image = cv2 . cvtColor ( image , cv2 . COLOR_BGR2RGB ) if self . transform is not None : transformed = self . transform ( image = image ) image = transformed [ \"image\" ] return image , label","title":"dataset.py"},{"location":"autoalbument/examples/imagenet/#searchyaml","text":"# @package _global_ _version : 2 # An internal value that indicates a version of the config schema. This value is used by # `autoalbument-search` and `autoalbument-migrate` to upgrade the config to the latest version if necessary. # Please do not change it manually. task : classification # Deep learning task. Should either be `classification` or `semantic_segmentation`. policy_model : # Settings for Policy Model that searches augmentation policies. task_factor : 0.1 # Multiplier for classification loss of a model. Faster AutoAugment uses classification loss to prevent augmentations # from transforming images of a particular class to another class. The authors of Faster AutoAugment use 0.1 as # default value. gp_factor : 10 # Multiplier for the gradient penalty for WGAN-GP training. 10 is the default value that was proposed in # `Improved Training of Wasserstein GANs`. temperature : 0.05 # Temperature for Relaxed Bernoulli distribution. The probability of applying a certain augmentation is sampled from # Relaxed Bernoulli distribution (because Bernoulli distribution is not differentiable). With lower values of # `temperature` Relaxed Bernoulli distribution behaves like Bernoulli distribution. In the paper, the authors # of Faster AutoAugment used 0.05 as a default value for `temperature`. num_sub_policies : 100 # Number of augmentation sub-policies. When an image passes through an augmentation pipeline, Faster AutoAugment # randomly chooses one sub-policy and uses augmentations from that sub-policy to transform an input image. A larger # number of sub-policies leads to a more diverse set of augmentations and better performance of a model trained on # augmented images. However, an increase in the number of sub-policies leads to the exponential growth of a search # space of augmentations, so you need more training data for Policy Model to find good augmentation policies. num_chunks : 6 # Number of chunks in a batch. Faster AutoAugment splits each batch of images into `num_chunks` chunks. Then it # applies the same sub-policy with the same parameters to each image in a chunk. This parameter controls the tradeoff # between the speed of augmentation search and diversity of augmentations. Larger `num_chunks` values will lead to # faster searching but less diverse set of augmentations. Note that this parameter is used only in the searching # phase. When you train a model with found sub-policies, Albumentations will apply a distinct set of transformations # to each image separately. operation_count : 4 # Number of consecutive augmentations in each sub-policy. Faster AutoAugment will sequentially apply `operation_count` # augmentations from a sub-policy to an image. Larger values of `operation_count` lead to better performance of # a model trained on augmented images. Simultaneously, larger values of `operation_count` affect the speed of search # and increase the searching time. classification_model : # Settings for Classification Model that is used for two purposes: # 1. As a model that performs classification of input images. # 2. As a Discriminator for Policy Model. num_classes : 1000 # Number of classes in the dataset. The dataset implementation should return an integer in the range # [0, num_classes - 1] as a class label of an image. architecture : resnet50 # The architecture of Classification Model. AutoAlbument uses models from # https://github.com/rwightman/pytorch-image-models/. Please refer to its documentation to get a list of available # models - https://rwightman.github.io/pytorch-image-models/#list-models-with-pretrained-weights. pretrained : false # Boolean flag that indicates whether the selected model architecture should load pretrained weights or use randomly # initialized weights. data : dataset : _target_ : dataset.ImageNetSearchDataset root : ~/data/imagenet split : train # Class for the PyTorch Dataset and arguments to it. AutoAlbument will create an object of this class using # the `instantiate` method from Hydra - https://hydra.cc/docs/next/patterns/instantiate_objects/overview/. # # Note that the target class value in the `_target_` argument should be located inside PYTHONPATH so Hydra could # find it. The directory with the config file is automatically added to PYTHONPATH, so the default value # `dataset.SearchDataset` points to the class `SearchDataset` from the `dataset.py` file. This `dataset.py` file is # located along with the `search.yaml` file in the same directory provided by `--config-dir`. # # As an alternative, you could provide a path to a Python file with the dataset using the `dataset_file` parameter # instead of the `dataset` parameter. The Python file should contain the implementation of a PyTorch dataset for # augmentation search. The dataset class should have named `SearchDataset`. The value in `dataset_file` could either # be a relative or an absolute path ; in the case of a relative path, the path should be relative to this config # file's location. # # - Example of a relative path: # dataset_file: dataset.py # # - Example of an absolute path: # dataset_file: /projects/pytorch/dataset.py # input_dtype : uint8 # The data type of input images. Two values are supported: # - uint8. In that case, all input images should be NumPy arrays with the np.uint8 data type and values in the range # [0, 255]. # - float32. In that case, all input images should be NumPy arrays with the np.float32 data type and values in the # range [0.0, 1.0]. preprocessing : - Resize : height : 256 width : 256 - RandomCrop : height : 224 width : 224 # A list of preprocessing augmentations that will be applied to each image before applying augmentations from # a policy. A preprocessing augmentation should be defined as `key`: `value`, where `key` is the name of augmentation # from Albumentations, and `value` is a dictionary with augmentation parameters. The found policy will also apply # those preprocessing augmentations before applying the main augmentations. # # Here is an example of an augmentation pipeline that first pads an image to the size 512x512 pixels, then resizes # the resulting image to the size 256x256 pixels and finally crops a random patch with the size 224x224 pixels. # # preprocessing: # - PadIfNeeded: # min_height: 512 # min_width: 512 # - Resize: # height: 256 # width: 256 # - RandomCrop: # height: 224 # width: 224 # normalization : mean : [ 0.485 , 0.456 , 0.406 ] std : [ 0.229 , 0.224 , 0.225 ] # Normalization values for images. For each image, the search pipeline will subtract `mean` and divide by `std`. # Normalization is applied after transforms defined in `preprocessing`. Note that regardless of `input_dtype`, # the normalization function will always receive a `float32` input with values in the range [0.0, 1.0], so you should # define `mean` and `std` values accordingly. dataloader : _target_ : torch.utils.data.DataLoader batch_size : 96 shuffle : true num_workers : 8 pin_memory : true drop_last : true # Parameters for the PyTorch DataLoader. Please refer to the PyTorch documentation for the description of parameters - # https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader. optim : main : _target_ : torch.optim.Adam lr : 1e-3 betas : [ 0 , 0.999 ] # Optimizer configuration for the main (either Classification or Semantic Segmentation) Model policy : _target_ : torch.optim.Adam lr : 1e-3 betas : [ 0 , 0.999 ] # Optimizer configuration for Policy Model seed : 42 # Random seed. If the value is not null, it will be passed to `seed_everything` - # https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.utilities.seed.html?highlight=seed_everything hydra : run : dir : ${config_dir:}/outputs/${now:%Y-%m-%d}/${now:%H-%M-%S} # Path to the directory that will contain all outputs produced by the search algorithm. `${config_dir:}` contains # path to the directory with the `search.yaml` config file. Please refer to the Hydra documentation for more # information - https://hydra.cc/docs/configure_hydra/workdir. trainer : # Configuration for PyTorch Lightning Trainer. You can read more about Trainer and its arguments at # https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html. max_epochs : 1 # Number of epochs to search for augmentation parameters. # More detailed description - https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html#max-epochs benchmark : true # If true enables cudnn.benchmark. # More detailed description - https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html#benchmark gpus : 1 # Number of GPUs to train on. Set to `0` or None` to use CPU for training. # More detailed description - https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html#gpus","title":"search.yaml"},{"location":"autoalbument/examples/list/","text":"List of examples \u00b6 Image classification on the CIFAR10 dataset . Image classification on the SVHN dataset . Image classification on the ImageNet dataset . Semantic segmentation on the Pascal VOC dataset . Semantic segmentation on the Cityscapes dataset . To run the search with an example config: autoalbument-search --config-dir </path/to/directory_with_dataset.py_and_search.yaml>","title":"List of examples"},{"location":"autoalbument/examples/list/#list-of-examples","text":"Image classification on the CIFAR10 dataset . Image classification on the SVHN dataset . Image classification on the ImageNet dataset . Semantic segmentation on the Pascal VOC dataset . Semantic segmentation on the Cityscapes dataset . To run the search with an example config: autoalbument-search --config-dir </path/to/directory_with_dataset.py_and_search.yaml>","title":"List of examples"},{"location":"autoalbument/examples/pascal_voc/","text":"Semantic segmentation on the Pascal VOC dataset \u00b6 The following files are also available on GitHub - https://github.com/albumentations-team/autoalbument/tree/master/examples/pascal_voc dataset.py \u00b6 import cv2 import numpy as np from torchvision.datasets import VOCSegmentation cv2 . setNumThreads ( 0 ) cv2 . ocl . setUseOpenCL ( False ) VOC_CLASSES = [ \"background\" , \"aeroplane\" , \"bicycle\" , \"bird\" , \"boat\" , \"bottle\" , \"bus\" , \"car\" , \"cat\" , \"chair\" , \"cow\" , \"diningtable\" , \"dog\" , \"horse\" , \"motorbike\" , \"person\" , \"potted plant\" , \"sheep\" , \"sofa\" , \"train\" , \"tv/monitor\" , ] VOC_COLORMAP = [ [ 0 , 0 , 0 ], [ 128 , 0 , 0 ], [ 0 , 128 , 0 ], [ 128 , 128 , 0 ], [ 0 , 0 , 128 ], [ 128 , 0 , 128 ], [ 0 , 128 , 128 ], [ 128 , 128 , 128 ], [ 64 , 0 , 0 ], [ 192 , 0 , 0 ], [ 64 , 128 , 0 ], [ 192 , 128 , 0 ], [ 64 , 0 , 128 ], [ 192 , 0 , 128 ], [ 64 , 128 , 128 ], [ 192 , 128 , 128 ], [ 0 , 64 , 0 ], [ 128 , 64 , 0 ], [ 0 , 192 , 0 ], [ 128 , 192 , 0 ], [ 0 , 64 , 128 ], ] class PascalVOCSearchDataset ( VOCSegmentation ): def __init__ ( self , root = \"~/data/pascal_voc\" , image_set = \"train\" , download = True , transform = None ): super () . __init__ ( root = root , image_set = image_set , download = download , transform = transform ) @staticmethod def _convert_to_segmentation_mask ( mask ): # This function converts a mask from the Pascal VOC format to the format required by AutoAlbument. # # Pascal VOC uses an RGB image to encode the segmentation mask for that image. RGB values of a pixel # encode the pixel's class. # # AutoAlbument requires a segmentation mask to be a NumPy array with the shape [height, width, num_classes]. # Each channel in this mask should encode values for a single class. Pixel in a mask channel should have # a value of 1.0 if the pixel of the image belongs to this class and 0.0 otherwise. height , width = mask . shape [: 2 ] segmentation_mask = np . zeros (( height , width , len ( VOC_COLORMAP )), dtype = np . float32 ) for label_index , label in enumerate ( VOC_COLORMAP ): segmentation_mask [:, :, label_index ] = np . all ( mask == label , axis =- 1 ) . astype ( float ) return segmentation_mask def __getitem__ ( self , index ): image = cv2 . imread ( self . images [ index ]) image = cv2 . cvtColor ( image , cv2 . COLOR_BGR2RGB ) mask = cv2 . imread ( self . masks [ index ]) mask = cv2 . cvtColor ( mask , cv2 . COLOR_BGR2RGB ) mask = self . _convert_to_segmentation_mask ( mask ) if self . transform is not None : transformed = self . transform ( image = image , mask = mask ) image = transformed [ \"image\" ] mask = transformed [ \"mask\" ] return image , mask search.yaml \u00b6 # @package _global_ _version : 2 # An internal value that indicates a version of the config schema. This value is used by # `autoalbument-search` and `autoalbument-migrate` to upgrade the config to the latest version if necessary. # Please do not change it manually. task : semantic_segmentation # Deep learning task. Should either be `classification` or `semantic_segmentation`. policy_model : # Settings for Policy Model that searches augmentation policies. task_factor : 0.1 # Multiplier for segmentation loss of a model. Faster AutoAugment uses segmentation loss to prevent augmentations # from transforming images of a particular class to another class. gp_factor : 10 # Multiplier for the gradient penalty for WGAN-GP training. 10 is the default value that was proposed in # `Improved Training of Wasserstein GANs`. temperature : 0.05 # Temperature for Relaxed Bernoulli distribution. The probability of applying a certain augmentation is sampled from # Relaxed Bernoulli distribution (because Bernoulli distribution is not differentiable). With lower values of # `temperature` Relaxed Bernoulli distribution behaves like Bernoulli distribution. In the paper, the authors # of Faster AutoAugment used 0.05 as a default value for `temperature`. num_sub_policies : 25 # Number of augmentation sub-policies. When an image passes through an augmentation pipeline, Faster AutoAugment # randomly chooses one sub-policy and uses augmentations from that sub-policy to transform an input image. A larger # number of sub-policies leads to a more diverse set of augmentations and better performance of a model trained on # augmented images. However, an increase in the number of sub-policies leads to the exponential growth of a search # space of augmentations, so you need more training data for Policy Model to find good augmentation policies. num_chunks : 4 # Number of chunks in a batch. Faster AutoAugment splits each batch of images into `num_chunks` chunks. Then it # applies the same sub-policy with the same parameters to each image in a chunk. This parameter controls the tradeoff # between the speed of augmentation search and diversity of augmentations. Larger `num_chunks` values will lead to # faster searching but less diverse set of augmentations. Note that this parameter is used only in the searching # phase. When you train a model with found sub-policies, Albumentations will apply a distinct set of transformations # to each image separately. operation_count : 4 # Number of consecutive augmentations in each sub-policy. Faster AutoAugment will sequentially apply `operation_count` # augmentations from a sub-policy to an image. Larger values of `operation_count` lead to better performance of # a model trained on augmented images. Simultaneously, larger values of `operation_count` affect the speed of search # and increase the searching time. semantic_segmentation_model : # Settings for Semantic Segmentation Model that is used for two purposes: # 1. As a model that performs semantic segmentation of input images. # 2. As a Discriminator for Policy Model. _target_ : autoalbument.faster_autoaugment.models.SemanticSegmentationModel # By default, AutoAlbument uses an instance of `autoalbument.faster_autoaugment.models.SemanticSegmentationModel` as # a semantic segmentation model. # This model takes four parameters: `num_classes`, `architecture`, `encoder_architecture` and `pretrained`. num_classes : 21 # The number of classes in the dataset. The dataset implementation should return a mask as a NumPy array with # the shape [height, width, num_classes]. In a case of binary segmentation you can set `num_classes` to 1. architecture : DeepLabV3Plus # The architecture of Semantic Segmentation Model. AutoAlbument uses models from # https://github.com/qubvel/segmentation_models.pytorch. Please refer to its documentation to get a list of available # models - https://github.com/qubvel/segmentation_models.pytorch#models-. encoder_architecture : resnet50 # The architecture of encoder in Semantic Segmentation Model. Please refer to Segmentation Models' documentation to # get a list of available encoders - https://github.com/qubvel/segmentation_models.pytorch#encoders- pretrained : true # Either boolean flag or string with that indicates whether the selected encoder architecture should load pretrained # weights or use randomly initialized weights. # - In the case of boolean flag `true` means using pretrained weights from ImageNet and `false` means using randomly # initialized weights. # - In the case of string the value should specify the name of the weights. For the list of available weights please # refer to https://github.com/qubvel/segmentation_models.pytorch#encoders- data : dataset : _target_ : dataset.PascalVOCSearchDataset root : ~/data/pascal_voc image_set : train download : false # Class for the PyTorch Dataset and arguments to it. AutoAlbument will create an object of this class using # the `instantiate` method from Hydra - https://hydra.cc/docs/next/patterns/instantiate_objects/overview/. # # Note that the target class value in the `_target_` argument should be located inside PYTHONPATH so Hydra could # find it. The directory with the config file is automatically added to PYTHONPATH, so the default value # `dataset.SearchDataset` points to the class `SearchDataset` from the `dataset.py` file. This `dataset.py` file is # located along with the `search.yaml` file in the same directory provided by `--config-dir`. # # As an alternative, you could provide a path to a Python file with the dataset using the `dataset_file` parameter # instead of the `dataset` parameter. The Python file should contain the implementation of a PyTorch dataset for # augmentation search. The dataset class should have named `SearchDataset`. The value in `dataset_file` could either # be a relative or an absolute path ; in the case of a relative path, the path should be relative to this config # file's location. # # - Example of a relative path: # dataset_file: dataset.py # # - Example of an absolute path: # dataset_file: /projects/pytorch/dataset.py # input_dtype : uint8 # The data type of input images. Two values are supported: # - uint8. In that case, all input images should be NumPy arrays with the np.uint8 data type and values in the range # [0, 255]. # - float32. In that case, all input images should be NumPy arrays with the np.float32 data type and values in the # range [0.0, 1.0]. preprocessing : - LongestMaxSize : max_size : 256 - PadIfNeeded : min_height : 256 min_width : 256 border_mode : 0 value : [ 0 , 0 , 0 ] # A list of preprocessing augmentations that will be applied to each image before applying augmentations from # a policy. A preprocessing augmentation should be defined as `key`: `value`, where `key` is the name of augmentation # from Albumentations, and `value` is a dictionary with augmentation parameters. The found policy will also apply # those preprocessing augmentations before applying the main augmentations. # # Here is an example of an augmentation pipeline that first pads an image to the size 512x512 pixels, then resizes # the resulting image to the size 256x256 pixels and finally crops a random patch with the size 224x224 pixels. # # preprocessing: # - PadIfNeeded: # min_height: 512 # min_width: 512 # - Resize: # height: 256 # width: 256 # - RandomCrop: # height: 224 # width: 224 # normalization : mean : [ 0.485 , 0.456 , 0.406 ] std : [ 0.229 , 0.224 , 0.225 ] # Normalization values for images. For each image, the search pipeline will subtract `mean` and divide by `std`. # Normalization is applied after transforms defined in `preprocessing`. Note that regardless of `input_dtype`, # the normalization function will always receive a `float32` input with values in the range [0.0, 1.0], so you should # define `mean` and `std` values accordingly. ImageNet normalization is used by default. dataloader : _target_ : torch.utils.data.DataLoader batch_size : 32 shuffle : true num_workers : 8 pin_memory : true drop_last : true # Parameters for the PyTorch DataLoader. Please refer to the PyTorch documentation for the description of parameters - # https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader. optim : main : _target_ : torch.optim.Adam lr : 1e-3 betas : [ 0 , 0.999 ] # Optimizer configuration for the main (either Classification or Semantic Segmentation) Model policy : _target_ : torch.optim.Adam lr : 1e-3 betas : [ 0 , 0.999 ] # Optimizer configuration for Policy Model seed : 42 # Random seed. If the value is not null, it will be passed to `seed_everything` - # https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.utilities.seed.html?highlight=seed_everything hydra : run : dir : ${config_dir:}/outputs/${now:%Y-%m-%d}/${now:%H-%M-%S} # Path to the directory that will contain all outputs produced by the search algorithm. `${config_dir:}` contains # path to the directory with the `search.yaml` config file. Please refer to the Hydra documentation for more # information - https://hydra.cc/docs/configure_hydra/workdir. trainer : # Configuration for PyTorch Lightning Trainer. You can read more about Trainer and its arguments at # https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html. max_epochs : 50 # Number of epochs to search for augmentation parameters. # More detailed description - https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html#max-epochs benchmark : true # If true enables cudnn.benchmark. # More detailed description - https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html#benchmark gpus : 1 # Number of GPUs to train on. Set to `0` or None` to use CPU for training. # More detailed description - https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html#gpus","title":"Semantic segmentation on the Pascal VOC dataset"},{"location":"autoalbument/examples/pascal_voc/#semantic-segmentation-on-the-pascal-voc-dataset","text":"The following files are also available on GitHub - https://github.com/albumentations-team/autoalbument/tree/master/examples/pascal_voc","title":"Semantic segmentation on the Pascal VOC dataset"},{"location":"autoalbument/examples/pascal_voc/#datasetpy","text":"import cv2 import numpy as np from torchvision.datasets import VOCSegmentation cv2 . setNumThreads ( 0 ) cv2 . ocl . setUseOpenCL ( False ) VOC_CLASSES = [ \"background\" , \"aeroplane\" , \"bicycle\" , \"bird\" , \"boat\" , \"bottle\" , \"bus\" , \"car\" , \"cat\" , \"chair\" , \"cow\" , \"diningtable\" , \"dog\" , \"horse\" , \"motorbike\" , \"person\" , \"potted plant\" , \"sheep\" , \"sofa\" , \"train\" , \"tv/monitor\" , ] VOC_COLORMAP = [ [ 0 , 0 , 0 ], [ 128 , 0 , 0 ], [ 0 , 128 , 0 ], [ 128 , 128 , 0 ], [ 0 , 0 , 128 ], [ 128 , 0 , 128 ], [ 0 , 128 , 128 ], [ 128 , 128 , 128 ], [ 64 , 0 , 0 ], [ 192 , 0 , 0 ], [ 64 , 128 , 0 ], [ 192 , 128 , 0 ], [ 64 , 0 , 128 ], [ 192 , 0 , 128 ], [ 64 , 128 , 128 ], [ 192 , 128 , 128 ], [ 0 , 64 , 0 ], [ 128 , 64 , 0 ], [ 0 , 192 , 0 ], [ 128 , 192 , 0 ], [ 0 , 64 , 128 ], ] class PascalVOCSearchDataset ( VOCSegmentation ): def __init__ ( self , root = \"~/data/pascal_voc\" , image_set = \"train\" , download = True , transform = None ): super () . __init__ ( root = root , image_set = image_set , download = download , transform = transform ) @staticmethod def _convert_to_segmentation_mask ( mask ): # This function converts a mask from the Pascal VOC format to the format required by AutoAlbument. # # Pascal VOC uses an RGB image to encode the segmentation mask for that image. RGB values of a pixel # encode the pixel's class. # # AutoAlbument requires a segmentation mask to be a NumPy array with the shape [height, width, num_classes]. # Each channel in this mask should encode values for a single class. Pixel in a mask channel should have # a value of 1.0 if the pixel of the image belongs to this class and 0.0 otherwise. height , width = mask . shape [: 2 ] segmentation_mask = np . zeros (( height , width , len ( VOC_COLORMAP )), dtype = np . float32 ) for label_index , label in enumerate ( VOC_COLORMAP ): segmentation_mask [:, :, label_index ] = np . all ( mask == label , axis =- 1 ) . astype ( float ) return segmentation_mask def __getitem__ ( self , index ): image = cv2 . imread ( self . images [ index ]) image = cv2 . cvtColor ( image , cv2 . COLOR_BGR2RGB ) mask = cv2 . imread ( self . masks [ index ]) mask = cv2 . cvtColor ( mask , cv2 . COLOR_BGR2RGB ) mask = self . _convert_to_segmentation_mask ( mask ) if self . transform is not None : transformed = self . transform ( image = image , mask = mask ) image = transformed [ \"image\" ] mask = transformed [ \"mask\" ] return image , mask","title":"dataset.py"},{"location":"autoalbument/examples/pascal_voc/#searchyaml","text":"# @package _global_ _version : 2 # An internal value that indicates a version of the config schema. This value is used by # `autoalbument-search` and `autoalbument-migrate` to upgrade the config to the latest version if necessary. # Please do not change it manually. task : semantic_segmentation # Deep learning task. Should either be `classification` or `semantic_segmentation`. policy_model : # Settings for Policy Model that searches augmentation policies. task_factor : 0.1 # Multiplier for segmentation loss of a model. Faster AutoAugment uses segmentation loss to prevent augmentations # from transforming images of a particular class to another class. gp_factor : 10 # Multiplier for the gradient penalty for WGAN-GP training. 10 is the default value that was proposed in # `Improved Training of Wasserstein GANs`. temperature : 0.05 # Temperature for Relaxed Bernoulli distribution. The probability of applying a certain augmentation is sampled from # Relaxed Bernoulli distribution (because Bernoulli distribution is not differentiable). With lower values of # `temperature` Relaxed Bernoulli distribution behaves like Bernoulli distribution. In the paper, the authors # of Faster AutoAugment used 0.05 as a default value for `temperature`. num_sub_policies : 25 # Number of augmentation sub-policies. When an image passes through an augmentation pipeline, Faster AutoAugment # randomly chooses one sub-policy and uses augmentations from that sub-policy to transform an input image. A larger # number of sub-policies leads to a more diverse set of augmentations and better performance of a model trained on # augmented images. However, an increase in the number of sub-policies leads to the exponential growth of a search # space of augmentations, so you need more training data for Policy Model to find good augmentation policies. num_chunks : 4 # Number of chunks in a batch. Faster AutoAugment splits each batch of images into `num_chunks` chunks. Then it # applies the same sub-policy with the same parameters to each image in a chunk. This parameter controls the tradeoff # between the speed of augmentation search and diversity of augmentations. Larger `num_chunks` values will lead to # faster searching but less diverse set of augmentations. Note that this parameter is used only in the searching # phase. When you train a model with found sub-policies, Albumentations will apply a distinct set of transformations # to each image separately. operation_count : 4 # Number of consecutive augmentations in each sub-policy. Faster AutoAugment will sequentially apply `operation_count` # augmentations from a sub-policy to an image. Larger values of `operation_count` lead to better performance of # a model trained on augmented images. Simultaneously, larger values of `operation_count` affect the speed of search # and increase the searching time. semantic_segmentation_model : # Settings for Semantic Segmentation Model that is used for two purposes: # 1. As a model that performs semantic segmentation of input images. # 2. As a Discriminator for Policy Model. _target_ : autoalbument.faster_autoaugment.models.SemanticSegmentationModel # By default, AutoAlbument uses an instance of `autoalbument.faster_autoaugment.models.SemanticSegmentationModel` as # a semantic segmentation model. # This model takes four parameters: `num_classes`, `architecture`, `encoder_architecture` and `pretrained`. num_classes : 21 # The number of classes in the dataset. The dataset implementation should return a mask as a NumPy array with # the shape [height, width, num_classes]. In a case of binary segmentation you can set `num_classes` to 1. architecture : DeepLabV3Plus # The architecture of Semantic Segmentation Model. AutoAlbument uses models from # https://github.com/qubvel/segmentation_models.pytorch. Please refer to its documentation to get a list of available # models - https://github.com/qubvel/segmentation_models.pytorch#models-. encoder_architecture : resnet50 # The architecture of encoder in Semantic Segmentation Model. Please refer to Segmentation Models' documentation to # get a list of available encoders - https://github.com/qubvel/segmentation_models.pytorch#encoders- pretrained : true # Either boolean flag or string with that indicates whether the selected encoder architecture should load pretrained # weights or use randomly initialized weights. # - In the case of boolean flag `true` means using pretrained weights from ImageNet and `false` means using randomly # initialized weights. # - In the case of string the value should specify the name of the weights. For the list of available weights please # refer to https://github.com/qubvel/segmentation_models.pytorch#encoders- data : dataset : _target_ : dataset.PascalVOCSearchDataset root : ~/data/pascal_voc image_set : train download : false # Class for the PyTorch Dataset and arguments to it. AutoAlbument will create an object of this class using # the `instantiate` method from Hydra - https://hydra.cc/docs/next/patterns/instantiate_objects/overview/. # # Note that the target class value in the `_target_` argument should be located inside PYTHONPATH so Hydra could # find it. The directory with the config file is automatically added to PYTHONPATH, so the default value # `dataset.SearchDataset` points to the class `SearchDataset` from the `dataset.py` file. This `dataset.py` file is # located along with the `search.yaml` file in the same directory provided by `--config-dir`. # # As an alternative, you could provide a path to a Python file with the dataset using the `dataset_file` parameter # instead of the `dataset` parameter. The Python file should contain the implementation of a PyTorch dataset for # augmentation search. The dataset class should have named `SearchDataset`. The value in `dataset_file` could either # be a relative or an absolute path ; in the case of a relative path, the path should be relative to this config # file's location. # # - Example of a relative path: # dataset_file: dataset.py # # - Example of an absolute path: # dataset_file: /projects/pytorch/dataset.py # input_dtype : uint8 # The data type of input images. Two values are supported: # - uint8. In that case, all input images should be NumPy arrays with the np.uint8 data type and values in the range # [0, 255]. # - float32. In that case, all input images should be NumPy arrays with the np.float32 data type and values in the # range [0.0, 1.0]. preprocessing : - LongestMaxSize : max_size : 256 - PadIfNeeded : min_height : 256 min_width : 256 border_mode : 0 value : [ 0 , 0 , 0 ] # A list of preprocessing augmentations that will be applied to each image before applying augmentations from # a policy. A preprocessing augmentation should be defined as `key`: `value`, where `key` is the name of augmentation # from Albumentations, and `value` is a dictionary with augmentation parameters. The found policy will also apply # those preprocessing augmentations before applying the main augmentations. # # Here is an example of an augmentation pipeline that first pads an image to the size 512x512 pixels, then resizes # the resulting image to the size 256x256 pixels and finally crops a random patch with the size 224x224 pixels. # # preprocessing: # - PadIfNeeded: # min_height: 512 # min_width: 512 # - Resize: # height: 256 # width: 256 # - RandomCrop: # height: 224 # width: 224 # normalization : mean : [ 0.485 , 0.456 , 0.406 ] std : [ 0.229 , 0.224 , 0.225 ] # Normalization values for images. For each image, the search pipeline will subtract `mean` and divide by `std`. # Normalization is applied after transforms defined in `preprocessing`. Note that regardless of `input_dtype`, # the normalization function will always receive a `float32` input with values in the range [0.0, 1.0], so you should # define `mean` and `std` values accordingly. ImageNet normalization is used by default. dataloader : _target_ : torch.utils.data.DataLoader batch_size : 32 shuffle : true num_workers : 8 pin_memory : true drop_last : true # Parameters for the PyTorch DataLoader. Please refer to the PyTorch documentation for the description of parameters - # https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader. optim : main : _target_ : torch.optim.Adam lr : 1e-3 betas : [ 0 , 0.999 ] # Optimizer configuration for the main (either Classification or Semantic Segmentation) Model policy : _target_ : torch.optim.Adam lr : 1e-3 betas : [ 0 , 0.999 ] # Optimizer configuration for Policy Model seed : 42 # Random seed. If the value is not null, it will be passed to `seed_everything` - # https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.utilities.seed.html?highlight=seed_everything hydra : run : dir : ${config_dir:}/outputs/${now:%Y-%m-%d}/${now:%H-%M-%S} # Path to the directory that will contain all outputs produced by the search algorithm. `${config_dir:}` contains # path to the directory with the `search.yaml` config file. Please refer to the Hydra documentation for more # information - https://hydra.cc/docs/configure_hydra/workdir. trainer : # Configuration for PyTorch Lightning Trainer. You can read more about Trainer and its arguments at # https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html. max_epochs : 50 # Number of epochs to search for augmentation parameters. # More detailed description - https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html#max-epochs benchmark : true # If true enables cudnn.benchmark. # More detailed description - https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html#benchmark gpus : 1 # Number of GPUs to train on. Set to `0` or None` to use CPU for training. # More detailed description - https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html#gpus","title":"search.yaml"},{"location":"autoalbument/examples/svhn/","text":"Image classification on the SVHN dataset \u00b6 The following files are also available on GitHub - https://github.com/albumentations-team/autoalbument/tree/master/examples/svhn dataset.py \u00b6 import cv2 import numpy as np import torch import torchvision cv2 . setNumThreads ( 0 ) cv2 . ocl . setUseOpenCL ( False ) class SVHNSearchDataset ( torchvision . datasets . SVHN ): def __getitem__ ( self , index ): image , label = self . data [ index ], int ( self . labels [ index ]) image = np . transpose ( image , ( 1 , 2 , 0 )) if self . transform is not None : transformed = self . transform ( image = image ) image = transformed [ \"image\" ] return image , label class ConcatSVHNSearchDataset ( torch . utils . data . ConcatDataset ): def __init__ ( self , root , download , transform = None ): datasets = [ SVHNSearchDataset ( root = root , split = \"train\" , download = download , transform = transform ), SVHNSearchDataset ( root = root , split = \"extra\" , download = download , transform = transform ), ] super () . __init__ ( datasets ) search.yaml \u00b6 # @package _global_ _version : 2 # An internal value that indicates a version of the config schema. This value is used by # `autoalbument-search` and `autoalbument-migrate` to upgrade the config to the latest version if necessary. # Please do not change it manually. task : classification # Deep learning task. Should either be `classification` or `semantic_segmentation`. # Settings for Policy Model that searches augmentation policies. policy_model : # Multiplier for classification loss of a model. Faster AutoAugment uses classification loss to prevent augmentations # from transforming images of a particular class to another class. The authors of Faster AutoAugment use 0.1 as # default value. task_factor : 0.1 # Multiplier for the gradient penalty for WGAN-GP training. 10 is the default value that was proposed in # `Improved Training of Wasserstein GANs`. gp_factor : 10 # Temperature for Relaxed Bernoulli distribution. The probability of applying a certain augmentation is sampled from # Relaxed Bernoulli distribution (because Bernoulli distribution is not differentiable). With lower values of # `temperature` Relaxed Bernoulli distribution behaves like Bernoulli distribution. In the paper, the authors # of Faster AutoAugment used 0.05 as a default value for `temperature`. temperature : 0.05 # Number of augmentation sub-policies. When an image passes through an augmentation pipeline, Faster AutoAugment # randomly chooses one sub-policy and uses augmentations from that sub-policy to transform an input image. A larger # number of sub-policies leads to a more diverse set of augmentations and better performance of a model trained on # augmented images. However, an increase in the number of sub-policies leads to the exponential growth of a search # space of augmentations, so you need more training data for Policy Model to find good augmentation policies. num_sub_policies : 100 # Number of chunks in a batch. Faster AutoAugment splits each batch of images into `num_chunks` chunks. Then it # applies the same sub-policy with the same parameters to each image in a chunk. This parameter controls the tradeoff # between the speed of augmentation search and diversity of augmentations. Larger `num_chunks` values will lead to # faster searching but less diverse set of augmentations. Note that this parameter is used only in the searching # phase. When you train a model with found sub-policies, Albumentations will apply a distinct set of transformations # to each image separately. num_chunks : 8 # Number of consecutive augmentations in each sub-policy. Faster AutoAugment will sequentially apply `operation_count` # augmentations from a sub-policy to an image. Larger values of `operation_count` lead to better performance of # a model trained on augmented images. Simultaneously, larger values of `operation_count` affect the speed of search # and increase the searching time. operation_count : 4 # Settings for Classification Model that is used for two purposes: # 1. As a model that performs classification of input images. # 2. As a Discriminator for Policy Model. classification_model : # A custom classification model is used. This model is defined inside the `model.py` file which is located # in the same directory with `search.yaml` and `dataset.py`. _target_ : model.SVHNClassificationModel # # As an alternative, you could use a built-in AutoAldbument model using the following config: # # _target_: autoalbument.faster_autoaugment.models.ClassificationModel # # # Number of classes in the dataset. The dataset implementation should return an integer in the range # # [0, num_classes - 1] as a class label of an image. # num_classes: 10 # # # The architecture of Classification Model. AutoAlbument uses models from # # https://github.com/rwightman/pytorch-image-models/. Please refer to its documentation to get a list of available # # models - https://rwightman.github.io/pytorch-image-models/#list-models-with-pretrained-weights. # architecture: resnet18 # # # Boolean flag that indicates whether the selected model architecture should load pretrained weights or use randomly # # initialized weights. # pretrained: False data : # Class for the PyTorch Dataset and arguments to it. AutoAlbument will create an object of this class using # the `instantiate` method from Hydra - https://hydra.cc/docs/next/patterns/instantiate_objects/overview/. # # Note that the target class value in the `_target_` argument should be located inside PYTHONPATH so Hydra could # find it. The directory with the config file is automatically added to PYTHONPATH, so the default value # `dataset.SearchDataset` points to the class `SearchDataset` from the `dataset.py` file. This `dataset.py` file is # located along with the `search.yaml` file in the same directory provided by `--config-dir`. # # As an alternative, you could provide a path to a Python file with the dataset using the `dataset_file` parameter # instead of the `dataset` parameter. The Python file should contain the implementation of a PyTorch dataset for # augmentation search. The dataset class should have named `SearchDataset`. The value in `dataset_file` could either # be a relative or an absolute path ; in the case of a relative path, the path should be relative to this config # file's location. # # - Example of a relative path: # dataset_file: dataset.py # # - Example of an absolute path: # dataset_file: /projects/pytorch/dataset.py # dataset : _target_ : dataset.ConcatSVHNSearchDataset root : ~/data/svhn download : true # The data type of input images. Two values are supported: # - uint8. In that case, all input images should be NumPy arrays with the np.uint8 data type and values in the range # [0, 255]. # - float32. In that case, all input images should be NumPy arrays with the np.float32 data type and values in the # range [0.0, 1.0]. input_dtype : uint8 # A list of preprocessing augmentations that will be applied to each image before applying augmentations from # a policy. A preprocessing augmentation should be defined as `key`: `value`, where `key` is the name of augmentation # from Albumentations, and `value` is a dictionary with augmentation parameters. The found policy will also apply # those preprocessing augmentations before applying the main augmentations. # # Here is an example of an augmentation pipeline that first pads an image to the size 512x512 pixels, then resizes # the resulting image to the size 256x256 pixels and finally crops a random patch with the size 224x224 pixels. # # preprocessing: # - PadIfNeeded: # min_height: 512 # min_width: 512 # - Resize: # height: 256 # width: 256 # - RandomCrop: # height: 224 # width: 224 # preprocessing : null # Normalization values for images. For each image, the search pipeline will subtract `mean` and divide by `std`. # Normalization is applied after transforms defined in `preprocessing`. Note that regardless of `input_dtype`, # the normalization function will always receive a `float32` input with values in the range [0.0, 1.0], so you should # define `mean` and `std` values accordingly. normalization : mean : [ 0.4376821 , 0.4437697 , 0.47280442 ] std : [ 0.19803012 , 0.20101562 , 0.19703614 ] # Parameters for the PyTorch DataLoader. Please refer to the PyTorch documentation for the description of parameters - # https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader. dataloader : _target_ : torch.utils.data.DataLoader batch_size : 128 shuffle : true num_workers : 8 pin_memory : true drop_last : true optim : # Number of epochs to search parameters of augmentations. main : _target_ : torch.optim.Adam lr : 1e-3 betas : [ 0 , 0.999 ] # Optimizer configuration for Policy Model policy : _target_ : torch.optim.Adam lr : 1e-3 betas : [ 0 , 0.999 ] seed : 42 # Random seed. If the value is not null, it will be passed to `seed_everything` - # https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.utilities.seed.html?highlight=seed_everything hydra : run : # Path to the directory that will contain all outputs produced by the search algorithm. `${config_dir:}` contains # path to the directory with the `search.yaml` config file. Please refer to the Hydra documentation for more # information - https://hydra.cc/docs/configure_hydra/workdir. dir : ${config_dir:}/outputs/${now:%Y-%m-%d}/${now:%H-%M-%S} trainer : # Configuration for PyTorch Lightning Trainer. You can read more about Trainer and its arguments at # https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html. max_epochs : 4 # Number of epochs to search for augmentation parameters. # More detailed description - https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html#max-epochs benchmark : true # If true enables cudnn.benchmark. # More detailed description - https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html#benchmark gpus : 1 # Number of GPUs to train on. Set to `0` or None` to use CPU for training. # More detailed description - https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html#gpus model.py \u00b6 \"\"\"WideResNet code from https://github.com/xternalz/WideResNet-pytorch\"\"\" import torch import torch.nn as nn import torch.nn.functional as F from autoalbument.faster_autoaugment.models import BaseDiscriminator class BasicBlock ( nn . Module ): def __init__ ( self , in_planes , out_planes , stride ): super ( BasicBlock , self ) . __init__ () self . bn1 = nn . BatchNorm2d ( in_planes ) self . relu1 = nn . ReLU ( inplace = True ) self . conv1 = nn . Conv2d ( in_planes , out_planes , kernel_size = 3 , stride = stride , padding = 1 , bias = False ) self . bn2 = nn . BatchNorm2d ( out_planes ) self . relu2 = nn . ReLU ( inplace = True ) self . conv2 = nn . Conv2d ( out_planes , out_planes , kernel_size = 3 , stride = 1 , padding = 1 , bias = False ) self . equal_in_out = in_planes == out_planes self . conv_shortcut = ( ( not self . equal_in_out ) and nn . Conv2d ( in_planes , out_planes , kernel_size = 1 , stride = stride , padding = 0 , bias = False ) or None ) def forward ( self , x ): if not self . equal_in_out : x = self . relu1 ( self . bn1 ( x )) else : out = self . relu1 ( self . bn1 ( x )) out = self . relu2 ( self . bn2 ( self . conv1 ( out if self . equal_in_out else x ))) out = self . conv2 ( out ) return torch . add ( x if self . equal_in_out else self . conv_shortcut ( x ), out ) class NetworkBlock ( nn . Module ): def __init__ ( self , nb_layers , in_planes , out_planes , block , stride ): super ( NetworkBlock , self ) . __init__ () self . layer = self . _make_layer ( block , in_planes , out_planes , nb_layers , stride ) def _make_layer ( self , block , in_planes , out_planes , nb_layers , stride ): layers = [] for i in range ( int ( nb_layers )): layers . append ( block ( i == 0 and in_planes or out_planes , out_planes , i == 0 and stride or 1 )) return nn . Sequential ( * layers ) def forward ( self , x ): return self . layer ( x ) class WideResNet ( nn . Module ): def __init__ ( self , depth , num_classes , widen_factor = 1 ): super ( WideResNet , self ) . __init__ () n_channels = [ 16 , 16 * widen_factor , 32 * widen_factor , 64 * widen_factor ] assert ( depth - 4 ) % 6 == 0 n = ( depth - 4 ) / 6 block = BasicBlock # 1st conv before any network block self . conv1 = nn . Conv2d ( 3 , n_channels [ 0 ], kernel_size = 3 , stride = 1 , padding = 1 , bias = False ) # 1st block self . block1 = NetworkBlock ( n , n_channels [ 0 ], n_channels [ 1 ], block , 1 ) # 2nd block self . block2 = NetworkBlock ( n , n_channels [ 1 ], n_channels [ 2 ], block , 2 ) # 3rd block self . block3 = NetworkBlock ( n , n_channels [ 2 ], n_channels [ 3 ], block , 2 ) # global average pooling and classifier self . bn1 = nn . BatchNorm2d ( n_channels [ 3 ]) self . relu = nn . ReLU ( inplace = True ) self . fc = nn . Linear ( n_channels [ 3 ], num_classes ) self . n_channels = n_channels [ 3 ] for m in self . modules (): if isinstance ( m , nn . Conv2d ): nn . init . kaiming_normal_ ( m . weight , mode = \"fan_out\" , nonlinearity = \"relu\" ) elif isinstance ( m , nn . BatchNorm2d ): m . weight . data . fill_ ( 1 ) m . bias . data . zero_ () elif isinstance ( m , nn . Linear ): m . bias . data . zero_ () def forward_features ( self , x ): x = self . conv1 ( x ) x = self . block1 ( x ) x = self . block2 ( x ) x = self . block3 ( x ) x = self . relu ( self . bn1 ( x )) x = F . avg_pool2d ( x , 8 , 1 , 0 ) x = x . view ( - 1 , self . n_channels ) return x def forward_classifier ( self , x ): return self . fc ( x ) def forward ( self , x ): x = self . forward_features ( x ) x = self . forward_classifier ( x ) return x def wide_resnet_28x10 ( num_classes ): return WideResNet ( depth = 28 , widen_factor = 10 , num_classes = num_classes ) class SVHNClassificationModel ( BaseDiscriminator ): def __init__ ( self , * args , ** kwargs ): super () . __init__ () self . base_model = wide_resnet_28x10 ( num_classes = 10 ) num_features = self . base_model . fc . in_features self . discriminator = nn . Sequential ( nn . Linear ( num_features , num_features ), nn . ReLU (), nn . Linear ( num_features , 1 ) ) def forward ( self , input ): x = self . base_model . forward_features ( input ) return self . base_model . forward_classifier ( x ), self . discriminator ( x ) . view ( - 1 )","title":"Image classification on the SVHN dataset"},{"location":"autoalbument/examples/svhn/#image-classification-on-the-svhn-dataset","text":"The following files are also available on GitHub - https://github.com/albumentations-team/autoalbument/tree/master/examples/svhn","title":"Image classification on the SVHN dataset"},{"location":"autoalbument/examples/svhn/#datasetpy","text":"import cv2 import numpy as np import torch import torchvision cv2 . setNumThreads ( 0 ) cv2 . ocl . setUseOpenCL ( False ) class SVHNSearchDataset ( torchvision . datasets . SVHN ): def __getitem__ ( self , index ): image , label = self . data [ index ], int ( self . labels [ index ]) image = np . transpose ( image , ( 1 , 2 , 0 )) if self . transform is not None : transformed = self . transform ( image = image ) image = transformed [ \"image\" ] return image , label class ConcatSVHNSearchDataset ( torch . utils . data . ConcatDataset ): def __init__ ( self , root , download , transform = None ): datasets = [ SVHNSearchDataset ( root = root , split = \"train\" , download = download , transform = transform ), SVHNSearchDataset ( root = root , split = \"extra\" , download = download , transform = transform ), ] super () . __init__ ( datasets )","title":"dataset.py"},{"location":"autoalbument/examples/svhn/#searchyaml","text":"# @package _global_ _version : 2 # An internal value that indicates a version of the config schema. This value is used by # `autoalbument-search` and `autoalbument-migrate` to upgrade the config to the latest version if necessary. # Please do not change it manually. task : classification # Deep learning task. Should either be `classification` or `semantic_segmentation`. # Settings for Policy Model that searches augmentation policies. policy_model : # Multiplier for classification loss of a model. Faster AutoAugment uses classification loss to prevent augmentations # from transforming images of a particular class to another class. The authors of Faster AutoAugment use 0.1 as # default value. task_factor : 0.1 # Multiplier for the gradient penalty for WGAN-GP training. 10 is the default value that was proposed in # `Improved Training of Wasserstein GANs`. gp_factor : 10 # Temperature for Relaxed Bernoulli distribution. The probability of applying a certain augmentation is sampled from # Relaxed Bernoulli distribution (because Bernoulli distribution is not differentiable). With lower values of # `temperature` Relaxed Bernoulli distribution behaves like Bernoulli distribution. In the paper, the authors # of Faster AutoAugment used 0.05 as a default value for `temperature`. temperature : 0.05 # Number of augmentation sub-policies. When an image passes through an augmentation pipeline, Faster AutoAugment # randomly chooses one sub-policy and uses augmentations from that sub-policy to transform an input image. A larger # number of sub-policies leads to a more diverse set of augmentations and better performance of a model trained on # augmented images. However, an increase in the number of sub-policies leads to the exponential growth of a search # space of augmentations, so you need more training data for Policy Model to find good augmentation policies. num_sub_policies : 100 # Number of chunks in a batch. Faster AutoAugment splits each batch of images into `num_chunks` chunks. Then it # applies the same sub-policy with the same parameters to each image in a chunk. This parameter controls the tradeoff # between the speed of augmentation search and diversity of augmentations. Larger `num_chunks` values will lead to # faster searching but less diverse set of augmentations. Note that this parameter is used only in the searching # phase. When you train a model with found sub-policies, Albumentations will apply a distinct set of transformations # to each image separately. num_chunks : 8 # Number of consecutive augmentations in each sub-policy. Faster AutoAugment will sequentially apply `operation_count` # augmentations from a sub-policy to an image. Larger values of `operation_count` lead to better performance of # a model trained on augmented images. Simultaneously, larger values of `operation_count` affect the speed of search # and increase the searching time. operation_count : 4 # Settings for Classification Model that is used for two purposes: # 1. As a model that performs classification of input images. # 2. As a Discriminator for Policy Model. classification_model : # A custom classification model is used. This model is defined inside the `model.py` file which is located # in the same directory with `search.yaml` and `dataset.py`. _target_ : model.SVHNClassificationModel # # As an alternative, you could use a built-in AutoAldbument model using the following config: # # _target_: autoalbument.faster_autoaugment.models.ClassificationModel # # # Number of classes in the dataset. The dataset implementation should return an integer in the range # # [0, num_classes - 1] as a class label of an image. # num_classes: 10 # # # The architecture of Classification Model. AutoAlbument uses models from # # https://github.com/rwightman/pytorch-image-models/. Please refer to its documentation to get a list of available # # models - https://rwightman.github.io/pytorch-image-models/#list-models-with-pretrained-weights. # architecture: resnet18 # # # Boolean flag that indicates whether the selected model architecture should load pretrained weights or use randomly # # initialized weights. # pretrained: False data : # Class for the PyTorch Dataset and arguments to it. AutoAlbument will create an object of this class using # the `instantiate` method from Hydra - https://hydra.cc/docs/next/patterns/instantiate_objects/overview/. # # Note that the target class value in the `_target_` argument should be located inside PYTHONPATH so Hydra could # find it. The directory with the config file is automatically added to PYTHONPATH, so the default value # `dataset.SearchDataset` points to the class `SearchDataset` from the `dataset.py` file. This `dataset.py` file is # located along with the `search.yaml` file in the same directory provided by `--config-dir`. # # As an alternative, you could provide a path to a Python file with the dataset using the `dataset_file` parameter # instead of the `dataset` parameter. The Python file should contain the implementation of a PyTorch dataset for # augmentation search. The dataset class should have named `SearchDataset`. The value in `dataset_file` could either # be a relative or an absolute path ; in the case of a relative path, the path should be relative to this config # file's location. # # - Example of a relative path: # dataset_file: dataset.py # # - Example of an absolute path: # dataset_file: /projects/pytorch/dataset.py # dataset : _target_ : dataset.ConcatSVHNSearchDataset root : ~/data/svhn download : true # The data type of input images. Two values are supported: # - uint8. In that case, all input images should be NumPy arrays with the np.uint8 data type and values in the range # [0, 255]. # - float32. In that case, all input images should be NumPy arrays with the np.float32 data type and values in the # range [0.0, 1.0]. input_dtype : uint8 # A list of preprocessing augmentations that will be applied to each image before applying augmentations from # a policy. A preprocessing augmentation should be defined as `key`: `value`, where `key` is the name of augmentation # from Albumentations, and `value` is a dictionary with augmentation parameters. The found policy will also apply # those preprocessing augmentations before applying the main augmentations. # # Here is an example of an augmentation pipeline that first pads an image to the size 512x512 pixels, then resizes # the resulting image to the size 256x256 pixels and finally crops a random patch with the size 224x224 pixels. # # preprocessing: # - PadIfNeeded: # min_height: 512 # min_width: 512 # - Resize: # height: 256 # width: 256 # - RandomCrop: # height: 224 # width: 224 # preprocessing : null # Normalization values for images. For each image, the search pipeline will subtract `mean` and divide by `std`. # Normalization is applied after transforms defined in `preprocessing`. Note that regardless of `input_dtype`, # the normalization function will always receive a `float32` input with values in the range [0.0, 1.0], so you should # define `mean` and `std` values accordingly. normalization : mean : [ 0.4376821 , 0.4437697 , 0.47280442 ] std : [ 0.19803012 , 0.20101562 , 0.19703614 ] # Parameters for the PyTorch DataLoader. Please refer to the PyTorch documentation for the description of parameters - # https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader. dataloader : _target_ : torch.utils.data.DataLoader batch_size : 128 shuffle : true num_workers : 8 pin_memory : true drop_last : true optim : # Number of epochs to search parameters of augmentations. main : _target_ : torch.optim.Adam lr : 1e-3 betas : [ 0 , 0.999 ] # Optimizer configuration for Policy Model policy : _target_ : torch.optim.Adam lr : 1e-3 betas : [ 0 , 0.999 ] seed : 42 # Random seed. If the value is not null, it will be passed to `seed_everything` - # https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.utilities.seed.html?highlight=seed_everything hydra : run : # Path to the directory that will contain all outputs produced by the search algorithm. `${config_dir:}` contains # path to the directory with the `search.yaml` config file. Please refer to the Hydra documentation for more # information - https://hydra.cc/docs/configure_hydra/workdir. dir : ${config_dir:}/outputs/${now:%Y-%m-%d}/${now:%H-%M-%S} trainer : # Configuration for PyTorch Lightning Trainer. You can read more about Trainer and its arguments at # https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html. max_epochs : 4 # Number of epochs to search for augmentation parameters. # More detailed description - https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html#max-epochs benchmark : true # If true enables cudnn.benchmark. # More detailed description - https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html#benchmark gpus : 1 # Number of GPUs to train on. Set to `0` or None` to use CPU for training. # More detailed description - https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html#gpus","title":"search.yaml"},{"location":"autoalbument/examples/svhn/#modelpy","text":"\"\"\"WideResNet code from https://github.com/xternalz/WideResNet-pytorch\"\"\" import torch import torch.nn as nn import torch.nn.functional as F from autoalbument.faster_autoaugment.models import BaseDiscriminator class BasicBlock ( nn . Module ): def __init__ ( self , in_planes , out_planes , stride ): super ( BasicBlock , self ) . __init__ () self . bn1 = nn . BatchNorm2d ( in_planes ) self . relu1 = nn . ReLU ( inplace = True ) self . conv1 = nn . Conv2d ( in_planes , out_planes , kernel_size = 3 , stride = stride , padding = 1 , bias = False ) self . bn2 = nn . BatchNorm2d ( out_planes ) self . relu2 = nn . ReLU ( inplace = True ) self . conv2 = nn . Conv2d ( out_planes , out_planes , kernel_size = 3 , stride = 1 , padding = 1 , bias = False ) self . equal_in_out = in_planes == out_planes self . conv_shortcut = ( ( not self . equal_in_out ) and nn . Conv2d ( in_planes , out_planes , kernel_size = 1 , stride = stride , padding = 0 , bias = False ) or None ) def forward ( self , x ): if not self . equal_in_out : x = self . relu1 ( self . bn1 ( x )) else : out = self . relu1 ( self . bn1 ( x )) out = self . relu2 ( self . bn2 ( self . conv1 ( out if self . equal_in_out else x ))) out = self . conv2 ( out ) return torch . add ( x if self . equal_in_out else self . conv_shortcut ( x ), out ) class NetworkBlock ( nn . Module ): def __init__ ( self , nb_layers , in_planes , out_planes , block , stride ): super ( NetworkBlock , self ) . __init__ () self . layer = self . _make_layer ( block , in_planes , out_planes , nb_layers , stride ) def _make_layer ( self , block , in_planes , out_planes , nb_layers , stride ): layers = [] for i in range ( int ( nb_layers )): layers . append ( block ( i == 0 and in_planes or out_planes , out_planes , i == 0 and stride or 1 )) return nn . Sequential ( * layers ) def forward ( self , x ): return self . layer ( x ) class WideResNet ( nn . Module ): def __init__ ( self , depth , num_classes , widen_factor = 1 ): super ( WideResNet , self ) . __init__ () n_channels = [ 16 , 16 * widen_factor , 32 * widen_factor , 64 * widen_factor ] assert ( depth - 4 ) % 6 == 0 n = ( depth - 4 ) / 6 block = BasicBlock # 1st conv before any network block self . conv1 = nn . Conv2d ( 3 , n_channels [ 0 ], kernel_size = 3 , stride = 1 , padding = 1 , bias = False ) # 1st block self . block1 = NetworkBlock ( n , n_channels [ 0 ], n_channels [ 1 ], block , 1 ) # 2nd block self . block2 = NetworkBlock ( n , n_channels [ 1 ], n_channels [ 2 ], block , 2 ) # 3rd block self . block3 = NetworkBlock ( n , n_channels [ 2 ], n_channels [ 3 ], block , 2 ) # global average pooling and classifier self . bn1 = nn . BatchNorm2d ( n_channels [ 3 ]) self . relu = nn . ReLU ( inplace = True ) self . fc = nn . Linear ( n_channels [ 3 ], num_classes ) self . n_channels = n_channels [ 3 ] for m in self . modules (): if isinstance ( m , nn . Conv2d ): nn . init . kaiming_normal_ ( m . weight , mode = \"fan_out\" , nonlinearity = \"relu\" ) elif isinstance ( m , nn . BatchNorm2d ): m . weight . data . fill_ ( 1 ) m . bias . data . zero_ () elif isinstance ( m , nn . Linear ): m . bias . data . zero_ () def forward_features ( self , x ): x = self . conv1 ( x ) x = self . block1 ( x ) x = self . block2 ( x ) x = self . block3 ( x ) x = self . relu ( self . bn1 ( x )) x = F . avg_pool2d ( x , 8 , 1 , 0 ) x = x . view ( - 1 , self . n_channels ) return x def forward_classifier ( self , x ): return self . fc ( x ) def forward ( self , x ): x = self . forward_features ( x ) x = self . forward_classifier ( x ) return x def wide_resnet_28x10 ( num_classes ): return WideResNet ( depth = 28 , widen_factor = 10 , num_classes = num_classes ) class SVHNClassificationModel ( BaseDiscriminator ): def __init__ ( self , * args , ** kwargs ): super () . __init__ () self . base_model = wide_resnet_28x10 ( num_classes = 10 ) num_features = self . base_model . fc . in_features self . discriminator = nn . Sequential ( nn . Linear ( num_features , num_features ), nn . ReLU (), nn . Linear ( num_features , 1 ) ) def forward ( self , input ): x = self . base_model . forward_features ( input ) return self . base_model . forward_classifier ( x ), self . discriminator ( x ) . view ( - 1 )","title":"model.py"},{"location":"examples/","text":"List of examples \u00b6 Defining a simple augmentation pipeline for image augmentation Working with non-8-bit images Using Albumentations to augment bounding boxes for object detection tasks How to use Albumentations for detection tasks if you need to keep all bounding boxes Using Albumentations for a semantic segmentation task Using Albumentations to augment keypoints Applying the same augmentation with the same parameters to multiple images, masks, bounding boxes, or keypoints Weather augmentations in Albumentations Migrating from torchvision to Albumentations Debugging an augmentation pipeline with ReplayCompose How to save and load parameters of an augmentation pipeline Showcase. Cool augmentation examples on diverse set of images from various real-world tasks. Examples of how to use Albumentations with different deep learning frameworks \u00b6 PyTorch PyTorch and Albumentations for image classification PyTorch and Albumentations for semantic segmentation TensorFlow 2 Using Albumentations with Tensorflow","title":"List of examples"},{"location":"examples/#list-of-examples","text":"Defining a simple augmentation pipeline for image augmentation Working with non-8-bit images Using Albumentations to augment bounding boxes for object detection tasks How to use Albumentations for detection tasks if you need to keep all bounding boxes Using Albumentations for a semantic segmentation task Using Albumentations to augment keypoints Applying the same augmentation with the same parameters to multiple images, masks, bounding boxes, or keypoints Weather augmentations in Albumentations Migrating from torchvision to Albumentations Debugging an augmentation pipeline with ReplayCompose How to save and load parameters of an augmentation pipeline Showcase. Cool augmentation examples on diverse set of images from various real-world tasks.","title":"List of examples"},{"location":"examples/#examples-of-how-to-use-albumentations-with-different-deep-learning-frameworks","text":"PyTorch PyTorch and Albumentations for image classification PyTorch and Albumentations for semantic segmentation TensorFlow 2 Using Albumentations with Tensorflow","title":"Examples of how to use Albumentations with different deep learning frameworks"},{"location":"examples/example/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Defining a simple augmentation pipeline for image augmentation \u00b6 This example shows how you can use Albumentations to define a simple augmentation pipeline. Import the required libraries \u00b6 import random import cv2 from matplotlib import pyplot as plt import albumentations as A Define the visualization function \u00b6 def visualize ( image ): plt . figure ( figsize = ( 10 , 10 )) plt . axis ( 'off' ) plt . imshow ( image ) Read the image from the disk and convert it from the BGR color space to the RGB color space \u00b6 For historical reasons , OpenCV reads an image in BGR format (so color channels of the image have the following order: Blue, Green, Red). Albumentations uses the most common and popular RGB image format. So when using OpenCV, we need to convert the image format to RGB explicitly. image = cv2 . imread ( 'images/image_3.jpg' ) image = cv2 . cvtColor ( image , cv2 . COLOR_BGR2RGB ) visualize ( image ) Define a single augmentation, pass the image to it and receive the augmented image \u00b6 We fix the random seed for visualization purposes, so the augmentation will always produce the same result. In a real computer vision pipeline, you shouldn't fix the random seed before applying a transform to the image because, in that case, the pipeline will always output the same image. The purpose of image augmentation is to use different transformations each time. transform = A . HorizontalFlip ( p = 0.5 ) random . seed ( 7 ) augmented_image = transform ( image = image )[ 'image' ] visualize ( augmented_image ) transform = A . ShiftScaleRotate ( p = 0.5 ) random . seed ( 7 ) augmented_image = transform ( image = image )[ 'image' ] visualize ( augmented_image ) Define an augmentation pipeline using Compose , pass the image to it and receive the augmented image \u00b6 transform = A . Compose ([ A . CLAHE (), A . RandomRotate90 (), A . Transpose (), A . ShiftScaleRotate ( shift_limit = 0.0625 , scale_limit = 0.50 , rotate_limit = 45 , p =. 75 ), A . Blur ( blur_limit = 3 ), A . OpticalDistortion (), A . GridDistortion (), A . HueSaturationValue (), ]) random . seed ( 42 ) augmented_image = transform ( image = image )[ 'image' ] visualize ( augmented_image ) transform = A . Compose ([ A . RandomRotate90 (), A . Flip (), A . Transpose (), A . OneOf ([ A . IAAAdditiveGaussianNoise (), A . GaussNoise (), ], p = 0.2 ), A . OneOf ([ A . MotionBlur ( p =. 2 ), A . MedianBlur ( blur_limit = 3 , p = 0.1 ), A . Blur ( blur_limit = 3 , p = 0.1 ), ], p = 0.2 ), A . ShiftScaleRotate ( shift_limit = 0.0625 , scale_limit = 0.2 , rotate_limit = 45 , p = 0.2 ), A . OneOf ([ A . OpticalDistortion ( p = 0.3 ), A . GridDistortion ( p =. 1 ), A . IAAPiecewiseAffine ( p = 0.3 ), ], p = 0.2 ), A . OneOf ([ A . CLAHE ( clip_limit = 2 ), A . IAASharpen (), A . IAAEmboss (), A . RandomBrightnessContrast (), ], p = 0.3 ), A . HueSaturationValue ( p = 0.3 ), ]) random . seed ( 42 ) augmented_image = transform ( image = image )[ 'image' ] visualize ( augmented_image )","title":"Defining a simple augmentation pipeline for image augmentation"},{"location":"examples/example/#defining-a-simple-augmentation-pipeline-for-image-augmentation","text":"This example shows how you can use Albumentations to define a simple augmentation pipeline.","title":"Defining a simple augmentation pipeline for image augmentation"},{"location":"examples/example/#import-the-required-libraries","text":"import random import cv2 from matplotlib import pyplot as plt import albumentations as A","title":"Import the required libraries"},{"location":"examples/example/#define-the-visualization-function","text":"def visualize ( image ): plt . figure ( figsize = ( 10 , 10 )) plt . axis ( 'off' ) plt . imshow ( image )","title":"Define the visualization function"},{"location":"examples/example/#read-the-image-from-the-disk-and-convert-it-from-the-bgr-color-space-to-the-rgb-color-space","text":"For historical reasons , OpenCV reads an image in BGR format (so color channels of the image have the following order: Blue, Green, Red). Albumentations uses the most common and popular RGB image format. So when using OpenCV, we need to convert the image format to RGB explicitly. image = cv2 . imread ( 'images/image_3.jpg' ) image = cv2 . cvtColor ( image , cv2 . COLOR_BGR2RGB ) visualize ( image )","title":"Read the image from the disk and convert it from the BGR color space to the RGB color space"},{"location":"examples/example/#define-a-single-augmentation-pass-the-image-to-it-and-receive-the-augmented-image","text":"We fix the random seed for visualization purposes, so the augmentation will always produce the same result. In a real computer vision pipeline, you shouldn't fix the random seed before applying a transform to the image because, in that case, the pipeline will always output the same image. The purpose of image augmentation is to use different transformations each time. transform = A . HorizontalFlip ( p = 0.5 ) random . seed ( 7 ) augmented_image = transform ( image = image )[ 'image' ] visualize ( augmented_image ) transform = A . ShiftScaleRotate ( p = 0.5 ) random . seed ( 7 ) augmented_image = transform ( image = image )[ 'image' ] visualize ( augmented_image )","title":"Define a single augmentation, pass the image to it and receive the augmented image"},{"location":"examples/example/#define-an-augmentation-pipeline-using-compose-pass-the-image-to-it-and-receive-the-augmented-image","text":"transform = A . Compose ([ A . CLAHE (), A . RandomRotate90 (), A . Transpose (), A . ShiftScaleRotate ( shift_limit = 0.0625 , scale_limit = 0.50 , rotate_limit = 45 , p =. 75 ), A . Blur ( blur_limit = 3 ), A . OpticalDistortion (), A . GridDistortion (), A . HueSaturationValue (), ]) random . seed ( 42 ) augmented_image = transform ( image = image )[ 'image' ] visualize ( augmented_image ) transform = A . Compose ([ A . RandomRotate90 (), A . Flip (), A . Transpose (), A . OneOf ([ A . IAAAdditiveGaussianNoise (), A . GaussNoise (), ], p = 0.2 ), A . OneOf ([ A . MotionBlur ( p =. 2 ), A . MedianBlur ( blur_limit = 3 , p = 0.1 ), A . Blur ( blur_limit = 3 , p = 0.1 ), ], p = 0.2 ), A . ShiftScaleRotate ( shift_limit = 0.0625 , scale_limit = 0.2 , rotate_limit = 45 , p = 0.2 ), A . OneOf ([ A . OpticalDistortion ( p = 0.3 ), A . GridDistortion ( p =. 1 ), A . IAAPiecewiseAffine ( p = 0.3 ), ], p = 0.2 ), A . OneOf ([ A . CLAHE ( clip_limit = 2 ), A . IAASharpen (), A . IAAEmboss (), A . RandomBrightnessContrast (), ], p = 0.3 ), A . HueSaturationValue ( p = 0.3 ), ]) random . seed ( 42 ) augmented_image = transform ( image = image )[ 'image' ] visualize ( augmented_image )","title":"Define an augmentation pipeline using Compose, pass the image to it and receive the augmented image"},{"location":"examples/example_16_bit_tiff/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Working with non-8-bit images \u00b6 This example shows how you can augment 16-bit TIFF images. 16-bit images are used in satellite imagery. The following technique could also be applied to all non-8-bit images (i.e., 24-bit images, 32-bit images. etc.). Import the required libraries \u00b6 import random import cv2 from matplotlib import pyplot as plt import albumentations as A Define the visualization function \u00b6 def visualize ( image ): # Divide all values by 65535 so we can display the image using matplotlib image = image / 65535 plt . figure ( figsize = ( 10 , 10 )) plt . axis ( 'off' ) plt . imshow ( image ) Read the 16-bit TIFF image from the disk \u00b6 # The image is taken from http://www.brucelindbloom.com/index.html?ReferenceImages.html # \u00a9 Bruce Justin Lindbloom image = cv2 . imread ( 'images/DeltaE_16bit_gamma2.2.tif' , cv2 . IMREAD_UNCHANGED ) image = cv2 . cvtColor ( image , cv2 . COLOR_BGR2RGB ) visualize ( image ) Note: OpenCV may read incorrectly some TIFF files. Consider using tifffile - https://github.com/blink1073/tifffile Define an augmentation pipeline that works with 16-bit TIFF images \u00b6 Under the hood, Albumentations supports two data types that describe the intensity of pixels: - np.uint8 , an unsigned 8-bit integer that can define values between 0 and 255. - np.float32 , a floating-point number with single precision. For np.float32 input, Albumentations expects that value will lie in the range between 0.0 and 1.0. Albumentations has a dedicated transformation called ToFloat that takes a NumPy array with data types such as np.uint16 , np.uint32 , etc. (so any datatype that used values higher than 255 to represent pixel intensity) and converts it to a NumPy array with the np.float32 datatype. Additionally, this transformation divides all input values to lie in the range [0.0, 1.0] . By default, if the input data type is np.uint16 , all values are divided by 65536, and if the input data type is np.uint32 , all values are divided by 4294967295. You can specify your divider in the max_value parameter. The augmentation pipeline for non-8-bit images consists of the following stages: First, you use the ToFloat transform to convert an input image to float32. All values in the converted image will lie in the range [0.0, 1.0] . Then you use all the necessary image transforms. Optionally you could use the FromFloat transform at the end of the augmentation pipeline to convert the image back to its original data type. transform = A . Compose ([ A . ToFloat ( max_value = 65535.0 ), A . RandomRotate90 (), A . Flip (), A . OneOf ([ A . MotionBlur ( p = 0.2 ), A . MedianBlur ( blur_limit = 3 , p = 0.1 ), A . Blur ( blur_limit = 3 , p = 0.1 ), ], p = 0.2 ), A . ShiftScaleRotate ( shift_limit = 0.0625 , scale_limit = 0.2 , rotate_limit = 45 , p = 0.2 ), A . OneOf ([ A . OpticalDistortion ( p = 0.3 ), A . GridDistortion ( p = 0.1 ), ], p = 0.2 ), A . HueSaturationValue ( hue_shift_limit = 20 , sat_shift_limit = 0.1 , val_shift_limit = 0.1 , p = 0.3 ), A . FromFloat ( max_value = 65535.0 ), ]) We fix the random seed for visualization purposes, so the augmentation will always produce the same result. In a real computer vision pipeline, you shouldn't fix the random seed before applying a transform to the image because, in that case, the pipeline will always output the same image. The purpose of image augmentation is to use different transformations each time. random . seed ( 7 ) augmented = transform ( image = image ) visualize ( augmented [ 'image' ])","title":"Working with non-8-bit images"},{"location":"examples/example_16_bit_tiff/#working-with-non-8-bit-images","text":"This example shows how you can augment 16-bit TIFF images. 16-bit images are used in satellite imagery. The following technique could also be applied to all non-8-bit images (i.e., 24-bit images, 32-bit images. etc.).","title":"Working with non-8-bit images"},{"location":"examples/example_16_bit_tiff/#import-the-required-libraries","text":"import random import cv2 from matplotlib import pyplot as plt import albumentations as A","title":"Import the required libraries"},{"location":"examples/example_16_bit_tiff/#define-the-visualization-function","text":"def visualize ( image ): # Divide all values by 65535 so we can display the image using matplotlib image = image / 65535 plt . figure ( figsize = ( 10 , 10 )) plt . axis ( 'off' ) plt . imshow ( image )","title":"Define the visualization function"},{"location":"examples/example_16_bit_tiff/#read-the-16-bit-tiff-image-from-the-disk","text":"# The image is taken from http://www.brucelindbloom.com/index.html?ReferenceImages.html # \u00a9 Bruce Justin Lindbloom image = cv2 . imread ( 'images/DeltaE_16bit_gamma2.2.tif' , cv2 . IMREAD_UNCHANGED ) image = cv2 . cvtColor ( image , cv2 . COLOR_BGR2RGB ) visualize ( image ) Note: OpenCV may read incorrectly some TIFF files. Consider using tifffile - https://github.com/blink1073/tifffile","title":"Read the 16-bit TIFF image from the disk"},{"location":"examples/example_16_bit_tiff/#define-an-augmentation-pipeline-that-works-with-16-bit-tiff-images","text":"Under the hood, Albumentations supports two data types that describe the intensity of pixels: - np.uint8 , an unsigned 8-bit integer that can define values between 0 and 255. - np.float32 , a floating-point number with single precision. For np.float32 input, Albumentations expects that value will lie in the range between 0.0 and 1.0. Albumentations has a dedicated transformation called ToFloat that takes a NumPy array with data types such as np.uint16 , np.uint32 , etc. (so any datatype that used values higher than 255 to represent pixel intensity) and converts it to a NumPy array with the np.float32 datatype. Additionally, this transformation divides all input values to lie in the range [0.0, 1.0] . By default, if the input data type is np.uint16 , all values are divided by 65536, and if the input data type is np.uint32 , all values are divided by 4294967295. You can specify your divider in the max_value parameter. The augmentation pipeline for non-8-bit images consists of the following stages: First, you use the ToFloat transform to convert an input image to float32. All values in the converted image will lie in the range [0.0, 1.0] . Then you use all the necessary image transforms. Optionally you could use the FromFloat transform at the end of the augmentation pipeline to convert the image back to its original data type. transform = A . Compose ([ A . ToFloat ( max_value = 65535.0 ), A . RandomRotate90 (), A . Flip (), A . OneOf ([ A . MotionBlur ( p = 0.2 ), A . MedianBlur ( blur_limit = 3 , p = 0.1 ), A . Blur ( blur_limit = 3 , p = 0.1 ), ], p = 0.2 ), A . ShiftScaleRotate ( shift_limit = 0.0625 , scale_limit = 0.2 , rotate_limit = 45 , p = 0.2 ), A . OneOf ([ A . OpticalDistortion ( p = 0.3 ), A . GridDistortion ( p = 0.1 ), ], p = 0.2 ), A . HueSaturationValue ( hue_shift_limit = 20 , sat_shift_limit = 0.1 , val_shift_limit = 0.1 , p = 0.3 ), A . FromFloat ( max_value = 65535.0 ), ]) We fix the random seed for visualization purposes, so the augmentation will always produce the same result. In a real computer vision pipeline, you shouldn't fix the random seed before applying a transform to the image because, in that case, the pipeline will always output the same image. The purpose of image augmentation is to use different transformations each time. random . seed ( 7 ) augmented = transform ( image = image ) visualize ( augmented [ 'image' ])","title":"Define an augmentation pipeline that works with 16-bit TIFF images"},{"location":"examples/example_bboxes/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Using Albumentations to augment bounding boxes for object detection tasks \u00b6 Import the required libraries \u00b6 % matplotlib inline import random import cv2 from matplotlib import pyplot as plt import albumentations as A Define functions to visualize bounding boxes and class labels on an image \u00b6 The visualization function is based on https://github.com/facebookresearch/Detectron/blob/master/detectron/utils/vis.py BOX_COLOR = ( 255 , 0 , 0 ) # Red TEXT_COLOR = ( 255 , 255 , 255 ) # White def visualize_bbox ( img , bbox , class_name , color = BOX_COLOR , thickness = 2 ): \"\"\"Visualizes a single bounding box on the image\"\"\" x_min , y_min , w , h = bbox x_min , x_max , y_min , y_max = int ( x_min ), int ( x_min + w ), int ( y_min ), int ( y_min + h ) cv2 . rectangle ( img , ( x_min , y_min ), ( x_max , y_max ), color = color , thickness = thickness ) (( text_width , text_height ), _ ) = cv2 . getTextSize ( class_name , cv2 . FONT_HERSHEY_SIMPLEX , 0.35 , 1 ) cv2 . rectangle ( img , ( x_min , y_min - int ( 1.3 * text_height )), ( x_min + text_width , y_min ), BOX_COLOR , - 1 ) cv2 . putText ( img , text = class_name , org = ( x_min , y_min - int ( 0.3 * text_height )), fontFace = cv2 . FONT_HERSHEY_SIMPLEX , fontScale = 0.35 , color = TEXT_COLOR , lineType = cv2 . LINE_AA , ) return img def visualize ( image , bboxes , category_ids , category_id_to_name ): img = image . copy () for bbox , category_id in zip ( bboxes , category_ids ): class_name = category_id_to_name [ category_id ] img = visualize_bbox ( img , bbox , class_name ) plt . figure ( figsize = ( 12 , 12 )) plt . axis ( 'off' ) plt . imshow ( img ) Get an image and annotations for it \u00b6 For this example we will use an image from the COCO dataset that have two associated bounding boxes. The image is available at http://cocodataset.org/#explore?id=386298 Load the image from the disk \u00b6 image = cv2 . imread ( 'images/000000386298.jpg' ) image = cv2 . cvtColor ( image , cv2 . COLOR_BGR2RGB ) Define two bounding boxes with coordinates and class labels \u00b6 Coordinates for those bounding boxes are declared using the coco format. Each bounding box is described using four values [x_min, y_min, width, height] . For the detailed description of different formats for bounding boxes coordinates, please refer to the documentation article about bounding boxes - https://albumentations.ai/docs/getting_started/bounding_boxes_augmentation/ . bboxes = [[ 5.66 , 138.95 , 147.09 , 164.88 ], [ 366.7 , 80.84 , 132.8 , 181.84 ]] category_ids = [ 17 , 18 ] # We will use the mapping from category_id to the class name # to visualize the class label for the bounding box on the image category_id_to_name = { 17 : 'cat' , 18 : 'dog' } Visuaize the original image with bounding boxes \u00b6 visualize ( image , bboxes , category_ids , category_id_to_name ) Define an augmentation pipeline \u00b6 To make an augmentation pipeline that works with bounding boxes, you need to pass an instance of BboxParams to Compose . In BboxParams you need to specify the format of coordinates for bounding boxes and optionally a few other parameters. For the detailed description of BboxParams please refer to the documentation article about bounding boxes - https://albumentations.ai/docs/getting_started/bounding_boxes_augmentation/ . transform = A . Compose ( [ A . HorizontalFlip ( p = 0.5 )], bbox_params = A . BboxParams ( format = 'coco' , label_fields = [ 'category_ids' ]), ) We fix the random seed for visualization purposes, so the augmentation will always produce the same result. In a real computer vision pipeline, you shouldn't fix the random seed before applying a transform to the image because, in that case, the pipeline will always output the same image. The purpose of image augmentation is to use different transformations each time. random . seed ( 7 ) transformed = transform ( image = image , bboxes = bboxes , category_ids = category_ids ) visualize ( transformed [ 'image' ], transformed [ 'bboxes' ], transformed [ 'category_ids' ], category_id_to_name , ) Another example \u00b6 transform = A . Compose ( [ A . ShiftScaleRotate ( p = 0.5 )], bbox_params = A . BboxParams ( format = 'coco' , label_fields = [ 'category_ids' ]), ) random . seed ( 7 ) transformed = transform ( image = image , bboxes = bboxes , category_ids = category_ids ) visualize ( transformed [ 'image' ], transformed [ 'bboxes' ], transformed [ 'category_ids' ], category_id_to_name , ) Define a complex augmentation piepline \u00b6 transform = A . Compose ([ A . HorizontalFlip ( p = 0.5 ), A . ShiftScaleRotate ( p = 0.5 ), A . RandomBrightnessContrast ( p = 0.3 ), A . RGBShift ( r_shift_limit = 30 , g_shift_limit = 30 , b_shift_limit = 30 , p = 0.3 ), ], bbox_params = A . BboxParams ( format = 'coco' , label_fields = [ 'category_ids' ]), ) random . seed ( 7 ) transformed = transform ( image = image , bboxes = bboxes , category_ids = category_ids ) visualize ( transformed [ 'image' ], transformed [ 'bboxes' ], transformed [ 'category_ids' ], category_id_to_name , ) min_area and min_visibility parameters \u00b6 The size of bounding boxes could change if you apply spatial augmentations, for example, when you crop a part of an image or when you resize an image. min_area and min_visibility parameters control what Albumentations should do to the augmented bounding boxes if their size has changed after augmentation. The size of bounding boxes could change if you apply spatial augmentations, for example, when you crop a part of an image or when you resize an image. min_area is a value in pixels. If the area of a bounding box after augmentation becomes smaller than min_area , Albumentations will drop that box. So the returned list of augmented bounding boxes won't contain that bounding box. min_visibility is a value between 0 and 1. If the ratio of the bounding box area after augmentation to the area of the bounding box before augmentation becomes smaller than min_visibility , Albumentations will drop that box. So if the augmentation process cuts the most of the bounding box, that box won't be present in the returned list of the augmented bounding boxes. Define an augmentation pipeline with the default values for min_area and min_visibilty \u00b6 If you don't pass the min_area and min_visibility parameters, Albumentations will use 0 as a default value for them. transform = A . Compose ( [ A . CenterCrop ( height = 280 , width = 280 , p = 1 )], bbox_params = A . BboxParams ( format = 'coco' , label_fields = [ 'category_ids' ]), ) transformed = transform ( image = image , bboxes = bboxes , category_ids = category_ids ) visualize ( transformed [ 'image' ], transformed [ 'bboxes' ], transformed [ 'category_ids' ], category_id_to_name , ) As you see the output contains two bounding boxes. Define an augmentation pipeline with min_area \u00b6 Next, we will set the min_area value to 4500 pixels. transform = A . Compose ( [ A . CenterCrop ( height = 280 , width = 280 , p = 1 )], bbox_params = A . BboxParams ( format = 'coco' , min_area = 4500 , label_fields = [ 'category_ids' ]), ) transformed = transform ( image = image , bboxes = bboxes , category_ids = category_ids ) visualize ( transformed [ 'image' ], transformed [ 'bboxes' ], transformed [ 'category_ids' ], category_id_to_name , ) The output contains only one bounding box because the area of the second bounding box became lower than 4500 pixels. Define an augmentation pipeline with min_visibility \u00b6 Finally, we will set min_visibility to 0.3. So if the area of the output bounding box is less than 30% of the original area, Albumentations won't return that bounding box. transform = A . Compose ( [ A . CenterCrop ( height = 280 , width = 280 , p = 1 )], bbox_params = A . BboxParams ( format = 'coco' , min_visibility = 0.3 , label_fields = [ 'category_ids' ]), ) transformed = transform ( image = image , bboxes = bboxes , category_ids = category_ids ) visualize ( transformed [ 'image' ], transformed [ 'bboxes' ], transformed [ 'category_ids' ], category_id_to_name , ) The output doesn't contain any bounding box. Note that you can declare both the min_area and min_visibility parameters simultaneously in one BboxParams instance.","title":"Using Albumentations to augment bounding boxes for object detection tasks"},{"location":"examples/example_bboxes/#using-albumentations-to-augment-bounding-boxes-for-object-detection-tasks","text":"","title":"Using Albumentations to augment bounding boxes for object detection tasks"},{"location":"examples/example_bboxes/#import-the-required-libraries","text":"% matplotlib inline import random import cv2 from matplotlib import pyplot as plt import albumentations as A","title":"Import the required libraries"},{"location":"examples/example_bboxes/#define-functions-to-visualize-bounding-boxes-and-class-labels-on-an-image","text":"The visualization function is based on https://github.com/facebookresearch/Detectron/blob/master/detectron/utils/vis.py BOX_COLOR = ( 255 , 0 , 0 ) # Red TEXT_COLOR = ( 255 , 255 , 255 ) # White def visualize_bbox ( img , bbox , class_name , color = BOX_COLOR , thickness = 2 ): \"\"\"Visualizes a single bounding box on the image\"\"\" x_min , y_min , w , h = bbox x_min , x_max , y_min , y_max = int ( x_min ), int ( x_min + w ), int ( y_min ), int ( y_min + h ) cv2 . rectangle ( img , ( x_min , y_min ), ( x_max , y_max ), color = color , thickness = thickness ) (( text_width , text_height ), _ ) = cv2 . getTextSize ( class_name , cv2 . FONT_HERSHEY_SIMPLEX , 0.35 , 1 ) cv2 . rectangle ( img , ( x_min , y_min - int ( 1.3 * text_height )), ( x_min + text_width , y_min ), BOX_COLOR , - 1 ) cv2 . putText ( img , text = class_name , org = ( x_min , y_min - int ( 0.3 * text_height )), fontFace = cv2 . FONT_HERSHEY_SIMPLEX , fontScale = 0.35 , color = TEXT_COLOR , lineType = cv2 . LINE_AA , ) return img def visualize ( image , bboxes , category_ids , category_id_to_name ): img = image . copy () for bbox , category_id in zip ( bboxes , category_ids ): class_name = category_id_to_name [ category_id ] img = visualize_bbox ( img , bbox , class_name ) plt . figure ( figsize = ( 12 , 12 )) plt . axis ( 'off' ) plt . imshow ( img )","title":"Define functions to visualize bounding boxes and class labels on an image"},{"location":"examples/example_bboxes/#get-an-image-and-annotations-for-it","text":"For this example we will use an image from the COCO dataset that have two associated bounding boxes. The image is available at http://cocodataset.org/#explore?id=386298","title":"Get an image and annotations for it"},{"location":"examples/example_bboxes/#load-the-image-from-the-disk","text":"image = cv2 . imread ( 'images/000000386298.jpg' ) image = cv2 . cvtColor ( image , cv2 . COLOR_BGR2RGB )","title":"Load the image from the disk"},{"location":"examples/example_bboxes/#define-two-bounding-boxes-with-coordinates-and-class-labels","text":"Coordinates for those bounding boxes are declared using the coco format. Each bounding box is described using four values [x_min, y_min, width, height] . For the detailed description of different formats for bounding boxes coordinates, please refer to the documentation article about bounding boxes - https://albumentations.ai/docs/getting_started/bounding_boxes_augmentation/ . bboxes = [[ 5.66 , 138.95 , 147.09 , 164.88 ], [ 366.7 , 80.84 , 132.8 , 181.84 ]] category_ids = [ 17 , 18 ] # We will use the mapping from category_id to the class name # to visualize the class label for the bounding box on the image category_id_to_name = { 17 : 'cat' , 18 : 'dog' }","title":"Define two bounding boxes with coordinates and class labels"},{"location":"examples/example_bboxes/#visuaize-the-original-image-with-bounding-boxes","text":"visualize ( image , bboxes , category_ids , category_id_to_name )","title":"Visuaize the original image with bounding boxes"},{"location":"examples/example_bboxes/#define-an-augmentation-pipeline","text":"To make an augmentation pipeline that works with bounding boxes, you need to pass an instance of BboxParams to Compose . In BboxParams you need to specify the format of coordinates for bounding boxes and optionally a few other parameters. For the detailed description of BboxParams please refer to the documentation article about bounding boxes - https://albumentations.ai/docs/getting_started/bounding_boxes_augmentation/ . transform = A . Compose ( [ A . HorizontalFlip ( p = 0.5 )], bbox_params = A . BboxParams ( format = 'coco' , label_fields = [ 'category_ids' ]), ) We fix the random seed for visualization purposes, so the augmentation will always produce the same result. In a real computer vision pipeline, you shouldn't fix the random seed before applying a transform to the image because, in that case, the pipeline will always output the same image. The purpose of image augmentation is to use different transformations each time. random . seed ( 7 ) transformed = transform ( image = image , bboxes = bboxes , category_ids = category_ids ) visualize ( transformed [ 'image' ], transformed [ 'bboxes' ], transformed [ 'category_ids' ], category_id_to_name , )","title":"Define an augmentation pipeline"},{"location":"examples/example_bboxes/#another-example","text":"transform = A . Compose ( [ A . ShiftScaleRotate ( p = 0.5 )], bbox_params = A . BboxParams ( format = 'coco' , label_fields = [ 'category_ids' ]), ) random . seed ( 7 ) transformed = transform ( image = image , bboxes = bboxes , category_ids = category_ids ) visualize ( transformed [ 'image' ], transformed [ 'bboxes' ], transformed [ 'category_ids' ], category_id_to_name , )","title":"Another example"},{"location":"examples/example_bboxes/#define-a-complex-augmentation-piepline","text":"transform = A . Compose ([ A . HorizontalFlip ( p = 0.5 ), A . ShiftScaleRotate ( p = 0.5 ), A . RandomBrightnessContrast ( p = 0.3 ), A . RGBShift ( r_shift_limit = 30 , g_shift_limit = 30 , b_shift_limit = 30 , p = 0.3 ), ], bbox_params = A . BboxParams ( format = 'coco' , label_fields = [ 'category_ids' ]), ) random . seed ( 7 ) transformed = transform ( image = image , bboxes = bboxes , category_ids = category_ids ) visualize ( transformed [ 'image' ], transformed [ 'bboxes' ], transformed [ 'category_ids' ], category_id_to_name , )","title":"Define a complex augmentation piepline"},{"location":"examples/example_bboxes/#min_area-and-min_visibility-parameters","text":"The size of bounding boxes could change if you apply spatial augmentations, for example, when you crop a part of an image or when you resize an image. min_area and min_visibility parameters control what Albumentations should do to the augmented bounding boxes if their size has changed after augmentation. The size of bounding boxes could change if you apply spatial augmentations, for example, when you crop a part of an image or when you resize an image. min_area is a value in pixels. If the area of a bounding box after augmentation becomes smaller than min_area , Albumentations will drop that box. So the returned list of augmented bounding boxes won't contain that bounding box. min_visibility is a value between 0 and 1. If the ratio of the bounding box area after augmentation to the area of the bounding box before augmentation becomes smaller than min_visibility , Albumentations will drop that box. So if the augmentation process cuts the most of the bounding box, that box won't be present in the returned list of the augmented bounding boxes.","title":"min_area and min_visibility parameters"},{"location":"examples/example_bboxes/#define-an-augmentation-pipeline-with-the-default-values-for-min_area-and-min_visibilty","text":"If you don't pass the min_area and min_visibility parameters, Albumentations will use 0 as a default value for them. transform = A . Compose ( [ A . CenterCrop ( height = 280 , width = 280 , p = 1 )], bbox_params = A . BboxParams ( format = 'coco' , label_fields = [ 'category_ids' ]), ) transformed = transform ( image = image , bboxes = bboxes , category_ids = category_ids ) visualize ( transformed [ 'image' ], transformed [ 'bboxes' ], transformed [ 'category_ids' ], category_id_to_name , ) As you see the output contains two bounding boxes.","title":"Define an augmentation pipeline with the default values for min_area and min_visibilty"},{"location":"examples/example_bboxes/#define-an-augmentation-pipeline-with-min_area","text":"Next, we will set the min_area value to 4500 pixels. transform = A . Compose ( [ A . CenterCrop ( height = 280 , width = 280 , p = 1 )], bbox_params = A . BboxParams ( format = 'coco' , min_area = 4500 , label_fields = [ 'category_ids' ]), ) transformed = transform ( image = image , bboxes = bboxes , category_ids = category_ids ) visualize ( transformed [ 'image' ], transformed [ 'bboxes' ], transformed [ 'category_ids' ], category_id_to_name , ) The output contains only one bounding box because the area of the second bounding box became lower than 4500 pixels.","title":"Define an augmentation pipeline with min_area"},{"location":"examples/example_bboxes/#define-an-augmentation-pipeline-with-min_visibility","text":"Finally, we will set min_visibility to 0.3. So if the area of the output bounding box is less than 30% of the original area, Albumentations won't return that bounding box. transform = A . Compose ( [ A . CenterCrop ( height = 280 , width = 280 , p = 1 )], bbox_params = A . BboxParams ( format = 'coco' , min_visibility = 0.3 , label_fields = [ 'category_ids' ]), ) transformed = transform ( image = image , bboxes = bboxes , category_ids = category_ids ) visualize ( transformed [ 'image' ], transformed [ 'bboxes' ], transformed [ 'category_ids' ], category_id_to_name , ) The output doesn't contain any bounding box. Note that you can declare both the min_area and min_visibility parameters simultaneously in one BboxParams instance.","title":"Define an augmentation pipeline with min_visibility"},{"location":"examples/example_bboxes2/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); How to use Albumentations for detection tasks if you need to keep all bounding boxes \u00b6 Some augmentations like RandomCrop and CenterCrop may transform an image so that it won't contain all original bounding boxes. This example shows how you can use the transform named RandomSizedBBoxSafeCrop to crop a part of the image but keep all bounding boxes from the original image. Import the required libraries \u00b6 import random import cv2 from matplotlib import pyplot as plt import albumentations as A Define functions to visualize bounding boxes and class labels on an image \u00b6 The visualization function is based on https://github.com/facebookresearch/Detectron/blob/master/detectron/utils/vis.py BOX_COLOR = ( 255 , 0 , 0 ) # Red TEXT_COLOR = ( 255 , 255 , 255 ) # White def visualize_bbox ( img , bbox , class_name , color = BOX_COLOR , thickness = 2 ): \"\"\"Visualizes a single bounding box on the image\"\"\" x_min , y_min , w , h = bbox x_min , x_max , y_min , y_max = int ( x_min ), int ( x_min + w ), int ( y_min ), int ( y_min + h ) cv2 . rectangle ( img , ( x_min , y_min ), ( x_max , y_max ), color = color , thickness = thickness ) (( text_width , text_height ), _ ) = cv2 . getTextSize ( class_name , cv2 . FONT_HERSHEY_SIMPLEX , 0.35 , 1 ) cv2 . rectangle ( img , ( x_min , y_min - int ( 1.3 * text_height )), ( x_min + text_width , y_min ), BOX_COLOR , - 1 ) cv2 . putText ( img , text = class_name , org = ( x_min , y_min - int ( 0.3 * text_height )), fontFace = cv2 . FONT_HERSHEY_SIMPLEX , fontScale = 0.35 , color = TEXT_COLOR , lineType = cv2 . LINE_AA , ) return img def visualize ( image , bboxes , category_ids , category_id_to_name ): img = image . copy () for bbox , category_id in zip ( bboxes , category_ids ): class_name = category_id_to_name [ category_id ] img = visualize_bbox ( img , bbox , class_name ) plt . figure ( figsize = ( 12 , 12 )) plt . axis ( 'off' ) plt . imshow ( img ) Get an image and annotations for it \u00b6 For this example we will use an image from the COCO dataset that have two associated bounding boxes. The image is available at http://cocodataset.org/#explore?id=386298 Load the image from the disk \u00b6 image = cv2 . imread ( 'images/000000386298.jpg' ) image = cv2 . cvtColor ( image , cv2 . COLOR_BGR2RGB ) Define two bounding boxes with coordinates and class labels \u00b6 Coordinates for those bounding boxes are declared using the coco format. Each bounding box is described using four values [x_min, y_min, width, height] . For the detailed description of different formats for bounding boxes coordinates, please refer to the documentation article about bounding boxes - https://albumentations.ai/docs/getting_started/bounding_boxes_augmentation/ . bboxes = [[ 5.66 , 138.95 , 147.09 , 164.88 ], [ 366.7 , 80.84 , 132.8 , 181.84 ]] category_ids = [ 17 , 18 ] # We will use the mapping from category_id to the class name # to visualize the class label for the bounding box on the image category_id_to_name = { 17 : 'cat' , 18 : 'dog' } Visuaize the image with bounding boxes \u00b6 visualize ( image , bboxes , category_ids , category_id_to_name ) Using RandomSizedBBoxSafeCrop to keep all bounding boxes from the original image \u00b6 RandomSizedBBoxSafeCrop crops a random part of the image. It ensures that the cropped part will contain all bounding boxes from the original image. Then the transform rescales the crop to height and width specified by the respective parameters. The erosion_rate parameter controls how much area of the original bounding box could be lost after cropping. erosion_rate = 0.2 means that the augmented bounding box's area could be up to 20% smaller than the area of the original bounding box. Define an augmentation pipeline \u00b6 transform = A . Compose ( [ A . RandomSizedBBoxSafeCrop ( width = 448 , height = 336 , erosion_rate = 0.2 )], bbox_params = A . BboxParams ( format = 'coco' , label_fields = [ 'category_ids' ]), ) Augment the input image with bounding boxes \u00b6 We fix the random seed for visualization purposes, so the augmentation will always produce the same result. In a real computer vision pipeline, you shouldn't fix the random seed before applying a transform to the image because, in that case, the pipeline will always output the same image. The purpose of image augmentation is to use different transformations each time. random . seed ( 7 ) transformed = transform ( image = image , bboxes = bboxes , category_ids = category_ids ) visualize ( transformed [ 'image' ], transformed [ 'bboxes' ], transformed [ 'category_ids' ], category_id_to_name , ) A few more examples with different random seeds \u00b6 random . seed ( 3 ) transformed = transform ( image = image , bboxes = bboxes , category_ids = category_ids ) visualize ( transformed [ 'image' ], transformed [ 'bboxes' ], transformed [ 'category_ids' ], category_id_to_name , ) random . seed ( 444 ) transformed = transform ( image = image , bboxes = bboxes , category_ids = category_ids ) visualize ( transformed [ 'image' ], transformed [ 'bboxes' ], transformed [ 'category_ids' ], category_id_to_name , )","title":"How to use Albumentations for detection tasks if you need to keep all bounding boxes"},{"location":"examples/example_bboxes2/#how-to-use-albumentations-for-detection-tasks-if-you-need-to-keep-all-bounding-boxes","text":"Some augmentations like RandomCrop and CenterCrop may transform an image so that it won't contain all original bounding boxes. This example shows how you can use the transform named RandomSizedBBoxSafeCrop to crop a part of the image but keep all bounding boxes from the original image.","title":"How to use Albumentations for detection tasks if you need to keep all bounding boxes"},{"location":"examples/example_bboxes2/#import-the-required-libraries","text":"import random import cv2 from matplotlib import pyplot as plt import albumentations as A","title":"Import the required libraries"},{"location":"examples/example_bboxes2/#define-functions-to-visualize-bounding-boxes-and-class-labels-on-an-image","text":"The visualization function is based on https://github.com/facebookresearch/Detectron/blob/master/detectron/utils/vis.py BOX_COLOR = ( 255 , 0 , 0 ) # Red TEXT_COLOR = ( 255 , 255 , 255 ) # White def visualize_bbox ( img , bbox , class_name , color = BOX_COLOR , thickness = 2 ): \"\"\"Visualizes a single bounding box on the image\"\"\" x_min , y_min , w , h = bbox x_min , x_max , y_min , y_max = int ( x_min ), int ( x_min + w ), int ( y_min ), int ( y_min + h ) cv2 . rectangle ( img , ( x_min , y_min ), ( x_max , y_max ), color = color , thickness = thickness ) (( text_width , text_height ), _ ) = cv2 . getTextSize ( class_name , cv2 . FONT_HERSHEY_SIMPLEX , 0.35 , 1 ) cv2 . rectangle ( img , ( x_min , y_min - int ( 1.3 * text_height )), ( x_min + text_width , y_min ), BOX_COLOR , - 1 ) cv2 . putText ( img , text = class_name , org = ( x_min , y_min - int ( 0.3 * text_height )), fontFace = cv2 . FONT_HERSHEY_SIMPLEX , fontScale = 0.35 , color = TEXT_COLOR , lineType = cv2 . LINE_AA , ) return img def visualize ( image , bboxes , category_ids , category_id_to_name ): img = image . copy () for bbox , category_id in zip ( bboxes , category_ids ): class_name = category_id_to_name [ category_id ] img = visualize_bbox ( img , bbox , class_name ) plt . figure ( figsize = ( 12 , 12 )) plt . axis ( 'off' ) plt . imshow ( img )","title":"Define functions to visualize bounding boxes and class labels on an image"},{"location":"examples/example_bboxes2/#get-an-image-and-annotations-for-it","text":"For this example we will use an image from the COCO dataset that have two associated bounding boxes. The image is available at http://cocodataset.org/#explore?id=386298","title":"Get an image and annotations for it"},{"location":"examples/example_bboxes2/#load-the-image-from-the-disk","text":"image = cv2 . imread ( 'images/000000386298.jpg' ) image = cv2 . cvtColor ( image , cv2 . COLOR_BGR2RGB )","title":"Load the image from the disk"},{"location":"examples/example_bboxes2/#define-two-bounding-boxes-with-coordinates-and-class-labels","text":"Coordinates for those bounding boxes are declared using the coco format. Each bounding box is described using four values [x_min, y_min, width, height] . For the detailed description of different formats for bounding boxes coordinates, please refer to the documentation article about bounding boxes - https://albumentations.ai/docs/getting_started/bounding_boxes_augmentation/ . bboxes = [[ 5.66 , 138.95 , 147.09 , 164.88 ], [ 366.7 , 80.84 , 132.8 , 181.84 ]] category_ids = [ 17 , 18 ] # We will use the mapping from category_id to the class name # to visualize the class label for the bounding box on the image category_id_to_name = { 17 : 'cat' , 18 : 'dog' }","title":"Define two bounding boxes with coordinates and class labels"},{"location":"examples/example_bboxes2/#visuaize-the-image-with-bounding-boxes","text":"visualize ( image , bboxes , category_ids , category_id_to_name )","title":"Visuaize the image with bounding boxes"},{"location":"examples/example_bboxes2/#using-randomsizedbboxsafecrop-to-keep-all-bounding-boxes-from-the-original-image","text":"RandomSizedBBoxSafeCrop crops a random part of the image. It ensures that the cropped part will contain all bounding boxes from the original image. Then the transform rescales the crop to height and width specified by the respective parameters. The erosion_rate parameter controls how much area of the original bounding box could be lost after cropping. erosion_rate = 0.2 means that the augmented bounding box's area could be up to 20% smaller than the area of the original bounding box.","title":"Using RandomSizedBBoxSafeCrop to keep all bounding boxes from the original image"},{"location":"examples/example_bboxes2/#define-an-augmentation-pipeline","text":"transform = A . Compose ( [ A . RandomSizedBBoxSafeCrop ( width = 448 , height = 336 , erosion_rate = 0.2 )], bbox_params = A . BboxParams ( format = 'coco' , label_fields = [ 'category_ids' ]), )","title":"Define an augmentation pipeline"},{"location":"examples/example_bboxes2/#augment-the-input-image-with-bounding-boxes","text":"We fix the random seed for visualization purposes, so the augmentation will always produce the same result. In a real computer vision pipeline, you shouldn't fix the random seed before applying a transform to the image because, in that case, the pipeline will always output the same image. The purpose of image augmentation is to use different transformations each time. random . seed ( 7 ) transformed = transform ( image = image , bboxes = bboxes , category_ids = category_ids ) visualize ( transformed [ 'image' ], transformed [ 'bboxes' ], transformed [ 'category_ids' ], category_id_to_name , )","title":"Augment the input image with bounding boxes"},{"location":"examples/example_bboxes2/#a-few-more-examples-with-different-random-seeds","text":"random . seed ( 3 ) transformed = transform ( image = image , bboxes = bboxes , category_ids = category_ids ) visualize ( transformed [ 'image' ], transformed [ 'bboxes' ], transformed [ 'category_ids' ], category_id_to_name , ) random . seed ( 444 ) transformed = transform ( image = image , bboxes = bboxes , category_ids = category_ids ) visualize ( transformed [ 'image' ], transformed [ 'bboxes' ], transformed [ 'category_ids' ], category_id_to_name , )","title":"A few more examples with different random seeds"},{"location":"examples/example_kaggle_salt/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Using Albumentations for a semantic segmentation task \u00b6 We will use images and data from the TGS Salt Identification Challenge . Import the required libraries \u00b6 import random import cv2 from matplotlib import pyplot as plt import albumentations as A Define a function to visualize images and masks \u00b6 def visualize ( image , mask , original_image = None , original_mask = None ): fontsize = 18 if original_image is None and original_mask is None : f , ax = plt . subplots ( 2 , 1 , figsize = ( 8 , 8 )) ax [ 0 ] . imshow ( image ) ax [ 1 ] . imshow ( mask ) else : f , ax = plt . subplots ( 2 , 2 , figsize = ( 8 , 8 )) ax [ 0 , 0 ] . imshow ( original_image ) ax [ 0 , 0 ] . set_title ( 'Original image' , fontsize = fontsize ) ax [ 1 , 0 ] . imshow ( original_mask ) ax [ 1 , 0 ] . set_title ( 'Original mask' , fontsize = fontsize ) ax [ 0 , 1 ] . imshow ( image ) ax [ 0 , 1 ] . set_title ( 'Transformed image' , fontsize = fontsize ) ax [ 1 , 1 ] . imshow ( mask ) ax [ 1 , 1 ] . set_title ( 'Transformed mask' , fontsize = fontsize ) Read an image and its mask from the disk \u00b6 image = cv2 . imread ( 'images/kaggle_salt/0fea4b5049_image.png' ) mask = cv2 . imread ( 'images/kaggle_salt/0fea4b5049.png' , cv2 . IMREAD_GRAYSCALE ) Original image \u00b6 print ( image . shape , mask . shape ) (101, 101, 3) (101, 101) original_height , original_width = image . shape [: 2 ] visualize ( image , mask ) # Padding UNet type architecture requires input image size be divisible by 2^N 2^N , where N N is the number of maxpooling layers. In the vanilla UNet N=5 N=5 \\Longrightarrow \\Longrightarrow , we need to pad input images to the closest divisible by 2^5 = 32 2^5 = 32 number, which is 128. This operation may be performed using PadIfNeeded transformation. It pads both the image and the mask on all four sides. Padding type (zero, constant, reflection) may be specified. The default padding is reflection padding. aug = A . PadIfNeeded ( min_height = 128 , min_width = 128 , p = 1 ) augmented = aug ( image = image , mask = mask ) image_padded = augmented [ 'image' ] mask_padded = augmented [ 'mask' ] print ( image_padded . shape , mask_padded . shape ) visualize ( image_padded , mask_padded , original_image = image , original_mask = mask ) (128, 128, 3) (128, 128) CenterCrop and Crop \u00b6 To get to the original image and mask from the padded version, we may use CenterCrop or Crop transformations. aug = A . CenterCrop ( p = 1 , height = original_height , width = original_width ) augmented = aug ( image = image_padded , mask = mask_padded ) image_center_cropped = augmented [ 'image' ] mask_center_cropped = augmented [ 'mask' ] print ( image_center_cropped . shape , mask_center_cropped . shape ) assert ( image - image_center_cropped ) . sum () == 0 assert ( mask - mask_center_cropped ) . sum () == 0 visualize ( image_padded , mask_padded , original_image = image_center_cropped , original_mask = mask_center_cropped ) (101, 101, 3) (101, 101) x_min = ( 128 - original_width ) // 2 y_min = ( 128 - original_height ) // 2 x_max = x_min + original_width y_max = y_min + original_height aug = A . Crop ( x_min = x_min , x_max = x_max , y_min = y_min , y_max = y_max , p = 1 ) augmented = aug ( image = image_padded , mask = mask_padded ) image_cropped = augmented [ 'image' ] mask_cropped = augmented [ 'mask' ] print ( image_cropped . shape , mask_cropped . shape ) assert ( image - image_cropped ) . sum () == 0 assert ( mask - mask_cropped ) . sum () == 0 visualize ( image_cropped , mask_cropped , original_image = image_padded , original_mask = mask_padded ) (101, 101, 3) (101, 101) Non destructive transformations. Dehidral group D4 \u00b6 For images for which there is no clear notion of top like this one, satellite and aerial imagery or medical imagery is typically a good idea to add transformations that do not add or lose the information. There are eight distinct ways to represent the same square on the plane. Combinations of the transformations HorizontalFlip , VerticalFlip , Transpose , RandomRotate90 will be able to get the original image to all eight states. ## HorizontalFlip aug = A . HorizontalFlip ( p = 1 ) augmented = aug ( image = image , mask = mask ) image_h_flipped = augmented [ 'image' ] mask_h_flipped = augmented [ 'mask' ] visualize ( image_h_flipped , mask_h_flipped , original_image = image , original_mask = mask ) VerticalFlip \u00b6 aug = A . VerticalFlip ( p = 1 ) augmented = aug ( image = image , mask = mask ) image_v_flipped = augmented [ 'image' ] mask_v_flipped = augmented [ 'mask' ] visualize ( image_v_flipped , mask_v_flipped , original_image = image , original_mask = mask ) RandomRotate90 (Randomly rotates by 0, 90, 180, 270 degrees) \u00b6 aug = A . RandomRotate90 ( p = 1 ) augmented = aug ( image = image , mask = mask ) image_rot90 = augmented [ 'image' ] mask_rot90 = augmented [ 'mask' ] visualize ( image_rot90 , mask_rot90 , original_image = image , original_mask = mask ) ## Transpose (switch X and Y axis) aug = A . Transpose ( p = 1 ) augmented = aug ( image = image , mask = mask ) image_transposed = augmented [ 'image' ] mask_transposed = augmented [ 'mask' ] visualize ( image_transposed , mask_transposed , original_image = image , original_mask = mask ) Non-rigid transformations: ElasticTransform, GridDistortion, OpticalDistortion \u00b6 In medical imaging problems, non-rigid transformations help to augment the data. It is unclear if they will help with this problem, but let's look at them. We will consider ElasticTransform , GridDistortion , OpticalDistortion . We fix the random seed for visualization purposes, so the augmentation will always produce the same result. In a real computer vision pipeline, you shouldn't fix the random seed before applying a transform to the image because, in that case, the pipeline will always output the same image. The purpose of image augmentation is to use different transformations each time. ElasticTransform \u00b6 aug = A . ElasticTransform ( p = 1 , alpha = 120 , sigma = 120 * 0.05 , alpha_affine = 120 * 0.03 ) random . seed ( 7 ) augmented = aug ( image = image , mask = mask ) image_elastic = augmented [ 'image' ] mask_elastic = augmented [ 'mask' ] visualize ( image_elastic , mask_elastic , original_image = image , original_mask = mask ) GridDistortion \u00b6 aug = A . GridDistortion ( p = 1 ) random . seed ( 7 ) augmented = aug ( image = image , mask = mask ) image_grid = augmented [ 'image' ] mask_grid = augmented [ 'mask' ] visualize ( image_grid , mask_grid , original_image = image , original_mask = mask ) OpticalDistortion \u00b6 aug = A . OpticalDistortion ( distort_limit = 2 , shift_limit = 0.5 , p = 1 ) random . seed ( 7 ) augmented = aug ( image = image , mask = mask ) image_optical = augmented [ 'image' ] mask_optical = augmented [ 'mask' ] visualize ( image_optical , mask_optical , original_image = image , original_mask = mask ) RandomSizedCrop \u00b6 One may combine RandomCrop and RandomScale but there is a transformation RandomSizedCrop that allows to combine them into one transformation. aug = A . RandomSizedCrop ( min_max_height = ( 50 , 101 ), height = original_height , width = original_width , p = 1 ) random . seed ( 7 ) augmented = aug ( image = image , mask = mask ) image_scaled = augmented [ 'image' ] mask_scaled = augmented [ 'mask' ] visualize ( image_scaled , mask_scaled , original_image = image , original_mask = mask ) Let's try to combine different transformations \u00b6 Light non-destructive augmentations. aug = A . Compose ([ A . VerticalFlip ( p = 0.5 ), A . RandomRotate90 ( p = 0.5 )] ) random . seed ( 7 ) augmented = aug ( image = image , mask = mask ) image_light = augmented [ 'image' ] mask_light = augmented [ 'mask' ] visualize ( image_light , mask_light , original_image = image , original_mask = mask ) Let's add non rigid transformations and RandomSizedCrop \u00b6 Medium augmentations \u00b6 aug = A . Compose ([ A . OneOf ([ A . RandomSizedCrop ( min_max_height = ( 50 , 101 ), height = original_height , width = original_width , p = 0.5 ), A . PadIfNeeded ( min_height = original_height , min_width = original_width , p = 0.5 ) ], p = 1 ), A . VerticalFlip ( p = 0.5 ), A . RandomRotate90 ( p = 0.5 ), A . OneOf ([ A . ElasticTransform ( p = 0.5 , alpha = 120 , sigma = 120 * 0.05 , alpha_affine = 120 * 0.03 ), A . GridDistortion ( p = 0.5 ), A . OpticalDistortion ( distort_limit = 1 , shift_limit = 0.5 , p = 1 ), ], p = 0.8 )]) random . seed ( 11 ) augmented = aug ( image = image , mask = mask ) image_medium = augmented [ 'image' ] mask_medium = augmented [ 'mask' ] visualize ( image_medium , mask_medium , original_image = image , original_mask = mask ) Let's add non-spatial stransformations. \u00b6 Many non-spatial transformations like CLAHE , RandomBrightness , RandomContrast , RandomGamma can be also added. They will be applied only to the image and not the mask. aug = A . Compose ([ A . OneOf ([ A . RandomSizedCrop ( min_max_height = ( 50 , 101 ), height = original_height , width = original_width , p = 0.5 ), A . PadIfNeeded ( min_height = original_height , min_width = original_width , p = 0.5 ) ], p = 1 ), A . VerticalFlip ( p = 0.5 ), A . RandomRotate90 ( p = 0.5 ), A . OneOf ([ A . ElasticTransform ( alpha = 120 , sigma = 120 * 0.05 , alpha_affine = 120 * 0.03 , p = 0.5 ), A . GridDistortion ( p = 0.5 ), A . OpticalDistortion ( distort_limit = 2 , shift_limit = 0.5 , p = 1 ) ], p = 0.8 ), A . CLAHE ( p = 0.8 ), A . RandomBrightnessContrast ( p = 0.8 ), A . RandomGamma ( p = 0.8 )]) random . seed ( 11 ) augmented = aug ( image = image , mask = mask ) image_heavy = augmented [ 'image' ] mask_heavy = augmented [ 'mask' ] visualize ( image_heavy , mask_heavy , original_image = image , original_mask = mask )","title":"Using Albumentations for a semantic segmentation task"},{"location":"examples/example_kaggle_salt/#using-albumentations-for-a-semantic-segmentation-task","text":"We will use images and data from the TGS Salt Identification Challenge .","title":"Using Albumentations for a semantic segmentation task"},{"location":"examples/example_kaggle_salt/#import-the-required-libraries","text":"import random import cv2 from matplotlib import pyplot as plt import albumentations as A","title":"Import the required libraries"},{"location":"examples/example_kaggle_salt/#define-a-function-to-visualize-images-and-masks","text":"def visualize ( image , mask , original_image = None , original_mask = None ): fontsize = 18 if original_image is None and original_mask is None : f , ax = plt . subplots ( 2 , 1 , figsize = ( 8 , 8 )) ax [ 0 ] . imshow ( image ) ax [ 1 ] . imshow ( mask ) else : f , ax = plt . subplots ( 2 , 2 , figsize = ( 8 , 8 )) ax [ 0 , 0 ] . imshow ( original_image ) ax [ 0 , 0 ] . set_title ( 'Original image' , fontsize = fontsize ) ax [ 1 , 0 ] . imshow ( original_mask ) ax [ 1 , 0 ] . set_title ( 'Original mask' , fontsize = fontsize ) ax [ 0 , 1 ] . imshow ( image ) ax [ 0 , 1 ] . set_title ( 'Transformed image' , fontsize = fontsize ) ax [ 1 , 1 ] . imshow ( mask ) ax [ 1 , 1 ] . set_title ( 'Transformed mask' , fontsize = fontsize )","title":"Define a function to visualize images and masks"},{"location":"examples/example_kaggle_salt/#read-an-image-and-its-mask-from-the-disk","text":"image = cv2 . imread ( 'images/kaggle_salt/0fea4b5049_image.png' ) mask = cv2 . imread ( 'images/kaggle_salt/0fea4b5049.png' , cv2 . IMREAD_GRAYSCALE )","title":"Read an image and its mask from the disk"},{"location":"examples/example_kaggle_salt/#original-image","text":"print ( image . shape , mask . shape ) (101, 101, 3) (101, 101) original_height , original_width = image . shape [: 2 ] visualize ( image , mask ) # Padding UNet type architecture requires input image size be divisible by 2^N 2^N , where N N is the number of maxpooling layers. In the vanilla UNet N=5 N=5 \\Longrightarrow \\Longrightarrow , we need to pad input images to the closest divisible by 2^5 = 32 2^5 = 32 number, which is 128. This operation may be performed using PadIfNeeded transformation. It pads both the image and the mask on all four sides. Padding type (zero, constant, reflection) may be specified. The default padding is reflection padding. aug = A . PadIfNeeded ( min_height = 128 , min_width = 128 , p = 1 ) augmented = aug ( image = image , mask = mask ) image_padded = augmented [ 'image' ] mask_padded = augmented [ 'mask' ] print ( image_padded . shape , mask_padded . shape ) visualize ( image_padded , mask_padded , original_image = image , original_mask = mask ) (128, 128, 3) (128, 128)","title":"Original image "},{"location":"examples/example_kaggle_salt/#centercrop-and-crop","text":"To get to the original image and mask from the padded version, we may use CenterCrop or Crop transformations. aug = A . CenterCrop ( p = 1 , height = original_height , width = original_width ) augmented = aug ( image = image_padded , mask = mask_padded ) image_center_cropped = augmented [ 'image' ] mask_center_cropped = augmented [ 'mask' ] print ( image_center_cropped . shape , mask_center_cropped . shape ) assert ( image - image_center_cropped ) . sum () == 0 assert ( mask - mask_center_cropped ) . sum () == 0 visualize ( image_padded , mask_padded , original_image = image_center_cropped , original_mask = mask_center_cropped ) (101, 101, 3) (101, 101) x_min = ( 128 - original_width ) // 2 y_min = ( 128 - original_height ) // 2 x_max = x_min + original_width y_max = y_min + original_height aug = A . Crop ( x_min = x_min , x_max = x_max , y_min = y_min , y_max = y_max , p = 1 ) augmented = aug ( image = image_padded , mask = mask_padded ) image_cropped = augmented [ 'image' ] mask_cropped = augmented [ 'mask' ] print ( image_cropped . shape , mask_cropped . shape ) assert ( image - image_cropped ) . sum () == 0 assert ( mask - mask_cropped ) . sum () == 0 visualize ( image_cropped , mask_cropped , original_image = image_padded , original_mask = mask_padded ) (101, 101, 3) (101, 101)","title":"CenterCrop and Crop"},{"location":"examples/example_kaggle_salt/#non-destructive-transformations-dehidral-group-d4","text":"For images for which there is no clear notion of top like this one, satellite and aerial imagery or medical imagery is typically a good idea to add transformations that do not add or lose the information. There are eight distinct ways to represent the same square on the plane. Combinations of the transformations HorizontalFlip , VerticalFlip , Transpose , RandomRotate90 will be able to get the original image to all eight states. ## HorizontalFlip aug = A . HorizontalFlip ( p = 1 ) augmented = aug ( image = image , mask = mask ) image_h_flipped = augmented [ 'image' ] mask_h_flipped = augmented [ 'mask' ] visualize ( image_h_flipped , mask_h_flipped , original_image = image , original_mask = mask )","title":"Non destructive transformations. Dehidral group D4"},{"location":"examples/example_kaggle_salt/#verticalflip","text":"aug = A . VerticalFlip ( p = 1 ) augmented = aug ( image = image , mask = mask ) image_v_flipped = augmented [ 'image' ] mask_v_flipped = augmented [ 'mask' ] visualize ( image_v_flipped , mask_v_flipped , original_image = image , original_mask = mask )","title":"VerticalFlip"},{"location":"examples/example_kaggle_salt/#randomrotate90-randomly-rotates-by-0-90-180-270-degrees","text":"aug = A . RandomRotate90 ( p = 1 ) augmented = aug ( image = image , mask = mask ) image_rot90 = augmented [ 'image' ] mask_rot90 = augmented [ 'mask' ] visualize ( image_rot90 , mask_rot90 , original_image = image , original_mask = mask ) ## Transpose (switch X and Y axis) aug = A . Transpose ( p = 1 ) augmented = aug ( image = image , mask = mask ) image_transposed = augmented [ 'image' ] mask_transposed = augmented [ 'mask' ] visualize ( image_transposed , mask_transposed , original_image = image , original_mask = mask )","title":"RandomRotate90 (Randomly rotates by 0, 90, 180, 270 degrees)"},{"location":"examples/example_kaggle_salt/#non-rigid-transformations-elastictransform-griddistortion-opticaldistortion","text":"In medical imaging problems, non-rigid transformations help to augment the data. It is unclear if they will help with this problem, but let's look at them. We will consider ElasticTransform , GridDistortion , OpticalDistortion . We fix the random seed for visualization purposes, so the augmentation will always produce the same result. In a real computer vision pipeline, you shouldn't fix the random seed before applying a transform to the image because, in that case, the pipeline will always output the same image. The purpose of image augmentation is to use different transformations each time.","title":"Non-rigid transformations: ElasticTransform, GridDistortion, OpticalDistortion"},{"location":"examples/example_kaggle_salt/#elastictransform","text":"aug = A . ElasticTransform ( p = 1 , alpha = 120 , sigma = 120 * 0.05 , alpha_affine = 120 * 0.03 ) random . seed ( 7 ) augmented = aug ( image = image , mask = mask ) image_elastic = augmented [ 'image' ] mask_elastic = augmented [ 'mask' ] visualize ( image_elastic , mask_elastic , original_image = image , original_mask = mask )","title":"ElasticTransform"},{"location":"examples/example_kaggle_salt/#griddistortion","text":"aug = A . GridDistortion ( p = 1 ) random . seed ( 7 ) augmented = aug ( image = image , mask = mask ) image_grid = augmented [ 'image' ] mask_grid = augmented [ 'mask' ] visualize ( image_grid , mask_grid , original_image = image , original_mask = mask )","title":"GridDistortion"},{"location":"examples/example_kaggle_salt/#opticaldistortion","text":"aug = A . OpticalDistortion ( distort_limit = 2 , shift_limit = 0.5 , p = 1 ) random . seed ( 7 ) augmented = aug ( image = image , mask = mask ) image_optical = augmented [ 'image' ] mask_optical = augmented [ 'mask' ] visualize ( image_optical , mask_optical , original_image = image , original_mask = mask )","title":"OpticalDistortion"},{"location":"examples/example_kaggle_salt/#randomsizedcrop","text":"One may combine RandomCrop and RandomScale but there is a transformation RandomSizedCrop that allows to combine them into one transformation. aug = A . RandomSizedCrop ( min_max_height = ( 50 , 101 ), height = original_height , width = original_width , p = 1 ) random . seed ( 7 ) augmented = aug ( image = image , mask = mask ) image_scaled = augmented [ 'image' ] mask_scaled = augmented [ 'mask' ] visualize ( image_scaled , mask_scaled , original_image = image , original_mask = mask )","title":"RandomSizedCrop"},{"location":"examples/example_kaggle_salt/#lets-try-to-combine-different-transformations","text":"Light non-destructive augmentations. aug = A . Compose ([ A . VerticalFlip ( p = 0.5 ), A . RandomRotate90 ( p = 0.5 )] ) random . seed ( 7 ) augmented = aug ( image = image , mask = mask ) image_light = augmented [ 'image' ] mask_light = augmented [ 'mask' ] visualize ( image_light , mask_light , original_image = image , original_mask = mask )","title":"Let's try to combine different transformations"},{"location":"examples/example_kaggle_salt/#lets-add-non-rigid-transformations-and-randomsizedcrop","text":"","title":"Let's add non rigid transformations and RandomSizedCrop"},{"location":"examples/example_kaggle_salt/#medium-augmentations","text":"aug = A . Compose ([ A . OneOf ([ A . RandomSizedCrop ( min_max_height = ( 50 , 101 ), height = original_height , width = original_width , p = 0.5 ), A . PadIfNeeded ( min_height = original_height , min_width = original_width , p = 0.5 ) ], p = 1 ), A . VerticalFlip ( p = 0.5 ), A . RandomRotate90 ( p = 0.5 ), A . OneOf ([ A . ElasticTransform ( p = 0.5 , alpha = 120 , sigma = 120 * 0.05 , alpha_affine = 120 * 0.03 ), A . GridDistortion ( p = 0.5 ), A . OpticalDistortion ( distort_limit = 1 , shift_limit = 0.5 , p = 1 ), ], p = 0.8 )]) random . seed ( 11 ) augmented = aug ( image = image , mask = mask ) image_medium = augmented [ 'image' ] mask_medium = augmented [ 'mask' ] visualize ( image_medium , mask_medium , original_image = image , original_mask = mask )","title":"Medium augmentations"},{"location":"examples/example_kaggle_salt/#lets-add-non-spatial-stransformations","text":"Many non-spatial transformations like CLAHE , RandomBrightness , RandomContrast , RandomGamma can be also added. They will be applied only to the image and not the mask. aug = A . Compose ([ A . OneOf ([ A . RandomSizedCrop ( min_max_height = ( 50 , 101 ), height = original_height , width = original_width , p = 0.5 ), A . PadIfNeeded ( min_height = original_height , min_width = original_width , p = 0.5 ) ], p = 1 ), A . VerticalFlip ( p = 0.5 ), A . RandomRotate90 ( p = 0.5 ), A . OneOf ([ A . ElasticTransform ( alpha = 120 , sigma = 120 * 0.05 , alpha_affine = 120 * 0.03 , p = 0.5 ), A . GridDistortion ( p = 0.5 ), A . OpticalDistortion ( distort_limit = 2 , shift_limit = 0.5 , p = 1 ) ], p = 0.8 ), A . CLAHE ( p = 0.8 ), A . RandomBrightnessContrast ( p = 0.8 ), A . RandomGamma ( p = 0.8 )]) random . seed ( 11 ) augmented = aug ( image = image , mask = mask ) image_heavy = augmented [ 'image' ] mask_heavy = augmented [ 'mask' ] visualize ( image_heavy , mask_heavy , original_image = image , original_mask = mask )","title":"Let's add non-spatial stransformations."},{"location":"examples/example_keypoints/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Using Albumentations to augment keypoints \u00b6 In this notebook we will show how to apply Albumentations to the keypoint augmentation problem. Please refer to A list of transforms and their supported targets to see which spatial-level augmentations support keypoints. You can use any pixel-level augmentation to an image with keypoints because pixel-level augmentations don't affect keypoints. Note : by default, augmentations that work with keypoints don't change keypoints' labels after transformation. If keypoints' labels are side-specific, that may pose a problem. For example, if you have a keypoint named left arm and apply a HorizontalFlip augmentation, you will get a keypoint with the same left arm label, but it will now look like a right arm keypoint. See a picture at the end of this article for a visual example. If you work with such type of keypoints, consider using SymmetricKeypoints augmentations from albumentations-experimental that are created precisely to handle that case. Import the required libraries \u00b6 import random import cv2 from matplotlib import pyplot as plt import albumentations as A Define a function to visualize keypoints on an image \u00b6 KEYPOINT_COLOR = ( 0 , 255 , 0 ) # Green def vis_keypoints ( image , keypoints , color = KEYPOINT_COLOR , diameter = 15 ): image = image . copy () for ( x , y ) in keypoints : cv2 . circle ( image , ( int ( x ), int ( y )), diameter , ( 0 , 255 , 0 ), - 1 ) plt . figure ( figsize = ( 8 , 8 )) plt . axis ( 'off' ) plt . imshow ( image ) Get an image and annotations for it \u00b6 image = cv2 . imread ( 'images/keypoints_image.jpg' ) image = cv2 . cvtColor ( image , cv2 . COLOR_BGR2RGB ) Define keypoints \u00b6 We will use the xy format for keypoints' coordinates. Each keypoint is defined with two coordinates, x is the position on the x-axis, and y is the position on the y-axis. Please refer to this article with the detailed description of formats for keypoints' coordinates - https://albumentations.ai/docs/getting_started/keypoints_augmentation/ keypoints = [ ( 100 , 100 ), ( 720 , 410 ), ( 1100 , 400 ), ( 1700 , 30 ), ( 300 , 650 ), ( 1570 , 590 ), ( 560 , 800 ), ( 1300 , 750 ), ( 900 , 1000 ), ( 910 , 780 ), ( 670 , 670 ), ( 830 , 670 ), ( 1000 , 670 ), ( 1150 , 670 ), ( 820 , 900 ), ( 1000 , 900 ), ] Visualize the original image with keypoints \u00b6 vis_keypoints ( image , keypoints ) Define a simple augmentation pipeline \u00b6 transform = A . Compose ( [ A . HorizontalFlip ( p = 1 )], keypoint_params = A . KeypointParams ( format = 'xy' ) ) transformed = transform ( image = image , keypoints = keypoints ) vis_keypoints ( transformed [ 'image' ], transformed [ 'keypoints' ]) A few more examples of augmentation pipelines \u00b6 transform = A . Compose ( [ A . VerticalFlip ( p = 1 )], keypoint_params = A . KeypointParams ( format = 'xy' ) ) transformed = transform ( image = image , keypoints = keypoints ) vis_keypoints ( transformed [ 'image' ], transformed [ 'keypoints' ]) We fix the random seed for visualization purposes, so the augmentation will always produce the same result. In a real computer vision pipeline, you shouldn't fix the random seed before applying a transform to the image because, in that case, the pipeline will always output the same image. The purpose of image augmentation is to use different transformations each time. random . seed ( 7 ) transform = A . Compose ( [ A . RandomCrop ( width = 768 , height = 768 , p = 1 )], keypoint_params = A . KeypointParams ( format = 'xy' ) ) transformed = transform ( image = image , keypoints = keypoints ) vis_keypoints ( transformed [ 'image' ], transformed [ 'keypoints' ]) random . seed ( 7 ) transform = A . Compose ( [ A . Rotate ( p = 0.5 )], keypoint_params = A . KeypointParams ( format = 'xy' ) ) transformed = transform ( image = image , keypoints = keypoints ) vis_keypoints ( transformed [ 'image' ], transformed [ 'keypoints' ]) transform = A . Compose ( [ A . CenterCrop ( height = 512 , width = 512 , p = 1 )], keypoint_params = A . KeypointParams ( format = 'xy' ) ) transformed = transform ( image = image , keypoints = keypoints ) vis_keypoints ( transformed [ 'image' ], transformed [ 'keypoints' ]) random . seed ( 7 ) transform = A . Compose ( [ A . ShiftScaleRotate ( p = 0.5 )], keypoint_params = A . KeypointParams ( format = 'xy' ) ) transformed = transform ( image = image , keypoints = keypoints ) vis_keypoints ( transformed [ 'image' ], transformed [ 'keypoints' ]) An example of complex augmentation pipeline \u00b6 random . seed ( 7 ) transform = A . Compose ([ A . RandomSizedCrop ( min_max_height = ( 256 , 1025 ), height = 512 , width = 512 , p = 0.5 ), A . HorizontalFlip ( p = 0.5 ), A . OneOf ([ A . HueSaturationValue ( p = 0.5 ), A . RGBShift ( p = 0.7 ) ], p = 1 ), A . RandomBrightnessContrast ( p = 0.5 ) ], keypoint_params = A . KeypointParams ( format = 'xy' ), ) transformed = transform ( image = image , keypoints = keypoints ) vis_keypoints ( transformed [ 'image' ], transformed [ 'keypoints' ])","title":"Using Albumentations to augment keypoints"},{"location":"examples/example_keypoints/#using-albumentations-to-augment-keypoints","text":"In this notebook we will show how to apply Albumentations to the keypoint augmentation problem. Please refer to A list of transforms and their supported targets to see which spatial-level augmentations support keypoints. You can use any pixel-level augmentation to an image with keypoints because pixel-level augmentations don't affect keypoints. Note : by default, augmentations that work with keypoints don't change keypoints' labels after transformation. If keypoints' labels are side-specific, that may pose a problem. For example, if you have a keypoint named left arm and apply a HorizontalFlip augmentation, you will get a keypoint with the same left arm label, but it will now look like a right arm keypoint. See a picture at the end of this article for a visual example. If you work with such type of keypoints, consider using SymmetricKeypoints augmentations from albumentations-experimental that are created precisely to handle that case.","title":"Using Albumentations to augment keypoints"},{"location":"examples/example_keypoints/#import-the-required-libraries","text":"import random import cv2 from matplotlib import pyplot as plt import albumentations as A","title":"Import the required libraries"},{"location":"examples/example_keypoints/#define-a-function-to-visualize-keypoints-on-an-image","text":"KEYPOINT_COLOR = ( 0 , 255 , 0 ) # Green def vis_keypoints ( image , keypoints , color = KEYPOINT_COLOR , diameter = 15 ): image = image . copy () for ( x , y ) in keypoints : cv2 . circle ( image , ( int ( x ), int ( y )), diameter , ( 0 , 255 , 0 ), - 1 ) plt . figure ( figsize = ( 8 , 8 )) plt . axis ( 'off' ) plt . imshow ( image )","title":"Define a function to visualize keypoints on an image"},{"location":"examples/example_keypoints/#get-an-image-and-annotations-for-it","text":"image = cv2 . imread ( 'images/keypoints_image.jpg' ) image = cv2 . cvtColor ( image , cv2 . COLOR_BGR2RGB )","title":"Get an image and annotations for it"},{"location":"examples/example_keypoints/#define-keypoints","text":"We will use the xy format for keypoints' coordinates. Each keypoint is defined with two coordinates, x is the position on the x-axis, and y is the position on the y-axis. Please refer to this article with the detailed description of formats for keypoints' coordinates - https://albumentations.ai/docs/getting_started/keypoints_augmentation/ keypoints = [ ( 100 , 100 ), ( 720 , 410 ), ( 1100 , 400 ), ( 1700 , 30 ), ( 300 , 650 ), ( 1570 , 590 ), ( 560 , 800 ), ( 1300 , 750 ), ( 900 , 1000 ), ( 910 , 780 ), ( 670 , 670 ), ( 830 , 670 ), ( 1000 , 670 ), ( 1150 , 670 ), ( 820 , 900 ), ( 1000 , 900 ), ]","title":"Define keypoints"},{"location":"examples/example_keypoints/#visualize-the-original-image-with-keypoints","text":"vis_keypoints ( image , keypoints )","title":"Visualize the original image with keypoints"},{"location":"examples/example_keypoints/#define-a-simple-augmentation-pipeline","text":"transform = A . Compose ( [ A . HorizontalFlip ( p = 1 )], keypoint_params = A . KeypointParams ( format = 'xy' ) ) transformed = transform ( image = image , keypoints = keypoints ) vis_keypoints ( transformed [ 'image' ], transformed [ 'keypoints' ])","title":"Define a simple augmentation pipeline"},{"location":"examples/example_keypoints/#a-few-more-examples-of-augmentation-pipelines","text":"transform = A . Compose ( [ A . VerticalFlip ( p = 1 )], keypoint_params = A . KeypointParams ( format = 'xy' ) ) transformed = transform ( image = image , keypoints = keypoints ) vis_keypoints ( transformed [ 'image' ], transformed [ 'keypoints' ]) We fix the random seed for visualization purposes, so the augmentation will always produce the same result. In a real computer vision pipeline, you shouldn't fix the random seed before applying a transform to the image because, in that case, the pipeline will always output the same image. The purpose of image augmentation is to use different transformations each time. random . seed ( 7 ) transform = A . Compose ( [ A . RandomCrop ( width = 768 , height = 768 , p = 1 )], keypoint_params = A . KeypointParams ( format = 'xy' ) ) transformed = transform ( image = image , keypoints = keypoints ) vis_keypoints ( transformed [ 'image' ], transformed [ 'keypoints' ]) random . seed ( 7 ) transform = A . Compose ( [ A . Rotate ( p = 0.5 )], keypoint_params = A . KeypointParams ( format = 'xy' ) ) transformed = transform ( image = image , keypoints = keypoints ) vis_keypoints ( transformed [ 'image' ], transformed [ 'keypoints' ]) transform = A . Compose ( [ A . CenterCrop ( height = 512 , width = 512 , p = 1 )], keypoint_params = A . KeypointParams ( format = 'xy' ) ) transformed = transform ( image = image , keypoints = keypoints ) vis_keypoints ( transformed [ 'image' ], transformed [ 'keypoints' ]) random . seed ( 7 ) transform = A . Compose ( [ A . ShiftScaleRotate ( p = 0.5 )], keypoint_params = A . KeypointParams ( format = 'xy' ) ) transformed = transform ( image = image , keypoints = keypoints ) vis_keypoints ( transformed [ 'image' ], transformed [ 'keypoints' ])","title":"A few more examples of augmentation pipelines"},{"location":"examples/example_keypoints/#an-example-of-complex-augmentation-pipeline","text":"random . seed ( 7 ) transform = A . Compose ([ A . RandomSizedCrop ( min_max_height = ( 256 , 1025 ), height = 512 , width = 512 , p = 0.5 ), A . HorizontalFlip ( p = 0.5 ), A . OneOf ([ A . HueSaturationValue ( p = 0.5 ), A . RGBShift ( p = 0.7 ) ], p = 1 ), A . RandomBrightnessContrast ( p = 0.5 ) ], keypoint_params = A . KeypointParams ( format = 'xy' ), ) transformed = transform ( image = image , keypoints = keypoints ) vis_keypoints ( transformed [ 'image' ], transformed [ 'keypoints' ])","title":"An example of complex augmentation pipeline"},{"location":"examples/example_multi_target/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Applying the same augmentation with the same parameters to multiple images, masks, bounding boxes, or keypoints \u00b6 Sometimes you want to apply the same set of augmentations to multiple input objects of the same type. For example, you might have a set of frames from the video, and you want to augment them in the same way. Or you may have multiple masks for the same image, and you want to apply the same augmentation for all of them. In Albumentations, you can declare additional targets and their types using the additional_targets argument to Compose . For the name of an additional target, you can use any string value that is also a valid argument name in Python. Later, you will use those names to pass additional targets to a transformation pipeline. So you can't use a string that starts with a digit, such as '0image' because it is not a valid Python argument name. The type could be either image , mask , bboxes , or keypoints . An example definition of Compose that supports multiple inputs of the same type may be the following: import albumentations as A transform = A.Compose( [HorizontalFlip(p=0.5), ...], additional_targets={ 'image1': 'image', 'image2': 'image', ... 'imageN': 'image', 'bboxes1': 'bboxes', 'bboxes1': 'bboxes', ... 'bboxesM': 'bboxes', 'keypoints1': 'keypoints', 'keypoints2': 'keypoints', ... 'keypointsK': 'keypoints', 'mask1': 'mask', 'mask2': 'mask', ... 'maskL': 'mask' }) ) Note : there is also an alternative way to apply the same augmentation to multiple inputs such as images, masks, etc. ReplayCompose is a tool that could record augmentation parameters applied to one set of inputs (e.g., an image and an associated mask) and then use the recorded values to augment another set of inputs in the same way. You can read more about ReplayCompose here . Import the required libraries \u00b6 import random import cv2 from matplotlib import pyplot as plt import albumentations as A Define a function to visualize an image \u00b6 def visualize ( image ): plt . figure ( figsize = ( 10 , 10 )) plt . axis ( 'off' ) plt . imshow ( image ) Load images from the disk \u00b6 image = cv2 . imread ( 'images/multi_target_1.jpg' ) image = cv2 . cvtColor ( image , cv2 . COLOR_BGR2RGB ) image0 = cv2 . imread ( 'images/multi_target_2.jpg' ) image0 = cv2 . cvtColor ( image0 , cv2 . COLOR_BGR2RGB ) image1 = cv2 . imread ( 'images/multi_target_3.jpg' ) image1 = cv2 . cvtColor ( image1 , cv2 . COLOR_BGR2RGB ) Show original images \u00b6 visualize ( image ) visualize ( image0 ) visualize ( image1 ) Define an augmentation pipeline \u00b6 The pipeline expects three images as inputs named image , image0 , and image1 . Then the pipeline will augment those three images in the same way. So it will apply the same set of transformations with the same parameters. transform = A . Compose ( [ A . VerticalFlip ( p = 1 )], additional_targets = { 'image0' : 'image' , 'image1' : 'image' } ) transformed = transform ( image = image , image0 = image0 , image1 = image1 ) visualize ( transformed [ 'image' ]) visualize ( transformed [ 'image0' ]) visualize ( transformed [ 'image1' ]) An example of more complex pipeline \u00b6 transform = A . Compose ( [ A . HorizontalFlip ( p = 0.5 ), A . ShiftScaleRotate ( p = 0.5 ), A . RandomBrightnessContrast ( p = 0.2 ), A . RGBShift ( p = 0.2 ), ], additional_targets = { 'image0' : 'image' , 'image1' : 'image' } ) random . seed ( 42 ) transformed = transform ( image = image , image0 = image0 , image1 = image1 ) visualize ( transformed [ 'image' ]) visualize ( transformed [ 'image0' ]) visualize ( transformed [ 'image1' ])","title":"Applying the same augmentation with the same parameters to multiple images, masks, bounding boxes, or keypoints"},{"location":"examples/example_multi_target/#applying-the-same-augmentation-with-the-same-parameters-to-multiple-images-masks-bounding-boxes-or-keypoints","text":"Sometimes you want to apply the same set of augmentations to multiple input objects of the same type. For example, you might have a set of frames from the video, and you want to augment them in the same way. Or you may have multiple masks for the same image, and you want to apply the same augmentation for all of them. In Albumentations, you can declare additional targets and their types using the additional_targets argument to Compose . For the name of an additional target, you can use any string value that is also a valid argument name in Python. Later, you will use those names to pass additional targets to a transformation pipeline. So you can't use a string that starts with a digit, such as '0image' because it is not a valid Python argument name. The type could be either image , mask , bboxes , or keypoints . An example definition of Compose that supports multiple inputs of the same type may be the following: import albumentations as A transform = A.Compose( [HorizontalFlip(p=0.5), ...], additional_targets={ 'image1': 'image', 'image2': 'image', ... 'imageN': 'image', 'bboxes1': 'bboxes', 'bboxes1': 'bboxes', ... 'bboxesM': 'bboxes', 'keypoints1': 'keypoints', 'keypoints2': 'keypoints', ... 'keypointsK': 'keypoints', 'mask1': 'mask', 'mask2': 'mask', ... 'maskL': 'mask' }) ) Note : there is also an alternative way to apply the same augmentation to multiple inputs such as images, masks, etc. ReplayCompose is a tool that could record augmentation parameters applied to one set of inputs (e.g., an image and an associated mask) and then use the recorded values to augment another set of inputs in the same way. You can read more about ReplayCompose here .","title":"Applying the same augmentation with the same parameters to multiple images, masks, bounding boxes, or keypoints"},{"location":"examples/example_multi_target/#import-the-required-libraries","text":"import random import cv2 from matplotlib import pyplot as plt import albumentations as A","title":"Import the required libraries"},{"location":"examples/example_multi_target/#define-a-function-to-visualize-an-image","text":"def visualize ( image ): plt . figure ( figsize = ( 10 , 10 )) plt . axis ( 'off' ) plt . imshow ( image )","title":"Define a function to visualize an image"},{"location":"examples/example_multi_target/#load-images-from-the-disk","text":"image = cv2 . imread ( 'images/multi_target_1.jpg' ) image = cv2 . cvtColor ( image , cv2 . COLOR_BGR2RGB ) image0 = cv2 . imread ( 'images/multi_target_2.jpg' ) image0 = cv2 . cvtColor ( image0 , cv2 . COLOR_BGR2RGB ) image1 = cv2 . imread ( 'images/multi_target_3.jpg' ) image1 = cv2 . cvtColor ( image1 , cv2 . COLOR_BGR2RGB )","title":"Load images from the disk"},{"location":"examples/example_multi_target/#show-original-images","text":"visualize ( image ) visualize ( image0 ) visualize ( image1 )","title":"Show original images"},{"location":"examples/example_multi_target/#define-an-augmentation-pipeline","text":"The pipeline expects three images as inputs named image , image0 , and image1 . Then the pipeline will augment those three images in the same way. So it will apply the same set of transformations with the same parameters. transform = A . Compose ( [ A . VerticalFlip ( p = 1 )], additional_targets = { 'image0' : 'image' , 'image1' : 'image' } ) transformed = transform ( image = image , image0 = image0 , image1 = image1 ) visualize ( transformed [ 'image' ]) visualize ( transformed [ 'image0' ]) visualize ( transformed [ 'image1' ])","title":"Define an augmentation pipeline"},{"location":"examples/example_multi_target/#an-example-of-more-complex-pipeline","text":"transform = A . Compose ( [ A . HorizontalFlip ( p = 0.5 ), A . ShiftScaleRotate ( p = 0.5 ), A . RandomBrightnessContrast ( p = 0.2 ), A . RGBShift ( p = 0.2 ), ], additional_targets = { 'image0' : 'image' , 'image1' : 'image' } ) random . seed ( 42 ) transformed = transform ( image = image , image0 = image0 , image1 = image1 ) visualize ( transformed [ 'image' ]) visualize ( transformed [ 'image0' ]) visualize ( transformed [ 'image1' ])","title":"An example of more complex pipeline"},{"location":"examples/example_weather_transforms/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Weather augmentations in Albumentations \u00b6 This notebook demonstrates weather augmentations that are supported by Albumentations. Import the required libraries \u00b6 import random import cv2 from matplotlib import pyplot as plt import albumentations as A Define a function to visualize an image \u00b6 def visualize ( image ): plt . figure ( figsize = ( 20 , 10 )) plt . axis ( 'off' ) plt . imshow ( image ) Load the image from the disk \u00b6 image = cv2 . imread ( 'images/weather_example.jpg' ) image = cv2 . cvtColor ( image , cv2 . COLOR_BGR2RGB ) Visualize the original image \u00b6 visualize ( image ) RandomRain \u00b6 We fix the random seed for visualization purposes, so the augmentation will always produce the same result. In a real computer vision pipeline, you shouldn't fix the random seed before applying a transform to the image because, in that case, the pipeline will always output the same image. The purpose of image augmentation is to use different transformations each time. transform = A . Compose ( [ A . RandomRain ( brightness_coefficient = 0.9 , drop_width = 1 , blur_value = 5 , p = 1 )], ) random . seed ( 7 ) transformed = transform ( image = image ) visualize ( transformed [ 'image' ]) RandomSnow \u00b6 transform = A . Compose ( [ A . RandomSnow ( brightness_coeff = 2.5 , snow_point_lower = 0.3 , snow_point_upper = 0.5 , p = 1 )], ) random . seed ( 7 ) transformed = transform ( image = image ) visualize ( transformed [ 'image' ]) RandomSunFlare \u00b6 transform = A . Compose ( [ A . RandomSunFlare ( flare_roi = ( 0 , 0 , 1 , 0.5 ), angle_lower = 0.5 , p = 1 )], ) random . seed ( 7 ) transformed = transform ( image = image ) visualize ( transformed [ 'image' ]) RandomShadow \u00b6 transform = A . Compose ( [ A . RandomShadow ( num_shadows_lower = 1 , num_shadows_upper = 1 , shadow_dimension = 5 , shadow_roi = ( 0 , 0.5 , 1 , 1 ), p = 1 )], ) random . seed ( 7 ) transformed = transform ( image = image ) visualize ( transformed [ 'image' ]) RandomFog \u00b6 transform = A . Compose ( [ A . RandomFog ( fog_coef_lower = 0.7 , fog_coef_upper = 0.8 , alpha_coef = 0.1 , p = 1 )], ) random . seed ( 7 ) transformed = transform ( image = image ) visualize ( transformed [ 'image' ]) RandomShadow \u00b6 transform = A . Compose ( [ A . RandomShadow ( num_shadows_lower = 1 , num_shadows_upper = 1 , shadow_dimension = 5 , shadow_roi = ( 0 , 0.5 , 1 , 1 ), p = 1 )], ) random . seed ( 7 ) transformed = transform ( image = image ) visualize ( transformed [ 'image' ]) RandomFog \u00b6 transform = A . Compose ( [ A . RandomFog ( fog_coef_lower = 0.7 , fog_coef_upper = 0.8 , alpha_coef = 0.1 , p = 1 )], ) random . seed ( 7 ) transformed = transform ( image = image ) visualize ( transformed [ 'image' ])","title":"Weather augmentations in Albumentations"},{"location":"examples/example_weather_transforms/#weather-augmentations-in-albumentations","text":"This notebook demonstrates weather augmentations that are supported by Albumentations.","title":"Weather augmentations in Albumentations"},{"location":"examples/example_weather_transforms/#import-the-required-libraries","text":"import random import cv2 from matplotlib import pyplot as plt import albumentations as A","title":"Import the required libraries"},{"location":"examples/example_weather_transforms/#define-a-function-to-visualize-an-image","text":"def visualize ( image ): plt . figure ( figsize = ( 20 , 10 )) plt . axis ( 'off' ) plt . imshow ( image )","title":"Define a function to visualize an image"},{"location":"examples/example_weather_transforms/#load-the-image-from-the-disk","text":"image = cv2 . imread ( 'images/weather_example.jpg' ) image = cv2 . cvtColor ( image , cv2 . COLOR_BGR2RGB )","title":"Load the image from the disk"},{"location":"examples/example_weather_transforms/#visualize-the-original-image","text":"visualize ( image )","title":"Visualize the original image"},{"location":"examples/example_weather_transforms/#randomrain","text":"We fix the random seed for visualization purposes, so the augmentation will always produce the same result. In a real computer vision pipeline, you shouldn't fix the random seed before applying a transform to the image because, in that case, the pipeline will always output the same image. The purpose of image augmentation is to use different transformations each time. transform = A . Compose ( [ A . RandomRain ( brightness_coefficient = 0.9 , drop_width = 1 , blur_value = 5 , p = 1 )], ) random . seed ( 7 ) transformed = transform ( image = image ) visualize ( transformed [ 'image' ])","title":"RandomRain"},{"location":"examples/example_weather_transforms/#randomsnow","text":"transform = A . Compose ( [ A . RandomSnow ( brightness_coeff = 2.5 , snow_point_lower = 0.3 , snow_point_upper = 0.5 , p = 1 )], ) random . seed ( 7 ) transformed = transform ( image = image ) visualize ( transformed [ 'image' ])","title":"RandomSnow"},{"location":"examples/example_weather_transforms/#randomsunflare","text":"transform = A . Compose ( [ A . RandomSunFlare ( flare_roi = ( 0 , 0 , 1 , 0.5 ), angle_lower = 0.5 , p = 1 )], ) random . seed ( 7 ) transformed = transform ( image = image ) visualize ( transformed [ 'image' ])","title":"RandomSunFlare"},{"location":"examples/example_weather_transforms/#randomshadow","text":"transform = A . Compose ( [ A . RandomShadow ( num_shadows_lower = 1 , num_shadows_upper = 1 , shadow_dimension = 5 , shadow_roi = ( 0 , 0.5 , 1 , 1 ), p = 1 )], ) random . seed ( 7 ) transformed = transform ( image = image ) visualize ( transformed [ 'image' ])","title":"RandomShadow"},{"location":"examples/example_weather_transforms/#randomfog","text":"transform = A . Compose ( [ A . RandomFog ( fog_coef_lower = 0.7 , fog_coef_upper = 0.8 , alpha_coef = 0.1 , p = 1 )], ) random . seed ( 7 ) transformed = transform ( image = image ) visualize ( transformed [ 'image' ])","title":"RandomFog"},{"location":"examples/example_weather_transforms/#randomshadow_1","text":"transform = A . Compose ( [ A . RandomShadow ( num_shadows_lower = 1 , num_shadows_upper = 1 , shadow_dimension = 5 , shadow_roi = ( 0 , 0.5 , 1 , 1 ), p = 1 )], ) random . seed ( 7 ) transformed = transform ( image = image ) visualize ( transformed [ 'image' ])","title":"RandomShadow"},{"location":"examples/example_weather_transforms/#randomfog_1","text":"transform = A . Compose ( [ A . RandomFog ( fog_coef_lower = 0.7 , fog_coef_upper = 0.8 , alpha_coef = 0.1 , p = 1 )], ) random . seed ( 7 ) transformed = transform ( image = image ) visualize ( transformed [ 'image' ])","title":"RandomFog"},{"location":"examples/migrating_from_torchvision_to_albumentations/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Migrating from torchvision to Albumentations \u00b6 This notebook shows how you can use Albumentations instead of torchvision to perform data augmentation. Import the required libraries \u00b6 from PIL import Image import cv2 import numpy as np from torch.utils.data import Dataset from torchvision import transforms import albumentations as A from albumentations.pytorch import ToTensorV2 An example pipeline that uses torchvision \u00b6 class TorchvisionDataset ( Dataset ): def __init__ ( self , file_paths , labels , transform = None ): self . file_paths = file_paths self . labels = labels self . transform = transform def __len__ ( self ): return len ( self . file_paths ) def __getitem__ ( self , idx ): label = self . labels [ idx ] file_path = self . file_paths [ idx ] # Read an image with PIL image = Image . open ( file_path ) if self . transform : image = self . transform ( image ) return image , label torchvision_transform = transforms . Compose ([ transforms . Resize (( 256 , 256 )), transforms . RandomCrop ( 224 ), transforms . RandomHorizontalFlip (), transforms . ToTensor (), transforms . Normalize ( mean = [ 0.485 , 0.456 , 0.406 ], std = [ 0.229 , 0.224 , 0.225 ], ) ]) torchvision_dataset = TorchvisionDataset ( file_paths = [ './images/image_1.jpg' , './images/image_2.jpg' , './images/image_3.jpg' ], labels = [ 1 , 2 , 3 ], transform = torchvision_transform , ) The same pipeline with Albumentations \u00b6 class AlbumentationsDataset ( Dataset ): \"\"\"__init__ and __len__ functions are the same as in TorchvisionDataset\"\"\" def __init__ ( self , file_paths , labels , transform = None ): self . file_paths = file_paths self . labels = labels self . transform = transform def __len__ ( self ): return len ( self . file_paths ) def __getitem__ ( self , idx ): label = self . labels [ idx ] file_path = self . file_paths [ idx ] # Read an image with OpenCV image = cv2 . imread ( file_path ) # By default OpenCV uses BGR color space for color images, # so we need to convert the image to RGB color space. image = cv2 . cvtColor ( image , cv2 . COLOR_BGR2RGB ) if self . transform : augmented = self . transform ( image = image ) image = augmented [ 'image' ] return image , label albumentations_transform = A . Compose ([ A . Resize ( 256 , 256 ), A . RandomCrop ( 224 , 224 ), A . HorizontalFlip (), A . Normalize ( mean = [ 0.485 , 0.456 , 0.406 ], std = [ 0.229 , 0.224 , 0.225 ], ), ToTensorV2 () ]) albumentations_dataset = AlbumentationsDataset ( file_paths = [ './images/image_1.jpg' , './images/image_2.jpg' , './images/image_3.jpg' ], labels = [ 1 , 2 , 3 ], transform = albumentations_transform , ) Using albumentations with PIL \u00b6 You can use PIL instead of OpenCV while working with Albumentations, but in that case, you need to convert a PIL image to a NumPy array before applying transformations. Them you need to convert the augmented image back from a NumPy array to a PIL image. class AlbumentationsPilDataset ( Dataset ): \"\"\"__init__ and __len__ functions are the same as in TorchvisionDataset\"\"\" def __init__ ( self , file_paths , labels , transform = None ): self . file_paths = file_paths self . labels = labels self . transform = transform def __len__ ( self ): return len ( self . file_paths ) def __getitem__ ( self , idx ): label = self . labels [ idx ] file_path = self . file_paths [ idx ] image = Image . open ( file_path ) if self . transform : # Convert PIL image to numpy array image_np = np . array ( image ) # Apply transformations augmented = self . transform ( image = image_np ) # Convert numpy array to PIL Image image = Image . fromarray ( augmented [ 'image' ]) return image , label albumentations_pil_transform = A . Compose ([ A . Resize ( 256 , 256 ), A . RandomCrop ( 224 , 224 ), A . HorizontalFlip (), ]) # Note that this dataset will output PIL images and not numpy arrays nor PyTorch tensors albumentations_pil_dataset = AlbumentationsPilDataset ( file_paths = [ './images/image_1.jpg' , './images/image_2.jpg' , './images/image_3.jpg' ], labels = [ 1 , 2 , 3 ], transform = albumentations_pil_transform , ) Albumentations equivalents for torchvision transforms \u00b6 torchvision transform albumentations transform albumentations example Compose Compose Compose([Resize(256, 256), RandomCrop(224, 224)]) CenterCrop CenterCrop CenterCrop(256, 256) ColorJitter HueSaturationValue HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.5) Pad PadIfNeeded PadIfNeeded(min_height=512, min_width=512) RandomAffine ShiftScaleRotate ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=45, p=0.5) RandomCrop RandomCrop RandomCrop(256, 256) RandomGrayscale ToGray ToGray(p=0.5) RandomHorizontalFlip HorizontalFlip HorizontalFlip(p=0.5) RandomRotation Rotate Rotate(limit=45, p=0.5) RandomVerticalFlip VerticalFlip VerticalFlip(p=0.5) Resize Resize Resize(256, 256) Normalize Normalize Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])","title":"Migrating from torchvision to Albumentations"},{"location":"examples/migrating_from_torchvision_to_albumentations/#migrating-from-torchvision-to-albumentations","text":"This notebook shows how you can use Albumentations instead of torchvision to perform data augmentation.","title":"Migrating from torchvision to Albumentations"},{"location":"examples/migrating_from_torchvision_to_albumentations/#import-the-required-libraries","text":"from PIL import Image import cv2 import numpy as np from torch.utils.data import Dataset from torchvision import transforms import albumentations as A from albumentations.pytorch import ToTensorV2","title":"Import the required libraries"},{"location":"examples/migrating_from_torchvision_to_albumentations/#an-example-pipeline-that-uses-torchvision","text":"class TorchvisionDataset ( Dataset ): def __init__ ( self , file_paths , labels , transform = None ): self . file_paths = file_paths self . labels = labels self . transform = transform def __len__ ( self ): return len ( self . file_paths ) def __getitem__ ( self , idx ): label = self . labels [ idx ] file_path = self . file_paths [ idx ] # Read an image with PIL image = Image . open ( file_path ) if self . transform : image = self . transform ( image ) return image , label torchvision_transform = transforms . Compose ([ transforms . Resize (( 256 , 256 )), transforms . RandomCrop ( 224 ), transforms . RandomHorizontalFlip (), transforms . ToTensor (), transforms . Normalize ( mean = [ 0.485 , 0.456 , 0.406 ], std = [ 0.229 , 0.224 , 0.225 ], ) ]) torchvision_dataset = TorchvisionDataset ( file_paths = [ './images/image_1.jpg' , './images/image_2.jpg' , './images/image_3.jpg' ], labels = [ 1 , 2 , 3 ], transform = torchvision_transform , )","title":"An example pipeline that uses torchvision"},{"location":"examples/migrating_from_torchvision_to_albumentations/#the-same-pipeline-with-albumentations","text":"class AlbumentationsDataset ( Dataset ): \"\"\"__init__ and __len__ functions are the same as in TorchvisionDataset\"\"\" def __init__ ( self , file_paths , labels , transform = None ): self . file_paths = file_paths self . labels = labels self . transform = transform def __len__ ( self ): return len ( self . file_paths ) def __getitem__ ( self , idx ): label = self . labels [ idx ] file_path = self . file_paths [ idx ] # Read an image with OpenCV image = cv2 . imread ( file_path ) # By default OpenCV uses BGR color space for color images, # so we need to convert the image to RGB color space. image = cv2 . cvtColor ( image , cv2 . COLOR_BGR2RGB ) if self . transform : augmented = self . transform ( image = image ) image = augmented [ 'image' ] return image , label albumentations_transform = A . Compose ([ A . Resize ( 256 , 256 ), A . RandomCrop ( 224 , 224 ), A . HorizontalFlip (), A . Normalize ( mean = [ 0.485 , 0.456 , 0.406 ], std = [ 0.229 , 0.224 , 0.225 ], ), ToTensorV2 () ]) albumentations_dataset = AlbumentationsDataset ( file_paths = [ './images/image_1.jpg' , './images/image_2.jpg' , './images/image_3.jpg' ], labels = [ 1 , 2 , 3 ], transform = albumentations_transform , )","title":"The same pipeline with Albumentations"},{"location":"examples/migrating_from_torchvision_to_albumentations/#using-albumentations-with-pil","text":"You can use PIL instead of OpenCV while working with Albumentations, but in that case, you need to convert a PIL image to a NumPy array before applying transformations. Them you need to convert the augmented image back from a NumPy array to a PIL image. class AlbumentationsPilDataset ( Dataset ): \"\"\"__init__ and __len__ functions are the same as in TorchvisionDataset\"\"\" def __init__ ( self , file_paths , labels , transform = None ): self . file_paths = file_paths self . labels = labels self . transform = transform def __len__ ( self ): return len ( self . file_paths ) def __getitem__ ( self , idx ): label = self . labels [ idx ] file_path = self . file_paths [ idx ] image = Image . open ( file_path ) if self . transform : # Convert PIL image to numpy array image_np = np . array ( image ) # Apply transformations augmented = self . transform ( image = image_np ) # Convert numpy array to PIL Image image = Image . fromarray ( augmented [ 'image' ]) return image , label albumentations_pil_transform = A . Compose ([ A . Resize ( 256 , 256 ), A . RandomCrop ( 224 , 224 ), A . HorizontalFlip (), ]) # Note that this dataset will output PIL images and not numpy arrays nor PyTorch tensors albumentations_pil_dataset = AlbumentationsPilDataset ( file_paths = [ './images/image_1.jpg' , './images/image_2.jpg' , './images/image_3.jpg' ], labels = [ 1 , 2 , 3 ], transform = albumentations_pil_transform , )","title":"Using albumentations with PIL"},{"location":"examples/migrating_from_torchvision_to_albumentations/#albumentations-equivalents-for-torchvision-transforms","text":"torchvision transform albumentations transform albumentations example Compose Compose Compose([Resize(256, 256), RandomCrop(224, 224)]) CenterCrop CenterCrop CenterCrop(256, 256) ColorJitter HueSaturationValue HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.5) Pad PadIfNeeded PadIfNeeded(min_height=512, min_width=512) RandomAffine ShiftScaleRotate ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=45, p=0.5) RandomCrop RandomCrop RandomCrop(256, 256) RandomGrayscale ToGray ToGray(p=0.5) RandomHorizontalFlip HorizontalFlip HorizontalFlip(p=0.5) RandomRotation Rotate Rotate(limit=45, p=0.5) RandomVerticalFlip VerticalFlip VerticalFlip(p=0.5) Resize Resize Resize(256, 256) Normalize Normalize Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])","title":"Albumentations equivalents for torchvision transforms"},{"location":"examples/pytorch_classification/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); PyTorch and Albumentations for image classification \u00b6 This example shows how to use Albumentations for image classification. We will use the Cats vs. Docs dataset . The task will be to detect whether an image contains a cat or a dog. Import the required libraries \u00b6 from collections import defaultdict import copy import random import os import shutil from urllib.request import urlretrieve import albumentations as A from albumentations.pytorch import ToTensorV2 import cv2 import matplotlib.pyplot as plt from tqdm import tqdm import torch import torch.backends.cudnn as cudnn import torch.nn as nn import torch.optim from torch.utils.data import Dataset , DataLoader import torchvision.models as models cudnn . benchmark = True Define functions to download an archived dataset and unpack it \u00b6 class TqdmUpTo ( tqdm ): def update_to ( self , b = 1 , bsize = 1 , tsize = None ): if tsize is not None : self . total = tsize self . update ( b * bsize - self . n ) def download_url ( url , filepath ): directory = os . path . dirname ( os . path . abspath ( filepath )) os . makedirs ( directory , exist_ok = True ) if os . path . exists ( filepath ): print ( \"Filepath already exists. Skipping download.\" ) return with TqdmUpTo ( unit = \"B\" , unit_scale = True , unit_divisor = 1024 , miniters = 1 , desc = os . path . basename ( filepath )) as t : urlretrieve ( url , filename = filepath , reporthook = t . update_to , data = None ) t . total = t . n def extract_archive ( filepath ): extract_dir = os . path . dirname ( os . path . abspath ( filepath )) shutil . unpack_archive ( filepath , extract_dir ) Set the root directory for the downloaded dataset \u00b6 dataset_directory = os . path . join ( os . environ [ \"HOME\" ], \"datasets/cats-vs-dogs\" ) Download and extract the Cats vs. Docs dataset \u00b6 filepath = os . path . join ( dataset_directory , \"kagglecatsanddogs_3367a.zip\" ) download_url ( url = \"https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_3367a.zip\" , filepath = filepath , ) extract_archive ( filepath ) Filepath already exists. Skipping download. Split files from the dataset into the train and validation sets \u00b6 Some files in the dataset are broken, so we will use only those image files that OpenCV could load correctly. We will use 20000 images for training, 4936 images for validation, and 10 images for testing. root_directory = os . path . join ( dataset_directory , \"PetImages\" ) cat_directory = os . path . join ( root_directory , \"Cat\" ) dog_directory = os . path . join ( root_directory , \"Dog\" ) cat_images_filepaths = sorted ([ os . path . join ( cat_directory , f ) for f in os . listdir ( cat_directory )]) dog_images_filepaths = sorted ([ os . path . join ( dog_directory , f ) for f in os . listdir ( dog_directory )]) images_filepaths = [ * cat_images_filepaths , * dog_images_filepaths ] correct_images_filepaths = [ i for i in images_filepaths if cv2 . imread ( i ) is not None ] random . seed ( 42 ) random . shuffle ( correct_images_filepaths ) train_images_filepaths = correct_images_filepaths [: 20000 ] val_images_filepaths = correct_images_filepaths [ 20000 : - 10 ] test_images_filepaths = correct_images_filepaths [ - 10 :] print ( len ( train_images_filepaths ), len ( val_images_filepaths ), len ( test_images_filepaths )) 20000 4936 10 Define a function to visualize images and their labels \u00b6 Let's define a function that will take a list of images' file paths and their labels and visualize them in a grid. Correct labels are colored green, and incorrectly predicted labels are colored red. def display_image_grid ( images_filepaths , predicted_labels = (), cols = 5 ): rows = len ( images_filepaths ) // cols figure , ax = plt . subplots ( nrows = rows , ncols = cols , figsize = ( 12 , 6 )) for i , image_filepath in enumerate ( images_filepaths ): image = cv2 . imread ( image_filepath ) image = cv2 . cvtColor ( image , cv2 . COLOR_BGR2RGB ) true_label = os . path . normpath ( image_filepath ) . split ( os . sep )[ - 2 ] predicted_label = predicted_labels [ i ] if predicted_labels else true_label color = \"green\" if true_label == predicted_label else \"red\" ax . ravel ()[ i ] . imshow ( image ) ax . ravel ()[ i ] . set_title ( predicted_label , color = color ) ax . ravel ()[ i ] . set_axis_off () plt . tight_layout () plt . show () display_image_grid ( test_images_filepaths ) Define a PyTorch dataset class \u00b6 Next, we define a PyTorch dataset. If you are new to PyTorch datasets, please refer to this tutorial - https://pytorch.org/tutorials/beginner/data_loading_tutorial.html . Out task is binary classification - a model needs to predict whether an image contains a cat or a dog. Our labels will mark the probability that an image contains a cat. So the correct label for an image with a cat will be 1.0 , and the correct label for an image with a dog will be 0.0 . __init__ will receive an optional transform argument. It is a transformation function of the Albumentations augmentation pipeline. Then in __getitem__ , the Dataset class will use that function to augment an image and return it along with the correct label. class CatsVsDogsDataset ( Dataset ): def __init__ ( self , images_filepaths , transform = None ): self . images_filepaths = images_filepaths self . transform = transform def __len__ ( self ): return len ( self . images_filepaths ) def __getitem__ ( self , idx ): image_filepath = self . images_filepaths [ idx ] image = cv2 . imread ( image_filepath ) image = cv2 . cvtColor ( image , cv2 . COLOR_BGR2RGB ) if os . path . normpath ( image_filepath ) . split ( os . sep )[ - 2 ] == \"Cat\" : label = 1.0 else : label = 0.0 if self . transform is not None : image = self . transform ( image = image )[ \"image\" ] return image , label Use Albumentations to define transformation functions for the train and validation datasets \u00b6 We use Albumentations to define augmentation pipelines for training and validation datasets. In both pipelines, we first resize an input image, so its smallest size is 160px, then we take a 128px by 128px crop. For the training dataset, we also apply more augmentations to that crop. Next, we will normalize an image. We first divide all pixel values of an image by 255, so each pixel's value will lie in a range [0.0, 1.0] . Then we will subtract mean pixel values and divide values by the standard deviation. mean and std in augmentation pipelines are taken from the ImageNet dataset . Still, they transfer reasonably well to the Cats vs. Dogs dataset. After that, we will apply ToTensorV2 that converts a NumPy array to a PyTorch tensor, which will serve as an input to a neural network. Note that in the validation pipeline we will use A.CenterCrop instead of A.RandomCrop because we want out validation results to be deterministic (so that they will not depend upon a random location of a crop). train_transform = A . Compose ( [ A . SmallestMaxSize ( max_size = 160 ), A . ShiftScaleRotate ( shift_limit = 0.05 , scale_limit = 0.05 , rotate_limit = 15 , p = 0.5 ), A . RandomCrop ( height = 128 , width = 128 ), A . RGBShift ( r_shift_limit = 15 , g_shift_limit = 15 , b_shift_limit = 15 , p = 0.5 ), A . RandomBrightnessContrast ( p = 0.5 ), A . Normalize ( mean = ( 0.485 , 0.456 , 0.406 ), std = ( 0.229 , 0.224 , 0.225 )), ToTensorV2 (), ] ) train_dataset = CatsVsDogsDataset ( images_filepaths = train_images_filepaths , transform = train_transform ) val_transform = A . Compose ( [ A . SmallestMaxSize ( max_size = 160 ), A . CenterCrop ( height = 128 , width = 128 ), A . Normalize ( mean = ( 0.485 , 0.456 , 0.406 ), std = ( 0.229 , 0.224 , 0.225 )), ToTensorV2 (), ] ) val_dataset = CatsVsDogsDataset ( images_filepaths = val_images_filepaths , transform = val_transform ) Also let's define a function that takes a dataset and visualizes different augmentations applied to the same image. def visualize_augmentations ( dataset , idx = 0 , samples = 10 , cols = 5 ): dataset = copy . deepcopy ( dataset ) dataset . transform = A . Compose ([ t for t in dataset . transform if not isinstance ( t , ( A . Normalize , ToTensorV2 ))]) rows = samples // cols figure , ax = plt . subplots ( nrows = rows , ncols = cols , figsize = ( 12 , 6 )) for i in range ( samples ): image , _ = dataset [ idx ] ax . ravel ()[ i ] . imshow ( image ) ax . ravel ()[ i ] . set_axis_off () plt . tight_layout () plt . show () random . seed ( 42 ) visualize_augmentations ( train_dataset ) Define helpers for training \u00b6 We define a few helpers for our training pipeline. calculate_accuracy takes model predictions and true labels and will return accuracy for those predictions. MetricMonitor helps to track metrics such as accuracy or loss during training and validation def calculate_accuracy ( output , target ): output = torch . sigmoid ( output ) >= 0.5 target = target == 1.0 return torch . true_divide (( target == output ) . sum ( dim = 0 ), output . size ( 0 )) . item () class MetricMonitor : def __init__ ( self , float_precision = 3 ): self . float_precision = float_precision self . reset () def reset ( self ): self . metrics = defaultdict ( lambda : { \"val\" : 0 , \"count\" : 0 , \"avg\" : 0 }) def update ( self , metric_name , val ): metric = self . metrics [ metric_name ] metric [ \"val\" ] += val metric [ \"count\" ] += 1 metric [ \"avg\" ] = metric [ \"val\" ] / metric [ \"count\" ] def __str__ ( self ): return \" | \" . join ( [ \" {metric_name} : {avg:. {float_precision} f}\" . format ( metric_name = metric_name , avg = metric [ \"avg\" ], float_precision = self . float_precision ) for ( metric_name , metric ) in self . metrics . items () ] ) Define training parameters \u00b6 Here we define a few training parameters such as model architecture, learning rate, batch size, epochs, etc params = { \"model\" : \"resnet50\" , \"device\" : \"cuda\" , \"lr\" : 0.001 , \"batch_size\" : 64 , \"num_workers\" : 4 , \"epochs\" : 10 , } Create all required objects and functions for training and validation \u00b6 model = getattr ( models , params [ \"model\" ])( pretrained = False , num_classes = 1 ,) model = model . to ( params [ \"device\" ]) criterion = nn . BCEWithLogitsLoss () . to ( params [ \"device\" ]) optimizer = torch . optim . Adam ( model . parameters (), lr = params [ \"lr\" ]) train_loader = DataLoader ( train_dataset , batch_size = params [ \"batch_size\" ], shuffle = True , num_workers = params [ \"num_workers\" ], pin_memory = True , ) val_loader = DataLoader ( val_dataset , batch_size = params [ \"batch_size\" ], shuffle = False , num_workers = params [ \"num_workers\" ], pin_memory = True , ) def train ( train_loader , model , criterion , optimizer , epoch , params ): metric_monitor = MetricMonitor () model . train () stream = tqdm ( train_loader ) for i , ( images , target ) in enumerate ( stream , start = 1 ): images = images . to ( params [ \"device\" ], non_blocking = True ) target = target . to ( params [ \"device\" ], non_blocking = True ) . float () . view ( - 1 , 1 ) output = model ( images ) loss = criterion ( output , target ) accuracy = calculate_accuracy ( output , target ) metric_monitor . update ( \"Loss\" , loss . item ()) metric_monitor . update ( \"Accuracy\" , accuracy ) optimizer . zero_grad () loss . backward () optimizer . step () stream . set_description ( \"Epoch: {epoch} . Train. {metric_monitor} \" . format ( epoch = epoch , metric_monitor = metric_monitor ) ) def validate ( val_loader , model , criterion , epoch , params ): metric_monitor = MetricMonitor () model . eval () stream = tqdm ( val_loader ) with torch . no_grad (): for i , ( images , target ) in enumerate ( stream , start = 1 ): images = images . to ( params [ \"device\" ], non_blocking = True ) target = target . to ( params [ \"device\" ], non_blocking = True ) . float () . view ( - 1 , 1 ) output = model ( images ) loss = criterion ( output , target ) accuracy = calculate_accuracy ( output , target ) metric_monitor . update ( \"Loss\" , loss . item ()) metric_monitor . update ( \"Accuracy\" , accuracy ) stream . set_description ( \"Epoch: {epoch} . Validation. {metric_monitor} \" . format ( epoch = epoch , metric_monitor = metric_monitor ) ) Train a model \u00b6 for epoch in range ( 1 , params [ \"epochs\" ] + 1 ): train ( train_loader , model , criterion , optimizer , epoch , params ) validate ( val_loader , model , criterion , epoch , params ) Epoch: 1. Train. Loss: 0.700 | Accuracy: 0.598: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 313/313 [00:38<00:00, 8.04it/s] Epoch: 1. Validation. Loss: 0.684 | Accuracy: 0.663: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 78/78 [00:03<00:00, 23.46it/s] Epoch: 2. Train. Loss: 0.611 | Accuracy: 0.675: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 313/313 [00:37<00:00, 8.24it/s] Epoch: 2. Validation. Loss: 0.581 | Accuracy: 0.689: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 78/78 [00:03<00:00, 23.25it/s] Epoch: 3. Train. Loss: 0.513 | Accuracy: 0.752: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 313/313 [00:38<00:00, 8.22it/s] Epoch: 3. Validation. Loss: 0.408 | Accuracy: 0.818: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 78/78 [00:03<00:00, 23.61it/s] Epoch: 4. Train. Loss: 0.440 | Accuracy: 0.796: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 313/313 [00:37<00:00, 8.24it/s] Epoch: 4. Validation. Loss: 0.374 | Accuracy: 0.829: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 78/78 [00:03<00:00, 22.89it/s] Epoch: 5. Train. Loss: 0.391 | Accuracy: 0.821: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 313/313 [00:37<00:00, 8.25it/s] Epoch: 5. Validation. Loss: 0.345 | Accuracy: 0.853: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 78/78 [00:03<00:00, 23.03it/s] Epoch: 6. Train. Loss: 0.343 | Accuracy: 0.845: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 313/313 [00:38<00:00, 8.22it/s] Epoch: 6. Validation. Loss: 0.304 | Accuracy: 0.861: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 78/78 [00:03<00:00, 23.88it/s] Epoch: 7. Train. Loss: 0.312 | Accuracy: 0.858: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 313/313 [00:38<00:00, 8.23it/s] Epoch: 7. Validation. Loss: 0.259 | Accuracy: 0.886: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 78/78 [00:03<00:00, 23.29it/s] Epoch: 8. Train. Loss: 0.284 | Accuracy: 0.875: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 313/313 [00:38<00:00, 8.21it/s] Epoch: 8. Validation. Loss: 0.304 | Accuracy: 0.882: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 78/78 [00:03<00:00, 23.81it/s] Epoch: 9. Train. Loss: 0.265 | Accuracy: 0.884: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 313/313 [00:38<00:00, 8.18it/s] Epoch: 9. Validation. Loss: 0.255 | Accuracy: 0.888: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 78/78 [00:03<00:00, 23.78it/s] Epoch: 10. Train. Loss: 0.248 | Accuracy: 0.890: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 313/313 [00:38<00:00, 8.21it/s] Epoch: 10. Validation. Loss: 0.222 | Accuracy: 0.909: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 78/78 [00:03<00:00, 23.90it/s] Predict labels for images and visualize those predictions \u00b6 Now we have a trained model, so let's try to predict labels for some images and see whether those predictions are correct. First we make the CatsVsDogsInferenceDataset PyTorch dataset. Its code is similar to the training and validation datasets, but the inference dataset returns only an image and not an associated label (because in the real world we usually don't have access to the true labels and want to infer them using our trained model). class CatsVsDogsInferenceDataset ( Dataset ): def __init__ ( self , images_filepaths , transform = None ): self . images_filepaths = images_filepaths self . transform = transform def __len__ ( self ): return len ( self . images_filepaths ) def __getitem__ ( self , idx ): image_filepath = self . images_filepaths [ idx ] image = cv2 . imread ( image_filepath ) image = cv2 . cvtColor ( image , cv2 . COLOR_BGR2RGB ) if self . transform is not None : image = self . transform ( image = image )[ \"image\" ] return image test_transform = A . Compose ( [ A . SmallestMaxSize ( max_size = 160 ), A . CenterCrop ( height = 128 , width = 128 ), A . Normalize ( mean = ( 0.485 , 0.456 , 0.406 ), std = ( 0.229 , 0.224 , 0.225 )), ToTensorV2 (), ] ) test_dataset = CatsVsDogsInferenceDataset ( images_filepaths = test_images_filepaths , transform = test_transform ) test_loader = DataLoader ( test_dataset , batch_size = params [ \"batch_size\" ], shuffle = False , num_workers = params [ \"num_workers\" ], pin_memory = True , ) model = model . eval () predicted_labels = [] with torch . no_grad (): for images in test_loader : images = images . to ( params [ \"device\" ], non_blocking = True ) output = model ( images ) predictions = ( torch . sigmoid ( output ) >= 0.5 )[:, 0 ] . cpu () . numpy () predicted_labels += [ \"Cat\" if is_cat else \"Dog\" for is_cat in predictions ] display_image_grid ( test_images_filepaths , predicted_labels ) As we see our model predicted correct labels for 7 out of 10 images. If you train the model for more epochs, you will obtain better results.","title":"PyTorch and Albumentations for image classification"},{"location":"examples/pytorch_classification/#pytorch-and-albumentations-for-image-classification","text":"This example shows how to use Albumentations for image classification. We will use the Cats vs. Docs dataset . The task will be to detect whether an image contains a cat or a dog.","title":"PyTorch and Albumentations for image classification"},{"location":"examples/pytorch_classification/#import-the-required-libraries","text":"from collections import defaultdict import copy import random import os import shutil from urllib.request import urlretrieve import albumentations as A from albumentations.pytorch import ToTensorV2 import cv2 import matplotlib.pyplot as plt from tqdm import tqdm import torch import torch.backends.cudnn as cudnn import torch.nn as nn import torch.optim from torch.utils.data import Dataset , DataLoader import torchvision.models as models cudnn . benchmark = True","title":"Import the required libraries"},{"location":"examples/pytorch_classification/#define-functions-to-download-an-archived-dataset-and-unpack-it","text":"class TqdmUpTo ( tqdm ): def update_to ( self , b = 1 , bsize = 1 , tsize = None ): if tsize is not None : self . total = tsize self . update ( b * bsize - self . n ) def download_url ( url , filepath ): directory = os . path . dirname ( os . path . abspath ( filepath )) os . makedirs ( directory , exist_ok = True ) if os . path . exists ( filepath ): print ( \"Filepath already exists. Skipping download.\" ) return with TqdmUpTo ( unit = \"B\" , unit_scale = True , unit_divisor = 1024 , miniters = 1 , desc = os . path . basename ( filepath )) as t : urlretrieve ( url , filename = filepath , reporthook = t . update_to , data = None ) t . total = t . n def extract_archive ( filepath ): extract_dir = os . path . dirname ( os . path . abspath ( filepath )) shutil . unpack_archive ( filepath , extract_dir )","title":"Define functions to download an archived dataset and unpack it"},{"location":"examples/pytorch_classification/#set-the-root-directory-for-the-downloaded-dataset","text":"dataset_directory = os . path . join ( os . environ [ \"HOME\" ], \"datasets/cats-vs-dogs\" )","title":"Set the root directory for the downloaded dataset"},{"location":"examples/pytorch_classification/#download-and-extract-the-cats-vs-docs-dataset","text":"filepath = os . path . join ( dataset_directory , \"kagglecatsanddogs_3367a.zip\" ) download_url ( url = \"https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_3367a.zip\" , filepath = filepath , ) extract_archive ( filepath ) Filepath already exists. Skipping download.","title":"Download and extract the Cats vs. Docs dataset"},{"location":"examples/pytorch_classification/#split-files-from-the-dataset-into-the-train-and-validation-sets","text":"Some files in the dataset are broken, so we will use only those image files that OpenCV could load correctly. We will use 20000 images for training, 4936 images for validation, and 10 images for testing. root_directory = os . path . join ( dataset_directory , \"PetImages\" ) cat_directory = os . path . join ( root_directory , \"Cat\" ) dog_directory = os . path . join ( root_directory , \"Dog\" ) cat_images_filepaths = sorted ([ os . path . join ( cat_directory , f ) for f in os . listdir ( cat_directory )]) dog_images_filepaths = sorted ([ os . path . join ( dog_directory , f ) for f in os . listdir ( dog_directory )]) images_filepaths = [ * cat_images_filepaths , * dog_images_filepaths ] correct_images_filepaths = [ i for i in images_filepaths if cv2 . imread ( i ) is not None ] random . seed ( 42 ) random . shuffle ( correct_images_filepaths ) train_images_filepaths = correct_images_filepaths [: 20000 ] val_images_filepaths = correct_images_filepaths [ 20000 : - 10 ] test_images_filepaths = correct_images_filepaths [ - 10 :] print ( len ( train_images_filepaths ), len ( val_images_filepaths ), len ( test_images_filepaths )) 20000 4936 10","title":"Split files from the dataset into the train and validation sets"},{"location":"examples/pytorch_classification/#define-a-function-to-visualize-images-and-their-labels","text":"Let's define a function that will take a list of images' file paths and their labels and visualize them in a grid. Correct labels are colored green, and incorrectly predicted labels are colored red. def display_image_grid ( images_filepaths , predicted_labels = (), cols = 5 ): rows = len ( images_filepaths ) // cols figure , ax = plt . subplots ( nrows = rows , ncols = cols , figsize = ( 12 , 6 )) for i , image_filepath in enumerate ( images_filepaths ): image = cv2 . imread ( image_filepath ) image = cv2 . cvtColor ( image , cv2 . COLOR_BGR2RGB ) true_label = os . path . normpath ( image_filepath ) . split ( os . sep )[ - 2 ] predicted_label = predicted_labels [ i ] if predicted_labels else true_label color = \"green\" if true_label == predicted_label else \"red\" ax . ravel ()[ i ] . imshow ( image ) ax . ravel ()[ i ] . set_title ( predicted_label , color = color ) ax . ravel ()[ i ] . set_axis_off () plt . tight_layout () plt . show () display_image_grid ( test_images_filepaths )","title":"Define a function to visualize images and their labels"},{"location":"examples/pytorch_classification/#define-a-pytorch-dataset-class","text":"Next, we define a PyTorch dataset. If you are new to PyTorch datasets, please refer to this tutorial - https://pytorch.org/tutorials/beginner/data_loading_tutorial.html . Out task is binary classification - a model needs to predict whether an image contains a cat or a dog. Our labels will mark the probability that an image contains a cat. So the correct label for an image with a cat will be 1.0 , and the correct label for an image with a dog will be 0.0 . __init__ will receive an optional transform argument. It is a transformation function of the Albumentations augmentation pipeline. Then in __getitem__ , the Dataset class will use that function to augment an image and return it along with the correct label. class CatsVsDogsDataset ( Dataset ): def __init__ ( self , images_filepaths , transform = None ): self . images_filepaths = images_filepaths self . transform = transform def __len__ ( self ): return len ( self . images_filepaths ) def __getitem__ ( self , idx ): image_filepath = self . images_filepaths [ idx ] image = cv2 . imread ( image_filepath ) image = cv2 . cvtColor ( image , cv2 . COLOR_BGR2RGB ) if os . path . normpath ( image_filepath ) . split ( os . sep )[ - 2 ] == \"Cat\" : label = 1.0 else : label = 0.0 if self . transform is not None : image = self . transform ( image = image )[ \"image\" ] return image , label","title":"Define a PyTorch dataset class"},{"location":"examples/pytorch_classification/#use-albumentations-to-define-transformation-functions-for-the-train-and-validation-datasets","text":"We use Albumentations to define augmentation pipelines for training and validation datasets. In both pipelines, we first resize an input image, so its smallest size is 160px, then we take a 128px by 128px crop. For the training dataset, we also apply more augmentations to that crop. Next, we will normalize an image. We first divide all pixel values of an image by 255, so each pixel's value will lie in a range [0.0, 1.0] . Then we will subtract mean pixel values and divide values by the standard deviation. mean and std in augmentation pipelines are taken from the ImageNet dataset . Still, they transfer reasonably well to the Cats vs. Dogs dataset. After that, we will apply ToTensorV2 that converts a NumPy array to a PyTorch tensor, which will serve as an input to a neural network. Note that in the validation pipeline we will use A.CenterCrop instead of A.RandomCrop because we want out validation results to be deterministic (so that they will not depend upon a random location of a crop). train_transform = A . Compose ( [ A . SmallestMaxSize ( max_size = 160 ), A . ShiftScaleRotate ( shift_limit = 0.05 , scale_limit = 0.05 , rotate_limit = 15 , p = 0.5 ), A . RandomCrop ( height = 128 , width = 128 ), A . RGBShift ( r_shift_limit = 15 , g_shift_limit = 15 , b_shift_limit = 15 , p = 0.5 ), A . RandomBrightnessContrast ( p = 0.5 ), A . Normalize ( mean = ( 0.485 , 0.456 , 0.406 ), std = ( 0.229 , 0.224 , 0.225 )), ToTensorV2 (), ] ) train_dataset = CatsVsDogsDataset ( images_filepaths = train_images_filepaths , transform = train_transform ) val_transform = A . Compose ( [ A . SmallestMaxSize ( max_size = 160 ), A . CenterCrop ( height = 128 , width = 128 ), A . Normalize ( mean = ( 0.485 , 0.456 , 0.406 ), std = ( 0.229 , 0.224 , 0.225 )), ToTensorV2 (), ] ) val_dataset = CatsVsDogsDataset ( images_filepaths = val_images_filepaths , transform = val_transform ) Also let's define a function that takes a dataset and visualizes different augmentations applied to the same image. def visualize_augmentations ( dataset , idx = 0 , samples = 10 , cols = 5 ): dataset = copy . deepcopy ( dataset ) dataset . transform = A . Compose ([ t for t in dataset . transform if not isinstance ( t , ( A . Normalize , ToTensorV2 ))]) rows = samples // cols figure , ax = plt . subplots ( nrows = rows , ncols = cols , figsize = ( 12 , 6 )) for i in range ( samples ): image , _ = dataset [ idx ] ax . ravel ()[ i ] . imshow ( image ) ax . ravel ()[ i ] . set_axis_off () plt . tight_layout () plt . show () random . seed ( 42 ) visualize_augmentations ( train_dataset )","title":"Use Albumentations to define transformation functions for the train and validation datasets"},{"location":"examples/pytorch_classification/#define-helpers-for-training","text":"We define a few helpers for our training pipeline. calculate_accuracy takes model predictions and true labels and will return accuracy for those predictions. MetricMonitor helps to track metrics such as accuracy or loss during training and validation def calculate_accuracy ( output , target ): output = torch . sigmoid ( output ) >= 0.5 target = target == 1.0 return torch . true_divide (( target == output ) . sum ( dim = 0 ), output . size ( 0 )) . item () class MetricMonitor : def __init__ ( self , float_precision = 3 ): self . float_precision = float_precision self . reset () def reset ( self ): self . metrics = defaultdict ( lambda : { \"val\" : 0 , \"count\" : 0 , \"avg\" : 0 }) def update ( self , metric_name , val ): metric = self . metrics [ metric_name ] metric [ \"val\" ] += val metric [ \"count\" ] += 1 metric [ \"avg\" ] = metric [ \"val\" ] / metric [ \"count\" ] def __str__ ( self ): return \" | \" . join ( [ \" {metric_name} : {avg:. {float_precision} f}\" . format ( metric_name = metric_name , avg = metric [ \"avg\" ], float_precision = self . float_precision ) for ( metric_name , metric ) in self . metrics . items () ] )","title":"Define helpers for training"},{"location":"examples/pytorch_classification/#define-training-parameters","text":"Here we define a few training parameters such as model architecture, learning rate, batch size, epochs, etc params = { \"model\" : \"resnet50\" , \"device\" : \"cuda\" , \"lr\" : 0.001 , \"batch_size\" : 64 , \"num_workers\" : 4 , \"epochs\" : 10 , }","title":"Define training parameters"},{"location":"examples/pytorch_classification/#create-all-required-objects-and-functions-for-training-and-validation","text":"model = getattr ( models , params [ \"model\" ])( pretrained = False , num_classes = 1 ,) model = model . to ( params [ \"device\" ]) criterion = nn . BCEWithLogitsLoss () . to ( params [ \"device\" ]) optimizer = torch . optim . Adam ( model . parameters (), lr = params [ \"lr\" ]) train_loader = DataLoader ( train_dataset , batch_size = params [ \"batch_size\" ], shuffle = True , num_workers = params [ \"num_workers\" ], pin_memory = True , ) val_loader = DataLoader ( val_dataset , batch_size = params [ \"batch_size\" ], shuffle = False , num_workers = params [ \"num_workers\" ], pin_memory = True , ) def train ( train_loader , model , criterion , optimizer , epoch , params ): metric_monitor = MetricMonitor () model . train () stream = tqdm ( train_loader ) for i , ( images , target ) in enumerate ( stream , start = 1 ): images = images . to ( params [ \"device\" ], non_blocking = True ) target = target . to ( params [ \"device\" ], non_blocking = True ) . float () . view ( - 1 , 1 ) output = model ( images ) loss = criterion ( output , target ) accuracy = calculate_accuracy ( output , target ) metric_monitor . update ( \"Loss\" , loss . item ()) metric_monitor . update ( \"Accuracy\" , accuracy ) optimizer . zero_grad () loss . backward () optimizer . step () stream . set_description ( \"Epoch: {epoch} . Train. {metric_monitor} \" . format ( epoch = epoch , metric_monitor = metric_monitor ) ) def validate ( val_loader , model , criterion , epoch , params ): metric_monitor = MetricMonitor () model . eval () stream = tqdm ( val_loader ) with torch . no_grad (): for i , ( images , target ) in enumerate ( stream , start = 1 ): images = images . to ( params [ \"device\" ], non_blocking = True ) target = target . to ( params [ \"device\" ], non_blocking = True ) . float () . view ( - 1 , 1 ) output = model ( images ) loss = criterion ( output , target ) accuracy = calculate_accuracy ( output , target ) metric_monitor . update ( \"Loss\" , loss . item ()) metric_monitor . update ( \"Accuracy\" , accuracy ) stream . set_description ( \"Epoch: {epoch} . Validation. {metric_monitor} \" . format ( epoch = epoch , metric_monitor = metric_monitor ) )","title":"Create all required objects and functions for training and validation"},{"location":"examples/pytorch_classification/#train-a-model","text":"for epoch in range ( 1 , params [ \"epochs\" ] + 1 ): train ( train_loader , model , criterion , optimizer , epoch , params ) validate ( val_loader , model , criterion , epoch , params ) Epoch: 1. Train. Loss: 0.700 | Accuracy: 0.598: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 313/313 [00:38<00:00, 8.04it/s] Epoch: 1. Validation. Loss: 0.684 | Accuracy: 0.663: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 78/78 [00:03<00:00, 23.46it/s] Epoch: 2. Train. Loss: 0.611 | Accuracy: 0.675: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 313/313 [00:37<00:00, 8.24it/s] Epoch: 2. Validation. Loss: 0.581 | Accuracy: 0.689: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 78/78 [00:03<00:00, 23.25it/s] Epoch: 3. Train. Loss: 0.513 | Accuracy: 0.752: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 313/313 [00:38<00:00, 8.22it/s] Epoch: 3. Validation. Loss: 0.408 | Accuracy: 0.818: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 78/78 [00:03<00:00, 23.61it/s] Epoch: 4. Train. Loss: 0.440 | Accuracy: 0.796: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 313/313 [00:37<00:00, 8.24it/s] Epoch: 4. Validation. Loss: 0.374 | Accuracy: 0.829: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 78/78 [00:03<00:00, 22.89it/s] Epoch: 5. Train. Loss: 0.391 | Accuracy: 0.821: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 313/313 [00:37<00:00, 8.25it/s] Epoch: 5. Validation. Loss: 0.345 | Accuracy: 0.853: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 78/78 [00:03<00:00, 23.03it/s] Epoch: 6. Train. Loss: 0.343 | Accuracy: 0.845: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 313/313 [00:38<00:00, 8.22it/s] Epoch: 6. Validation. Loss: 0.304 | Accuracy: 0.861: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 78/78 [00:03<00:00, 23.88it/s] Epoch: 7. Train. Loss: 0.312 | Accuracy: 0.858: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 313/313 [00:38<00:00, 8.23it/s] Epoch: 7. Validation. Loss: 0.259 | Accuracy: 0.886: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 78/78 [00:03<00:00, 23.29it/s] Epoch: 8. Train. Loss: 0.284 | Accuracy: 0.875: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 313/313 [00:38<00:00, 8.21it/s] Epoch: 8. Validation. Loss: 0.304 | Accuracy: 0.882: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 78/78 [00:03<00:00, 23.81it/s] Epoch: 9. Train. Loss: 0.265 | Accuracy: 0.884: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 313/313 [00:38<00:00, 8.18it/s] Epoch: 9. Validation. Loss: 0.255 | Accuracy: 0.888: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 78/78 [00:03<00:00, 23.78it/s] Epoch: 10. Train. Loss: 0.248 | Accuracy: 0.890: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 313/313 [00:38<00:00, 8.21it/s] Epoch: 10. Validation. Loss: 0.222 | Accuracy: 0.909: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 78/78 [00:03<00:00, 23.90it/s]","title":"Train a model"},{"location":"examples/pytorch_classification/#predict-labels-for-images-and-visualize-those-predictions","text":"Now we have a trained model, so let's try to predict labels for some images and see whether those predictions are correct. First we make the CatsVsDogsInferenceDataset PyTorch dataset. Its code is similar to the training and validation datasets, but the inference dataset returns only an image and not an associated label (because in the real world we usually don't have access to the true labels and want to infer them using our trained model). class CatsVsDogsInferenceDataset ( Dataset ): def __init__ ( self , images_filepaths , transform = None ): self . images_filepaths = images_filepaths self . transform = transform def __len__ ( self ): return len ( self . images_filepaths ) def __getitem__ ( self , idx ): image_filepath = self . images_filepaths [ idx ] image = cv2 . imread ( image_filepath ) image = cv2 . cvtColor ( image , cv2 . COLOR_BGR2RGB ) if self . transform is not None : image = self . transform ( image = image )[ \"image\" ] return image test_transform = A . Compose ( [ A . SmallestMaxSize ( max_size = 160 ), A . CenterCrop ( height = 128 , width = 128 ), A . Normalize ( mean = ( 0.485 , 0.456 , 0.406 ), std = ( 0.229 , 0.224 , 0.225 )), ToTensorV2 (), ] ) test_dataset = CatsVsDogsInferenceDataset ( images_filepaths = test_images_filepaths , transform = test_transform ) test_loader = DataLoader ( test_dataset , batch_size = params [ \"batch_size\" ], shuffle = False , num_workers = params [ \"num_workers\" ], pin_memory = True , ) model = model . eval () predicted_labels = [] with torch . no_grad (): for images in test_loader : images = images . to ( params [ \"device\" ], non_blocking = True ) output = model ( images ) predictions = ( torch . sigmoid ( output ) >= 0.5 )[:, 0 ] . cpu () . numpy () predicted_labels += [ \"Cat\" if is_cat else \"Dog\" for is_cat in predictions ] display_image_grid ( test_images_filepaths , predicted_labels ) As we see our model predicted correct labels for 7 out of 10 images. If you train the model for more epochs, you will obtain better results.","title":"Predict labels for images and visualize those predictions"},{"location":"examples/pytorch_semantic_segmentation/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); PyTorch and Albumentations for semantic segmentation \u00b6 This example shows how to use Albumentations for binary semantic segmentation. We will use the The Oxford-IIIT Pet Dataset . The task will be to classify each pixel of an input image either as pet or background . Install the required libraries \u00b6 We will use TernausNet , a library that provides pretrained UNet models for the semantic segmentation task. ! pip install ternausnet > / dev / null Import the required libraries \u00b6 from collections import defaultdict import copy import random import os import shutil from urllib.request import urlretrieve import albumentations as A import albumentations.augmentations.functional as F from albumentations.pytorch import ToTensorV2 import cv2 import matplotlib.pyplot as plt import numpy as np import ternausnet.models from tqdm import tqdm import torch import torch.backends.cudnn as cudnn import torch.nn as nn import torch.optim from torch.utils.data import Dataset , DataLoader cudnn . benchmark = True Define functions to download an archived dataset and unpack it \u00b6 class TqdmUpTo ( tqdm ): def update_to ( self , b = 1 , bsize = 1 , tsize = None ): if tsize is not None : self . total = tsize self . update ( b * bsize - self . n ) def download_url ( url , filepath ): directory = os . path . dirname ( os . path . abspath ( filepath )) os . makedirs ( directory , exist_ok = True ) if os . path . exists ( filepath ): print ( \"Dataset already exists on the disk. Skipping download.\" ) return with TqdmUpTo ( unit = \"B\" , unit_scale = True , unit_divisor = 1024 , miniters = 1 , desc = os . path . basename ( filepath )) as t : urlretrieve ( url , filename = filepath , reporthook = t . update_to , data = None ) t . total = t . n def extract_archive ( filepath ): extract_dir = os . path . dirname ( os . path . abspath ( filepath )) shutil . unpack_archive ( filepath , extract_dir ) Set the root directory for the downloaded dataset \u00b6 dataset_directory = os . path . join ( os . environ [ \"HOME\" ], \"datasets/oxford-iiit-pet\" ) Download and extract the Cats vs. Docs dataset \u00b6 filepath = os . path . join ( dataset_directory , \"images.tar.gz\" ) download_url ( url = \"https://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz\" , filepath = filepath , ) extract_archive ( filepath ) Dataset already exists on the disk. Skipping download. filepath = os . path . join ( dataset_directory , \"annotations.tar.gz\" ) download_url ( url = \"https://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz\" , filepath = filepath , ) extract_archive ( filepath ) Dataset already exists on the disk. Skipping download. Split files from the dataset into the train and validation sets \u00b6 Some files in the dataset are broken, so we will use only those image files that OpenCV could load correctly. We will use 6000 images for training, 1374 images for validation, and 10 images for testing. root_directory = os . path . join ( dataset_directory ) images_directory = os . path . join ( root_directory , \"images\" ) masks_directory = os . path . join ( root_directory , \"annotations\" , \"trimaps\" ) images_filenames = list ( sorted ( os . listdir ( images_directory ))) correct_images_filenames = [ i for i in images_filenames if cv2 . imread ( os . path . join ( images_directory , i )) is not None ] random . seed ( 42 ) random . shuffle ( correct_images_filenames ) train_images_filenames = correct_images_filenames [: 6000 ] val_images_filenames = correct_images_filenames [ 6000 : - 10 ] test_images_filenames = images_filenames [ - 10 :] print ( len ( train_images_filenames ), len ( val_images_filenames ), len ( test_images_filenames )) 6000 1374 10 Define a function to preprocess a mask \u00b6 The dataset contains pixel-level trimap segmentation. For each image, there is an associated PNG file with a mask. The size of a mask equals to the size of the related image. Each pixel in a mask image can take one of three values: 1 , 2 , or 3 . 1 means that this pixel of an image belongs to the class pet , 2 - to the class background , 3 - to the class border . Since this example demonstrates a task of binary segmentation (that is assigning one of two classes to each pixel), we will preprocess the mask, so it will contain only two uniques values: 0.0 if a pixel is a background and 1.0 if a pixel is a pet or a border. def preprocess_mask ( mask ): mask = mask . astype ( np . float32 ) mask [ mask == 2.0 ] = 0.0 mask [( mask == 1.0 ) | ( mask == 3.0 )] = 1.0 return mask Define a function to visualize images and their labels \u00b6 Let's define a visualization function that will take a list of images' file names, a path to the directory with images, a path to the directory with masks, and an optional argument with predicted masks (we will use this argument later to show predictions of a model). def display_image_grid ( images_filenames , images_directory , masks_directory , predicted_masks = None ): cols = 3 if predicted_masks else 2 rows = len ( images_filenames ) figure , ax = plt . subplots ( nrows = rows , ncols = cols , figsize = ( 10 , 24 )) for i , image_filename in enumerate ( images_filenames ): image = cv2 . imread ( os . path . join ( images_directory , image_filename )) image = cv2 . cvtColor ( image , cv2 . COLOR_BGR2RGB ) mask = cv2 . imread ( os . path . join ( masks_directory , image_filename . replace ( \".jpg\" , \".png\" )), cv2 . IMREAD_UNCHANGED ,) mask = preprocess_mask ( mask ) ax [ i , 0 ] . imshow ( image ) ax [ i , 1 ] . imshow ( mask , interpolation = \"nearest\" ) ax [ i , 0 ] . set_title ( \"Image\" ) ax [ i , 1 ] . set_title ( \"Ground truth mask\" ) ax [ i , 0 ] . set_axis_off () ax [ i , 1 ] . set_axis_off () if predicted_masks : predicted_mask = predicted_masks [ i ] ax [ i , 2 ] . imshow ( predicted_mask , interpolation = \"nearest\" ) ax [ i , 2 ] . set_title ( \"Predicted mask\" ) ax [ i , 2 ] . set_axis_off () plt . tight_layout () plt . show () display_image_grid ( test_images_filenames , images_directory , masks_directory ) Image sizes for training and prediction \u00b6 Often, images that you use for training and inference have different heights and widths and different aspect ratios. That fact brings two challenges to a deep learning pipeline: - PyTorch requires all images in a batch to have the same height and width. - If a neural network is not fully convolutional, you have to use the same width and height for all images during training and inference. Fully convolutional architectures, such as UNet, can work with images of any size. There are three common ways to deal with those challenges: 1. Resize all images and masks to a fixed size (e.g., 256x256 pixels) during training. After a model predicts a mask with that fixed size during inference, resize the mask to the original image size. This approach is simple, but it has a few drawbacks: - The predicted mask is smaller than the image, and the mask may lose some context and important details of the original image. - This approach may be problematic if images in your dataset have different aspect ratios. For example, suppose you are resizing an image with the size 1024x512 pixels (so an image with an aspect ratio of 2:1) to 256x256 pixels (1:1 aspect ratio). In that case, this transformation will distort the image and may also affect the quality of predictions. 2. If you use a fully convolutional neural network, you can train a model with image crops, but use original images for inference. This option usually provides the best tradeoff between quality, speed of training, and hardware requirements. 3. Do not alter the sizes of images and use source images both for training and inference. With this approach, you won't lose any information. However, original images could be quite large, so they may require a lot of GPU memory. Also, this approach requires more training time to obtain good results. Some architectures, such as UNet, require that an image's size must be divisible by a downsampling factor of a network (usually 32), so you may also need to pad an image with borders. Albumentations provides a particular transformation for that case. The following example shows how different types of images look. example_image_filename = correct_images_filenames [ 0 ] image = cv2 . imread ( os . path . join ( images_directory , example_image_filename )) image = cv2 . cvtColor ( image , cv2 . COLOR_BGR2RGB ) resized_image = F . resize ( image , height = 256 , width = 256 ) padded_image = F . pad ( image , min_height = 512 , min_width = 512 ) padded_constant_image = F . pad ( image , min_height = 512 , min_width = 512 , border_mode = cv2 . BORDER_CONSTANT ) cropped_image = F . center_crop ( image , crop_height = 256 , crop_width = 256 ) figure , ax = plt . subplots ( nrows = 1 , ncols = 5 , figsize = ( 18 , 10 )) ax . ravel ()[ 0 ] . imshow ( image ) ax . ravel ()[ 0 ] . set_title ( \"Original image\" ) ax . ravel ()[ 1 ] . imshow ( resized_image ) ax . ravel ()[ 1 ] . set_title ( \"Resized image\" ) ax . ravel ()[ 2 ] . imshow ( cropped_image ) ax . ravel ()[ 2 ] . set_title ( \"Cropped image\" ) ax . ravel ()[ 3 ] . imshow ( padded_image ) ax . ravel ()[ 3 ] . set_title ( \"Image padded with reflection\" ) ax . ravel ()[ 4 ] . imshow ( padded_constant_image ) ax . ravel ()[ 4 ] . set_title ( \"Image padded with constant padding\" ) plt . tight_layout () plt . show () In this tutorial, we will explore all three approaches for dealing with image sizes. Approach 1. Resize all images and masks to a fixed size (e.g., 256x256 pixels). \u00b6 Define a PyTorch dataset class \u00b6 Next, we define a PyTorch dataset. If you are new to PyTorch datasets, please refer to this tutorial - https://pytorch.org/tutorials/beginner/data_loading_tutorial.html . __init__ will receive an optional transform argument. It is a transformation function of the Albumentations augmentation pipeline. Then in __getitem__ , the Dataset class will use that function to augment an image and a mask and return their augmented versions. class OxfordPetDataset ( Dataset ): def __init__ ( self , images_filenames , images_directory , masks_directory , transform = None ): self . images_filenames = images_filenames self . images_directory = images_directory self . masks_directory = masks_directory self . transform = transform def __len__ ( self ): return len ( self . images_filenames ) def __getitem__ ( self , idx ): image_filename = self . images_filenames [ idx ] image = cv2 . imread ( os . path . join ( self . images_directory , image_filename )) image = cv2 . cvtColor ( image , cv2 . COLOR_BGR2RGB ) mask = cv2 . imread ( os . path . join ( self . masks_directory , image_filename . replace ( \".jpg\" , \".png\" )), cv2 . IMREAD_UNCHANGED , ) mask = preprocess_mask ( mask ) if self . transform is not None : transformed = self . transform ( image = image , mask = mask ) image = transformed [ \"image\" ] mask = transformed [ \"mask\" ] return image , mask Next, we create augmentation pipelines for the training and validation datasets. Note that we use A.Resize(256, 256) to resize input images and masks to the size 256x256 pixels. train_transform = A . Compose ( [ A . Resize ( 256 , 256 ), A . ShiftScaleRotate ( shift_limit = 0.2 , scale_limit = 0.2 , rotate_limit = 30 , p = 0.5 ), A . RGBShift ( r_shift_limit = 25 , g_shift_limit = 25 , b_shift_limit = 25 , p = 0.5 ), A . RandomBrightnessContrast ( brightness_limit = 0.3 , contrast_limit = 0.3 , p = 0.5 ), A . Normalize ( mean = ( 0.485 , 0.456 , 0.406 ), std = ( 0.229 , 0.224 , 0.225 )), ToTensorV2 (), ] ) train_dataset = OxfordPetDataset ( train_images_filenames , images_directory , masks_directory , transform = train_transform ,) val_transform = A . Compose ( [ A . Resize ( 256 , 256 ), A . Normalize ( mean = ( 0.485 , 0.456 , 0.406 ), std = ( 0.229 , 0.224 , 0.225 )), ToTensorV2 ()] ) val_dataset = OxfordPetDataset ( val_images_filenames , images_directory , masks_directory , transform = val_transform ,) Let's define a function that takes a dataset and visualizes different augmentations applied to the same image and the associated mask. def visualize_augmentations ( dataset , idx = 0 , samples = 5 ): dataset = copy . deepcopy ( dataset ) dataset . transform = A . Compose ([ t for t in dataset . transform if not isinstance ( t , ( A . Normalize , ToTensorV2 ))]) figure , ax = plt . subplots ( nrows = samples , ncols = 2 , figsize = ( 10 , 24 )) for i in range ( samples ): image , mask = dataset [ idx ] ax [ i , 0 ] . imshow ( image ) ax [ i , 1 ] . imshow ( mask , interpolation = \"nearest\" ) ax [ i , 0 ] . set_title ( \"Augmented image\" ) ax [ i , 1 ] . set_title ( \"Augmented mask\" ) ax [ i , 0 ] . set_axis_off () ax [ i , 1 ] . set_axis_off () plt . tight_layout () plt . show () random . seed ( 42 ) visualize_augmentations ( train_dataset , idx = 55 ) Define helpers for training \u00b6 MetricMonitor helps to track metrics such as accuracy or loss during training and validation. class MetricMonitor : def __init__ ( self , float_precision = 3 ): self . float_precision = float_precision self . reset () def reset ( self ): self . metrics = defaultdict ( lambda : { \"val\" : 0 , \"count\" : 0 , \"avg\" : 0 }) def update ( self , metric_name , val ): metric = self . metrics [ metric_name ] metric [ \"val\" ] += val metric [ \"count\" ] += 1 metric [ \"avg\" ] = metric [ \"val\" ] / metric [ \"count\" ] def __str__ ( self ): return \" | \" . join ( [ \" {metric_name} : {avg:. {float_precision} f}\" . format ( metric_name = metric_name , avg = metric [ \"avg\" ], float_precision = self . float_precision ) for ( metric_name , metric ) in self . metrics . items () ] ) Define functions for training and validation \u00b6 def train ( train_loader , model , criterion , optimizer , epoch , params ): metric_monitor = MetricMonitor () model . train () stream = tqdm ( train_loader ) for i , ( images , target ) in enumerate ( stream , start = 1 ): images = images . to ( params [ \"device\" ], non_blocking = True ) target = target . to ( params [ \"device\" ], non_blocking = True ) output = model ( images ) . squeeze ( 1 ) loss = criterion ( output , target ) metric_monitor . update ( \"Loss\" , loss . item ()) optimizer . zero_grad () loss . backward () optimizer . step () stream . set_description ( \"Epoch: {epoch} . Train. {metric_monitor} \" . format ( epoch = epoch , metric_monitor = metric_monitor ) ) def validate ( val_loader , model , criterion , epoch , params ): metric_monitor = MetricMonitor () model . eval () stream = tqdm ( val_loader ) with torch . no_grad (): for i , ( images , target ) in enumerate ( stream , start = 1 ): images = images . to ( params [ \"device\" ], non_blocking = True ) target = target . to ( params [ \"device\" ], non_blocking = True ) output = model ( images ) . squeeze ( 1 ) loss = criterion ( output , target ) metric_monitor . update ( \"Loss\" , loss . item ()) stream . set_description ( \"Epoch: {epoch} . Validation. {metric_monitor} \" . format ( epoch = epoch , metric_monitor = metric_monitor ) ) def create_model ( params ): model = getattr ( ternausnet . models , params [ \"model\" ])( pretrained = True ) model = model . to ( params [ \"device\" ]) return model def train_and_validate ( model , train_dataset , val_dataset , params ): train_loader = DataLoader ( train_dataset , batch_size = params [ \"batch_size\" ], shuffle = True , num_workers = params [ \"num_workers\" ], pin_memory = True , ) val_loader = DataLoader ( val_dataset , batch_size = params [ \"batch_size\" ], shuffle = False , num_workers = params [ \"num_workers\" ], pin_memory = True , ) criterion = nn . BCEWithLogitsLoss () . to ( params [ \"device\" ]) optimizer = torch . optim . Adam ( model . parameters (), lr = params [ \"lr\" ]) for epoch in range ( 1 , params [ \"epochs\" ] + 1 ): train ( train_loader , model , criterion , optimizer , epoch , params ) validate ( val_loader , model , criterion , epoch , params ) return model def predict ( model , params , test_dataset , batch_size ): test_loader = DataLoader ( test_dataset , batch_size = batch_size , shuffle = False , num_workers = params [ \"num_workers\" ], pin_memory = True , ) model . eval () predictions = [] with torch . no_grad (): for images , ( original_heights , original_widths ) in test_loader : images = images . to ( params [ \"device\" ], non_blocking = True ) output = model ( images ) probabilities = torch . sigmoid ( output . squeeze ( 1 )) predicted_masks = ( probabilities >= 0.5 ) . float () * 1 predicted_masks = predicted_masks . cpu () . numpy () for predicted_mask , original_height , original_width in zip ( predicted_masks , original_heights . numpy (), original_widths . numpy () ): predictions . append (( predicted_mask , original_height , original_width )) return predictions Define training parameters \u00b6 Here we define a few training parameters such as model architecture, learning rate, batch size, epochs, etc. params = { \"model\" : \"UNet11\" , \"device\" : \"cuda\" , \"lr\" : 0.001 , \"batch_size\" : 16 , \"num_workers\" : 4 , \"epochs\" : 10 , } Train a model \u00b6 model = create_model ( params ) model = train_and_validate ( model , train_dataset , val_dataset , params ) Epoch: 1. Train. Loss: 0.415: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 375/375 [01:42<00:00, 3.66it/s] Epoch: 1. Validation. Loss: 0.210: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 86/86 [00:09<00:00, 9.55it/s] Epoch: 2. Train. Loss: 0.257: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 375/375 [01:40<00:00, 3.75it/s] Epoch: 2. Validation. Loss: 0.178: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 86/86 [00:08<00:00, 10.62it/s] Epoch: 3. Train. Loss: 0.221: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 375/375 [01:39<00:00, 3.75it/s] Epoch: 3. Validation. Loss: 0.168: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 86/86 [00:08<00:00, 10.58it/s] Epoch: 4. Train. Loss: 0.209: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 375/375 [01:40<00:00, 3.75it/s] Epoch: 4. Validation. Loss: 0.156: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 86/86 [00:08<00:00, 10.57it/s] Epoch: 5. Train. Loss: 0.190: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 375/375 [01:40<00:00, 3.75it/s] Epoch: 5. Validation. Loss: 0.149: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 86/86 [00:08<00:00, 10.57it/s] Epoch: 6. Train. Loss: 0.179: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 375/375 [01:39<00:00, 3.75it/s] Epoch: 6. Validation. Loss: 0.155: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 86/86 [00:08<00:00, 10.55it/s] Epoch: 7. Train. Loss: 0.175: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 375/375 [01:40<00:00, 3.75it/s] Epoch: 7. Validation. Loss: 0.147: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 86/86 [00:08<00:00, 10.59it/s] Epoch: 8. Train. Loss: 0.167: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 375/375 [01:40<00:00, 3.75it/s] Epoch: 8. Validation. Loss: 0.146: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 86/86 [00:08<00:00, 10.61it/s] Epoch: 9. Train. Loss: 0.165: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 375/375 [01:40<00:00, 3.75it/s] Epoch: 9. Validation. Loss: 0.131: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 86/86 [00:08<00:00, 10.56it/s] Epoch: 10. Train. Loss: 0.156: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 375/375 [01:40<00:00, 3.75it/s] Epoch: 10. Validation. Loss: 0.140: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 86/86 [00:08<00:00, 10.60it/s] Predict labels for images and visualize those predictions \u00b6 Now we have a trained model, so let's try to predict masks for some images. Note that the __getitem__ method returns not only an image but also the original height and width of an image. We will use those values to resize a predicted mask from the size of 256x256 pixels to the original image's size. class OxfordPetInferenceDataset ( Dataset ): def __init__ ( self , images_filenames , images_directory , transform = None ): self . images_filenames = images_filenames self . images_directory = images_directory self . transform = transform def __len__ ( self ): return len ( self . images_filenames ) def __getitem__ ( self , idx ): image_filename = self . images_filenames [ idx ] image = cv2 . imread ( os . path . join ( self . images_directory , image_filename )) image = cv2 . cvtColor ( image , cv2 . COLOR_BGR2RGB ) original_size = tuple ( image . shape [: 2 ]) if self . transform is not None : transformed = self . transform ( image = image ) image = transformed [ \"image\" ] return image , original_size test_transform = A . Compose ( [ A . Resize ( 256 , 256 ), A . Normalize ( mean = ( 0.485 , 0.456 , 0.406 ), std = ( 0.229 , 0.224 , 0.225 )), ToTensorV2 ()] ) test_dataset = OxfordPetInferenceDataset ( test_images_filenames , images_directory , transform = test_transform ,) predictions = predict ( model , params , test_dataset , batch_size = 16 ) Next, we will resize the predicted masks with the size of 256x256 pixels to the original images' size. predicted_masks = [] for predicted_256x256_mask , original_height , original_width in predictions : full_sized_mask = F . resize ( predicted_256x256_mask , height = original_height , width = original_width , interpolation = cv2 . INTER_NEAREST ) predicted_masks . append ( full_sized_mask ) display_image_grid ( test_images_filenames , images_directory , masks_directory , predicted_masks = predicted_masks ) Approach 2. Train on crops, predict masks for full-sized images \u00b6 We will reuse most of the code from the previous example. Heights and widths of the same images in the dataset are less than the crop size (256x256 pixels), so we first apply A.PadIfNeeded(min_height=256, min_width=256) which will pad an image if its height or width is less than 256 pixels. train_transform = A . Compose ( [ A . PadIfNeeded ( min_height = 256 , min_width = 256 ), A . RandomCrop ( 256 , 256 ), A . ShiftScaleRotate ( shift_limit = 0.05 , scale_limit = 0.05 , rotate_limit = 15 , p = 0.5 ), A . RGBShift ( r_shift_limit = 15 , g_shift_limit = 15 , b_shift_limit = 15 , p = 0.5 ), A . RandomBrightnessContrast ( p = 0.5 ), A . Normalize ( mean = ( 0.485 , 0.456 , 0.406 ), std = ( 0.229 , 0.224 , 0.225 )), ToTensorV2 (), ] ) train_dataset = OxfordPetDataset ( train_images_filenames , images_directory , masks_directory , transform = train_transform ,) val_transform = A . Compose ( [ A . PadIfNeeded ( min_height = 256 , min_width = 256 ), A . CenterCrop ( 256 , 256 ), A . Normalize ( mean = ( 0.485 , 0.456 , 0.406 ), std = ( 0.229 , 0.224 , 0.225 )), ToTensorV2 (), ] ) val_dataset = OxfordPetDataset ( val_images_filenames , images_directory , masks_directory , transform = val_transform ,) params = { \"model\" : \"UNet11\" , \"device\" : \"cuda\" , \"lr\" : 0.001 , \"batch_size\" : 16 , \"num_workers\" : 4 , \"epochs\" : 10 , } model = create_model ( params ) model = train_and_validate ( model , train_dataset , val_dataset , params ) Epoch: 1. Train. Loss: 0.445: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 375/375 [01:40<00:00, 3.75it/s] Epoch: 1. Validation. Loss: 0.279: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 86/86 [00:08<00:00, 10.49it/s] Epoch: 2. Train. Loss: 0.311: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 375/375 [01:39<00:00, 3.75it/s] Epoch: 2. Validation. Loss: 0.238: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 86/86 [00:08<00:00, 10.51it/s] Epoch: 3. Train. Loss: 0.259: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 375/375 [01:39<00:00, 3.75it/s] Epoch: 3. Validation. Loss: 0.206: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 86/86 [00:08<00:00, 10.54it/s] Epoch: 4. Train. Loss: 0.244: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 375/375 [01:39<00:00, 3.75it/s] Epoch: 4. Validation. Loss: 0.211: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 86/86 [00:08<00:00, 10.54it/s] Epoch: 5. Train. Loss: 0.224: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 375/375 [01:40<00:00, 3.74it/s] Epoch: 5. Validation. Loss: 0.270: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 86/86 [00:08<00:00, 10.47it/s] Epoch: 6. Train. Loss: 0.207: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 375/375 [01:40<00:00, 3.75it/s] Epoch: 6. Validation. Loss: 0.169: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 86/86 [00:08<00:00, 10.56it/s] Epoch: 7. Train. Loss: 0.212: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 375/375 [01:40<00:00, 3.75it/s] Epoch: 7. Validation. Loss: 0.169: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 86/86 [00:08<00:00, 10.56it/s] Epoch: 8. Train. Loss: 0.189: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 375/375 [01:40<00:00, 3.75it/s] Epoch: 8. Validation. Loss: 0.201: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 86/86 [00:08<00:00, 10.52it/s] Epoch: 9. Train. Loss: 0.185: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 375/375 [01:39<00:00, 3.75it/s] Epoch: 9. Validation. Loss: 0.162: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 86/86 [00:08<00:00, 10.54it/s] Epoch: 10. Train. Loss: 0.187: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 375/375 [01:39<00:00, 3.75it/s] Epoch: 10. Validation. Loss: 0.159: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 86/86 [00:08<00:00, 10.49it/s] All images in the test dataset have a max side with size 500 pixels. Since PyTorch requires that all images in a batch must have the same dimensions, and also UNet requires that the size of an image will be divisible by 16, we will apply A.PadIfNeeded(min_height=512, min_width=512, border_mode=cv2.BORDER_CONSTANT) . That augmentation will pad image borders with zeros so the image size will become 512x512 pixels. test_transform = A . Compose ( [ A . PadIfNeeded ( min_height = 512 , min_width = 512 , border_mode = cv2 . BORDER_CONSTANT ), A . Normalize ( mean = ( 0.485 , 0.456 , 0.406 ), std = ( 0.229 , 0.224 , 0.225 )), ToTensorV2 (), ] ) test_dataset = OxfordPetInferenceDataset ( test_images_filenames , images_directory , transform = test_transform ,) predictions = predict ( model , params , test_dataset , batch_size = 16 ) Since we received masks for padded images, we need to crop a part of the original image size from the padded mask. predicted_masks = [] for predicted_padded_mask , original_height , original_width in predictions : cropped_mask = F . center_crop ( predicted_padded_mask , original_height , original_width ) predicted_masks . append ( cropped_mask ) display_image_grid ( test_images_filenames , images_directory , masks_directory , predicted_masks = predicted_masks ) Approach 3. Use original images. \u00b6 We could also use original images without resizing or cropping them. However, there is a problem with this dataset. A few images in the dataset are so large that even with batch_size=1 , they require more than 11Gb of GPU memory for training. So as a tradeoff, we will first apply the A.LongestMaxSize(512) augmentation that will ensure that an image's largest size is no more than 512 pixels. That augmentation will affect only 137 out of 7384 dataset images. Next will use A.PadIfNeeded(min_height=512, min_width=512) to ensure that all images in a batch will have size 512x512 pixels. train_transform = A . Compose ( [ A . LongestMaxSize ( 512 ), A . PadIfNeeded ( min_height = 512 , min_width = 512 ), A . ShiftScaleRotate ( shift_limit = 0.05 , scale_limit = 0.05 , rotate_limit = 15 , p = 0.5 ), A . RGBShift ( r_shift_limit = 15 , g_shift_limit = 15 , b_shift_limit = 15 , p = 0.5 ), A . RandomBrightnessContrast ( p = 0.5 ), A . Normalize ( mean = ( 0.485 , 0.456 , 0.406 ), std = ( 0.229 , 0.224 , 0.225 )), ToTensorV2 (), ] ) train_dataset = OxfordPetDataset ( train_images_filenames , images_directory , masks_directory , transform = train_transform ,) val_transform = A . Compose ( [ A . LongestMaxSize ( 512 ), A . PadIfNeeded ( min_height = 512 , min_width = 512 , border_mode = cv2 . BORDER_CONSTANT ), A . Normalize ( mean = ( 0.485 , 0.456 , 0.406 ), std = ( 0.229 , 0.224 , 0.225 )), ToTensorV2 (), ] ) val_dataset = OxfordPetDataset ( val_images_filenames , images_directory , masks_directory , transform = val_transform ,) params = { \"model\" : \"UNet11\" , \"device\" : \"cuda\" , \"lr\" : 0.001 , \"batch_size\" : 8 , \"num_workers\" : 4 , \"epochs\" : 10 , } model = create_model ( params ) model = train_and_validate ( model , train_dataset , val_dataset , params ) Epoch: 1. Train. Loss: 0.442: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 750/750 [06:58<00:00, 1.79it/s] Epoch: 1. Validation. Loss: 0.225: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 172/172 [00:35<00:00, 4.80it/s] Epoch: 2. Train. Loss: 0.283: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 750/750 [06:54<00:00, 1.81it/s] Epoch: 2. Validation. Loss: 0.188: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 172/172 [00:34<00:00, 4.99it/s] Epoch: 3. Train. Loss: 0.234: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 750/750 [06:53<00:00, 1.81it/s] Epoch: 3. Validation. Loss: 0.154: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 172/172 [00:34<00:00, 4.96it/s] Epoch: 4. Train. Loss: 0.211: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 750/750 [06:53<00:00, 1.81it/s] Epoch: 4. Validation. Loss: 0.136: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 172/172 [00:34<00:00, 4.99it/s] Epoch: 5. Train. Loss: 0.196: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 750/750 [06:53<00:00, 1.81it/s] Epoch: 5. Validation. Loss: 0.131: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 172/172 [00:34<00:00, 4.96it/s] Epoch: 6. Train. Loss: 0.187: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 750/750 [06:53<00:00, 1.81it/s] Epoch: 6. Validation. Loss: 0.151: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 172/172 [00:34<00:00, 4.98it/s] Epoch: 7. Train. Loss: 0.177: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 750/750 [06:53<00:00, 1.81it/s] Epoch: 7. Validation. Loss: 0.127: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 172/172 [00:34<00:00, 4.98it/s] Epoch: 8. Train. Loss: 0.171: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 750/750 [06:53<00:00, 1.81it/s] Epoch: 8. Validation. Loss: 0.113: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 172/172 [00:34<00:00, 4.99it/s] Epoch: 9. Train. Loss: 0.162: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 750/750 [06:54<00:00, 1.81it/s] Epoch: 9. Validation. Loss: 0.143: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 172/172 [00:34<00:00, 4.94it/s] Epoch: 10. Train. Loss: 0.157: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 750/750 [06:53<00:00, 1.81it/s] Epoch: 10. Validation. Loss: 0.115: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 172/172 [00:34<00:00, 4.97it/s] Next, we will use the same code that we were using in Approach 2 to make predictions. test_transform = A . Compose ( [ A . PadIfNeeded ( min_height = 512 , min_width = 512 , border_mode = cv2 . BORDER_CONSTANT ), A . Normalize ( mean = ( 0.485 , 0.456 , 0.406 ), std = ( 0.229 , 0.224 , 0.225 )), ToTensorV2 (), ] ) test_dataset = OxfordPetInferenceDataset ( test_images_filenames , images_directory , transform = test_transform ,) predictions = predict ( model , params , test_dataset , batch_size = 16 ) predicted_masks = [] for predicted_padded_mask , original_height , original_width in predictions : cropped_mask = F . center_crop ( predicted_padded_mask , original_height , original_width ) predicted_masks . append ( cropped_mask ) display_image_grid ( test_images_filenames , images_directory , masks_directory , predicted_masks = predicted_masks )","title":"PyTorch and Albumentations for semantic segmentation"},{"location":"examples/pytorch_semantic_segmentation/#pytorch-and-albumentations-for-semantic-segmentation","text":"This example shows how to use Albumentations for binary semantic segmentation. We will use the The Oxford-IIIT Pet Dataset . The task will be to classify each pixel of an input image either as pet or background .","title":"PyTorch and Albumentations for semantic segmentation"},{"location":"examples/pytorch_semantic_segmentation/#install-the-required-libraries","text":"We will use TernausNet , a library that provides pretrained UNet models for the semantic segmentation task. ! pip install ternausnet > / dev / null","title":"Install the required libraries"},{"location":"examples/pytorch_semantic_segmentation/#import-the-required-libraries","text":"from collections import defaultdict import copy import random import os import shutil from urllib.request import urlretrieve import albumentations as A import albumentations.augmentations.functional as F from albumentations.pytorch import ToTensorV2 import cv2 import matplotlib.pyplot as plt import numpy as np import ternausnet.models from tqdm import tqdm import torch import torch.backends.cudnn as cudnn import torch.nn as nn import torch.optim from torch.utils.data import Dataset , DataLoader cudnn . benchmark = True","title":"Import the required libraries"},{"location":"examples/pytorch_semantic_segmentation/#define-functions-to-download-an-archived-dataset-and-unpack-it","text":"class TqdmUpTo ( tqdm ): def update_to ( self , b = 1 , bsize = 1 , tsize = None ): if tsize is not None : self . total = tsize self . update ( b * bsize - self . n ) def download_url ( url , filepath ): directory = os . path . dirname ( os . path . abspath ( filepath )) os . makedirs ( directory , exist_ok = True ) if os . path . exists ( filepath ): print ( \"Dataset already exists on the disk. Skipping download.\" ) return with TqdmUpTo ( unit = \"B\" , unit_scale = True , unit_divisor = 1024 , miniters = 1 , desc = os . path . basename ( filepath )) as t : urlretrieve ( url , filename = filepath , reporthook = t . update_to , data = None ) t . total = t . n def extract_archive ( filepath ): extract_dir = os . path . dirname ( os . path . abspath ( filepath )) shutil . unpack_archive ( filepath , extract_dir )","title":"Define functions to download an archived dataset and unpack it"},{"location":"examples/pytorch_semantic_segmentation/#set-the-root-directory-for-the-downloaded-dataset","text":"dataset_directory = os . path . join ( os . environ [ \"HOME\" ], \"datasets/oxford-iiit-pet\" )","title":"Set the root directory for the downloaded dataset"},{"location":"examples/pytorch_semantic_segmentation/#download-and-extract-the-cats-vs-docs-dataset","text":"filepath = os . path . join ( dataset_directory , \"images.tar.gz\" ) download_url ( url = \"https://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz\" , filepath = filepath , ) extract_archive ( filepath ) Dataset already exists on the disk. Skipping download. filepath = os . path . join ( dataset_directory , \"annotations.tar.gz\" ) download_url ( url = \"https://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz\" , filepath = filepath , ) extract_archive ( filepath ) Dataset already exists on the disk. Skipping download.","title":"Download and extract the Cats vs. Docs dataset"},{"location":"examples/pytorch_semantic_segmentation/#split-files-from-the-dataset-into-the-train-and-validation-sets","text":"Some files in the dataset are broken, so we will use only those image files that OpenCV could load correctly. We will use 6000 images for training, 1374 images for validation, and 10 images for testing. root_directory = os . path . join ( dataset_directory ) images_directory = os . path . join ( root_directory , \"images\" ) masks_directory = os . path . join ( root_directory , \"annotations\" , \"trimaps\" ) images_filenames = list ( sorted ( os . listdir ( images_directory ))) correct_images_filenames = [ i for i in images_filenames if cv2 . imread ( os . path . join ( images_directory , i )) is not None ] random . seed ( 42 ) random . shuffle ( correct_images_filenames ) train_images_filenames = correct_images_filenames [: 6000 ] val_images_filenames = correct_images_filenames [ 6000 : - 10 ] test_images_filenames = images_filenames [ - 10 :] print ( len ( train_images_filenames ), len ( val_images_filenames ), len ( test_images_filenames )) 6000 1374 10","title":"Split files from the dataset into the train and validation sets"},{"location":"examples/pytorch_semantic_segmentation/#define-a-function-to-preprocess-a-mask","text":"The dataset contains pixel-level trimap segmentation. For each image, there is an associated PNG file with a mask. The size of a mask equals to the size of the related image. Each pixel in a mask image can take one of three values: 1 , 2 , or 3 . 1 means that this pixel of an image belongs to the class pet , 2 - to the class background , 3 - to the class border . Since this example demonstrates a task of binary segmentation (that is assigning one of two classes to each pixel), we will preprocess the mask, so it will contain only two uniques values: 0.0 if a pixel is a background and 1.0 if a pixel is a pet or a border. def preprocess_mask ( mask ): mask = mask . astype ( np . float32 ) mask [ mask == 2.0 ] = 0.0 mask [( mask == 1.0 ) | ( mask == 3.0 )] = 1.0 return mask","title":"Define a function to preprocess a mask"},{"location":"examples/pytorch_semantic_segmentation/#define-a-function-to-visualize-images-and-their-labels","text":"Let's define a visualization function that will take a list of images' file names, a path to the directory with images, a path to the directory with masks, and an optional argument with predicted masks (we will use this argument later to show predictions of a model). def display_image_grid ( images_filenames , images_directory , masks_directory , predicted_masks = None ): cols = 3 if predicted_masks else 2 rows = len ( images_filenames ) figure , ax = plt . subplots ( nrows = rows , ncols = cols , figsize = ( 10 , 24 )) for i , image_filename in enumerate ( images_filenames ): image = cv2 . imread ( os . path . join ( images_directory , image_filename )) image = cv2 . cvtColor ( image , cv2 . COLOR_BGR2RGB ) mask = cv2 . imread ( os . path . join ( masks_directory , image_filename . replace ( \".jpg\" , \".png\" )), cv2 . IMREAD_UNCHANGED ,) mask = preprocess_mask ( mask ) ax [ i , 0 ] . imshow ( image ) ax [ i , 1 ] . imshow ( mask , interpolation = \"nearest\" ) ax [ i , 0 ] . set_title ( \"Image\" ) ax [ i , 1 ] . set_title ( \"Ground truth mask\" ) ax [ i , 0 ] . set_axis_off () ax [ i , 1 ] . set_axis_off () if predicted_masks : predicted_mask = predicted_masks [ i ] ax [ i , 2 ] . imshow ( predicted_mask , interpolation = \"nearest\" ) ax [ i , 2 ] . set_title ( \"Predicted mask\" ) ax [ i , 2 ] . set_axis_off () plt . tight_layout () plt . show () display_image_grid ( test_images_filenames , images_directory , masks_directory )","title":"Define a function to visualize images and their labels"},{"location":"examples/pytorch_semantic_segmentation/#image-sizes-for-training-and-prediction","text":"Often, images that you use for training and inference have different heights and widths and different aspect ratios. That fact brings two challenges to a deep learning pipeline: - PyTorch requires all images in a batch to have the same height and width. - If a neural network is not fully convolutional, you have to use the same width and height for all images during training and inference. Fully convolutional architectures, such as UNet, can work with images of any size. There are three common ways to deal with those challenges: 1. Resize all images and masks to a fixed size (e.g., 256x256 pixels) during training. After a model predicts a mask with that fixed size during inference, resize the mask to the original image size. This approach is simple, but it has a few drawbacks: - The predicted mask is smaller than the image, and the mask may lose some context and important details of the original image. - This approach may be problematic if images in your dataset have different aspect ratios. For example, suppose you are resizing an image with the size 1024x512 pixels (so an image with an aspect ratio of 2:1) to 256x256 pixels (1:1 aspect ratio). In that case, this transformation will distort the image and may also affect the quality of predictions. 2. If you use a fully convolutional neural network, you can train a model with image crops, but use original images for inference. This option usually provides the best tradeoff between quality, speed of training, and hardware requirements. 3. Do not alter the sizes of images and use source images both for training and inference. With this approach, you won't lose any information. However, original images could be quite large, so they may require a lot of GPU memory. Also, this approach requires more training time to obtain good results. Some architectures, such as UNet, require that an image's size must be divisible by a downsampling factor of a network (usually 32), so you may also need to pad an image with borders. Albumentations provides a particular transformation for that case. The following example shows how different types of images look. example_image_filename = correct_images_filenames [ 0 ] image = cv2 . imread ( os . path . join ( images_directory , example_image_filename )) image = cv2 . cvtColor ( image , cv2 . COLOR_BGR2RGB ) resized_image = F . resize ( image , height = 256 , width = 256 ) padded_image = F . pad ( image , min_height = 512 , min_width = 512 ) padded_constant_image = F . pad ( image , min_height = 512 , min_width = 512 , border_mode = cv2 . BORDER_CONSTANT ) cropped_image = F . center_crop ( image , crop_height = 256 , crop_width = 256 ) figure , ax = plt . subplots ( nrows = 1 , ncols = 5 , figsize = ( 18 , 10 )) ax . ravel ()[ 0 ] . imshow ( image ) ax . ravel ()[ 0 ] . set_title ( \"Original image\" ) ax . ravel ()[ 1 ] . imshow ( resized_image ) ax . ravel ()[ 1 ] . set_title ( \"Resized image\" ) ax . ravel ()[ 2 ] . imshow ( cropped_image ) ax . ravel ()[ 2 ] . set_title ( \"Cropped image\" ) ax . ravel ()[ 3 ] . imshow ( padded_image ) ax . ravel ()[ 3 ] . set_title ( \"Image padded with reflection\" ) ax . ravel ()[ 4 ] . imshow ( padded_constant_image ) ax . ravel ()[ 4 ] . set_title ( \"Image padded with constant padding\" ) plt . tight_layout () plt . show () In this tutorial, we will explore all three approaches for dealing with image sizes.","title":"Image sizes for training and prediction"},{"location":"examples/pytorch_semantic_segmentation/#approach-1-resize-all-images-and-masks-to-a-fixed-size-eg-256x256-pixels","text":"","title":"Approach 1. Resize all images and masks to a fixed size (e.g., 256x256 pixels)."},{"location":"examples/pytorch_semantic_segmentation/#define-a-pytorch-dataset-class","text":"Next, we define a PyTorch dataset. If you are new to PyTorch datasets, please refer to this tutorial - https://pytorch.org/tutorials/beginner/data_loading_tutorial.html . __init__ will receive an optional transform argument. It is a transformation function of the Albumentations augmentation pipeline. Then in __getitem__ , the Dataset class will use that function to augment an image and a mask and return their augmented versions. class OxfordPetDataset ( Dataset ): def __init__ ( self , images_filenames , images_directory , masks_directory , transform = None ): self . images_filenames = images_filenames self . images_directory = images_directory self . masks_directory = masks_directory self . transform = transform def __len__ ( self ): return len ( self . images_filenames ) def __getitem__ ( self , idx ): image_filename = self . images_filenames [ idx ] image = cv2 . imread ( os . path . join ( self . images_directory , image_filename )) image = cv2 . cvtColor ( image , cv2 . COLOR_BGR2RGB ) mask = cv2 . imread ( os . path . join ( self . masks_directory , image_filename . replace ( \".jpg\" , \".png\" )), cv2 . IMREAD_UNCHANGED , ) mask = preprocess_mask ( mask ) if self . transform is not None : transformed = self . transform ( image = image , mask = mask ) image = transformed [ \"image\" ] mask = transformed [ \"mask\" ] return image , mask Next, we create augmentation pipelines for the training and validation datasets. Note that we use A.Resize(256, 256) to resize input images and masks to the size 256x256 pixels. train_transform = A . Compose ( [ A . Resize ( 256 , 256 ), A . ShiftScaleRotate ( shift_limit = 0.2 , scale_limit = 0.2 , rotate_limit = 30 , p = 0.5 ), A . RGBShift ( r_shift_limit = 25 , g_shift_limit = 25 , b_shift_limit = 25 , p = 0.5 ), A . RandomBrightnessContrast ( brightness_limit = 0.3 , contrast_limit = 0.3 , p = 0.5 ), A . Normalize ( mean = ( 0.485 , 0.456 , 0.406 ), std = ( 0.229 , 0.224 , 0.225 )), ToTensorV2 (), ] ) train_dataset = OxfordPetDataset ( train_images_filenames , images_directory , masks_directory , transform = train_transform ,) val_transform = A . Compose ( [ A . Resize ( 256 , 256 ), A . Normalize ( mean = ( 0.485 , 0.456 , 0.406 ), std = ( 0.229 , 0.224 , 0.225 )), ToTensorV2 ()] ) val_dataset = OxfordPetDataset ( val_images_filenames , images_directory , masks_directory , transform = val_transform ,) Let's define a function that takes a dataset and visualizes different augmentations applied to the same image and the associated mask. def visualize_augmentations ( dataset , idx = 0 , samples = 5 ): dataset = copy . deepcopy ( dataset ) dataset . transform = A . Compose ([ t for t in dataset . transform if not isinstance ( t , ( A . Normalize , ToTensorV2 ))]) figure , ax = plt . subplots ( nrows = samples , ncols = 2 , figsize = ( 10 , 24 )) for i in range ( samples ): image , mask = dataset [ idx ] ax [ i , 0 ] . imshow ( image ) ax [ i , 1 ] . imshow ( mask , interpolation = \"nearest\" ) ax [ i , 0 ] . set_title ( \"Augmented image\" ) ax [ i , 1 ] . set_title ( \"Augmented mask\" ) ax [ i , 0 ] . set_axis_off () ax [ i , 1 ] . set_axis_off () plt . tight_layout () plt . show () random . seed ( 42 ) visualize_augmentations ( train_dataset , idx = 55 )","title":"Define a PyTorch dataset class"},{"location":"examples/pytorch_semantic_segmentation/#define-helpers-for-training","text":"MetricMonitor helps to track metrics such as accuracy or loss during training and validation. class MetricMonitor : def __init__ ( self , float_precision = 3 ): self . float_precision = float_precision self . reset () def reset ( self ): self . metrics = defaultdict ( lambda : { \"val\" : 0 , \"count\" : 0 , \"avg\" : 0 }) def update ( self , metric_name , val ): metric = self . metrics [ metric_name ] metric [ \"val\" ] += val metric [ \"count\" ] += 1 metric [ \"avg\" ] = metric [ \"val\" ] / metric [ \"count\" ] def __str__ ( self ): return \" | \" . join ( [ \" {metric_name} : {avg:. {float_precision} f}\" . format ( metric_name = metric_name , avg = metric [ \"avg\" ], float_precision = self . float_precision ) for ( metric_name , metric ) in self . metrics . items () ] )","title":"Define helpers for training"},{"location":"examples/pytorch_semantic_segmentation/#define-functions-for-training-and-validation","text":"def train ( train_loader , model , criterion , optimizer , epoch , params ): metric_monitor = MetricMonitor () model . train () stream = tqdm ( train_loader ) for i , ( images , target ) in enumerate ( stream , start = 1 ): images = images . to ( params [ \"device\" ], non_blocking = True ) target = target . to ( params [ \"device\" ], non_blocking = True ) output = model ( images ) . squeeze ( 1 ) loss = criterion ( output , target ) metric_monitor . update ( \"Loss\" , loss . item ()) optimizer . zero_grad () loss . backward () optimizer . step () stream . set_description ( \"Epoch: {epoch} . Train. {metric_monitor} \" . format ( epoch = epoch , metric_monitor = metric_monitor ) ) def validate ( val_loader , model , criterion , epoch , params ): metric_monitor = MetricMonitor () model . eval () stream = tqdm ( val_loader ) with torch . no_grad (): for i , ( images , target ) in enumerate ( stream , start = 1 ): images = images . to ( params [ \"device\" ], non_blocking = True ) target = target . to ( params [ \"device\" ], non_blocking = True ) output = model ( images ) . squeeze ( 1 ) loss = criterion ( output , target ) metric_monitor . update ( \"Loss\" , loss . item ()) stream . set_description ( \"Epoch: {epoch} . Validation. {metric_monitor} \" . format ( epoch = epoch , metric_monitor = metric_monitor ) ) def create_model ( params ): model = getattr ( ternausnet . models , params [ \"model\" ])( pretrained = True ) model = model . to ( params [ \"device\" ]) return model def train_and_validate ( model , train_dataset , val_dataset , params ): train_loader = DataLoader ( train_dataset , batch_size = params [ \"batch_size\" ], shuffle = True , num_workers = params [ \"num_workers\" ], pin_memory = True , ) val_loader = DataLoader ( val_dataset , batch_size = params [ \"batch_size\" ], shuffle = False , num_workers = params [ \"num_workers\" ], pin_memory = True , ) criterion = nn . BCEWithLogitsLoss () . to ( params [ \"device\" ]) optimizer = torch . optim . Adam ( model . parameters (), lr = params [ \"lr\" ]) for epoch in range ( 1 , params [ \"epochs\" ] + 1 ): train ( train_loader , model , criterion , optimizer , epoch , params ) validate ( val_loader , model , criterion , epoch , params ) return model def predict ( model , params , test_dataset , batch_size ): test_loader = DataLoader ( test_dataset , batch_size = batch_size , shuffle = False , num_workers = params [ \"num_workers\" ], pin_memory = True , ) model . eval () predictions = [] with torch . no_grad (): for images , ( original_heights , original_widths ) in test_loader : images = images . to ( params [ \"device\" ], non_blocking = True ) output = model ( images ) probabilities = torch . sigmoid ( output . squeeze ( 1 )) predicted_masks = ( probabilities >= 0.5 ) . float () * 1 predicted_masks = predicted_masks . cpu () . numpy () for predicted_mask , original_height , original_width in zip ( predicted_masks , original_heights . numpy (), original_widths . numpy () ): predictions . append (( predicted_mask , original_height , original_width )) return predictions","title":"Define functions for training and validation"},{"location":"examples/pytorch_semantic_segmentation/#define-training-parameters","text":"Here we define a few training parameters such as model architecture, learning rate, batch size, epochs, etc. params = { \"model\" : \"UNet11\" , \"device\" : \"cuda\" , \"lr\" : 0.001 , \"batch_size\" : 16 , \"num_workers\" : 4 , \"epochs\" : 10 , }","title":"Define training parameters"},{"location":"examples/pytorch_semantic_segmentation/#train-a-model","text":"model = create_model ( params ) model = train_and_validate ( model , train_dataset , val_dataset , params ) Epoch: 1. Train. Loss: 0.415: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 375/375 [01:42<00:00, 3.66it/s] Epoch: 1. Validation. Loss: 0.210: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 86/86 [00:09<00:00, 9.55it/s] Epoch: 2. Train. Loss: 0.257: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 375/375 [01:40<00:00, 3.75it/s] Epoch: 2. Validation. Loss: 0.178: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 86/86 [00:08<00:00, 10.62it/s] Epoch: 3. Train. Loss: 0.221: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 375/375 [01:39<00:00, 3.75it/s] Epoch: 3. Validation. Loss: 0.168: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 86/86 [00:08<00:00, 10.58it/s] Epoch: 4. Train. Loss: 0.209: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 375/375 [01:40<00:00, 3.75it/s] Epoch: 4. Validation. Loss: 0.156: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 86/86 [00:08<00:00, 10.57it/s] Epoch: 5. Train. Loss: 0.190: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 375/375 [01:40<00:00, 3.75it/s] Epoch: 5. Validation. Loss: 0.149: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 86/86 [00:08<00:00, 10.57it/s] Epoch: 6. Train. Loss: 0.179: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 375/375 [01:39<00:00, 3.75it/s] Epoch: 6. Validation. Loss: 0.155: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 86/86 [00:08<00:00, 10.55it/s] Epoch: 7. Train. Loss: 0.175: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 375/375 [01:40<00:00, 3.75it/s] Epoch: 7. Validation. Loss: 0.147: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 86/86 [00:08<00:00, 10.59it/s] Epoch: 8. Train. Loss: 0.167: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 375/375 [01:40<00:00, 3.75it/s] Epoch: 8. Validation. Loss: 0.146: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 86/86 [00:08<00:00, 10.61it/s] Epoch: 9. Train. Loss: 0.165: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 375/375 [01:40<00:00, 3.75it/s] Epoch: 9. Validation. Loss: 0.131: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 86/86 [00:08<00:00, 10.56it/s] Epoch: 10. Train. Loss: 0.156: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 375/375 [01:40<00:00, 3.75it/s] Epoch: 10. Validation. Loss: 0.140: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 86/86 [00:08<00:00, 10.60it/s]","title":"Train a model"},{"location":"examples/pytorch_semantic_segmentation/#predict-labels-for-images-and-visualize-those-predictions","text":"Now we have a trained model, so let's try to predict masks for some images. Note that the __getitem__ method returns not only an image but also the original height and width of an image. We will use those values to resize a predicted mask from the size of 256x256 pixels to the original image's size. class OxfordPetInferenceDataset ( Dataset ): def __init__ ( self , images_filenames , images_directory , transform = None ): self . images_filenames = images_filenames self . images_directory = images_directory self . transform = transform def __len__ ( self ): return len ( self . images_filenames ) def __getitem__ ( self , idx ): image_filename = self . images_filenames [ idx ] image = cv2 . imread ( os . path . join ( self . images_directory , image_filename )) image = cv2 . cvtColor ( image , cv2 . COLOR_BGR2RGB ) original_size = tuple ( image . shape [: 2 ]) if self . transform is not None : transformed = self . transform ( image = image ) image = transformed [ \"image\" ] return image , original_size test_transform = A . Compose ( [ A . Resize ( 256 , 256 ), A . Normalize ( mean = ( 0.485 , 0.456 , 0.406 ), std = ( 0.229 , 0.224 , 0.225 )), ToTensorV2 ()] ) test_dataset = OxfordPetInferenceDataset ( test_images_filenames , images_directory , transform = test_transform ,) predictions = predict ( model , params , test_dataset , batch_size = 16 ) Next, we will resize the predicted masks with the size of 256x256 pixels to the original images' size. predicted_masks = [] for predicted_256x256_mask , original_height , original_width in predictions : full_sized_mask = F . resize ( predicted_256x256_mask , height = original_height , width = original_width , interpolation = cv2 . INTER_NEAREST ) predicted_masks . append ( full_sized_mask ) display_image_grid ( test_images_filenames , images_directory , masks_directory , predicted_masks = predicted_masks )","title":"Predict labels for images and visualize those predictions"},{"location":"examples/pytorch_semantic_segmentation/#approach-2-train-on-crops-predict-masks-for-full-sized-images","text":"We will reuse most of the code from the previous example. Heights and widths of the same images in the dataset are less than the crop size (256x256 pixels), so we first apply A.PadIfNeeded(min_height=256, min_width=256) which will pad an image if its height or width is less than 256 pixels. train_transform = A . Compose ( [ A . PadIfNeeded ( min_height = 256 , min_width = 256 ), A . RandomCrop ( 256 , 256 ), A . ShiftScaleRotate ( shift_limit = 0.05 , scale_limit = 0.05 , rotate_limit = 15 , p = 0.5 ), A . RGBShift ( r_shift_limit = 15 , g_shift_limit = 15 , b_shift_limit = 15 , p = 0.5 ), A . RandomBrightnessContrast ( p = 0.5 ), A . Normalize ( mean = ( 0.485 , 0.456 , 0.406 ), std = ( 0.229 , 0.224 , 0.225 )), ToTensorV2 (), ] ) train_dataset = OxfordPetDataset ( train_images_filenames , images_directory , masks_directory , transform = train_transform ,) val_transform = A . Compose ( [ A . PadIfNeeded ( min_height = 256 , min_width = 256 ), A . CenterCrop ( 256 , 256 ), A . Normalize ( mean = ( 0.485 , 0.456 , 0.406 ), std = ( 0.229 , 0.224 , 0.225 )), ToTensorV2 (), ] ) val_dataset = OxfordPetDataset ( val_images_filenames , images_directory , masks_directory , transform = val_transform ,) params = { \"model\" : \"UNet11\" , \"device\" : \"cuda\" , \"lr\" : 0.001 , \"batch_size\" : 16 , \"num_workers\" : 4 , \"epochs\" : 10 , } model = create_model ( params ) model = train_and_validate ( model , train_dataset , val_dataset , params ) Epoch: 1. Train. Loss: 0.445: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 375/375 [01:40<00:00, 3.75it/s] Epoch: 1. Validation. Loss: 0.279: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 86/86 [00:08<00:00, 10.49it/s] Epoch: 2. Train. Loss: 0.311: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 375/375 [01:39<00:00, 3.75it/s] Epoch: 2. Validation. Loss: 0.238: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 86/86 [00:08<00:00, 10.51it/s] Epoch: 3. Train. Loss: 0.259: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 375/375 [01:39<00:00, 3.75it/s] Epoch: 3. Validation. Loss: 0.206: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 86/86 [00:08<00:00, 10.54it/s] Epoch: 4. Train. Loss: 0.244: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 375/375 [01:39<00:00, 3.75it/s] Epoch: 4. Validation. Loss: 0.211: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 86/86 [00:08<00:00, 10.54it/s] Epoch: 5. Train. Loss: 0.224: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 375/375 [01:40<00:00, 3.74it/s] Epoch: 5. Validation. Loss: 0.270: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 86/86 [00:08<00:00, 10.47it/s] Epoch: 6. Train. Loss: 0.207: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 375/375 [01:40<00:00, 3.75it/s] Epoch: 6. Validation. Loss: 0.169: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 86/86 [00:08<00:00, 10.56it/s] Epoch: 7. Train. Loss: 0.212: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 375/375 [01:40<00:00, 3.75it/s] Epoch: 7. Validation. Loss: 0.169: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 86/86 [00:08<00:00, 10.56it/s] Epoch: 8. Train. Loss: 0.189: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 375/375 [01:40<00:00, 3.75it/s] Epoch: 8. Validation. Loss: 0.201: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 86/86 [00:08<00:00, 10.52it/s] Epoch: 9. Train. Loss: 0.185: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 375/375 [01:39<00:00, 3.75it/s] Epoch: 9. Validation. Loss: 0.162: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 86/86 [00:08<00:00, 10.54it/s] Epoch: 10. Train. Loss: 0.187: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 375/375 [01:39<00:00, 3.75it/s] Epoch: 10. Validation. Loss: 0.159: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 86/86 [00:08<00:00, 10.49it/s] All images in the test dataset have a max side with size 500 pixels. Since PyTorch requires that all images in a batch must have the same dimensions, and also UNet requires that the size of an image will be divisible by 16, we will apply A.PadIfNeeded(min_height=512, min_width=512, border_mode=cv2.BORDER_CONSTANT) . That augmentation will pad image borders with zeros so the image size will become 512x512 pixels. test_transform = A . Compose ( [ A . PadIfNeeded ( min_height = 512 , min_width = 512 , border_mode = cv2 . BORDER_CONSTANT ), A . Normalize ( mean = ( 0.485 , 0.456 , 0.406 ), std = ( 0.229 , 0.224 , 0.225 )), ToTensorV2 (), ] ) test_dataset = OxfordPetInferenceDataset ( test_images_filenames , images_directory , transform = test_transform ,) predictions = predict ( model , params , test_dataset , batch_size = 16 ) Since we received masks for padded images, we need to crop a part of the original image size from the padded mask. predicted_masks = [] for predicted_padded_mask , original_height , original_width in predictions : cropped_mask = F . center_crop ( predicted_padded_mask , original_height , original_width ) predicted_masks . append ( cropped_mask ) display_image_grid ( test_images_filenames , images_directory , masks_directory , predicted_masks = predicted_masks )","title":"Approach 2. Train on crops, predict masks for full-sized images"},{"location":"examples/pytorch_semantic_segmentation/#approach-3-use-original-images","text":"We could also use original images without resizing or cropping them. However, there is a problem with this dataset. A few images in the dataset are so large that even with batch_size=1 , they require more than 11Gb of GPU memory for training. So as a tradeoff, we will first apply the A.LongestMaxSize(512) augmentation that will ensure that an image's largest size is no more than 512 pixels. That augmentation will affect only 137 out of 7384 dataset images. Next will use A.PadIfNeeded(min_height=512, min_width=512) to ensure that all images in a batch will have size 512x512 pixels. train_transform = A . Compose ( [ A . LongestMaxSize ( 512 ), A . PadIfNeeded ( min_height = 512 , min_width = 512 ), A . ShiftScaleRotate ( shift_limit = 0.05 , scale_limit = 0.05 , rotate_limit = 15 , p = 0.5 ), A . RGBShift ( r_shift_limit = 15 , g_shift_limit = 15 , b_shift_limit = 15 , p = 0.5 ), A . RandomBrightnessContrast ( p = 0.5 ), A . Normalize ( mean = ( 0.485 , 0.456 , 0.406 ), std = ( 0.229 , 0.224 , 0.225 )), ToTensorV2 (), ] ) train_dataset = OxfordPetDataset ( train_images_filenames , images_directory , masks_directory , transform = train_transform ,) val_transform = A . Compose ( [ A . LongestMaxSize ( 512 ), A . PadIfNeeded ( min_height = 512 , min_width = 512 , border_mode = cv2 . BORDER_CONSTANT ), A . Normalize ( mean = ( 0.485 , 0.456 , 0.406 ), std = ( 0.229 , 0.224 , 0.225 )), ToTensorV2 (), ] ) val_dataset = OxfordPetDataset ( val_images_filenames , images_directory , masks_directory , transform = val_transform ,) params = { \"model\" : \"UNet11\" , \"device\" : \"cuda\" , \"lr\" : 0.001 , \"batch_size\" : 8 , \"num_workers\" : 4 , \"epochs\" : 10 , } model = create_model ( params ) model = train_and_validate ( model , train_dataset , val_dataset , params ) Epoch: 1. Train. Loss: 0.442: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 750/750 [06:58<00:00, 1.79it/s] Epoch: 1. Validation. Loss: 0.225: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 172/172 [00:35<00:00, 4.80it/s] Epoch: 2. Train. Loss: 0.283: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 750/750 [06:54<00:00, 1.81it/s] Epoch: 2. Validation. Loss: 0.188: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 172/172 [00:34<00:00, 4.99it/s] Epoch: 3. Train. Loss: 0.234: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 750/750 [06:53<00:00, 1.81it/s] Epoch: 3. Validation. Loss: 0.154: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 172/172 [00:34<00:00, 4.96it/s] Epoch: 4. Train. Loss: 0.211: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 750/750 [06:53<00:00, 1.81it/s] Epoch: 4. Validation. Loss: 0.136: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 172/172 [00:34<00:00, 4.99it/s] Epoch: 5. Train. Loss: 0.196: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 750/750 [06:53<00:00, 1.81it/s] Epoch: 5. Validation. Loss: 0.131: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 172/172 [00:34<00:00, 4.96it/s] Epoch: 6. Train. Loss: 0.187: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 750/750 [06:53<00:00, 1.81it/s] Epoch: 6. Validation. Loss: 0.151: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 172/172 [00:34<00:00, 4.98it/s] Epoch: 7. Train. Loss: 0.177: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 750/750 [06:53<00:00, 1.81it/s] Epoch: 7. Validation. Loss: 0.127: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 172/172 [00:34<00:00, 4.98it/s] Epoch: 8. Train. Loss: 0.171: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 750/750 [06:53<00:00, 1.81it/s] Epoch: 8. Validation. Loss: 0.113: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 172/172 [00:34<00:00, 4.99it/s] Epoch: 9. Train. Loss: 0.162: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 750/750 [06:54<00:00, 1.81it/s] Epoch: 9. Validation. Loss: 0.143: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 172/172 [00:34<00:00, 4.94it/s] Epoch: 10. Train. Loss: 0.157: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 750/750 [06:53<00:00, 1.81it/s] Epoch: 10. Validation. Loss: 0.115: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 172/172 [00:34<00:00, 4.97it/s] Next, we will use the same code that we were using in Approach 2 to make predictions. test_transform = A . Compose ( [ A . PadIfNeeded ( min_height = 512 , min_width = 512 , border_mode = cv2 . BORDER_CONSTANT ), A . Normalize ( mean = ( 0.485 , 0.456 , 0.406 ), std = ( 0.229 , 0.224 , 0.225 )), ToTensorV2 (), ] ) test_dataset = OxfordPetInferenceDataset ( test_images_filenames , images_directory , transform = test_transform ,) predictions = predict ( model , params , test_dataset , batch_size = 16 ) predicted_masks = [] for predicted_padded_mask , original_height , original_width in predictions : cropped_mask = F . center_crop ( predicted_padded_mask , original_height , original_width ) predicted_masks . append ( cropped_mask ) display_image_grid ( test_images_filenames , images_directory , masks_directory , predicted_masks = predicted_masks )","title":"Approach 3. Use original images."},{"location":"examples/replay/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Debugging an augmentation pipeline with ReplayCompose \u00b6 An augmentation pipeline has a lot of randomness inside it. It applies augmentations with some probabilities, and it samples parameters for those augmentations (such as a rotation angle or a level of changing brightness) from a random distribution. It could be very useful for debugging purposes to see which augmentations were applied to the image and look at the parameters of those augmentations. ReplayCompose tracks augmentation parameters. You can inspect those parameters or reapply them to another image. Import the required libraries \u00b6 import random import cv2 import matplotlib.pyplot as plt import albumentations as A Define the visualization function \u00b6 def visualize ( image ): plt . figure ( figsize = ( 10 , 10 )) plt . axis ( 'off' ) plt . imshow ( image ) Load an image from the disk \u00b6 image = cv2 . imread ( 'images/parrot.jpg' ) image = cv2 . cvtColor ( image , cv2 . COLOR_BGR2RGB ) visualize ( image ) Declare an augmentation pipeline using ReplayCompose \u00b6 transform = A . ReplayCompose ([ A . Resize ( 512 , 512 , always_apply = True ), A . RandomCrop ( 200 , 200 , always_apply = True ), A . OneOf ([ A . RGBShift (), A . HueSaturationValue () ]), ]) We fix the random seed for visualization purposes, so the augmentation will always produce the same result. In a real computer vision pipeline, you shouldn't fix the random seed before applying a transform to the image because, in that case, the pipeline will always output the same image. The purpose of image augmentation is to use different transformations each time. random . seed ( 7 ) data = transform ( image = image ) visualize ( data [ 'image' ]) data['replay'] contains information about augmentations that ere applied to the image. If applied is True , then the augmentation was applied to the image. params contains information about parameters that were used to transform the image. data [ 'replay' ] {'__class_fullname__': 'albumentations.core.composition.ReplayCompose', 'params': None, 'transforms': [{'__class_fullname__': 'albumentations.augmentations.transforms.Resize', 'always_apply': True, 'p': 1, 'height': 512, 'width': 512, 'interpolation': 1, 'params': {}, 'applied': True}, {'__class_fullname__': 'albumentations.augmentations.transforms.RandomCrop', 'always_apply': True, 'p': 1.0, 'height': 200, 'width': 200, 'params': {'h_start': 0.07243628666754276, 'w_start': 0.5358820043066892}, 'applied': True}, {'__class_fullname__': 'albumentations.core.composition.OneOf', 'params': None, 'transforms': [{'__class_fullname__': 'albumentations.augmentations.transforms.RGBShift', 'always_apply': False, 'p': 0.5, 'r_shift_limit': (-20, 20), 'g_shift_limit': (-20, 20), 'b_shift_limit': (-20, 20), 'params': None, 'applied': False}, {'__class_fullname__': 'albumentations.augmentations.transforms.HueSaturationValue', 'always_apply': False, 'p': 0.5, 'hue_shift_limit': (-20, 20), 'sat_shift_limit': (-30, 30), 'val_shift_limit': (-20, 20), 'params': {'hue_shift': -2.654172653504567, 'sat_shift': -25.808674585522866, 'val_shift': -16.371479466245397}, 'applied': True}], 'applied': True}], 'applied': True} Using ReplayCompose.replay to apply the same augmentations to another image \u00b6 To apply the same set of augmentations to a new target, you can use the ReplayCompose.replay function. Load new images \u00b6 image2 = cv2 . imread ( 'images/image_2.jpg' ) image2 = cv2 . cvtColor ( image2 , cv2 . COLOR_BGR2RGB ) visualize ( image2 ) image3 = cv2 . imread ( 'images/image_3.jpg' ) image3 = cv2 . cvtColor ( image3 , cv2 . COLOR_BGR2RGB ) visualize ( image3 ) Apply augmentations from data['replay'] to those images \u00b6 image2_data = A . ReplayCompose . replay ( data [ 'replay' ], image = image2 ) visualize ( image2_data [ 'image' ]) image3_data = A . ReplayCompose . replay ( data [ 'replay' ], image = image3 ) visualize ( image3_data [ 'image' ])","title":"Debugging an augmentation pipeline with ReplayCompose"},{"location":"examples/replay/#debugging-an-augmentation-pipeline-with-replaycompose","text":"An augmentation pipeline has a lot of randomness inside it. It applies augmentations with some probabilities, and it samples parameters for those augmentations (such as a rotation angle or a level of changing brightness) from a random distribution. It could be very useful for debugging purposes to see which augmentations were applied to the image and look at the parameters of those augmentations. ReplayCompose tracks augmentation parameters. You can inspect those parameters or reapply them to another image.","title":"Debugging an augmentation pipeline with ReplayCompose"},{"location":"examples/replay/#import-the-required-libraries","text":"import random import cv2 import matplotlib.pyplot as plt import albumentations as A","title":"Import the required libraries"},{"location":"examples/replay/#define-the-visualization-function","text":"def visualize ( image ): plt . figure ( figsize = ( 10 , 10 )) plt . axis ( 'off' ) plt . imshow ( image )","title":"Define the visualization function"},{"location":"examples/replay/#load-an-image-from-the-disk","text":"image = cv2 . imread ( 'images/parrot.jpg' ) image = cv2 . cvtColor ( image , cv2 . COLOR_BGR2RGB ) visualize ( image )","title":"Load an image from the disk"},{"location":"examples/replay/#declare-an-augmentation-pipeline-using-replaycompose","text":"transform = A . ReplayCompose ([ A . Resize ( 512 , 512 , always_apply = True ), A . RandomCrop ( 200 , 200 , always_apply = True ), A . OneOf ([ A . RGBShift (), A . HueSaturationValue () ]), ]) We fix the random seed for visualization purposes, so the augmentation will always produce the same result. In a real computer vision pipeline, you shouldn't fix the random seed before applying a transform to the image because, in that case, the pipeline will always output the same image. The purpose of image augmentation is to use different transformations each time. random . seed ( 7 ) data = transform ( image = image ) visualize ( data [ 'image' ]) data['replay'] contains information about augmentations that ere applied to the image. If applied is True , then the augmentation was applied to the image. params contains information about parameters that were used to transform the image. data [ 'replay' ] {'__class_fullname__': 'albumentations.core.composition.ReplayCompose', 'params': None, 'transforms': [{'__class_fullname__': 'albumentations.augmentations.transforms.Resize', 'always_apply': True, 'p': 1, 'height': 512, 'width': 512, 'interpolation': 1, 'params': {}, 'applied': True}, {'__class_fullname__': 'albumentations.augmentations.transforms.RandomCrop', 'always_apply': True, 'p': 1.0, 'height': 200, 'width': 200, 'params': {'h_start': 0.07243628666754276, 'w_start': 0.5358820043066892}, 'applied': True}, {'__class_fullname__': 'albumentations.core.composition.OneOf', 'params': None, 'transforms': [{'__class_fullname__': 'albumentations.augmentations.transforms.RGBShift', 'always_apply': False, 'p': 0.5, 'r_shift_limit': (-20, 20), 'g_shift_limit': (-20, 20), 'b_shift_limit': (-20, 20), 'params': None, 'applied': False}, {'__class_fullname__': 'albumentations.augmentations.transforms.HueSaturationValue', 'always_apply': False, 'p': 0.5, 'hue_shift_limit': (-20, 20), 'sat_shift_limit': (-30, 30), 'val_shift_limit': (-20, 20), 'params': {'hue_shift': -2.654172653504567, 'sat_shift': -25.808674585522866, 'val_shift': -16.371479466245397}, 'applied': True}], 'applied': True}], 'applied': True}","title":"Declare an augmentation pipeline using ReplayCompose"},{"location":"examples/replay/#using-replaycomposereplay-to-apply-the-same-augmentations-to-another-image","text":"To apply the same set of augmentations to a new target, you can use the ReplayCompose.replay function.","title":"Using ReplayCompose.replay to apply the same augmentations to another image"},{"location":"examples/replay/#load-new-images","text":"image2 = cv2 . imread ( 'images/image_2.jpg' ) image2 = cv2 . cvtColor ( image2 , cv2 . COLOR_BGR2RGB ) visualize ( image2 ) image3 = cv2 . imread ( 'images/image_3.jpg' ) image3 = cv2 . cvtColor ( image3 , cv2 . COLOR_BGR2RGB ) visualize ( image3 )","title":"Load new images"},{"location":"examples/replay/#apply-augmentations-from-datareplay-to-those-images","text":"image2_data = A . ReplayCompose . replay ( data [ 'replay' ], image = image2 ) visualize ( image2_data [ 'image' ]) image3_data = A . ReplayCompose . replay ( data [ 'replay' ], image = image3 ) visualize ( image3_data [ 'image' ])","title":"Apply augmentations from data['replay'] to those images"},{"location":"examples/serialization/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); How to save and load parameters of an augmentation pipeline \u00b6 Reproducibility is very important in deep learning. Data scientists and machine learning engineers need a way to save all parameters of deep learning pipelines such as model, optimizer, input datasets, and augmentation parameters and to be able to recreate the same pipeline using that data. Albumentations has built-in functionality to serialize the augmentation parameters and save them. Then you can use those parameters to recreate an augmentation pipeline. Import the required libraries \u00b6 import random import numpy as np import cv2 import matplotlib.pyplot as plt import albumentations as A Define the visualization function \u00b6 def visualize ( image ): plt . figure ( figsize = ( 6 , 6 )) plt . axis ( 'off' ) plt . imshow ( image ) Load an image from the disk \u00b6 image = cv2 . imread ( 'images/parrot.jpg' ) image = cv2 . cvtColor ( image , cv2 . COLOR_BGR2RGB ) visualize ( image ) Define an augmentation pipeline that we want to serialize \u00b6 transform = A . Compose ([ A . RandomCrop ( 768 , 768 ), A . OneOf ([ A . RGBShift (), A . HueSaturationValue () ]), ]) We can pass an instance of augmentation to the print function, and it will print the string representation of it. print ( transform ) Compose([ RandomCrop(always_apply=False, p=1.0, height=768, width=768), OneOf([ RGBShift(always_apply=False, p=0.5, r_shift_limit=(-20, 20), g_shift_limit=(-20, 20), b_shift_limit=(-20, 20)), HueSaturationValue(always_apply=False, p=0.5, hue_shift_limit=(-20, 20), sat_shift_limit=(-30, 30), val_shift_limit=(-20, 20)), ], p=0.5), ], p=1.0, bbox_params=None, keypoint_params=None, additional_targets={}) Next, we will fix the random seed to make augmentation reproducible for visualization purposes and augment an example image. random . seed ( 42 ) transformed = transform ( image = image ) visualize ( transformed [ 'image' ]) Serializing an augmentation pipeline to a JSON or YAML file \u00b6 To save the serialized representation of an augmentation pipeline to a JSON file, use the save function from Albumentations. A . save ( transform , '/tmp/transform.json' ) To load a serialized representation from a JSON file, use the load function from Albumentations. loaded_transform = A . load ( '/tmp/transform.json' ) print ( loaded_transform ) Compose([ RandomCrop(always_apply=False, p=1.0, height=768, width=768), OneOf([ RGBShift(always_apply=False, p=0.5, r_shift_limit=(-20, 20), g_shift_limit=(-20, 20), b_shift_limit=(-20, 20)), HueSaturationValue(always_apply=False, p=0.5, hue_shift_limit=(-20, 20), sat_shift_limit=(-30, 30), val_shift_limit=(-20, 20)), ], p=0.5), ], p=1.0, bbox_params=None, keypoint_params=None, additional_targets={}) Next, we will use the same random seed as before and apply the loaded augmentation pipeline to the same image. random . seed ( 42 ) transformed_from_loaded_transform = loaded_transform ( image = image ) visualize ( transformed_from_loaded_transform [ 'image' ]) assert np . array_equal ( transformed [ 'image' ], transformed_from_loaded_transform [ 'image' ]) As you see, it produced the same result. Using YAML insted of JSON \u00b6 You can also use YAML instead of JSON for serializing and deserializing of augmentation pipelines. To do that add the data_format='yaml' argument to the save and load functions. A . save ( transform , '/tmp/transform.yml' , data_format = 'yaml' ) loaded_transform = A . load ( '/tmp/transform.yml' , data_format = 'yaml' ) print ( loaded_transform ) Compose([ RandomCrop(always_apply=False, p=1.0, height=768, width=768), OneOf([ RGBShift(always_apply=False, p=0.5, r_shift_limit=(-20, 20), g_shift_limit=(-20, 20), b_shift_limit=(-20, 20)), HueSaturationValue(always_apply=False, p=0.5, hue_shift_limit=(-20, 20), sat_shift_limit=(-30, 30), val_shift_limit=(-20, 20)), ], p=0.5), ], p=1.0, bbox_params=None, keypoint_params=None, additional_targets={}) Serializing an augmentation pipeline to a Python dictionary \u00b6 If you need more control over a serialized pipeline, e.g., you want to save a serialized version to a database or send it to a server you can use the to_dict and from_dict functions. to_dict returns a Python dictionary that describes a pipeline. The dictionary will contain only primitive data types such as dictionaries, lists, strings, integers, and floats. To construct a pipeline from a dictionary, you need to call from_dict . transform_dict = A . to_dict ( transform ) loaded_transform = A . from_dict ( transform_dict ) print ( loaded_transform ) Compose([ RandomCrop(always_apply=False, p=1.0, height=768, width=768), OneOf([ RGBShift(always_apply=False, p=0.5, r_shift_limit=(-20, 20), g_shift_limit=(-20, 20), b_shift_limit=(-20, 20)), HueSaturationValue(always_apply=False, p=0.5, hue_shift_limit=(-20, 20), sat_shift_limit=(-30, 30), val_shift_limit=(-20, 20)), ], p=0.5), ], p=1.0, bbox_params=None, keypoint_params=None, additional_targets={}) Serializing and deserializing Lambda transforms \u00b6 Lambda transforms use custom transformation functions provided by a user. For those types of transforms, Albumentations saves only the name and the position in the augmentation pipeline. To deserialize an augmentation pipeline with Lambda transforms, you need to manually provide all Lambda transform instances using the lambda_transforms argument. Let's define a function that we will use to transform an image. def hflip_image ( image , ** kwargs ): return cv2 . flip ( image , 1 ) Next, we create a Lambda transform that will apply the hflip_image function to input images. Note that to make the transform serializable, you need to pass the name argument. hflip = A . Lambda ( name = 'hflip_image' , image = hflip_image , p = 0.5 ) transform = A . Compose ([ hflip ]) print ( transform ) Compose([ Lambda(name='hflip_image', image=<function hflip_image at 0x7feae89a77b8>, mask=<function noop at 0x7fead8ad2268>, keypoint=<function noop at 0x7fead8ad2268>, bbox=<function noop at 0x7fead8ad2268>, always_apply=False, p=0.5), ], p=1.0, bbox_params=None, keypoint_params=None, additional_targets={}) To check that transform is working, we will apply to an image. random . seed ( 7 ) transformed = transform ( image = image ) visualize ( transformed [ 'image' ]) To serialize a pipeline with a Lambda transform, use the save function as before. A . save ( transform , '/tmp/lambda_transform.json' ) To deserialize a pipeline that contains Lambda transforms, you need to pass names and instances of all Lambda transforms in a pipeline through the lambda_transforms argument. loaded_transform = A . load ( '/tmp/lambda_transform.json' , lambda_transforms = { 'hflip_image' : hflip }) print ( loaded_transform ) Compose([ Lambda(name='hflip_image', image=<function hflip_image at 0x7feae89a77b8>, mask=<function noop at 0x7fead8ad2268>, keypoint=<function noop at 0x7fead8ad2268>, bbox=<function noop at 0x7fead8ad2268>, always_apply=False, p=0.5), ], p=1.0, bbox_params=None, keypoint_params=None, additional_targets={}) Verify that the deserialized pipeline produces the same output. random . seed ( 7 ) transformed_from_loaded_transform = loaded_transform ( image = image ) assert np . array_equal ( transformed [ 'image' ], transformed_from_loaded_transform [ 'image' ]) To serialize and deserialize Lambda transforms to and from dictionaries use to_dict and from_dict . transform_dict = A . to_dict ( transform ) print ( transform_dict ) {'__version__': '0.4.5', 'transform': {'__class_fullname__': 'albumentations.core.composition.Compose', 'p': 1.0, 'transforms': [{'__type__': 'Lambda', '__name__': 'hflip_image'}], 'bbox_params': None, 'keypoint_params': None, 'additional_targets': {}}} loaded_transform = A . from_dict ( transform_dict , lambda_transforms = { 'hflip_image' : hflip })","title":"How to save and load parameters of an augmentation pipeline"},{"location":"examples/serialization/#how-to-save-and-load-parameters-of-an-augmentation-pipeline","text":"Reproducibility is very important in deep learning. Data scientists and machine learning engineers need a way to save all parameters of deep learning pipelines such as model, optimizer, input datasets, and augmentation parameters and to be able to recreate the same pipeline using that data. Albumentations has built-in functionality to serialize the augmentation parameters and save them. Then you can use those parameters to recreate an augmentation pipeline.","title":"How to save and load parameters of an augmentation pipeline"},{"location":"examples/serialization/#import-the-required-libraries","text":"import random import numpy as np import cv2 import matplotlib.pyplot as plt import albumentations as A","title":"Import the required libraries"},{"location":"examples/serialization/#define-the-visualization-function","text":"def visualize ( image ): plt . figure ( figsize = ( 6 , 6 )) plt . axis ( 'off' ) plt . imshow ( image )","title":"Define the visualization function"},{"location":"examples/serialization/#load-an-image-from-the-disk","text":"image = cv2 . imread ( 'images/parrot.jpg' ) image = cv2 . cvtColor ( image , cv2 . COLOR_BGR2RGB ) visualize ( image )","title":"Load an image from the disk"},{"location":"examples/serialization/#define-an-augmentation-pipeline-that-we-want-to-serialize","text":"transform = A . Compose ([ A . RandomCrop ( 768 , 768 ), A . OneOf ([ A . RGBShift (), A . HueSaturationValue () ]), ]) We can pass an instance of augmentation to the print function, and it will print the string representation of it. print ( transform ) Compose([ RandomCrop(always_apply=False, p=1.0, height=768, width=768), OneOf([ RGBShift(always_apply=False, p=0.5, r_shift_limit=(-20, 20), g_shift_limit=(-20, 20), b_shift_limit=(-20, 20)), HueSaturationValue(always_apply=False, p=0.5, hue_shift_limit=(-20, 20), sat_shift_limit=(-30, 30), val_shift_limit=(-20, 20)), ], p=0.5), ], p=1.0, bbox_params=None, keypoint_params=None, additional_targets={}) Next, we will fix the random seed to make augmentation reproducible for visualization purposes and augment an example image. random . seed ( 42 ) transformed = transform ( image = image ) visualize ( transformed [ 'image' ])","title":"Define an augmentation pipeline that we want to serialize"},{"location":"examples/serialization/#serializing-an-augmentation-pipeline-to-a-json-or-yaml-file","text":"To save the serialized representation of an augmentation pipeline to a JSON file, use the save function from Albumentations. A . save ( transform , '/tmp/transform.json' ) To load a serialized representation from a JSON file, use the load function from Albumentations. loaded_transform = A . load ( '/tmp/transform.json' ) print ( loaded_transform ) Compose([ RandomCrop(always_apply=False, p=1.0, height=768, width=768), OneOf([ RGBShift(always_apply=False, p=0.5, r_shift_limit=(-20, 20), g_shift_limit=(-20, 20), b_shift_limit=(-20, 20)), HueSaturationValue(always_apply=False, p=0.5, hue_shift_limit=(-20, 20), sat_shift_limit=(-30, 30), val_shift_limit=(-20, 20)), ], p=0.5), ], p=1.0, bbox_params=None, keypoint_params=None, additional_targets={}) Next, we will use the same random seed as before and apply the loaded augmentation pipeline to the same image. random . seed ( 42 ) transformed_from_loaded_transform = loaded_transform ( image = image ) visualize ( transformed_from_loaded_transform [ 'image' ]) assert np . array_equal ( transformed [ 'image' ], transformed_from_loaded_transform [ 'image' ]) As you see, it produced the same result.","title":"Serializing an augmentation pipeline to a JSON or YAML file"},{"location":"examples/serialization/#using-yaml-insted-of-json","text":"You can also use YAML instead of JSON for serializing and deserializing of augmentation pipelines. To do that add the data_format='yaml' argument to the save and load functions. A . save ( transform , '/tmp/transform.yml' , data_format = 'yaml' ) loaded_transform = A . load ( '/tmp/transform.yml' , data_format = 'yaml' ) print ( loaded_transform ) Compose([ RandomCrop(always_apply=False, p=1.0, height=768, width=768), OneOf([ RGBShift(always_apply=False, p=0.5, r_shift_limit=(-20, 20), g_shift_limit=(-20, 20), b_shift_limit=(-20, 20)), HueSaturationValue(always_apply=False, p=0.5, hue_shift_limit=(-20, 20), sat_shift_limit=(-30, 30), val_shift_limit=(-20, 20)), ], p=0.5), ], p=1.0, bbox_params=None, keypoint_params=None, additional_targets={})","title":"Using YAML insted of JSON"},{"location":"examples/serialization/#serializing-an-augmentation-pipeline-to-a-python-dictionary","text":"If you need more control over a serialized pipeline, e.g., you want to save a serialized version to a database or send it to a server you can use the to_dict and from_dict functions. to_dict returns a Python dictionary that describes a pipeline. The dictionary will contain only primitive data types such as dictionaries, lists, strings, integers, and floats. To construct a pipeline from a dictionary, you need to call from_dict . transform_dict = A . to_dict ( transform ) loaded_transform = A . from_dict ( transform_dict ) print ( loaded_transform ) Compose([ RandomCrop(always_apply=False, p=1.0, height=768, width=768), OneOf([ RGBShift(always_apply=False, p=0.5, r_shift_limit=(-20, 20), g_shift_limit=(-20, 20), b_shift_limit=(-20, 20)), HueSaturationValue(always_apply=False, p=0.5, hue_shift_limit=(-20, 20), sat_shift_limit=(-30, 30), val_shift_limit=(-20, 20)), ], p=0.5), ], p=1.0, bbox_params=None, keypoint_params=None, additional_targets={})","title":"Serializing an augmentation pipeline to a Python dictionary"},{"location":"examples/serialization/#serializing-and-deserializing-lambda-transforms","text":"Lambda transforms use custom transformation functions provided by a user. For those types of transforms, Albumentations saves only the name and the position in the augmentation pipeline. To deserialize an augmentation pipeline with Lambda transforms, you need to manually provide all Lambda transform instances using the lambda_transforms argument. Let's define a function that we will use to transform an image. def hflip_image ( image , ** kwargs ): return cv2 . flip ( image , 1 ) Next, we create a Lambda transform that will apply the hflip_image function to input images. Note that to make the transform serializable, you need to pass the name argument. hflip = A . Lambda ( name = 'hflip_image' , image = hflip_image , p = 0.5 ) transform = A . Compose ([ hflip ]) print ( transform ) Compose([ Lambda(name='hflip_image', image=<function hflip_image at 0x7feae89a77b8>, mask=<function noop at 0x7fead8ad2268>, keypoint=<function noop at 0x7fead8ad2268>, bbox=<function noop at 0x7fead8ad2268>, always_apply=False, p=0.5), ], p=1.0, bbox_params=None, keypoint_params=None, additional_targets={}) To check that transform is working, we will apply to an image. random . seed ( 7 ) transformed = transform ( image = image ) visualize ( transformed [ 'image' ]) To serialize a pipeline with a Lambda transform, use the save function as before. A . save ( transform , '/tmp/lambda_transform.json' ) To deserialize a pipeline that contains Lambda transforms, you need to pass names and instances of all Lambda transforms in a pipeline through the lambda_transforms argument. loaded_transform = A . load ( '/tmp/lambda_transform.json' , lambda_transforms = { 'hflip_image' : hflip }) print ( loaded_transform ) Compose([ Lambda(name='hflip_image', image=<function hflip_image at 0x7feae89a77b8>, mask=<function noop at 0x7fead8ad2268>, keypoint=<function noop at 0x7fead8ad2268>, bbox=<function noop at 0x7fead8ad2268>, always_apply=False, p=0.5), ], p=1.0, bbox_params=None, keypoint_params=None, additional_targets={}) Verify that the deserialized pipeline produces the same output. random . seed ( 7 ) transformed_from_loaded_transform = loaded_transform ( image = image ) assert np . array_equal ( transformed [ 'image' ], transformed_from_loaded_transform [ 'image' ]) To serialize and deserialize Lambda transforms to and from dictionaries use to_dict and from_dict . transform_dict = A . to_dict ( transform ) print ( transform_dict ) {'__version__': '0.4.5', 'transform': {'__class_fullname__': 'albumentations.core.composition.Compose', 'p': 1.0, 'transforms': [{'__type__': 'Lambda', '__name__': 'hflip_image'}], 'bbox_params': None, 'keypoint_params': None, 'additional_targets': {}}} loaded_transform = A . from_dict ( transform_dict , lambda_transforms = { 'hflip_image' : hflip })","title":"Serializing and deserializing Lambda transforms"},{"location":"examples/showcase/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Showcase. Cool augmentation examples on diverse set of images from various real-world tasks. \u00b6 Import libraries and define helper functions \u00b6 Import the required libraries \u00b6 import os import numpy as np import cv2 from matplotlib import pyplot as plt from skimage.color import label2rgb import albumentations as A import random Define visualization functions \u00b6 BOX_COLOR = ( 255 , 0 , 0 ) # Red TEXT_COLOR = ( 255 , 255 , 255 ) # White def visualize_bbox ( img , bbox , color = BOX_COLOR , thickness = 2 , ** kwargs ): x_min , y_min , w , h = bbox x_min , x_max , y_min , y_max = int ( x_min ), int ( x_min + w ), int ( y_min ), int ( y_min + h ) cv2 . rectangle ( img , ( x_min , y_min ), ( x_max , y_max ), color = color , thickness = thickness ) return img def visualize_titles ( img , bbox , title , color = BOX_COLOR , thickness = 2 , font_thickness = 2 , font_scale = 0.35 , ** kwargs ): x_min , y_min , w , h = bbox x_min , x_max , y_min , y_max = int ( x_min ), int ( x_min + w ), int ( y_min ), int ( y_min + h ) (( text_width , text_height ), _ ) = cv2 . getTextSize ( title , cv2 . FONT_HERSHEY_SIMPLEX , font_scale , font_thickness ) cv2 . rectangle ( img , ( x_min , y_min - int ( 1.3 * text_height )), ( x_min + text_width , y_min ), BOX_COLOR , - 1 ) cv2 . putText ( img , title , ( x_min , y_min - int ( 0.3 * text_height )), cv2 . FONT_HERSHEY_SIMPLEX , font_scale , TEXT_COLOR , font_thickness , lineType = cv2 . LINE_AA ) return img def augment_and_show ( aug , image , mask = None , bboxes = [], categories = [], category_id_to_name = [], filename = None , font_scale_orig = 0.35 , font_scale_aug = 0.35 , show_title = True , ** kwargs ): augmented = aug ( image = image , mask = mask , bboxes = bboxes , category_id = categories ) image = cv2 . cvtColor ( image , cv2 . COLOR_BGR2RGB ) image_aug = cv2 . cvtColor ( augmented [ 'image' ], cv2 . COLOR_BGR2RGB ) for bbox in bboxes : visualize_bbox ( image , bbox , ** kwargs ) for bbox in augmented [ 'bboxes' ]: visualize_bbox ( image_aug , bbox , ** kwargs ) if show_title : for bbox , cat_id in zip ( bboxes , categories ): visualize_titles ( image , bbox , category_id_to_name [ cat_id ], font_scale = font_scale_orig , ** kwargs ) for bbox , cat_id in zip ( augmented [ 'bboxes' ], augmented [ 'category_id' ]): visualize_titles ( image_aug , bbox , category_id_to_name [ cat_id ], font_scale = font_scale_aug , ** kwargs ) if mask is None : f , ax = plt . subplots ( 1 , 2 , figsize = ( 16 , 8 )) ax [ 0 ] . imshow ( image ) ax [ 0 ] . set_title ( 'Original image' ) ax [ 1 ] . imshow ( image_aug ) ax [ 1 ] . set_title ( 'Augmented image' ) else : f , ax = plt . subplots ( 2 , 2 , figsize = ( 16 , 16 )) if len ( mask . shape ) != 3 : mask = label2rgb ( mask , bg_label = 0 ) mask_aug = label2rgb ( augmented [ 'mask' ], bg_label = 0 ) else : mask = cv2 . cvtColor ( mask , cv2 . COLOR_BGR2RGB ) mask_aug = cv2 . cvtColor ( augmented [ 'mask' ], cv2 . COLOR_BGR2RGB ) ax [ 0 , 0 ] . imshow ( image ) ax [ 0 , 0 ] . set_title ( 'Original image' ) ax [ 0 , 1 ] . imshow ( image_aug ) ax [ 0 , 1 ] . set_title ( 'Augmented image' ) ax [ 1 , 0 ] . imshow ( mask , interpolation = 'nearest' ) ax [ 1 , 0 ] . set_title ( 'Original mask' ) ax [ 1 , 1 ] . imshow ( mask_aug , interpolation = 'nearest' ) ax [ 1 , 1 ] . set_title ( 'Augmented mask' ) f . tight_layout () if filename is not None : f . savefig ( filename ) return augmented [ 'image' ], augmented [ 'mask' ], augmented [ 'bboxes' ] def find_in_dir ( dirname ): return [ os . path . join ( dirname , fname ) for fname in sorted ( os . listdir ( dirname ))] Color augmentations \u00b6 image = cv2 . imread ( 'images/parrot.jpg' ) random . seed ( 42 ) light = A . Compose ([ A . RandomBrightnessContrast ( p = 1 ), A . RandomGamma ( p = 1 ), A . CLAHE ( p = 1 ), ], p = 1 ) medium = A . Compose ([ A . CLAHE ( p = 1 ), A . HueSaturationValue ( hue_shift_limit = 20 , sat_shift_limit = 50 , val_shift_limit = 50 , p = 1 ), ], p = 1 ) strong = A . Compose ([ A . ChannelShuffle ( p = 1 ), ], p = 1 ) r = augment_and_show ( light , image ) r = augment_and_show ( medium , image ) r = augment_and_show ( strong , image ) Inria Aerial Image Labeling Dataset \u00b6 random . seed ( 42 ) image , mask = cv2 . imread ( 'images/inria/inria_tyrol_w4_image.jpg' ), cv2 . imread ( 'images/inria/inria_tyrol_w4_mask.tif' , cv2 . IMREAD_GRAYSCALE ) image , mask = image [: 1024 , : 1024 ], mask [: 1024 ,: 1024 ] light = A . Compose ([ A . RandomSizedCrop (( 512 - 100 , 512 + 100 ), 512 , 512 ), A . ShiftScaleRotate (), A . RGBShift (), A . Blur (), A . GaussNoise (), A . ElasticTransform (), A . MaskDropout (( 10 , 15 ), p = 1 ), A . Cutout ( p = 1 ) ], p = 1 ) r = augment_and_show ( light , image , mask ) 2018 Data Science Bowl \u00b6 random . seed ( 42 ) image = cv2 . imread ( 'images/dsb2018/1a11552569160f0b1ea10bedbd628ce6c14f29edec5092034c2309c556df833e/images/1a11552569160f0b1ea10bedbd628ce6c14f29edec5092034c2309c556df833e.png' ) masks = [ cv2 . imread ( x , cv2 . IMREAD_GRAYSCALE ) for x in find_in_dir ( 'images/dsb2018/1a11552569160f0b1ea10bedbd628ce6c14f29edec5092034c2309c556df833e/masks' )] bboxes = [ cv2 . boundingRect ( cv2 . findNonZero ( mask )) for mask in masks ] label_image = np . zeros_like ( masks [ 0 ]) for i , mask in enumerate ( masks ): label_image += ( mask > 0 ) . astype ( np . uint8 ) * i light = A . Compose ([ A . RGBShift (), A . InvertImg (), A . Blur (), A . GaussNoise (), A . Flip (), A . RandomRotate90 (), A . RandomSizedCrop (( 512 - 100 , 512 + 100 ), 512 , 512 ), ], bbox_params = { 'format' : 'coco' , 'min_area' : 1 , 'min_visibility' : 0.5 , 'label_fields' : [ 'category_id' ]}, p = 1 ) label_ids = [ 0 ] * len ( bboxes ) label_names = [ 'Nuclei' ] r = augment_and_show ( light , image , label_image , bboxes , label_ids , label_names , show_title = False ) Mapilary Vistas \u00b6 from PIL import Image image = cv2 . imread ( 'images/vistas/_HnWguqEbRCphUquTMrCCA.jpg' ) labels = cv2 . imread ( 'images/vistas/_HnWguqEbRCphUquTMrCCA_labels.png' , cv2 . IMREAD_COLOR ) instances = np . array ( Image . open ( 'images/vistas/_HnWguqEbRCphUquTMrCCA_instances.png' ), dtype = np . uint16 ) IGNORED = 65 * 256 instances [( instances // 256 != 55 ) & ( instances // 256 != 44 ) & ( instances // 256 != 50 )] = IGNORED image = image [ 1000 : 2500 , 1000 : 2500 ] labels = labels [ 1000 : 2500 , 1000 : 2500 ] instances = instances [ 1000 : 2500 , 1000 : 2500 ] bboxes = [ cv2 . boundingRect ( cv2 . findNonZero (( instances == instance_id ) . astype ( np . uint8 ))) for instance_id in np . unique ( instances ) if instance_id != IGNORED ] instance_labels = [ instance_id // 256 for instance_id in np . unique ( instances ) if instance_id != IGNORED ] # coco_bboxes = [list(bbox) + [label] for bbox, label in zip(bboxes, instance_labels)] # coco_bboxes = A.convert_bboxes_to_albumentations(image.shape, coco_bboxes, source_format='coco') titles = [ \"Bird\" , \"Ground Animal\" , \"Curb\" , \"Fence\" , \"Guard Rail\" , \"Barrier\" , \"Wall\" , \"Bike Lane\" , \"Crosswalk - Plain\" , \"Curb Cut\" , \"Parking\" , \"Pedestrian Area\" , \"Rail Track\" , \"Road\" , \"Service Lane\" , \"Sidewalk\" , \"Bridge\" , \"Building\" , \"Tunnel\" , \"Person\" , \"Bicyclist\" , \"Motorcyclist\" , \"Other Rider\" , \"Lane Marking - Crosswalk\" , \"Lane Marking - General\" , \"Mountain\" , \"Sand\" , \"Sky\" , \"Snow\" , \"Terrain\" , \"Vegetation\" , \"Water\" , \"Banner\" , \"Bench\" , \"Bike Rack\" , \"Billboard\" , \"Catch Basin\" , \"CCTV Camera\" , \"Fire Hydrant\" , \"Junction Box\" , \"Mailbox\" , \"Manhole\" , \"Phone Booth\" , \"Pothole\" , \"Street Light\" , \"Pole\" , \"Traffic Sign Frame\" , \"Utility Pole\" , \"Traffic Light\" , \"Traffic Sign (Back)\" , \"Traffic Sign (Front)\" , \"Trash Can\" , \"Bicycle\" , \"Boat\" , \"Bus\" , \"Car\" , \"Caravan\" , \"Motorcycle\" , \"On Rails\" , \"Other Vehicle\" , \"Trailer\" , \"Truck\" , \"Wheeled Slow\" , \"Car Mount\" , \"Ego Vehicle\" , \"Unlabeled\" ] bbox_params = A . BboxParams ( format = 'coco' , min_area = 1 , min_visibility = 0.5 , label_fields = [ 'category_id' ]) light = A . Compose ([ A . HorizontalFlip ( p = 1 ), A . RandomSizedCrop (( 800 - 100 , 800 + 100 ), 600 , 600 ), A . GaussNoise ( var_limit = ( 100 , 150 ), p = 1 ), ], bbox_params = bbox_params , p = 1 ) medium = A . Compose ([ A . HorizontalFlip ( p = 1 ), A . RandomSizedCrop (( 800 - 100 , 800 + 100 ), 600 , 600 ), A . MotionBlur ( blur_limit = 17 , p = 1 ), ], bbox_params = bbox_params , p = 1 ) strong = A . Compose ([ A . HorizontalFlip ( p = 1 ), A . RandomSizedCrop (( 800 - 100 , 800 + 100 ), 600 , 600 ), A . RGBShift ( p = 1 ), A . Blur ( blur_limit = 11 , p = 1 ), A . RandomBrightness ( p = 1 ), A . CLAHE ( p = 1 ), ], bbox_params = bbox_params , p = 1 ) random . seed ( 13 ) r = augment_and_show ( light , image , labels , bboxes , instance_labels , titles , thickness = 2 , font_scale_orig = 2 , font_scale_aug = 1 ) random . seed ( 13 ) r = augment_and_show ( medium , image , labels , bboxes , instance_labels , titles , thickness = 2 , font_scale_orig = 2 , font_scale_aug = 1 ) random . seed ( 13 ) r = augment_and_show ( strong , image , labels , bboxes , instance_labels , titles , thickness = 2 , font_scale_orig = 2 , font_scale_aug = 1 )","title":"Showcase. Cool augmentation examples on diverse set of images from various real-world tasks."},{"location":"examples/showcase/#showcase-cool-augmentation-examples-on-diverse-set-of-images-from-various-real-world-tasks","text":"","title":"Showcase. Cool augmentation examples on diverse set of images from various real-world tasks."},{"location":"examples/showcase/#import-libraries-and-define-helper-functions","text":"","title":"Import libraries and define helper functions"},{"location":"examples/showcase/#import-the-required-libraries","text":"import os import numpy as np import cv2 from matplotlib import pyplot as plt from skimage.color import label2rgb import albumentations as A import random","title":"Import the required libraries"},{"location":"examples/showcase/#define-visualization-functions","text":"BOX_COLOR = ( 255 , 0 , 0 ) # Red TEXT_COLOR = ( 255 , 255 , 255 ) # White def visualize_bbox ( img , bbox , color = BOX_COLOR , thickness = 2 , ** kwargs ): x_min , y_min , w , h = bbox x_min , x_max , y_min , y_max = int ( x_min ), int ( x_min + w ), int ( y_min ), int ( y_min + h ) cv2 . rectangle ( img , ( x_min , y_min ), ( x_max , y_max ), color = color , thickness = thickness ) return img def visualize_titles ( img , bbox , title , color = BOX_COLOR , thickness = 2 , font_thickness = 2 , font_scale = 0.35 , ** kwargs ): x_min , y_min , w , h = bbox x_min , x_max , y_min , y_max = int ( x_min ), int ( x_min + w ), int ( y_min ), int ( y_min + h ) (( text_width , text_height ), _ ) = cv2 . getTextSize ( title , cv2 . FONT_HERSHEY_SIMPLEX , font_scale , font_thickness ) cv2 . rectangle ( img , ( x_min , y_min - int ( 1.3 * text_height )), ( x_min + text_width , y_min ), BOX_COLOR , - 1 ) cv2 . putText ( img , title , ( x_min , y_min - int ( 0.3 * text_height )), cv2 . FONT_HERSHEY_SIMPLEX , font_scale , TEXT_COLOR , font_thickness , lineType = cv2 . LINE_AA ) return img def augment_and_show ( aug , image , mask = None , bboxes = [], categories = [], category_id_to_name = [], filename = None , font_scale_orig = 0.35 , font_scale_aug = 0.35 , show_title = True , ** kwargs ): augmented = aug ( image = image , mask = mask , bboxes = bboxes , category_id = categories ) image = cv2 . cvtColor ( image , cv2 . COLOR_BGR2RGB ) image_aug = cv2 . cvtColor ( augmented [ 'image' ], cv2 . COLOR_BGR2RGB ) for bbox in bboxes : visualize_bbox ( image , bbox , ** kwargs ) for bbox in augmented [ 'bboxes' ]: visualize_bbox ( image_aug , bbox , ** kwargs ) if show_title : for bbox , cat_id in zip ( bboxes , categories ): visualize_titles ( image , bbox , category_id_to_name [ cat_id ], font_scale = font_scale_orig , ** kwargs ) for bbox , cat_id in zip ( augmented [ 'bboxes' ], augmented [ 'category_id' ]): visualize_titles ( image_aug , bbox , category_id_to_name [ cat_id ], font_scale = font_scale_aug , ** kwargs ) if mask is None : f , ax = plt . subplots ( 1 , 2 , figsize = ( 16 , 8 )) ax [ 0 ] . imshow ( image ) ax [ 0 ] . set_title ( 'Original image' ) ax [ 1 ] . imshow ( image_aug ) ax [ 1 ] . set_title ( 'Augmented image' ) else : f , ax = plt . subplots ( 2 , 2 , figsize = ( 16 , 16 )) if len ( mask . shape ) != 3 : mask = label2rgb ( mask , bg_label = 0 ) mask_aug = label2rgb ( augmented [ 'mask' ], bg_label = 0 ) else : mask = cv2 . cvtColor ( mask , cv2 . COLOR_BGR2RGB ) mask_aug = cv2 . cvtColor ( augmented [ 'mask' ], cv2 . COLOR_BGR2RGB ) ax [ 0 , 0 ] . imshow ( image ) ax [ 0 , 0 ] . set_title ( 'Original image' ) ax [ 0 , 1 ] . imshow ( image_aug ) ax [ 0 , 1 ] . set_title ( 'Augmented image' ) ax [ 1 , 0 ] . imshow ( mask , interpolation = 'nearest' ) ax [ 1 , 0 ] . set_title ( 'Original mask' ) ax [ 1 , 1 ] . imshow ( mask_aug , interpolation = 'nearest' ) ax [ 1 , 1 ] . set_title ( 'Augmented mask' ) f . tight_layout () if filename is not None : f . savefig ( filename ) return augmented [ 'image' ], augmented [ 'mask' ], augmented [ 'bboxes' ] def find_in_dir ( dirname ): return [ os . path . join ( dirname , fname ) for fname in sorted ( os . listdir ( dirname ))]","title":"Define visualization functions"},{"location":"examples/showcase/#color-augmentations","text":"image = cv2 . imread ( 'images/parrot.jpg' ) random . seed ( 42 ) light = A . Compose ([ A . RandomBrightnessContrast ( p = 1 ), A . RandomGamma ( p = 1 ), A . CLAHE ( p = 1 ), ], p = 1 ) medium = A . Compose ([ A . CLAHE ( p = 1 ), A . HueSaturationValue ( hue_shift_limit = 20 , sat_shift_limit = 50 , val_shift_limit = 50 , p = 1 ), ], p = 1 ) strong = A . Compose ([ A . ChannelShuffle ( p = 1 ), ], p = 1 ) r = augment_and_show ( light , image ) r = augment_and_show ( medium , image ) r = augment_and_show ( strong , image )","title":"Color augmentations"},{"location":"examples/showcase/#inria-aerial-image-labeling-dataset","text":"random . seed ( 42 ) image , mask = cv2 . imread ( 'images/inria/inria_tyrol_w4_image.jpg' ), cv2 . imread ( 'images/inria/inria_tyrol_w4_mask.tif' , cv2 . IMREAD_GRAYSCALE ) image , mask = image [: 1024 , : 1024 ], mask [: 1024 ,: 1024 ] light = A . Compose ([ A . RandomSizedCrop (( 512 - 100 , 512 + 100 ), 512 , 512 ), A . ShiftScaleRotate (), A . RGBShift (), A . Blur (), A . GaussNoise (), A . ElasticTransform (), A . MaskDropout (( 10 , 15 ), p = 1 ), A . Cutout ( p = 1 ) ], p = 1 ) r = augment_and_show ( light , image , mask )","title":"Inria Aerial Image Labeling Dataset"},{"location":"examples/showcase/#2018-data-science-bowl","text":"random . seed ( 42 ) image = cv2 . imread ( 'images/dsb2018/1a11552569160f0b1ea10bedbd628ce6c14f29edec5092034c2309c556df833e/images/1a11552569160f0b1ea10bedbd628ce6c14f29edec5092034c2309c556df833e.png' ) masks = [ cv2 . imread ( x , cv2 . IMREAD_GRAYSCALE ) for x in find_in_dir ( 'images/dsb2018/1a11552569160f0b1ea10bedbd628ce6c14f29edec5092034c2309c556df833e/masks' )] bboxes = [ cv2 . boundingRect ( cv2 . findNonZero ( mask )) for mask in masks ] label_image = np . zeros_like ( masks [ 0 ]) for i , mask in enumerate ( masks ): label_image += ( mask > 0 ) . astype ( np . uint8 ) * i light = A . Compose ([ A . RGBShift (), A . InvertImg (), A . Blur (), A . GaussNoise (), A . Flip (), A . RandomRotate90 (), A . RandomSizedCrop (( 512 - 100 , 512 + 100 ), 512 , 512 ), ], bbox_params = { 'format' : 'coco' , 'min_area' : 1 , 'min_visibility' : 0.5 , 'label_fields' : [ 'category_id' ]}, p = 1 ) label_ids = [ 0 ] * len ( bboxes ) label_names = [ 'Nuclei' ] r = augment_and_show ( light , image , label_image , bboxes , label_ids , label_names , show_title = False )","title":"2018 Data Science Bowl"},{"location":"examples/showcase/#mapilary-vistas","text":"from PIL import Image image = cv2 . imread ( 'images/vistas/_HnWguqEbRCphUquTMrCCA.jpg' ) labels = cv2 . imread ( 'images/vistas/_HnWguqEbRCphUquTMrCCA_labels.png' , cv2 . IMREAD_COLOR ) instances = np . array ( Image . open ( 'images/vistas/_HnWguqEbRCphUquTMrCCA_instances.png' ), dtype = np . uint16 ) IGNORED = 65 * 256 instances [( instances // 256 != 55 ) & ( instances // 256 != 44 ) & ( instances // 256 != 50 )] = IGNORED image = image [ 1000 : 2500 , 1000 : 2500 ] labels = labels [ 1000 : 2500 , 1000 : 2500 ] instances = instances [ 1000 : 2500 , 1000 : 2500 ] bboxes = [ cv2 . boundingRect ( cv2 . findNonZero (( instances == instance_id ) . astype ( np . uint8 ))) for instance_id in np . unique ( instances ) if instance_id != IGNORED ] instance_labels = [ instance_id // 256 for instance_id in np . unique ( instances ) if instance_id != IGNORED ] # coco_bboxes = [list(bbox) + [label] for bbox, label in zip(bboxes, instance_labels)] # coco_bboxes = A.convert_bboxes_to_albumentations(image.shape, coco_bboxes, source_format='coco') titles = [ \"Bird\" , \"Ground Animal\" , \"Curb\" , \"Fence\" , \"Guard Rail\" , \"Barrier\" , \"Wall\" , \"Bike Lane\" , \"Crosswalk - Plain\" , \"Curb Cut\" , \"Parking\" , \"Pedestrian Area\" , \"Rail Track\" , \"Road\" , \"Service Lane\" , \"Sidewalk\" , \"Bridge\" , \"Building\" , \"Tunnel\" , \"Person\" , \"Bicyclist\" , \"Motorcyclist\" , \"Other Rider\" , \"Lane Marking - Crosswalk\" , \"Lane Marking - General\" , \"Mountain\" , \"Sand\" , \"Sky\" , \"Snow\" , \"Terrain\" , \"Vegetation\" , \"Water\" , \"Banner\" , \"Bench\" , \"Bike Rack\" , \"Billboard\" , \"Catch Basin\" , \"CCTV Camera\" , \"Fire Hydrant\" , \"Junction Box\" , \"Mailbox\" , \"Manhole\" , \"Phone Booth\" , \"Pothole\" , \"Street Light\" , \"Pole\" , \"Traffic Sign Frame\" , \"Utility Pole\" , \"Traffic Light\" , \"Traffic Sign (Back)\" , \"Traffic Sign (Front)\" , \"Trash Can\" , \"Bicycle\" , \"Boat\" , \"Bus\" , \"Car\" , \"Caravan\" , \"Motorcycle\" , \"On Rails\" , \"Other Vehicle\" , \"Trailer\" , \"Truck\" , \"Wheeled Slow\" , \"Car Mount\" , \"Ego Vehicle\" , \"Unlabeled\" ] bbox_params = A . BboxParams ( format = 'coco' , min_area = 1 , min_visibility = 0.5 , label_fields = [ 'category_id' ]) light = A . Compose ([ A . HorizontalFlip ( p = 1 ), A . RandomSizedCrop (( 800 - 100 , 800 + 100 ), 600 , 600 ), A . GaussNoise ( var_limit = ( 100 , 150 ), p = 1 ), ], bbox_params = bbox_params , p = 1 ) medium = A . Compose ([ A . HorizontalFlip ( p = 1 ), A . RandomSizedCrop (( 800 - 100 , 800 + 100 ), 600 , 600 ), A . MotionBlur ( blur_limit = 17 , p = 1 ), ], bbox_params = bbox_params , p = 1 ) strong = A . Compose ([ A . HorizontalFlip ( p = 1 ), A . RandomSizedCrop (( 800 - 100 , 800 + 100 ), 600 , 600 ), A . RGBShift ( p = 1 ), A . Blur ( blur_limit = 11 , p = 1 ), A . RandomBrightness ( p = 1 ), A . CLAHE ( p = 1 ), ], bbox_params = bbox_params , p = 1 ) random . seed ( 13 ) r = augment_and_show ( light , image , labels , bboxes , instance_labels , titles , thickness = 2 , font_scale_orig = 2 , font_scale_aug = 1 ) random . seed ( 13 ) r = augment_and_show ( medium , image , labels , bboxes , instance_labels , titles , thickness = 2 , font_scale_orig = 2 , font_scale_aug = 1 ) random . seed ( 13 ) r = augment_and_show ( strong , image , labels , bboxes , instance_labels , titles , thickness = 2 , font_scale_orig = 2 , font_scale_aug = 1 )","title":"Mapilary Vistas"},{"location":"examples/tensorflow-example/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Using Albumentations with Tensorflow \u00b6 Author : Ayushman Buragohain ! pip install - q - U albumentations ! echo \"$(pip freeze | grep albumentations) is successfully installed\" albumentations==0.4.6 is successfully installed [Recommended] Update the version of tensorflow_datasets if you want to use it \u00b6 We'll we using an example from tensorflow_datasets . ! pip install -- upgrade tensorflow_datasets Run the example \u00b6 # necessary imports import tensorflow as tf import numpy as np import matplotlib.pyplot as plt import tensorflow_datasets as tfds from functools import partial from albumentations import ( Compose , RandomBrightness , JpegCompression , HueSaturationValue , RandomContrast , HorizontalFlip , Rotate ) AUTOTUNE = tf . data . experimental . AUTOTUNE tfds . __version__ '3.2.1' # load in the tf_flowers dataset data , info = tfds . load ( name = \"tf_flowers\" , split = \"train\" , as_supervised = True , with_info = True ) data <PrefetchDataset shapes: ((None, None, 3), ()), types: (tf.uint8, tf.int64)> info tfds.core.DatasetInfo( name='tf_flowers', version=3.0.1, description='A large set of images of flowers', homepage='https://www.tensorflow.org/tutorials/load_data/images', features=FeaturesDict({ 'image': Image(shape=(None, None, 3), dtype=tf.uint8), 'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=5), }), total_num_examples=3670, splits={ 'train': 3670, }, supervised_keys=('image', 'label'), citation=\"\"\"@ONLINE {tfflowers, author = \"The TensorFlow Team\", title = \"Flowers\", month = \"jan\", year = \"2019\", url = \"http://download.tensorflow.org/example_images/flower_photos.tgz\" }\"\"\", redistribution_info=, ) An Example Pipeline Using tf.image \u00b6 Process Data \u00b6 def process_image ( image , label , img_size ): # cast and normalize image image = tf . image . convert_image_dtype ( image , tf . float32 ) # apply simple augmentations image = tf . image . random_flip_left_right ( image ) image = tf . image . resize ( image ,[ img_size , img_size ]) return image , label ds_tf = data . map ( partial ( process_image , img_size = 120 ), num_parallel_calls = AUTOTUNE ) . batch ( 30 ) . prefetch ( AUTOTUNE ) ds_tf <PrefetchDataset shapes: ((None, 120, 120, 3), (None,)), types: (tf.float32, tf.int64)> View images from the dataset \u00b6 def view_image ( ds ): image , label = next ( iter ( ds )) # extract 1 batch from the dataset image = image . numpy () label = label . numpy () fig = plt . figure ( figsize = ( 22 , 22 )) for i in range ( 20 ): ax = fig . add_subplot ( 4 , 5 , i + 1 , xticks = [], yticks = []) ax . imshow ( image [ i ]) ax . set_title ( f \"Label: { label [ i ] } \" ) view_image ( ds_tf ) Using tf.image is very efficient to create a pipeline but the disadvantage is that with tf.image we can only apply limited amounts of augmentations to our input data . One way to solve is issue is to use tf.keras ImageDataGenerator class but albumentations is faster. An Example Pipeline using albumentations \u00b6 To integrate albumentations into our tensorflow pipeline we can create two functions : - Pipeline to apply augmentation . - a function that calls the above function and pass in our data through the pipeline. We can then wrap our 2nd Function under tf.numpy_function . italicized text## Create Pipeline to Process data # Instantiate augments # we can apply as many augments we want and adjust the values accordingly # here I have chosen the augments and their arguments at random transforms = Compose ([ Rotate ( limit = 40 ), RandomBrightness ( limit = 0.1 ), JpegCompression ( quality_lower = 85 , quality_upper = 100 , p = 0.5 ), HueSaturationValue ( hue_shift_limit = 20 , sat_shift_limit = 30 , val_shift_limit = 20 , p = 0.5 ), RandomContrast ( limit = 0.2 , p = 0.5 ), HorizontalFlip (), ]) def aug_fn ( image , img_size ): data = { \"image\" : image } aug_data = transforms ( ** data ) aug_img = aug_data [ \"image\" ] aug_img = tf . cast ( aug_img / 255.0 , tf . float32 ) aug_img = tf . image . resize ( aug_img , size = [ img_size , img_size ]) return aug_img def process_data ( image , label , img_size ): aug_img = tf . numpy_function ( func = aug_fn , inp = [ image , img_size ], Tout = tf . float32 ) return aug_img , label # create dataset ds_alb = data . map ( partial ( process_data , img_size = 120 ), num_parallel_calls = AUTOTUNE ) . prefetch ( AUTOTUNE ) ds_alb <PrefetchDataset shapes: (<unknown>, ()), types: (tf.float32, tf.int64)> Restoring dataset shapes. \u00b6 The datasets loses its shape after applying a tf.numpy_function, so this is necessary for the sequential model and when inheriting from the model class. def set_shapes ( img , label , img_shape = ( 120 , 120 , 3 )): img . set_shape ( img_shape ) label . set_shape ([]) return img , label ds_alb = ds_alb . map ( set_shapes , num_parallel_calls = AUTOTUNE ) . batch ( 32 ) . prefetch ( AUTOTUNE ) ds_alb <PrefetchDataset shapes: ((None, 120, 120, 3), (None,)), types: (tf.float32, tf.int64)> View images from the dataset \u00b6 view_image ( ds_alb ) We can then pass in this dataset to out model and call fit on our model Note : \u00b6 Some API's of tensorflow.keras.Model might not work, if you dont map the dataset with the set_shapes function. What works without setting shapes : \u00b6 from tensorflow.keras import models , layers from tensorflow import keras # Running the Model in eager mode using Sequential API def create_model ( input_shape ): return models . Sequential ([ layers . Conv2D ( 32 , ( 3 , 3 ), activation = 'relu' , input_shape = input_shape ), layers . MaxPooling2D (( 2 , 2 )), layers . Conv2D ( 64 , ( 3 , 3 ), activation = 'relu' ), layers . MaxPooling2D (( 2 , 2 )), layers . Conv2D ( 64 , ( 3 , 3 ), activation = 'relu' ), layers . Flatten (), layers . Dense ( 64 , activation = 'relu' ), layers . Dense ( 5 , activation = 'softmax' )]) model = create_model (( 120 , 120 , 3 )) model . compile ( optimizer = 'adam' , loss = 'sparse_categorical_crossentropy' , metrics = 'accuracy' , run_eagerly = True ) model . fit ( ds_alb , epochs = 2 ) Epoch 1/2 115/115 [==============================] - 28s 246ms/step - loss: 1.4670 - accuracy: 0.3545 Epoch 2/2 115/115 [==============================] - 25s 216ms/step - loss: 1.1873 - accuracy: 0.5166 <tensorflow.python.keras.callbacks.History at 0x7fb68ee2c400> # Functional API input = keras . Input ( shape = ( 120 , 120 , 3 )) x = keras . layers . Conv2D ( 32 , ( 3 , 3 ), activation = \"relu\" )( input ) x = keras . layers . MaxPooling2D (( 2 , 2 ))( x ) x = keras . layers . Conv2D ( 64 , ( 3 , 3 ), activation = 'relu' )( x ) x = keras . layers . MaxPooling2D (( 2 , 2 ))( x ) x = keras . layers . Conv2D ( 64 , ( 3 , 3 ), activation = 'relu' )( x ) x = keras . layers . Flatten ()( x ) x = keras . layers . Dense ( 64 , activation = 'relu' )( x ) x = keras . layers . Dense ( 5 , activation = 'softmax' )( x ) model = keras . Model ( inputs = input , outputs = x ) model . compile ( optimizer = 'adam' , loss = 'sparse_categorical_crossentropy' , metrics = 'accuracy' ) model . fit ( ds_alb , epochs = 2 ) Epoch 1/2 115/115 [==============================] - 21s 186ms/step - loss: 1.4250 - accuracy: 0.3943 Epoch 2/2 115/115 [==============================] - 22s 189ms/step - loss: 1.1752 - accuracy: 0.5256 <tensorflow.python.keras.callbacks.History at 0x7fb68e5eac88> # Transfer Learning [freeze base model layers]: Sequential API base_model = keras . applications . ResNet50 ( include_top = False , input_shape = ( 120 , 120 , 3 ), weights = \"imagenet\" ) base_model . trainable = False model = keras . models . Sequential ([ base_model , keras . layers . Conv2D ( 32 , ( 1 , 1 ), activation = \"relu\" ), keras . layers . Dropout ( 0.2 ), keras . layers . Flatten (), keras . layers . Dense ( 64 , activation = 'relu' ), keras . layers . Dense ( 5 , activation = 'softmax' ), ]) model . compile ( optimizer = 'adam' , loss = 'sparse_categorical_crossentropy' , metrics = 'accuracy' ) model . fit ( ds_alb , epochs = 2 ) Epoch 1/2 115/115 [==============================] - 29s 250ms/step - loss: 1.5457 - accuracy: 0.3052 Epoch 2/2 115/115 [==============================] - 27s 238ms/step - loss: 1.4697 - accuracy: 0.3638 <tensorflow.python.keras.callbacks.History at 0x7fb68f0386d8> # Transfer Learning [unfreeze all layers]: Sequential API base_model = keras . applications . ResNet50 ( include_top = False , input_shape = ( 120 , 120 , 3 ), weights = \"imagenet\" ) base_model . trainable = True model = keras . models . Sequential ([ base_model , keras . layers . Conv2D ( 32 , ( 1 , 1 ), activation = \"relu\" ), keras . layers . Flatten (), keras . layers . Dense ( 64 , activation = 'relu' ), keras . layers . Dense ( 5 , activation = 'softmax' ), ]) model . compile ( optimizer = 'adam' , loss = 'sparse_categorical_crossentropy' , metrics = 'accuracy' ) model . fit ( ds_alb , epochs = 2 ) Epoch 1/2 115/115 [==============================] - 46s 399ms/step - loss: 1.2163 - accuracy: 0.5708 Epoch 2/2 115/115 [==============================] - 45s 395ms/step - loss: 0.8039 - accuracy: 0.7204 <tensorflow.python.keras.callbacks.History at 0x7fb68b1a9f28> # Transfer Learning [freeze all layers of feature extractor]: Functional API base_model = keras . applications . ResNet50 ( include_top = False , input_shape = ( 120 , 120 , 3 ), weights = \"imagenet\" ) base_model . trainable = False input = keras . Input ( shape = ( 120 , 120 , 3 )) x = base_model ( input , training = False ) x = keras . layers . Conv2D ( 32 , ( 1 , 1 ), activation = \"relu\" )( x ) x = keras . layers . Dropout ( 0.2 )( x ) x = keras . layers . Flatten ()( x ) x = keras . layers . Dense ( 64 , activation = 'relu' )( x ) x = keras . layers . Dense ( 5 , activation = 'softmax' )( x ) model = keras . Model ( inputs = input , outputs = x ) model . compile ( optimizer = 'adam' , loss = 'sparse_categorical_crossentropy' , metrics = 'accuracy' ) model . fit ( ds_alb , epochs = 2 ) Epoch 1/2 115/115 [==============================] - 30s 261ms/step - loss: 1.5425 - accuracy: 0.3060 Epoch 2/2 115/115 [==============================] - 30s 258ms/step - loss: 1.4852 - accuracy: 0.3591 <tensorflow.python.keras.callbacks.History at 0x7fb688e2bb00> # Transfer Learning [freeze all layers of feature extractor]: Subclass API base_model = keras . applications . ResNet50 ( include_top = False , input_shape = ( 120 , 120 , 3 ), weights = \"imagenet\" ) base_model . trainable = False class MyModel ( keras . Model ): def __init__ ( self , base_model ): super ( MyModel , self ) . __init__ () self . base = base_model self . layer_1 = keras . layers . Flatten () self . layer_2 = keras . layers . Dense ( 64 , activation = 'relu' ) self . layer_3 = keras . layers . Dense ( 5 , activation = 'softmax' ) @tf . function def call ( self , xb ): x = self . base ( xb ) x = self . layer_1 ( x ) x = self . layer_2 ( x ) x = self . layer_3 ( x ) return x model = MyModel ( base_model = base_model ) model . compile ( optimizer = 'adam' , loss = 'sparse_categorical_crossentropy' , metrics = 'accuracy' ) model . fit ( ds_alb , epochs = 2 ) Epoch 1/2 115/115 [==============================] - 30s 257ms/step - loss: 1.5624 - accuracy: 0.3281 Epoch 2/2 115/115 [==============================] - 29s 256ms/step - loss: 1.4372 - accuracy: 0.3970 <tensorflow.python.keras.callbacks.History at 0x7fb68778d080> # Transfer Learning using [unfreeze all layers of feature extractor]: Subclass API base_model = keras . applications . ResNet50 ( include_top = False , input_shape = ( 120 , 120 , 3 ), weights = \"imagenet\" ) base_model . trainable = True class MyModel ( keras . Model ): def __init__ ( self , base_model ): super ( MyModel , self ) . __init__ () self . base = base_model self . layer_1 = keras . layers . Flatten () self . layer_2 = keras . layers . Dense ( 64 , activation = 'relu' ) self . layer_3 = keras . layers . Dense ( 5 , activation = 'softmax' ) @tf . function def call ( self , xb ): x = self . base ( xb ) x = self . layer_1 ( x ) x = self . layer_2 ( x ) x = self . layer_3 ( x ) return x model = MyModel ( base_model = base_model ) model . compile ( optimizer = 'adam' , loss = 'sparse_categorical_crossentropy' , metrics = 'accuracy' ) model . fit ( ds_alb , epochs = 2 ) Epoch 1/2 115/115 [==============================] - 46s 396ms/step - loss: 1.7472 - accuracy: 0.5422 Epoch 2/2 115/115 [==============================] - 45s 395ms/step - loss: 1.4129 - accuracy: 0.5714 <tensorflow.python.keras.callbacks.History at 0x7fb685e6da20> What works only if you set the shapes of the dataset : \u00b6 # Using Sequential API without transfer learning & Eager Execution def create_model ( input_shape ): return models . Sequential ([ layers . Conv2D ( 32 , ( 3 , 3 ), activation = 'relu' , input_shape = input_shape ), layers . MaxPooling2D (( 2 , 2 )), layers . Conv2D ( 64 , ( 3 , 3 ), activation = 'relu' ), layers . MaxPooling2D (( 2 , 2 )), layers . Conv2D ( 64 , ( 3 , 3 ), activation = 'relu' ), layers . Flatten (), layers . Dense ( 64 , activation = 'relu' ), layers . Dense ( 5 , activation = 'softmax' )]) model = create_model (( 120 , 120 , 3 )) model . compile ( optimizer = 'adam' , loss = 'sparse_categorical_crossentropy' , metrics = 'accuracy' ) model . fit ( ds_alb , epochs = 2 ) Epoch 1/2 115/115 [==============================] - 22s 192ms/step - loss: 1.4568 - accuracy: 0.3752 Epoch 2/2 115/115 [==============================] - 22s 194ms/step - loss: 1.1913 - accuracy: 0.5082 <tensorflow.python.keras.callbacks.History at 0x7fb6851346d8> # Using Subclass API without transfer learning & Eager Execution class MyModel ( keras . Model ): def __init__ ( self ): super ( MyModel , self ) . __init__ () self . conv1 = keras . layers . Conv2D ( 32 , ( 3 , 3 ), activation = 'relu' ) self . pool1 = keras . layers . MaxPooling2D (( 2 , 2 )) self . conv2 = keras . layers . Conv2D ( 64 , ( 3 , 3 ), activation = 'relu' ) self . pool2 = keras . layers . MaxPooling2D (( 2 , 2 )) self . conv3 = keras . layers . Conv2D ( 64 , ( 3 , 3 ), activation = 'relu' ) self . flat = keras . layers . Flatten () self . dense1 = keras . layers . Dense ( 64 , activation = 'relu' ) self . dense2 = keras . layers . Dense ( 5 , activation = 'softmax' ) def call ( self , xb ): x = self . conv1 ( xb ) x = self . pool1 ( x ) x = self . conv2 ( x ) x = self . pool2 ( x ) x = self . conv3 ( x ) x = self . flat ( x ) x = self . dense1 ( x ) x = self . dense2 ( x ) return x model = MyModel () model . compile ( optimizer = 'adam' , loss = 'sparse_categorical_crossentropy' , metrics = 'accuracy' ) model . fit ( ds_alb , epochs = 2 ) Epoch 1/2 115/115 [==============================] - 22s 194ms/step - loss: 1.4170 - accuracy: 0.3774 Epoch 2/2 115/115 [==============================] - 22s 192ms/step - loss: 1.1577 - accuracy: 0.5371 <tensorflow.python.keras.callbacks.History at 0x7fb6848369b0>","title":"Using Albumentations with Tensorflow"},{"location":"examples/tensorflow-example/#using-albumentations-with-tensorflow","text":"Author : Ayushman Buragohain ! pip install - q - U albumentations ! echo \"$(pip freeze | grep albumentations) is successfully installed\" albumentations==0.4.6 is successfully installed","title":"Using Albumentations with Tensorflow"},{"location":"examples/tensorflow-example/#recommended-update-the-version-of-tensorflow_datasets-if-you-want-to-use-it","text":"We'll we using an example from tensorflow_datasets . ! pip install -- upgrade tensorflow_datasets","title":"[Recommended] Update the version of tensorflow_datasets if you want to use it"},{"location":"examples/tensorflow-example/#run-the-example","text":"# necessary imports import tensorflow as tf import numpy as np import matplotlib.pyplot as plt import tensorflow_datasets as tfds from functools import partial from albumentations import ( Compose , RandomBrightness , JpegCompression , HueSaturationValue , RandomContrast , HorizontalFlip , Rotate ) AUTOTUNE = tf . data . experimental . AUTOTUNE tfds . __version__ '3.2.1' # load in the tf_flowers dataset data , info = tfds . load ( name = \"tf_flowers\" , split = \"train\" , as_supervised = True , with_info = True ) data <PrefetchDataset shapes: ((None, None, 3), ()), types: (tf.uint8, tf.int64)> info tfds.core.DatasetInfo( name='tf_flowers', version=3.0.1, description='A large set of images of flowers', homepage='https://www.tensorflow.org/tutorials/load_data/images', features=FeaturesDict({ 'image': Image(shape=(None, None, 3), dtype=tf.uint8), 'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=5), }), total_num_examples=3670, splits={ 'train': 3670, }, supervised_keys=('image', 'label'), citation=\"\"\"@ONLINE {tfflowers, author = \"The TensorFlow Team\", title = \"Flowers\", month = \"jan\", year = \"2019\", url = \"http://download.tensorflow.org/example_images/flower_photos.tgz\" }\"\"\", redistribution_info=, )","title":"Run the example"},{"location":"examples/tensorflow-example/#an-example-pipeline-using-tfimage","text":"","title":"An Example Pipeline Using tf.image"},{"location":"examples/tensorflow-example/#process-data","text":"def process_image ( image , label , img_size ): # cast and normalize image image = tf . image . convert_image_dtype ( image , tf . float32 ) # apply simple augmentations image = tf . image . random_flip_left_right ( image ) image = tf . image . resize ( image ,[ img_size , img_size ]) return image , label ds_tf = data . map ( partial ( process_image , img_size = 120 ), num_parallel_calls = AUTOTUNE ) . batch ( 30 ) . prefetch ( AUTOTUNE ) ds_tf <PrefetchDataset shapes: ((None, 120, 120, 3), (None,)), types: (tf.float32, tf.int64)>","title":"Process Data"},{"location":"examples/tensorflow-example/#view-images-from-the-dataset","text":"def view_image ( ds ): image , label = next ( iter ( ds )) # extract 1 batch from the dataset image = image . numpy () label = label . numpy () fig = plt . figure ( figsize = ( 22 , 22 )) for i in range ( 20 ): ax = fig . add_subplot ( 4 , 5 , i + 1 , xticks = [], yticks = []) ax . imshow ( image [ i ]) ax . set_title ( f \"Label: { label [ i ] } \" ) view_image ( ds_tf ) Using tf.image is very efficient to create a pipeline but the disadvantage is that with tf.image we can only apply limited amounts of augmentations to our input data . One way to solve is issue is to use tf.keras ImageDataGenerator class but albumentations is faster.","title":"View images from the dataset"},{"location":"examples/tensorflow-example/#an-example-pipeline-using-albumentations","text":"To integrate albumentations into our tensorflow pipeline we can create two functions : - Pipeline to apply augmentation . - a function that calls the above function and pass in our data through the pipeline. We can then wrap our 2nd Function under tf.numpy_function . italicized text## Create Pipeline to Process data # Instantiate augments # we can apply as many augments we want and adjust the values accordingly # here I have chosen the augments and their arguments at random transforms = Compose ([ Rotate ( limit = 40 ), RandomBrightness ( limit = 0.1 ), JpegCompression ( quality_lower = 85 , quality_upper = 100 , p = 0.5 ), HueSaturationValue ( hue_shift_limit = 20 , sat_shift_limit = 30 , val_shift_limit = 20 , p = 0.5 ), RandomContrast ( limit = 0.2 , p = 0.5 ), HorizontalFlip (), ]) def aug_fn ( image , img_size ): data = { \"image\" : image } aug_data = transforms ( ** data ) aug_img = aug_data [ \"image\" ] aug_img = tf . cast ( aug_img / 255.0 , tf . float32 ) aug_img = tf . image . resize ( aug_img , size = [ img_size , img_size ]) return aug_img def process_data ( image , label , img_size ): aug_img = tf . numpy_function ( func = aug_fn , inp = [ image , img_size ], Tout = tf . float32 ) return aug_img , label # create dataset ds_alb = data . map ( partial ( process_data , img_size = 120 ), num_parallel_calls = AUTOTUNE ) . prefetch ( AUTOTUNE ) ds_alb <PrefetchDataset shapes: (<unknown>, ()), types: (tf.float32, tf.int64)>","title":"An Example Pipeline using albumentations"},{"location":"examples/tensorflow-example/#restoring-dataset-shapes","text":"The datasets loses its shape after applying a tf.numpy_function, so this is necessary for the sequential model and when inheriting from the model class. def set_shapes ( img , label , img_shape = ( 120 , 120 , 3 )): img . set_shape ( img_shape ) label . set_shape ([]) return img , label ds_alb = ds_alb . map ( set_shapes , num_parallel_calls = AUTOTUNE ) . batch ( 32 ) . prefetch ( AUTOTUNE ) ds_alb <PrefetchDataset shapes: ((None, 120, 120, 3), (None,)), types: (tf.float32, tf.int64)>","title":"Restoring dataset shapes."},{"location":"examples/tensorflow-example/#view-images-from-the-dataset_1","text":"view_image ( ds_alb ) We can then pass in this dataset to out model and call fit on our model","title":"View images from the dataset"},{"location":"examples/tensorflow-example/#note","text":"Some API's of tensorflow.keras.Model might not work, if you dont map the dataset with the set_shapes function.","title":"Note:"},{"location":"examples/tensorflow-example/#what-works-without-setting-shapes","text":"from tensorflow.keras import models , layers from tensorflow import keras # Running the Model in eager mode using Sequential API def create_model ( input_shape ): return models . Sequential ([ layers . Conv2D ( 32 , ( 3 , 3 ), activation = 'relu' , input_shape = input_shape ), layers . MaxPooling2D (( 2 , 2 )), layers . Conv2D ( 64 , ( 3 , 3 ), activation = 'relu' ), layers . MaxPooling2D (( 2 , 2 )), layers . Conv2D ( 64 , ( 3 , 3 ), activation = 'relu' ), layers . Flatten (), layers . Dense ( 64 , activation = 'relu' ), layers . Dense ( 5 , activation = 'softmax' )]) model = create_model (( 120 , 120 , 3 )) model . compile ( optimizer = 'adam' , loss = 'sparse_categorical_crossentropy' , metrics = 'accuracy' , run_eagerly = True ) model . fit ( ds_alb , epochs = 2 ) Epoch 1/2 115/115 [==============================] - 28s 246ms/step - loss: 1.4670 - accuracy: 0.3545 Epoch 2/2 115/115 [==============================] - 25s 216ms/step - loss: 1.1873 - accuracy: 0.5166 <tensorflow.python.keras.callbacks.History at 0x7fb68ee2c400> # Functional API input = keras . Input ( shape = ( 120 , 120 , 3 )) x = keras . layers . Conv2D ( 32 , ( 3 , 3 ), activation = \"relu\" )( input ) x = keras . layers . MaxPooling2D (( 2 , 2 ))( x ) x = keras . layers . Conv2D ( 64 , ( 3 , 3 ), activation = 'relu' )( x ) x = keras . layers . MaxPooling2D (( 2 , 2 ))( x ) x = keras . layers . Conv2D ( 64 , ( 3 , 3 ), activation = 'relu' )( x ) x = keras . layers . Flatten ()( x ) x = keras . layers . Dense ( 64 , activation = 'relu' )( x ) x = keras . layers . Dense ( 5 , activation = 'softmax' )( x ) model = keras . Model ( inputs = input , outputs = x ) model . compile ( optimizer = 'adam' , loss = 'sparse_categorical_crossentropy' , metrics = 'accuracy' ) model . fit ( ds_alb , epochs = 2 ) Epoch 1/2 115/115 [==============================] - 21s 186ms/step - loss: 1.4250 - accuracy: 0.3943 Epoch 2/2 115/115 [==============================] - 22s 189ms/step - loss: 1.1752 - accuracy: 0.5256 <tensorflow.python.keras.callbacks.History at 0x7fb68e5eac88> # Transfer Learning [freeze base model layers]: Sequential API base_model = keras . applications . ResNet50 ( include_top = False , input_shape = ( 120 , 120 , 3 ), weights = \"imagenet\" ) base_model . trainable = False model = keras . models . Sequential ([ base_model , keras . layers . Conv2D ( 32 , ( 1 , 1 ), activation = \"relu\" ), keras . layers . Dropout ( 0.2 ), keras . layers . Flatten (), keras . layers . Dense ( 64 , activation = 'relu' ), keras . layers . Dense ( 5 , activation = 'softmax' ), ]) model . compile ( optimizer = 'adam' , loss = 'sparse_categorical_crossentropy' , metrics = 'accuracy' ) model . fit ( ds_alb , epochs = 2 ) Epoch 1/2 115/115 [==============================] - 29s 250ms/step - loss: 1.5457 - accuracy: 0.3052 Epoch 2/2 115/115 [==============================] - 27s 238ms/step - loss: 1.4697 - accuracy: 0.3638 <tensorflow.python.keras.callbacks.History at 0x7fb68f0386d8> # Transfer Learning [unfreeze all layers]: Sequential API base_model = keras . applications . ResNet50 ( include_top = False , input_shape = ( 120 , 120 , 3 ), weights = \"imagenet\" ) base_model . trainable = True model = keras . models . Sequential ([ base_model , keras . layers . Conv2D ( 32 , ( 1 , 1 ), activation = \"relu\" ), keras . layers . Flatten (), keras . layers . Dense ( 64 , activation = 'relu' ), keras . layers . Dense ( 5 , activation = 'softmax' ), ]) model . compile ( optimizer = 'adam' , loss = 'sparse_categorical_crossentropy' , metrics = 'accuracy' ) model . fit ( ds_alb , epochs = 2 ) Epoch 1/2 115/115 [==============================] - 46s 399ms/step - loss: 1.2163 - accuracy: 0.5708 Epoch 2/2 115/115 [==============================] - 45s 395ms/step - loss: 0.8039 - accuracy: 0.7204 <tensorflow.python.keras.callbacks.History at 0x7fb68b1a9f28> # Transfer Learning [freeze all layers of feature extractor]: Functional API base_model = keras . applications . ResNet50 ( include_top = False , input_shape = ( 120 , 120 , 3 ), weights = \"imagenet\" ) base_model . trainable = False input = keras . Input ( shape = ( 120 , 120 , 3 )) x = base_model ( input , training = False ) x = keras . layers . Conv2D ( 32 , ( 1 , 1 ), activation = \"relu\" )( x ) x = keras . layers . Dropout ( 0.2 )( x ) x = keras . layers . Flatten ()( x ) x = keras . layers . Dense ( 64 , activation = 'relu' )( x ) x = keras . layers . Dense ( 5 , activation = 'softmax' )( x ) model = keras . Model ( inputs = input , outputs = x ) model . compile ( optimizer = 'adam' , loss = 'sparse_categorical_crossentropy' , metrics = 'accuracy' ) model . fit ( ds_alb , epochs = 2 ) Epoch 1/2 115/115 [==============================] - 30s 261ms/step - loss: 1.5425 - accuracy: 0.3060 Epoch 2/2 115/115 [==============================] - 30s 258ms/step - loss: 1.4852 - accuracy: 0.3591 <tensorflow.python.keras.callbacks.History at 0x7fb688e2bb00> # Transfer Learning [freeze all layers of feature extractor]: Subclass API base_model = keras . applications . ResNet50 ( include_top = False , input_shape = ( 120 , 120 , 3 ), weights = \"imagenet\" ) base_model . trainable = False class MyModel ( keras . Model ): def __init__ ( self , base_model ): super ( MyModel , self ) . __init__ () self . base = base_model self . layer_1 = keras . layers . Flatten () self . layer_2 = keras . layers . Dense ( 64 , activation = 'relu' ) self . layer_3 = keras . layers . Dense ( 5 , activation = 'softmax' ) @tf . function def call ( self , xb ): x = self . base ( xb ) x = self . layer_1 ( x ) x = self . layer_2 ( x ) x = self . layer_3 ( x ) return x model = MyModel ( base_model = base_model ) model . compile ( optimizer = 'adam' , loss = 'sparse_categorical_crossentropy' , metrics = 'accuracy' ) model . fit ( ds_alb , epochs = 2 ) Epoch 1/2 115/115 [==============================] - 30s 257ms/step - loss: 1.5624 - accuracy: 0.3281 Epoch 2/2 115/115 [==============================] - 29s 256ms/step - loss: 1.4372 - accuracy: 0.3970 <tensorflow.python.keras.callbacks.History at 0x7fb68778d080> # Transfer Learning using [unfreeze all layers of feature extractor]: Subclass API base_model = keras . applications . ResNet50 ( include_top = False , input_shape = ( 120 , 120 , 3 ), weights = \"imagenet\" ) base_model . trainable = True class MyModel ( keras . Model ): def __init__ ( self , base_model ): super ( MyModel , self ) . __init__ () self . base = base_model self . layer_1 = keras . layers . Flatten () self . layer_2 = keras . layers . Dense ( 64 , activation = 'relu' ) self . layer_3 = keras . layers . Dense ( 5 , activation = 'softmax' ) @tf . function def call ( self , xb ): x = self . base ( xb ) x = self . layer_1 ( x ) x = self . layer_2 ( x ) x = self . layer_3 ( x ) return x model = MyModel ( base_model = base_model ) model . compile ( optimizer = 'adam' , loss = 'sparse_categorical_crossentropy' , metrics = 'accuracy' ) model . fit ( ds_alb , epochs = 2 ) Epoch 1/2 115/115 [==============================] - 46s 396ms/step - loss: 1.7472 - accuracy: 0.5422 Epoch 2/2 115/115 [==============================] - 45s 395ms/step - loss: 1.4129 - accuracy: 0.5714 <tensorflow.python.keras.callbacks.History at 0x7fb685e6da20>","title":"What works without setting shapes :"},{"location":"examples/tensorflow-example/#what-works-only-if-you-set-the-shapes-of-the-dataset","text":"# Using Sequential API without transfer learning & Eager Execution def create_model ( input_shape ): return models . Sequential ([ layers . Conv2D ( 32 , ( 3 , 3 ), activation = 'relu' , input_shape = input_shape ), layers . MaxPooling2D (( 2 , 2 )), layers . Conv2D ( 64 , ( 3 , 3 ), activation = 'relu' ), layers . MaxPooling2D (( 2 , 2 )), layers . Conv2D ( 64 , ( 3 , 3 ), activation = 'relu' ), layers . Flatten (), layers . Dense ( 64 , activation = 'relu' ), layers . Dense ( 5 , activation = 'softmax' )]) model = create_model (( 120 , 120 , 3 )) model . compile ( optimizer = 'adam' , loss = 'sparse_categorical_crossentropy' , metrics = 'accuracy' ) model . fit ( ds_alb , epochs = 2 ) Epoch 1/2 115/115 [==============================] - 22s 192ms/step - loss: 1.4568 - accuracy: 0.3752 Epoch 2/2 115/115 [==============================] - 22s 194ms/step - loss: 1.1913 - accuracy: 0.5082 <tensorflow.python.keras.callbacks.History at 0x7fb6851346d8> # Using Subclass API without transfer learning & Eager Execution class MyModel ( keras . Model ): def __init__ ( self ): super ( MyModel , self ) . __init__ () self . conv1 = keras . layers . Conv2D ( 32 , ( 3 , 3 ), activation = 'relu' ) self . pool1 = keras . layers . MaxPooling2D (( 2 , 2 )) self . conv2 = keras . layers . Conv2D ( 64 , ( 3 , 3 ), activation = 'relu' ) self . pool2 = keras . layers . MaxPooling2D (( 2 , 2 )) self . conv3 = keras . layers . Conv2D ( 64 , ( 3 , 3 ), activation = 'relu' ) self . flat = keras . layers . Flatten () self . dense1 = keras . layers . Dense ( 64 , activation = 'relu' ) self . dense2 = keras . layers . Dense ( 5 , activation = 'softmax' ) def call ( self , xb ): x = self . conv1 ( xb ) x = self . pool1 ( x ) x = self . conv2 ( x ) x = self . pool2 ( x ) x = self . conv3 ( x ) x = self . flat ( x ) x = self . dense1 ( x ) x = self . dense2 ( x ) return x model = MyModel () model . compile ( optimizer = 'adam' , loss = 'sparse_categorical_crossentropy' , metrics = 'accuracy' ) model . fit ( ds_alb , epochs = 2 ) Epoch 1/2 115/115 [==============================] - 22s 194ms/step - loss: 1.4170 - accuracy: 0.3774 Epoch 2/2 115/115 [==============================] - 22s 192ms/step - loss: 1.1577 - accuracy: 0.5371 <tensorflow.python.keras.callbacks.History at 0x7fb6848369b0>","title":"What works only if you set the shapes of the dataset :"},{"location":"experimental/installation/","text":"Installation \u00b6 Albumentations Experimental requires Python 3.5 or higher. Install the latest stable version from PyPI \u00b6 pip install -U albumentations_experimental Install the latest version from the master branch on GitHub \u00b6 pip install -U git+https://github.com/albumentations-team/albumentations_experimental","title":"Installation"},{"location":"experimental/installation/#installation","text":"Albumentations Experimental requires Python 3.5 or higher.","title":"Installation"},{"location":"experimental/installation/#install-the-latest-stable-version-from-pypi","text":"pip install -U albumentations_experimental","title":"Install the latest stable version from PyPI"},{"location":"experimental/installation/#install-the-latest-version-from-the-master-branch-on-github","text":"pip install -U git+https://github.com/albumentations-team/albumentations_experimental","title":"Install the latest version from the master branch on GitHub"},{"location":"experimental/overview/","text":"Albumentations Experimental Overview \u00b6 The Albumentations Experimental library provides experimental and cutting edge augmentation techniques on top of Albumentations. The source code is available at https://github.com/albumentations-team/albumentations_experimental . Why a separate library \u00b6 Albumentations provides stable and well-tested interfaces for performing augmentations. We don't want to pollute the library with features that may be prone to rapid changes in interfaces and behavior since they could break users' pipelines. But we also want to implement new, experimental features and see whether they will be useful. So we created Albumentations Experimental, a library that will help us to iterate faster and remove the need for striving for backward compatibility and rigorous testing. Beware, that each new version of Albumentations Experimental may contain backward-incompatible changes both in interfaces and behavior. When features in Albumentations Experimental are mature enough, we will port them to the main library with all our usual policies such as rigorous testing, extensive documentation, and stable behavior. Documentation \u00b6 API Reference Transforms (albumentations_experimental.augmentations.transforms)","title":"Albumentations Experimental Overview"},{"location":"experimental/overview/#albumentations-experimental-overview","text":"The Albumentations Experimental library provides experimental and cutting edge augmentation techniques on top of Albumentations. The source code is available at https://github.com/albumentations-team/albumentations_experimental .","title":"Albumentations Experimental Overview"},{"location":"experimental/overview/#why-a-separate-library","text":"Albumentations provides stable and well-tested interfaces for performing augmentations. We don't want to pollute the library with features that may be prone to rapid changes in interfaces and behavior since they could break users' pipelines. But we also want to implement new, experimental features and see whether they will be useful. So we created Albumentations Experimental, a library that will help us to iterate faster and remove the need for striving for backward compatibility and rigorous testing. Beware, that each new version of Albumentations Experimental may contain backward-incompatible changes both in interfaces and behavior. When features in Albumentations Experimental are mature enough, we will port them to the main library with all our usual policies such as rigorous testing, extensive documentation, and stable behavior.","title":"Why a separate library"},{"location":"experimental/overview/#documentation","text":"API Reference Transforms (albumentations_experimental.augmentations.transforms)","title":"Documentation"},{"location":"experimental/api_reference/augmentations/transforms/","text":"Albumentations Experimental Transforms (augmentations.transforms) \u00b6 \u00b6 class albumentations_experimental.augmentations.transforms.FlipSymmetricKeypoints ( symmetric_keypoints_horizontal = (), symmetric_keypoints_vertical = (), symmetric_keypoints_both = (), * args , ** kwargs ) [view source on GitHub] \u00b6 Flip the input horizontally around the y-axis. Parameters: Name Type Description p float probability of applying the transform. Default: 0.5. symmetric_keypoints_horizontal tuple, list, dict tuple of pairs containing indices of symmetric keypoints. Keypoints are considered as symmetric if horizontal flip swaps their semantics, e.g. left arm - right arm. If keypoint does not have pair then set as it is own index. Kyepoints length must be divisible to symmetric count. For example 5 symmetric pairs and 15 keypoints. symmetric_keypoints_vertical tuple, list, dict tuple of pairs containing indices of symmetric keypoints. Keypoints are considered as symmetric if vertical flip swaps their semantics, e.g. top corner - bottom corner. If keypoint does not have pair then set as it is own index. Kyepoints length must be divisible to symmetric count. For example 5 symmetric pairs and 15 keypoints. symmetric_keypoints_both tuple, list, dict tuple of pairs containing indices of symmetric keypoints. Keypoints are considered as symmetric if vertical and horizontal flip swaps their semantics, e.g. top left corner - bottom right corner. If keypoint does not have pair then set as it is own index. Kyepoints length must be divisible to symmetric count. For example 5 symmetric pairs and 15 keypoints. Targets: image, mask, bboxes, keypoints Image types: uint8, float32 class albumentations_experimental.augmentations.transforms.HorizontalFlipSymmetricKeypoints ( symmetric_keypoints , * args , ** kwargs ) [view source on GitHub] \u00b6 Flip the input horizontally around the y-axis. Parameters: Name Type Description p float probability of applying the transform. Default: 0.5. symmetric_keypoints tuple, list, dict tuple of pairs containing indices of symmetric keypoints. Keypoints are considered as symmetric if horizontal flip swaps their semantics, e.g. left arm - right arm. If keypoint does not have pair then set as it is own index. Kyepoints length must be divisible to symmetric count. For example 5 symmetric pairs and 15 keypoints. Targets: image, mask, bboxes, keypoints Image types: uint8, float32 class albumentations_experimental.augmentations.transforms.TransposeSymmetricKeypoints ( symmetric_keypoints = (), * args , ** kwargs ) [view source on GitHub] \u00b6 Flip the input horizontally around the y-axis. Parameters: Name Type Description p float probability of applying the transform. Default: 0.5. symmetric_keypoints tuple, list, dict tuple of pairs containing indices of symmetric keypoints. Keypoints are considered as symmetric if vertical and horizontal flip swaps their semantics, e.g. top left corner - bottom right corner. If keypoint does not have pair then set as it is own index. Kyepoints length must be divisible to symmetric count. For example 5 symmetric pairs and 15 keypoints. Targets: image, mask, bboxes, keypoints Image types: uint8, float32 class albumentations_experimental.augmentations.transforms.VerticalFlipSymmetricKeypoints ( symmetric_keypoints , * args , ** kwargs ) [view source on GitHub] \u00b6 Flip the input vertically around the x-axis. Parameters: Name Type Description p float probability of applying the transform. Default: 0.5. symmetric_keypoints tuple, list, dict tuple of pairs containing indices of symmetric keypoints. Keypoints are considered as symmetric if vertical flip swaps their semantics, e.g. top corner - bottom corner. If keypoint does not have pair then set as it is own index. Kyepoints length must be divisible to symmetric count. For example 5 symmetric pairs and 15 keypoints. Targets: image, mask, bboxes, keypoints Image types: uint8, float32","title":"Albumentations Experimental Transforms (augmentations.transforms)"},{"location":"experimental/api_reference/augmentations/transforms/#albumentations-experimental-transforms-augmentationstransforms","text":"","title":"Albumentations Experimental Transforms (augmentations.transforms)"},{"location":"experimental/api_reference/augmentations/transforms/#albumentations_experimental.augmentations.transforms","text":"","title":"albumentations_experimental.augmentations.transforms"},{"location":"experimental/api_reference/augmentations/transforms/#albumentations_experimental.augmentations.transforms.FlipSymmetricKeypoints","text":"Flip the input horizontally around the y-axis. Parameters: Name Type Description p float probability of applying the transform. Default: 0.5. symmetric_keypoints_horizontal tuple, list, dict tuple of pairs containing indices of symmetric keypoints. Keypoints are considered as symmetric if horizontal flip swaps their semantics, e.g. left arm - right arm. If keypoint does not have pair then set as it is own index. Kyepoints length must be divisible to symmetric count. For example 5 symmetric pairs and 15 keypoints. symmetric_keypoints_vertical tuple, list, dict tuple of pairs containing indices of symmetric keypoints. Keypoints are considered as symmetric if vertical flip swaps their semantics, e.g. top corner - bottom corner. If keypoint does not have pair then set as it is own index. Kyepoints length must be divisible to symmetric count. For example 5 symmetric pairs and 15 keypoints. symmetric_keypoints_both tuple, list, dict tuple of pairs containing indices of symmetric keypoints. Keypoints are considered as symmetric if vertical and horizontal flip swaps their semantics, e.g. top left corner - bottom right corner. If keypoint does not have pair then set as it is own index. Kyepoints length must be divisible to symmetric count. For example 5 symmetric pairs and 15 keypoints. Targets: image, mask, bboxes, keypoints Image types: uint8, float32","title":"FlipSymmetricKeypoints"},{"location":"experimental/api_reference/augmentations/transforms/#albumentations_experimental.augmentations.transforms.HorizontalFlipSymmetricKeypoints","text":"Flip the input horizontally around the y-axis. Parameters: Name Type Description p float probability of applying the transform. Default: 0.5. symmetric_keypoints tuple, list, dict tuple of pairs containing indices of symmetric keypoints. Keypoints are considered as symmetric if horizontal flip swaps their semantics, e.g. left arm - right arm. If keypoint does not have pair then set as it is own index. Kyepoints length must be divisible to symmetric count. For example 5 symmetric pairs and 15 keypoints. Targets: image, mask, bboxes, keypoints Image types: uint8, float32","title":"HorizontalFlipSymmetricKeypoints"},{"location":"experimental/api_reference/augmentations/transforms/#albumentations_experimental.augmentations.transforms.TransposeSymmetricKeypoints","text":"Flip the input horizontally around the y-axis. Parameters: Name Type Description p float probability of applying the transform. Default: 0.5. symmetric_keypoints tuple, list, dict tuple of pairs containing indices of symmetric keypoints. Keypoints are considered as symmetric if vertical and horizontal flip swaps their semantics, e.g. top left corner - bottom right corner. If keypoint does not have pair then set as it is own index. Kyepoints length must be divisible to symmetric count. For example 5 symmetric pairs and 15 keypoints. Targets: image, mask, bboxes, keypoints Image types: uint8, float32","title":"TransposeSymmetricKeypoints"},{"location":"experimental/api_reference/augmentations/transforms/#albumentations_experimental.augmentations.transforms.VerticalFlipSymmetricKeypoints","text":"Flip the input vertically around the x-axis. Parameters: Name Type Description p float probability of applying the transform. Default: 0.5. symmetric_keypoints tuple, list, dict tuple of pairs containing indices of symmetric keypoints. Keypoints are considered as symmetric if vertical flip swaps their semantics, e.g. top corner - bottom corner. If keypoint does not have pair then set as it is own index. Kyepoints length must be divisible to symmetric count. For example 5 symmetric pairs and 15 keypoints. Targets: image, mask, bboxes, keypoints Image types: uint8, float32","title":"VerticalFlipSymmetricKeypoints"},{"location":"external_resources/blog_posts_podcasts_talks/","text":"Blog posts, podcasts, talks, and videos about Albumentations \u00b6 Blog posts \u00b6 Custom Image Augmentation with Keras. Solving CIFAR-10 with Albumentations and TPU on Google Colab. . Road detection using segmentation models and albumentations libraries on Keras . Image Data Augmentation for TensorFlow 2, Keras and PyTorch with Albumentations in Python Explore image augmentations using a convenient tool Image Augmentation using PyTorch and Albumentations Employing the albumentation library in PyTorch workflows. Bonus: Helper for selecting appropriate values! Podcasts, talks, and videos \u00b6 PyConBY 2020: Eugene Khvedchenya - Albumentations: Fast and Flexible image augmentations Albumentations Framework: a fast image augmentations library | Interview with Dr. Vladimir Iglovikov Image Data Augmentation for TensorFlow 2, Keras and PyTorch with Albumentations in Python Bengali.AI competition - Ch 5. Image augmentations using albumentations Albumentations Tutorial for Data Augmentation","title":"Blog posts, podcasts, talks, and videos about Albumentations"},{"location":"external_resources/blog_posts_podcasts_talks/#blog-posts-podcasts-talks-and-videos-about-albumentations","text":"","title":"Blog posts, podcasts, talks, and videos about Albumentations"},{"location":"external_resources/blog_posts_podcasts_talks/#blog-posts","text":"Custom Image Augmentation with Keras. Solving CIFAR-10 with Albumentations and TPU on Google Colab. . Road detection using segmentation models and albumentations libraries on Keras . Image Data Augmentation for TensorFlow 2, Keras and PyTorch with Albumentations in Python Explore image augmentations using a convenient tool Image Augmentation using PyTorch and Albumentations Employing the albumentation library in PyTorch workflows. Bonus: Helper for selecting appropriate values!","title":"Blog posts"},{"location":"external_resources/blog_posts_podcasts_talks/#podcasts-talks-and-videos","text":"PyConBY 2020: Eugene Khvedchenya - Albumentations: Fast and Flexible image augmentations Albumentations Framework: a fast image augmentations library | Interview with Dr. Vladimir Iglovikov Image Data Augmentation for TensorFlow 2, Keras and PyTorch with Albumentations in Python Bengali.AI competition - Ch 5. Image augmentations using albumentations Albumentations Tutorial for Data Augmentation","title":"Podcasts, talks, and videos"},{"location":"external_resources/books/","text":"Books that mention Albumentations \u00b6 Deep Learning For Dummies . John Paul Mueller, Luca Massaron. May 2019. Data Science Programming All-in-One For Dummies . John Paul Mueller, Luca Massaron. January 2020. PyTorch Computer Vision Cookbook . Michael Avendi. March 2020. Approaching (Almost) Any Machine Learning Problem . Abhishek Thakur. June 2020.","title":"Books that mention Albumentations"},{"location":"external_resources/books/#books-that-mention-albumentations","text":"Deep Learning For Dummies . John Paul Mueller, Luca Massaron. May 2019. Data Science Programming All-in-One For Dummies . John Paul Mueller, Luca Massaron. January 2020. PyTorch Computer Vision Cookbook . Michael Avendi. March 2020. Approaching (Almost) Any Machine Learning Problem . Abhishek Thakur. June 2020.","title":"Books that mention Albumentations"},{"location":"getting_started/bounding_boxes_augmentation/","text":"Bounding boxes augmentation for object detection \u00b6 Different annotations formats \u00b6 Bounding boxes are rectangles that mark objects on an image. There are multiple formats of bounding boxes annotations. Each format uses its specific representation of bouning boxes coordinates. Albumentations supports four formats: pascal_voc , albumentations , coco , and yolo . Let's take a look at each of those formats and how they represent coordinates of bounding boxes. As an example, we will use an image from the dataset named Common Objects in Context . It contains one bounding box that marks a cat. The image width is 640 pixels, and its height is 480 pixels. The width of the bounding box is 322 pixels, and its height is 117 pixels. The bounding box has the following (x, y) coordinates of its corners: top-left is (x_min, y_min) or (98px, 345px) , top-right is (x_max, y_min) or (420px, 345px) , bottom-left is (x_min, y_max) or (98px, 462px) , bottom-right is (x_max, y_max) or (420px, 462px) . As you see, coordinates of the bounding box's corners are calculated with respect to the top-left corner of the image which has (x, y) coordinates (0, 0) . An example image with a bounding box from the COCO dataset pascal_voc \u00b6 pascal_voc is a format used by the Pascal VOC dataset . Coordinates of a bounding box are encoded with four values in pixels: [x_min, y_min, x_max, y_max] . x_min and y_min are coordinates of the top-left corner of the bounding box. x_max and y_max are coordinates of bottom-right corner of the bounding box. Coordinates of the example bounding box in this format are [98, 345, 420, 462] . albumentations \u00b6 albumentations is similar to pascal_voc , because it also uses four values [x_min, y_min, x_max, y_max] to represent a bounding box. But unlike pascal_voc , albumentations uses normalized values. To normalize values, we divide coordinates in pixels for the x- and y-axis by the width and the height of the image. Coordinates of the example bounding box in this format are [98 / 640, 345 / 480, 420 / 640, 462 / 480] which are [0.153125, 0.71875, 0.65625, 0.9625] . Albumentations uses this format internally to work with bounding boxes and augment them. coco \u00b6 coco is a format used by the Common Objects in Context COCO COCO dataset. In coco , a bounding box is defined by four values in pixels [x_min, y_min, width, height] . They are coordinates of the top-left corner along with the width and height of the bounding box. Coordinates of the example bounding box in this format are [98, 345, 322, 117] . yolo \u00b6 In yolo , a bounding box is represented by four values [x_center, y_center, width, height] . x_center and y_center are the normalized coordinates of the center of the bounding box. To make coordinates normalized, we take pixel values of x and y, which marks the center of the bounding box on the x- and y-axis. Then we divide the value of x by the width of the image and value of y by the height of the image. width and height represent the width and the height of the bounding box. They are normalized as well. Coordinates of the example bounding box in this format are [((420 + 98) / 2) / 640, ((462 + 345) / 2) / 480, 322 / 640, 117 / 480] which are [0.4046875, 0.840625, 0.503125, 0.24375] . How different formats represent coordinates of a bounding box Bounding boxes augmentation \u00b6 Just like with images and masks augmentation, the process of augmenting bounding boxes consists of 4 steps. You import the required libraries. You define an augmentation pipeline. You read images and bounding boxes from the disk. You pass an image and bounding boxes to the augmentation pipeline and receive augmented images and boxes. Note Some transforms in Albumentation don't support bounding boxes. If you try to use them you will get an exception. Please refer to this article to check whether a transform can augment bounding boxes. Step 1. Import the required libraries. \u00b6 import albumentations as A import cv2 Step 2. Define an augmentation pipeline. \u00b6 Here an example of a minimal declaration of an augmentation pipeline that works with bounding boxes. transform = A . Compose ([ A . RandomCrop ( width = 450 , height = 450 ), A . HorizontalFlip ( p = 0.5 ), A . RandomBrightnessContrast ( p = 0.2 ), ], bbox_params = A . BboxParams ( format = 'coco' )) Note that unlike image and masks augmentation, Compose now has an additional parameter bbox_params . You need to pass an instance of A.BboxParams to that argument. A.BboxParams specifies settings for working with bounding boxes. format sets the format for bounding boxes coordinates. It can either be pascal_voc , albumentations , coco or yolo . This value is required because Albumentation needs to know the coordinates' source format for bounding boxes to apply augmentations correctly. Besides format , A.BboxParams supports a few more settings. Here is an example of Compose that shows all available settings with A.BboxParams : transform = A . Compose ([ A . RandomCrop ( width = 450 , height = 450 ), A . HorizontalFlip ( p = 0.5 ), A . RandomBrightnessContrast ( p = 0.2 ), ], bbox_params = A . BboxParams ( format = 'coco' , min_area = 1024 , min_visibility = 0.1 , label_fields = [ 'class_labels' ])) min_area and min_visibility \u00b6 min_area and min_visibility parameters control what Albumentations should do to the augmented bounding boxes if their size has changed after augmentation. The size of bounding boxes could change if you apply spatial augmentations, for example, when you crop a part of an image or when you resize an image. min_area is a value in pixels. If the area of a bounding box after augmentation becomes smaller than min_area , Albumentations will drop that box. So the returned list of augmented bounding boxes won't contain that bounding box. min_visibility is a value between 0 and 1. If the ratio of the bounding box area after augmentation to the area of the bounding box before augmentation becomes smaller than min_visibility , Albumentations will drop that box. So if the augmentation process cuts the most of the bounding box, that box won't be present in the returned list of the augmented bounding boxes. Here is an example image that contains two bounding boxes. Bounding boxes coordinates are declared using the coco format. An example image with two bounding boxes First, we apply the CenterCrop augmentation without declaring parameters min_area and min_visibility . The augmented image contains two bounding boxes. An example image with two bounding boxes after applying augmentation Next, we apply the same CenterCrop augmentation, but now we also use the min_area parameter. Now, the augmented image contains only one bounding box, because the other bounding box's area after augmentation became smaller than min_area , so Albumentations dropped that bounding box. An example image with one bounding box after applying augmentation with 'min_area' Finally, we apply the CenterCrop augmentation with the min_visibility . After that augmentation, the resulting image doesn't contain any bounding box, because visibility of all bounding boxes after augmentation are below threshold set by min_visibility . An example image with zero bounding boxes after applying augmentation with 'min_visibility' Class labels for bounding boxes \u00b6 Besides coordinates, each bounding box should have an associated class label that tells which object lies inside the bounding box. There are two ways to pass a label for a bounding box. Let's say you have an example image with three objects: dog , cat , and sports ball . Bounding boxes coordinates in the coco format for those objects are [23, 74, 295, 388] , [377, 294, 252, 161] , and [333, 421, 49, 49] . An example image with 3 bounding boxes from the COCO dataset 1. You can pass labels along with bounding boxes coordinates by adding them as additional values to the list of coordinates. \u00b6 For the image above, bounding boxes with class labels will become [23, 74, 295, 388, 'dog'] , [377, 294, 252, 161, 'cat'] , and [333, 421, 49, 49, 'sports ball'] . Class labels could be of any type: integer, string, or any other Python data type. For example, integer values as class labels will look the following: [23, 74, 295, 388, 18] , [377, 294, 252, 161, 17] , and [333, 421, 49, 49, 37]. Also, you can use multiple class values for each bounding box, for example [23, 74, 295, 388, 'dog', 'animal'] , [377, 294, 252, 161, 'cat', 'animal'] , and [333, 421, 49, 49, 'sports ball', 'item'] . 2.You can pass labels for bounding boxes as a separate list (the preferred way). \u00b6 For example, if you have three bounding boxes like [23, 74, 295, 388] , [377, 294, 252, 161] , and [333, 421, 49, 49] you can create a separate list with values like ['cat', 'dog', 'sports ball'] , or [18, 17, 37] that contains class labels for those bounding boxes. Next, you pass that list with class labels as a separate argument to the transform function. Albumentations needs to know the names of all those lists with class labels to join them with augmented bounding boxes correctly. Then, if a bounding box is dropped after augmentation because it is no longer visible, Albumentations will drop the class label for that box as well. Use label_fields parameter to set names for all arguments in transform that will contain label descriptions for bounding boxes (more on that in Step 4). Step 3. Read images and bounding boxes from the disk. \u00b6 Read an image from the disk. image = cv2 . imread ( \"/path/to/image.jpg\" ) image = cv2 . cvtColor ( image , cv2 . COLOR_BGR2RGB ) Bounding boxes can be stored on the disk in different serialization formats: JSON, XML, YAML, CSV, etc. So the code to read bounding boxes depends on the actual format of data on the disk. After you read the data from the disk, you need to prepare bounding boxes for Albumentations. Albumentations expects that bounding boxes will be represented as a list of lists. Each list contains information about a single bounding box. A bounding box definition should have at list four elements that represent the coordinates of that bounding box. The actual meaning of those four values depends on the format of bounding boxes (either pascal_voc , albumentations , coco , or yolo ). Besides four coordinates, each definition of a bounding box may contain one or more extra values. You can use those extra values to store additional information about the bounding box, such as a class label of the object inside the box. During augmentation, Albumentations will not process those extra values. The library will return them as is along with the updated coordinates of the augmented bounding box. Step 4. Pass an image and bounding boxes to the augmentation pipeline and receive augmented images and boxes. \u00b6 As discussed in Step 2, there are two ways of passing class labels along with bounding boxes coordinates: 1. Pass class labels along with coordinates. \u00b6 So, if you have coordinates of three bounding boxes that look like this: bboxes = [ [ 23 , 74 , 295 , 388 ], [ 377 , 294 , 252 , 161 ], [ 333 , 421 , 49 , 49 ], ] you can add a class label for each bounding box as an additional element of the list along with four coordinates. So now a list with bounding boxes and their coordinates will look the following: bboxes = [ [ 23 , 74 , 295 , 388 , 'dog' ], [ 377 , 294 , 252 , 161 , 'cat' ], [ 333 , 421 , 49 , 49 , 'sports ball' ], ] or with multiple labels per each bounding box: bboxes = [ [ 23 , 74 , 295 , 388 , 'dog' , 'animal' ], [ 377 , 294 , 252 , 161 , 'cat' , 'animal' ], [ 333 , 421 , 49 , 49 , 'sports ball' , 'item' ], ] You can use any data type for declaring class labels. It can be string, integer, or any other Python data type. Next, you pass an image and bounding boxes for it to the transform function and receive the augmented image and bounding boxes. transformed = transform ( image = image , bboxes = bboxes ) transformed_image = transformed [ 'image' ] transformed_bboxes = transformed [ 'bboxes' ] Example input and output data for bounding boxes augmentation 2. Pass class labels in a separate argument to transform (the preferred way). \u00b6 Let's say you have coordinates of three bounding boxes bboxes = [ [ 23 , 74 , 295 , 388 ], [ 377 , 294 , 252 , 161 ], [ 333 , 421 , 49 , 49 ], ] You can create a separate list that contains class labels for those bounding boxes: class_labels = [ 'cat' , 'dog' , 'parrot' ] Then you pass both bounding boxes and class labels to transform . Note that to pass class labels, you need to use the name of the argument that you declared in label_fields when creating an instance of Compose in step 2. In our case, we set the name of the argument to class_labels . transformed = transform ( image = image , bboxes = bboxes , class_labels = class_labels ) transformed_image = transformed [ 'image' ] transformed_bboxes = transformed [ 'bboxes' ] transformed_class_labels = transformed [ 'class_labels' ] Example input and output data for bounding boxes augmentation with a separate argument for class labels Note that label_fields expects a list, so you can set multiple fields that contain labels for your bounding boxes. So if you declare Compose like transform = A . Compose ([ A . RandomCrop ( width = 450 , height = 450 ), A . HorizontalFlip ( p = 0.5 ), A . RandomBrightnessContrast ( p = 0.2 ), ], bbox_params = A . BboxParams ( format = 'coco' , label_fields = [ 'class_labels' , 'class_categories' ]))) you can use those multiple arguments to pass info about class labels, like class_labels = [ 'cat' , 'dog' , 'parrot' ] class_categories = [ 'animal' , 'animal' , 'item' ] transformed = transform ( image = image , bboxes = bboxes , class_labels = class_labels , class_categories = class_categories ) transformed_image = transformed [ 'image' ] transformed_bboxes = transformed [ 'bboxes' ] transformed_class_labels = transformed [ 'class_labels' ] transformed_class_categories = transformed [ 'class_categories' ] Examples \u00b6 Using Albumentations to augment bounding boxes for object detection tasks How to use Albumentations for detection tasks if you need to keep all bounding boxes Showcase. Cool augmentation examples on diverse set of images from various real-world tasks.","title":"Bounding boxes augmentation for object detection"},{"location":"getting_started/bounding_boxes_augmentation/#bounding-boxes-augmentation-for-object-detection","text":"","title":"Bounding boxes augmentation for object detection"},{"location":"getting_started/bounding_boxes_augmentation/#different-annotations-formats","text":"Bounding boxes are rectangles that mark objects on an image. There are multiple formats of bounding boxes annotations. Each format uses its specific representation of bouning boxes coordinates. Albumentations supports four formats: pascal_voc , albumentations , coco , and yolo . Let's take a look at each of those formats and how they represent coordinates of bounding boxes. As an example, we will use an image from the dataset named Common Objects in Context . It contains one bounding box that marks a cat. The image width is 640 pixels, and its height is 480 pixels. The width of the bounding box is 322 pixels, and its height is 117 pixels. The bounding box has the following (x, y) coordinates of its corners: top-left is (x_min, y_min) or (98px, 345px) , top-right is (x_max, y_min) or (420px, 345px) , bottom-left is (x_min, y_max) or (98px, 462px) , bottom-right is (x_max, y_max) or (420px, 462px) . As you see, coordinates of the bounding box's corners are calculated with respect to the top-left corner of the image which has (x, y) coordinates (0, 0) . An example image with a bounding box from the COCO dataset","title":"Different annotations formats"},{"location":"getting_started/bounding_boxes_augmentation/#pascal_voc","text":"pascal_voc is a format used by the Pascal VOC dataset . Coordinates of a bounding box are encoded with four values in pixels: [x_min, y_min, x_max, y_max] . x_min and y_min are coordinates of the top-left corner of the bounding box. x_max and y_max are coordinates of bottom-right corner of the bounding box. Coordinates of the example bounding box in this format are [98, 345, 420, 462] .","title":"pascal_voc"},{"location":"getting_started/bounding_boxes_augmentation/#albumentations","text":"albumentations is similar to pascal_voc , because it also uses four values [x_min, y_min, x_max, y_max] to represent a bounding box. But unlike pascal_voc , albumentations uses normalized values. To normalize values, we divide coordinates in pixels for the x- and y-axis by the width and the height of the image. Coordinates of the example bounding box in this format are [98 / 640, 345 / 480, 420 / 640, 462 / 480] which are [0.153125, 0.71875, 0.65625, 0.9625] . Albumentations uses this format internally to work with bounding boxes and augment them.","title":"albumentations"},{"location":"getting_started/bounding_boxes_augmentation/#coco","text":"coco is a format used by the Common Objects in Context COCO COCO dataset. In coco , a bounding box is defined by four values in pixels [x_min, y_min, width, height] . They are coordinates of the top-left corner along with the width and height of the bounding box. Coordinates of the example bounding box in this format are [98, 345, 322, 117] .","title":"coco"},{"location":"getting_started/bounding_boxes_augmentation/#yolo","text":"In yolo , a bounding box is represented by four values [x_center, y_center, width, height] . x_center and y_center are the normalized coordinates of the center of the bounding box. To make coordinates normalized, we take pixel values of x and y, which marks the center of the bounding box on the x- and y-axis. Then we divide the value of x by the width of the image and value of y by the height of the image. width and height represent the width and the height of the bounding box. They are normalized as well. Coordinates of the example bounding box in this format are [((420 + 98) / 2) / 640, ((462 + 345) / 2) / 480, 322 / 640, 117 / 480] which are [0.4046875, 0.840625, 0.503125, 0.24375] . How different formats represent coordinates of a bounding box","title":"yolo"},{"location":"getting_started/bounding_boxes_augmentation/#bounding-boxes-augmentation","text":"Just like with images and masks augmentation, the process of augmenting bounding boxes consists of 4 steps. You import the required libraries. You define an augmentation pipeline. You read images and bounding boxes from the disk. You pass an image and bounding boxes to the augmentation pipeline and receive augmented images and boxes. Note Some transforms in Albumentation don't support bounding boxes. If you try to use them you will get an exception. Please refer to this article to check whether a transform can augment bounding boxes.","title":"Bounding boxes augmentation"},{"location":"getting_started/bounding_boxes_augmentation/#step-1-import-the-required-libraries","text":"import albumentations as A import cv2","title":"Step 1. Import the required libraries."},{"location":"getting_started/bounding_boxes_augmentation/#step-2-define-an-augmentation-pipeline","text":"Here an example of a minimal declaration of an augmentation pipeline that works with bounding boxes. transform = A . Compose ([ A . RandomCrop ( width = 450 , height = 450 ), A . HorizontalFlip ( p = 0.5 ), A . RandomBrightnessContrast ( p = 0.2 ), ], bbox_params = A . BboxParams ( format = 'coco' )) Note that unlike image and masks augmentation, Compose now has an additional parameter bbox_params . You need to pass an instance of A.BboxParams to that argument. A.BboxParams specifies settings for working with bounding boxes. format sets the format for bounding boxes coordinates. It can either be pascal_voc , albumentations , coco or yolo . This value is required because Albumentation needs to know the coordinates' source format for bounding boxes to apply augmentations correctly. Besides format , A.BboxParams supports a few more settings. Here is an example of Compose that shows all available settings with A.BboxParams : transform = A . Compose ([ A . RandomCrop ( width = 450 , height = 450 ), A . HorizontalFlip ( p = 0.5 ), A . RandomBrightnessContrast ( p = 0.2 ), ], bbox_params = A . BboxParams ( format = 'coco' , min_area = 1024 , min_visibility = 0.1 , label_fields = [ 'class_labels' ]))","title":"Step 2. Define an augmentation pipeline."},{"location":"getting_started/bounding_boxes_augmentation/#min_area-and-min_visibility","text":"min_area and min_visibility parameters control what Albumentations should do to the augmented bounding boxes if their size has changed after augmentation. The size of bounding boxes could change if you apply spatial augmentations, for example, when you crop a part of an image or when you resize an image. min_area is a value in pixels. If the area of a bounding box after augmentation becomes smaller than min_area , Albumentations will drop that box. So the returned list of augmented bounding boxes won't contain that bounding box. min_visibility is a value between 0 and 1. If the ratio of the bounding box area after augmentation to the area of the bounding box before augmentation becomes smaller than min_visibility , Albumentations will drop that box. So if the augmentation process cuts the most of the bounding box, that box won't be present in the returned list of the augmented bounding boxes. Here is an example image that contains two bounding boxes. Bounding boxes coordinates are declared using the coco format. An example image with two bounding boxes First, we apply the CenterCrop augmentation without declaring parameters min_area and min_visibility . The augmented image contains two bounding boxes. An example image with two bounding boxes after applying augmentation Next, we apply the same CenterCrop augmentation, but now we also use the min_area parameter. Now, the augmented image contains only one bounding box, because the other bounding box's area after augmentation became smaller than min_area , so Albumentations dropped that bounding box. An example image with one bounding box after applying augmentation with 'min_area' Finally, we apply the CenterCrop augmentation with the min_visibility . After that augmentation, the resulting image doesn't contain any bounding box, because visibility of all bounding boxes after augmentation are below threshold set by min_visibility . An example image with zero bounding boxes after applying augmentation with 'min_visibility'","title":"min_area and min_visibility"},{"location":"getting_started/bounding_boxes_augmentation/#class-labels-for-bounding-boxes","text":"Besides coordinates, each bounding box should have an associated class label that tells which object lies inside the bounding box. There are two ways to pass a label for a bounding box. Let's say you have an example image with three objects: dog , cat , and sports ball . Bounding boxes coordinates in the coco format for those objects are [23, 74, 295, 388] , [377, 294, 252, 161] , and [333, 421, 49, 49] . An example image with 3 bounding boxes from the COCO dataset","title":"Class labels for bounding boxes"},{"location":"getting_started/bounding_boxes_augmentation/#1-you-can-pass-labels-along-with-bounding-boxes-coordinates-by-adding-them-as-additional-values-to-the-list-of-coordinates","text":"For the image above, bounding boxes with class labels will become [23, 74, 295, 388, 'dog'] , [377, 294, 252, 161, 'cat'] , and [333, 421, 49, 49, 'sports ball'] . Class labels could be of any type: integer, string, or any other Python data type. For example, integer values as class labels will look the following: [23, 74, 295, 388, 18] , [377, 294, 252, 161, 17] , and [333, 421, 49, 49, 37]. Also, you can use multiple class values for each bounding box, for example [23, 74, 295, 388, 'dog', 'animal'] , [377, 294, 252, 161, 'cat', 'animal'] , and [333, 421, 49, 49, 'sports ball', 'item'] .","title":"1. You can pass labels along with bounding boxes coordinates by adding them as additional values to the list of coordinates."},{"location":"getting_started/bounding_boxes_augmentation/#2you-can-pass-labels-for-bounding-boxes-as-a-separate-list-the-preferred-way","text":"For example, if you have three bounding boxes like [23, 74, 295, 388] , [377, 294, 252, 161] , and [333, 421, 49, 49] you can create a separate list with values like ['cat', 'dog', 'sports ball'] , or [18, 17, 37] that contains class labels for those bounding boxes. Next, you pass that list with class labels as a separate argument to the transform function. Albumentations needs to know the names of all those lists with class labels to join them with augmented bounding boxes correctly. Then, if a bounding box is dropped after augmentation because it is no longer visible, Albumentations will drop the class label for that box as well. Use label_fields parameter to set names for all arguments in transform that will contain label descriptions for bounding boxes (more on that in Step 4).","title":"2.You can pass labels for bounding boxes as a separate list (the preferred way)."},{"location":"getting_started/bounding_boxes_augmentation/#step-3-read-images-and-bounding-boxes-from-the-disk","text":"Read an image from the disk. image = cv2 . imread ( \"/path/to/image.jpg\" ) image = cv2 . cvtColor ( image , cv2 . COLOR_BGR2RGB ) Bounding boxes can be stored on the disk in different serialization formats: JSON, XML, YAML, CSV, etc. So the code to read bounding boxes depends on the actual format of data on the disk. After you read the data from the disk, you need to prepare bounding boxes for Albumentations. Albumentations expects that bounding boxes will be represented as a list of lists. Each list contains information about a single bounding box. A bounding box definition should have at list four elements that represent the coordinates of that bounding box. The actual meaning of those four values depends on the format of bounding boxes (either pascal_voc , albumentations , coco , or yolo ). Besides four coordinates, each definition of a bounding box may contain one or more extra values. You can use those extra values to store additional information about the bounding box, such as a class label of the object inside the box. During augmentation, Albumentations will not process those extra values. The library will return them as is along with the updated coordinates of the augmented bounding box.","title":"Step 3. Read images and bounding boxes from the disk."},{"location":"getting_started/bounding_boxes_augmentation/#step-4-pass-an-image-and-bounding-boxes-to-the-augmentation-pipeline-and-receive-augmented-images-and-boxes","text":"As discussed in Step 2, there are two ways of passing class labels along with bounding boxes coordinates:","title":"Step 4. Pass an image and bounding boxes to the augmentation pipeline and receive augmented images and boxes."},{"location":"getting_started/bounding_boxes_augmentation/#1-pass-class-labels-along-with-coordinates","text":"So, if you have coordinates of three bounding boxes that look like this: bboxes = [ [ 23 , 74 , 295 , 388 ], [ 377 , 294 , 252 , 161 ], [ 333 , 421 , 49 , 49 ], ] you can add a class label for each bounding box as an additional element of the list along with four coordinates. So now a list with bounding boxes and their coordinates will look the following: bboxes = [ [ 23 , 74 , 295 , 388 , 'dog' ], [ 377 , 294 , 252 , 161 , 'cat' ], [ 333 , 421 , 49 , 49 , 'sports ball' ], ] or with multiple labels per each bounding box: bboxes = [ [ 23 , 74 , 295 , 388 , 'dog' , 'animal' ], [ 377 , 294 , 252 , 161 , 'cat' , 'animal' ], [ 333 , 421 , 49 , 49 , 'sports ball' , 'item' ], ] You can use any data type for declaring class labels. It can be string, integer, or any other Python data type. Next, you pass an image and bounding boxes for it to the transform function and receive the augmented image and bounding boxes. transformed = transform ( image = image , bboxes = bboxes ) transformed_image = transformed [ 'image' ] transformed_bboxes = transformed [ 'bboxes' ] Example input and output data for bounding boxes augmentation","title":"1. Pass class labels along with coordinates."},{"location":"getting_started/bounding_boxes_augmentation/#2-pass-class-labels-in-a-separate-argument-to-transform-the-preferred-way","text":"Let's say you have coordinates of three bounding boxes bboxes = [ [ 23 , 74 , 295 , 388 ], [ 377 , 294 , 252 , 161 ], [ 333 , 421 , 49 , 49 ], ] You can create a separate list that contains class labels for those bounding boxes: class_labels = [ 'cat' , 'dog' , 'parrot' ] Then you pass both bounding boxes and class labels to transform . Note that to pass class labels, you need to use the name of the argument that you declared in label_fields when creating an instance of Compose in step 2. In our case, we set the name of the argument to class_labels . transformed = transform ( image = image , bboxes = bboxes , class_labels = class_labels ) transformed_image = transformed [ 'image' ] transformed_bboxes = transformed [ 'bboxes' ] transformed_class_labels = transformed [ 'class_labels' ] Example input and output data for bounding boxes augmentation with a separate argument for class labels Note that label_fields expects a list, so you can set multiple fields that contain labels for your bounding boxes. So if you declare Compose like transform = A . Compose ([ A . RandomCrop ( width = 450 , height = 450 ), A . HorizontalFlip ( p = 0.5 ), A . RandomBrightnessContrast ( p = 0.2 ), ], bbox_params = A . BboxParams ( format = 'coco' , label_fields = [ 'class_labels' , 'class_categories' ]))) you can use those multiple arguments to pass info about class labels, like class_labels = [ 'cat' , 'dog' , 'parrot' ] class_categories = [ 'animal' , 'animal' , 'item' ] transformed = transform ( image = image , bboxes = bboxes , class_labels = class_labels , class_categories = class_categories ) transformed_image = transformed [ 'image' ] transformed_bboxes = transformed [ 'bboxes' ] transformed_class_labels = transformed [ 'class_labels' ] transformed_class_categories = transformed [ 'class_categories' ]","title":"2. Pass class labels in a separate argument to transform (the preferred way)."},{"location":"getting_started/bounding_boxes_augmentation/#examples","text":"Using Albumentations to augment bounding boxes for object detection tasks How to use Albumentations for detection tasks if you need to keep all bounding boxes Showcase. Cool augmentation examples on diverse set of images from various real-world tasks.","title":"Examples"},{"location":"getting_started/image_augmentation/","text":"Image augmentation for classification \u00b6 We can divide the process of image augmentation into four steps: Import albumentations and a library to read images from the disk (e.g., OpenCV). Define an augmentation pipeline. Read images from the disk. Pass images to the augmentation pipeline and receive augmented images. Step 1. Import the required libraries. \u00b6 Import Albumentations import albumentations as A Import a library to read images from the disk. In this example, we will use OpenCV . It is an open-source computer vision library that supports many image formats. Albumentations has OpenCV as a dependency, so you already have OpenCV installed. import cv2 Step 2. Define an augmentation pipeline. \u00b6 To define an augmentation pipeline, you need to create an instance of the Compose class. As an argument to the Compose class, you need to pass a list of augmentations you want to apply. A call to Compose will return a transform function that will perform image augmentation. Let's look at an example: transform = A . Compose ([ A . RandomCrop ( width = 256 , height = 256 ), A . HorizontalFlip ( p = 0.5 ), A . RandomBrightnessContrast ( p = 0.2 ), ]) In the example, Compose receives a list with three augmentations: A.RandomCrop , A.HorizontalFlip , and A.RandomBrighntessContrast . You can find the full list of all available augmentations in the GitHub repository and in the API Docs . A demo playground that demonstrates how augmentations will transform the input image is available at https://albumentations-demo.herokuapp.com . To create an augmentation, you create an instance of the required augmentation class and pass augmentation parameters to it. A.RandomCrop receives two parameters, height and width . A.RandomCrop(width=256, height=256) means that A.RandomCrop will take an input image, extract a random patch with size 256 by 256 pixels from it and then pass the result to the next augmentation in the pipeline (in this case to A.HorizontalFlip ). A.HorizontalFlip in this example has one parameter named p . p is a special parameter that is supported by almost all augmentations. It controls the probability of applying the augmentation. p=0.5 means that with a probability of 50%, the transform will flip the image horizontally, and with a probability of 50%, the transform won't modify the input image. A.RandomBrighntessContrast in the example also has one parameter, p . With a probability of 20%, this augmentation will change the brightness and contrast of the image received from A.HorizontalFlip . And with a probability of 80%, it will keep the received image unchanged. A visualized version of the augmentation pipeline. You pass an image to it, the image goes through all transformations, and then you receive an augmented image from the pipeline. Step 3. Read images from the disk. \u00b6 To pass an image to the augmentation pipeline, you need to read it from the disk. The pipeline expects to receive an image in the form of a NumPy array. If it is a color image, it should have three channels in the following order: Red, Green, Blue (so a regular RGB image). To read images from the disk, you can use OpenCV - a popular library for image processing. It supports a lot of input formats and is installed along with Albumentations since Albumentations utilizes that library under the hood for a lot of augmentations. To import OpenCV import cv2 To read an image with OpenCV image = cv2 . imread ( \"/path/to/image.jpg\" ) image = cv2 . cvtColor ( image , cv2 . COLOR_BGR2RGB ) Note the usage of cv2.cvtColor . For historical reasons , OpenCV reads an image in BGR format (so color channels of the image have the following order: Blue, Green, Red). Albumentations uses the most common and popular RGB image format. So when using OpenCV, we need to convert the image format to RGB explicitly. Besides OpenCV, you can use other image processing libraries. Pillow \u00b6 Pillow is a popular Python image processing library. Install Pillow pip install pillow Import Pillow and NumPy (we need NumPy to convert a Pillow image to a NumPy array. NumPy is already installed along with Albumentations). from PIL import Image import numpy as np Read an image with Pillow and convert it to a NumPy array. pillow_image = Image . open ( \"image.jpg\" ) image = np . array ( pillow_image ) Step 4. Pass images to the augmentation pipeline and receive augmented images. \u00b6 To pass an image to the augmentation pipeline you need to call the transform function created by a call to A.Compose at Step 2. In the image argument to that function, you need to pass an image that you want to augment. transformed = transform ( image = image ) transform will return a dictionary with a single key image . Value at that key will contain an augmented image. transformed_image = transformed [ \"image\" ] To augment the next image, you need to call transform again and pass a new image as the image argument: another_transformed_image = transform ( image = another_image )[ \"image\" ] Each augmentation will change the input image with the probability set by the parameter p . Also, many augmentations have parameters that control the magnitude of changes that will be applied to an image. For example, A.RandomBrightnessContrast has two parameters: brightness_limit that controls the magnitude of adjusting brightness and contrast_limit that controls the magnitude of adjusting contrast. The bigger the value, the more the augmentation will change an image. During augmentation, a magnitude of the transformation is sampled from a uniform distribution limited by brightness_limit and contrast_limit . That means that if you make multiple calls to transform with the same input image, you will get a different output image each time. transform = A . Compose ([ A . RandomBrightnessContrast ( brightness_limit = 1 , contrast_limit = 1 , p = 1.0 ), ]) transformed_image_1 = transform ( image = image )[ 'image' ] transformed_image_2 = transform ( image = image )[ 'image' ] transformed_image_3 = transform ( image = image )[ 'image' ] Examples \u00b6 Defining a simple augmentation pipeline for image augmentation Working with non-8-bit images Weather augmentations in Albumentations Showcase. Cool augmentation examples on diverse set of images from various real-world tasks.","title":"Image augmentation for classification"},{"location":"getting_started/image_augmentation/#image-augmentation-for-classification","text":"We can divide the process of image augmentation into four steps: Import albumentations and a library to read images from the disk (e.g., OpenCV). Define an augmentation pipeline. Read images from the disk. Pass images to the augmentation pipeline and receive augmented images.","title":"Image augmentation for classification"},{"location":"getting_started/image_augmentation/#step-1-import-the-required-libraries","text":"Import Albumentations import albumentations as A Import a library to read images from the disk. In this example, we will use OpenCV . It is an open-source computer vision library that supports many image formats. Albumentations has OpenCV as a dependency, so you already have OpenCV installed. import cv2","title":"Step 1. Import the required libraries."},{"location":"getting_started/image_augmentation/#step-2-define-an-augmentation-pipeline","text":"To define an augmentation pipeline, you need to create an instance of the Compose class. As an argument to the Compose class, you need to pass a list of augmentations you want to apply. A call to Compose will return a transform function that will perform image augmentation. Let's look at an example: transform = A . Compose ([ A . RandomCrop ( width = 256 , height = 256 ), A . HorizontalFlip ( p = 0.5 ), A . RandomBrightnessContrast ( p = 0.2 ), ]) In the example, Compose receives a list with three augmentations: A.RandomCrop , A.HorizontalFlip , and A.RandomBrighntessContrast . You can find the full list of all available augmentations in the GitHub repository and in the API Docs . A demo playground that demonstrates how augmentations will transform the input image is available at https://albumentations-demo.herokuapp.com . To create an augmentation, you create an instance of the required augmentation class and pass augmentation parameters to it. A.RandomCrop receives two parameters, height and width . A.RandomCrop(width=256, height=256) means that A.RandomCrop will take an input image, extract a random patch with size 256 by 256 pixels from it and then pass the result to the next augmentation in the pipeline (in this case to A.HorizontalFlip ). A.HorizontalFlip in this example has one parameter named p . p is a special parameter that is supported by almost all augmentations. It controls the probability of applying the augmentation. p=0.5 means that with a probability of 50%, the transform will flip the image horizontally, and with a probability of 50%, the transform won't modify the input image. A.RandomBrighntessContrast in the example also has one parameter, p . With a probability of 20%, this augmentation will change the brightness and contrast of the image received from A.HorizontalFlip . And with a probability of 80%, it will keep the received image unchanged. A visualized version of the augmentation pipeline. You pass an image to it, the image goes through all transformations, and then you receive an augmented image from the pipeline.","title":"Step 2. Define an augmentation pipeline."},{"location":"getting_started/image_augmentation/#step-3-read-images-from-the-disk","text":"To pass an image to the augmentation pipeline, you need to read it from the disk. The pipeline expects to receive an image in the form of a NumPy array. If it is a color image, it should have three channels in the following order: Red, Green, Blue (so a regular RGB image). To read images from the disk, you can use OpenCV - a popular library for image processing. It supports a lot of input formats and is installed along with Albumentations since Albumentations utilizes that library under the hood for a lot of augmentations. To import OpenCV import cv2 To read an image with OpenCV image = cv2 . imread ( \"/path/to/image.jpg\" ) image = cv2 . cvtColor ( image , cv2 . COLOR_BGR2RGB ) Note the usage of cv2.cvtColor . For historical reasons , OpenCV reads an image in BGR format (so color channels of the image have the following order: Blue, Green, Red). Albumentations uses the most common and popular RGB image format. So when using OpenCV, we need to convert the image format to RGB explicitly. Besides OpenCV, you can use other image processing libraries.","title":"Step 3. Read images from the disk."},{"location":"getting_started/image_augmentation/#pillow","text":"Pillow is a popular Python image processing library. Install Pillow pip install pillow Import Pillow and NumPy (we need NumPy to convert a Pillow image to a NumPy array. NumPy is already installed along with Albumentations). from PIL import Image import numpy as np Read an image with Pillow and convert it to a NumPy array. pillow_image = Image . open ( \"image.jpg\" ) image = np . array ( pillow_image )","title":"Pillow"},{"location":"getting_started/image_augmentation/#step-4-pass-images-to-the-augmentation-pipeline-and-receive-augmented-images","text":"To pass an image to the augmentation pipeline you need to call the transform function created by a call to A.Compose at Step 2. In the image argument to that function, you need to pass an image that you want to augment. transformed = transform ( image = image ) transform will return a dictionary with a single key image . Value at that key will contain an augmented image. transformed_image = transformed [ \"image\" ] To augment the next image, you need to call transform again and pass a new image as the image argument: another_transformed_image = transform ( image = another_image )[ \"image\" ] Each augmentation will change the input image with the probability set by the parameter p . Also, many augmentations have parameters that control the magnitude of changes that will be applied to an image. For example, A.RandomBrightnessContrast has two parameters: brightness_limit that controls the magnitude of adjusting brightness and contrast_limit that controls the magnitude of adjusting contrast. The bigger the value, the more the augmentation will change an image. During augmentation, a magnitude of the transformation is sampled from a uniform distribution limited by brightness_limit and contrast_limit . That means that if you make multiple calls to transform with the same input image, you will get a different output image each time. transform = A . Compose ([ A . RandomBrightnessContrast ( brightness_limit = 1 , contrast_limit = 1 , p = 1.0 ), ]) transformed_image_1 = transform ( image = image )[ 'image' ] transformed_image_2 = transform ( image = image )[ 'image' ] transformed_image_3 = transform ( image = image )[ 'image' ]","title":"Step 4. Pass images to the augmentation pipeline and receive augmented images."},{"location":"getting_started/image_augmentation/#examples","text":"Defining a simple augmentation pipeline for image augmentation Working with non-8-bit images Weather augmentations in Albumentations Showcase. Cool augmentation examples on diverse set of images from various real-world tasks.","title":"Examples"},{"location":"getting_started/installation/","text":"Installation \u00b6 Albumentations requires Python 3.6 or higher. Install the latest stable version from PyPI \u00b6 pip install -U albumentations Install the latest version from the master branch on GitHub \u00b6 pip install -U git+https://github.com/albumentations-team/albumentations Note on OpenCV dependencies \u00b6 By default, pip downloads a wheel distribution of Albumentations. This distribution has opencv-python-headless as its dependency. However, Albumentations also depends on imgaug , which has opencv-python (non-headless-version) as its dependency. Because of this dependency clash, you will get both opencv-python-headless and opencv-python installed in your Python environment. In future versions, we plan to get rid of imgaug as a dependency, and Albumentations will require only opencv-python-headless as its dependency. However, if you need a workaround for the current version, or you already have some OpenCV distribution (such as opencv-python-headless , opencv-python , opencv-contrib-python or opencv-contrib-python-headless ) installed in your Python environment, you can force Albumentations to use it by providing the --no-binary imgaug,albumentations argument to pip, e.g. pip install -U albumentations --no-binary imgaug,albumentations pip will use the following logic to determine the required OpenCV distribution: If your Python environment already contains opencv-python , opencv-contrib-python , opencv-contrib-python-headless or opencv-python-headless pip will use it. If your Python environment doesn't contain any OpenCV distribution from step 1, pip will download opencv-python-headless . Install the latest stable version from conda-forge \u00b6 If you are using Anaconda or Miniconda you can install Albumentations from conda-forge: conda install -c conda-forge imgaug conda install -c conda-forge albumentations","title":"Installation"},{"location":"getting_started/installation/#installation","text":"Albumentations requires Python 3.6 or higher.","title":"Installation"},{"location":"getting_started/installation/#install-the-latest-stable-version-from-pypi","text":"pip install -U albumentations","title":"Install the latest stable version from PyPI"},{"location":"getting_started/installation/#install-the-latest-version-from-the-master-branch-on-github","text":"pip install -U git+https://github.com/albumentations-team/albumentations","title":"Install the latest version from the master branch on GitHub"},{"location":"getting_started/installation/#note-on-opencv-dependencies","text":"By default, pip downloads a wheel distribution of Albumentations. This distribution has opencv-python-headless as its dependency. However, Albumentations also depends on imgaug , which has opencv-python (non-headless-version) as its dependency. Because of this dependency clash, you will get both opencv-python-headless and opencv-python installed in your Python environment. In future versions, we plan to get rid of imgaug as a dependency, and Albumentations will require only opencv-python-headless as its dependency. However, if you need a workaround for the current version, or you already have some OpenCV distribution (such as opencv-python-headless , opencv-python , opencv-contrib-python or opencv-contrib-python-headless ) installed in your Python environment, you can force Albumentations to use it by providing the --no-binary imgaug,albumentations argument to pip, e.g. pip install -U albumentations --no-binary imgaug,albumentations pip will use the following logic to determine the required OpenCV distribution: If your Python environment already contains opencv-python , opencv-contrib-python , opencv-contrib-python-headless or opencv-python-headless pip will use it. If your Python environment doesn't contain any OpenCV distribution from step 1, pip will download opencv-python-headless .","title":"Note on OpenCV dependencies"},{"location":"getting_started/installation/#install-the-latest-stable-version-from-conda-forge","text":"If you are using Anaconda or Miniconda you can install Albumentations from conda-forge: conda install -c conda-forge imgaug conda install -c conda-forge albumentations","title":"Install the latest stable version from conda-forge"},{"location":"getting_started/keypoints_augmentation/","text":"Keypoints augmentation \u00b6 Computer vision tasks such as human pose estimation, face detection, and emotion recognition usually work with keypoints on the image. In the case of pose estimation, keypoints mark human joints such as shoulder, elbow, wrist, knee, etc. Keypoints annotations along with visualized edges between keypoints. Images are from the COCO dataset . In the case of face detection, keypoints mark important areas of the face such as eyes, nose, corners of the mouth, etc. Facial keypoints. Source: the \"Facial Keypoints Detection\" competition on Kaggle . To define a keypoint, you usually need two values, x and y coordinates of the keypoint. Coordinates of the keypoint are calculated with respect to the top-left corner of the image which has (x, y) coordinates (0, 0) . Often keypoints have associated labels such as right_elbow , left_wrist , etc. An example image with five keypoints from the COCO dataset Some classical computer vision algorithms, such as SIFT, may use four values to describe a keypoint. In addition to the x and y coordinates, there are keypoint scale and keypoint angle. Albumentations support those values as well. A keypoint may also has associated scale and angle values Keypoint angles are counter-clockwise. For example, in the following image, the angle value is 65\u00b0. You can read more about angle of rotation in the Wikipedia article . Supported formats for keypoints' coordinates. \u00b6 xy . A keypoint is defined by x and y coordinates in pixels. yx . A keypoint is defined by y and x coordinates in pixels. xya . A keypoint is defined by x and y coordinates in pixels and the angle. xys . A keypoint is defined by x and y coordinates in pixels, and the scale. xyas . A keypoint is defined by x and y coordinates in pixels, the angle, and the scale. xysa . A keypoint is defined by x and y coordinates in pixels, the scale, and the angle. Augmenting keypoints \u00b6 The process of augmenting keypoints looks very similar to the bounding boxes augmentation. It consists of 4 steps. You import the required libraries. You define an augmentation pipeline. You read images and keypoints from the disk. You pass an image and keypoints to the augmentation pipeline and receive augmented images and keypoints. Note Some transforms in Albumentation don't support keypoints. If you try to use them you will get an exception. Please refer to this article to check whether a transform can augment keypoints. Step 1. Import the required libraries. \u00b6 import albumentations as A import cv2 Step 2. Define an augmentation pipeline. \u00b6 Here an example of a minimal declaration of an augmentation pipeline that works with keypoints. transform = A . Compose ([ A . RandomCrop ( width = 330 , height = 330 ), A . RandomBrightnessContrast ( p = 0.2 ), ], keypoint_params = A . KeypointParams ( format = 'xy' )) Note that just like with bounding boxes, Compose has an additional parameter that defines the format for keypoints' coordinates. In the case of keypoints, it is called keypoint_params . Here we pass an instance of A.KeypointParams that says that xy coordinates format should be used. Besides format , A.KeypointParams supports a few more settings. Here is an example of Compose that shows all available settings with A.KeypointParams transform = A . Compose ([ A . RandomCrop ( width = 330 , height = 330 ), A . RandomBrightnessContrast ( p = 0.2 ), ], keypoint_params = A . KeypointParams ( format = 'xy' , label_fields = [ 'class_labels' ], remove_invisible = True , angle_in_degrees = True )) label_fields \u00b6 In some computer vision tasks, keypoints have not only coordinates but associated labels as well. For example, in pose estimation, each keypoint has a label such as elbow , knee or wrist . You need to pass those labels in a separate argument (or arguments, because you can use multiple fields) to the transform function that will augment keypoints. label_fields defines names of those fields. Step 4 describes how you need to use the transform function. remove_invisible \u00b6 After the augmentation, some keypoints may become invisible because they will be located outside of the augmented image's visible area. For example, if you crop a part of the image, all the keypoints outside of the cropped area will become invisible. If remove_invisible is set to True , Albumentations won't return invisible keypoints. remove_invisible is set to True by default, so if you don't pass that argument, Albumentations won't return invisible keypoints. angle_in_degrees \u00b6 If angle_in_degrees is set to True (this is the default value), then Albumentations expects that the angle value in formats xya , xyas , and xysa is defined in angles. If angle_in_degrees is set to False , Albumentations expects that the angle value is specified in radians. This setting doesn't affect xy and yx formats, because those formats don't use angles. 3. Read images and keypoints from the disk. \u00b6 Read an image from the disk. image = cv2 . imread ( \"/path/to/image.jpg\" ) image = cv2 . cvtColor ( image , cv2 . COLOR_BGR2RGB ) Keypoints can be stored on the disk in different serialization formats: JSON, XML, YAML, CSV, etc. So the code to read keypoints depends on the actual format of data on the disk. After you read the data from the disk, you need to prepare keypoints for Albumentations. Albumentations expects that keypoint will be represented as a list of lists. Each list contains information about a single keypoint. A definition of keypoint should have two to four elements depending on the selected format of keypoints. The first two elements are x and y coordinates of a keypoint in pixels (or y and x coordinates in the yx format). The third and fourth elements may be the angle and the scale of keypoint if you select a format that uses those values. Step 4. Pass an image and keypoints to the augmentation pipeline and receive augmented images and boxes. \u00b6 Let's say you have an example image with five keypoints. A list with those five keypoints' coordinates in the xy format will look the following: keypoints = [ ( 264 , 203 ), ( 86 , 88 ), ( 254 , 160 ), ( 193 , 103 ), ( 65 , 341 ), ] Then you pass those keypoints to the transform function along with the image and receive the augmented versions of image and keypoints. transformed = transform ( image = image , keypoints = keypoints ) transformed_image = transformed [ 'image' ] transformed_keypoints = transformed [ 'keypoints' ] The augmented image with augmented keypoints If you set remove_invisible to False in keypoint_params , then Albumentations will return all keypoints, even if they lie outside the visible area. In the example image below, you can see that the keypoint for the right hip is located outside the image, but Albumentations still retuned it. The area outside the image is highlighted in yellow. When remove_invisible is set to False Albumentations will return all keypoints, even those located outside the image If keypoints have associated class labels, you need to create a list that contains those labels: class_labels = [ 'left_elbow' , 'right_elbow' , 'left_wrist' , 'right_wrist' , 'right_hip' , ] Also, you need to declare the name of the argument to transform that will contain those labels. For declaration, you need to use the label_fields parameters of A.KeypointParams . For example, we could use the class_labels name for the argument with labels. transform = A . Compose ([ A . RandomCrop ( width = 330 , height = 330 ), A . RandomBrightnessContrast ( p = 0.2 ), ], keypoint_params = A . KeypointParams ( format = 'xy' , label_fields = [ 'class_labels' ])) Next, you pass both keypoints' coordinates and class labels to transform . transformed = transform ( image = image , keypoints = keypoints , class_labels = class_labels ) transformed_image = transformed [ 'image' ] transformed_keypoints = transformed [ 'keypoints' ] transformed_class_labels = transformed [ 'class_labels' ] Note that label_fields expects a list, so you can set multiple fields that contain labels for your keypoints. So if you declare Compose like transform = A . Compose ([ A . RandomCrop ( width = 330 , height = 330 ), A . RandomBrightnessContrast ( p = 0.2 ), ], keypoint_params = A . KeypointParams ( format = 'xy' , label_fields = [ 'class_labels' , 'class_sides' ])) you can use those multiple arguments to pass info about class labels, like class_labels = [ 'left_elbow' , 'right_elbow' , 'left_wrist' , 'right_wrist' , 'right_hip' , ] class_sides = [ 'left' , 'right' , 'left' , 'right' , 'right' ] transformed = transform ( image = image , keypoints = keypoints , class_labels = class_labels , class_sides = class_sides ) transformed_class_sides = transformed [ 'class_sides' ] transformed_class_labels = transformed [ 'class_labels' ] transformed_keypoints = transformed [ 'keypoints' ] transformed_image = transformed [ 'image' ] Example input and output data for keypoints augmentation with two separate arguments for class labels Note Some augmentations may affect class labels and make them incorrect. For example, the HorizontalFlip augmentation mirrors the input image. When you apply that augmentation to keypoints that mark the side of body parts (left or right), those keypoints will point to the wrong side (since left on the mirrored image becomes right ). So when you are creating an augmentation pipeline look carefully which augmentations could be applied to the input data. HorizontalFlip may make keypoints' labels incorrect Examples \u00b6 Using Albumentations to augment keypoints","title":"Keypoints augmentation"},{"location":"getting_started/keypoints_augmentation/#keypoints-augmentation","text":"Computer vision tasks such as human pose estimation, face detection, and emotion recognition usually work with keypoints on the image. In the case of pose estimation, keypoints mark human joints such as shoulder, elbow, wrist, knee, etc. Keypoints annotations along with visualized edges between keypoints. Images are from the COCO dataset . In the case of face detection, keypoints mark important areas of the face such as eyes, nose, corners of the mouth, etc. Facial keypoints. Source: the \"Facial Keypoints Detection\" competition on Kaggle . To define a keypoint, you usually need two values, x and y coordinates of the keypoint. Coordinates of the keypoint are calculated with respect to the top-left corner of the image which has (x, y) coordinates (0, 0) . Often keypoints have associated labels such as right_elbow , left_wrist , etc. An example image with five keypoints from the COCO dataset Some classical computer vision algorithms, such as SIFT, may use four values to describe a keypoint. In addition to the x and y coordinates, there are keypoint scale and keypoint angle. Albumentations support those values as well. A keypoint may also has associated scale and angle values Keypoint angles are counter-clockwise. For example, in the following image, the angle value is 65\u00b0. You can read more about angle of rotation in the Wikipedia article .","title":"Keypoints augmentation"},{"location":"getting_started/keypoints_augmentation/#supported-formats-for-keypoints-coordinates","text":"xy . A keypoint is defined by x and y coordinates in pixels. yx . A keypoint is defined by y and x coordinates in pixels. xya . A keypoint is defined by x and y coordinates in pixels and the angle. xys . A keypoint is defined by x and y coordinates in pixels, and the scale. xyas . A keypoint is defined by x and y coordinates in pixels, the angle, and the scale. xysa . A keypoint is defined by x and y coordinates in pixels, the scale, and the angle.","title":"Supported formats for keypoints' coordinates."},{"location":"getting_started/keypoints_augmentation/#augmenting-keypoints","text":"The process of augmenting keypoints looks very similar to the bounding boxes augmentation. It consists of 4 steps. You import the required libraries. You define an augmentation pipeline. You read images and keypoints from the disk. You pass an image and keypoints to the augmentation pipeline and receive augmented images and keypoints. Note Some transforms in Albumentation don't support keypoints. If you try to use them you will get an exception. Please refer to this article to check whether a transform can augment keypoints.","title":"Augmenting keypoints"},{"location":"getting_started/keypoints_augmentation/#step-1-import-the-required-libraries","text":"import albumentations as A import cv2","title":"Step 1. Import the required libraries."},{"location":"getting_started/keypoints_augmentation/#step-2-define-an-augmentation-pipeline","text":"Here an example of a minimal declaration of an augmentation pipeline that works with keypoints. transform = A . Compose ([ A . RandomCrop ( width = 330 , height = 330 ), A . RandomBrightnessContrast ( p = 0.2 ), ], keypoint_params = A . KeypointParams ( format = 'xy' )) Note that just like with bounding boxes, Compose has an additional parameter that defines the format for keypoints' coordinates. In the case of keypoints, it is called keypoint_params . Here we pass an instance of A.KeypointParams that says that xy coordinates format should be used. Besides format , A.KeypointParams supports a few more settings. Here is an example of Compose that shows all available settings with A.KeypointParams transform = A . Compose ([ A . RandomCrop ( width = 330 , height = 330 ), A . RandomBrightnessContrast ( p = 0.2 ), ], keypoint_params = A . KeypointParams ( format = 'xy' , label_fields = [ 'class_labels' ], remove_invisible = True , angle_in_degrees = True ))","title":"Step 2. Define an augmentation pipeline."},{"location":"getting_started/keypoints_augmentation/#label_fields","text":"In some computer vision tasks, keypoints have not only coordinates but associated labels as well. For example, in pose estimation, each keypoint has a label such as elbow , knee or wrist . You need to pass those labels in a separate argument (or arguments, because you can use multiple fields) to the transform function that will augment keypoints. label_fields defines names of those fields. Step 4 describes how you need to use the transform function.","title":"label_fields"},{"location":"getting_started/keypoints_augmentation/#remove_invisible","text":"After the augmentation, some keypoints may become invisible because they will be located outside of the augmented image's visible area. For example, if you crop a part of the image, all the keypoints outside of the cropped area will become invisible. If remove_invisible is set to True , Albumentations won't return invisible keypoints. remove_invisible is set to True by default, so if you don't pass that argument, Albumentations won't return invisible keypoints.","title":"remove_invisible"},{"location":"getting_started/keypoints_augmentation/#angle_in_degrees","text":"If angle_in_degrees is set to True (this is the default value), then Albumentations expects that the angle value in formats xya , xyas , and xysa is defined in angles. If angle_in_degrees is set to False , Albumentations expects that the angle value is specified in radians. This setting doesn't affect xy and yx formats, because those formats don't use angles.","title":"angle_in_degrees"},{"location":"getting_started/keypoints_augmentation/#3-read-images-and-keypoints-from-the-disk","text":"Read an image from the disk. image = cv2 . imread ( \"/path/to/image.jpg\" ) image = cv2 . cvtColor ( image , cv2 . COLOR_BGR2RGB ) Keypoints can be stored on the disk in different serialization formats: JSON, XML, YAML, CSV, etc. So the code to read keypoints depends on the actual format of data on the disk. After you read the data from the disk, you need to prepare keypoints for Albumentations. Albumentations expects that keypoint will be represented as a list of lists. Each list contains information about a single keypoint. A definition of keypoint should have two to four elements depending on the selected format of keypoints. The first two elements are x and y coordinates of a keypoint in pixels (or y and x coordinates in the yx format). The third and fourth elements may be the angle and the scale of keypoint if you select a format that uses those values.","title":"3. Read images and keypoints from the disk."},{"location":"getting_started/keypoints_augmentation/#step-4-pass-an-image-and-keypoints-to-the-augmentation-pipeline-and-receive-augmented-images-and-boxes","text":"Let's say you have an example image with five keypoints. A list with those five keypoints' coordinates in the xy format will look the following: keypoints = [ ( 264 , 203 ), ( 86 , 88 ), ( 254 , 160 ), ( 193 , 103 ), ( 65 , 341 ), ] Then you pass those keypoints to the transform function along with the image and receive the augmented versions of image and keypoints. transformed = transform ( image = image , keypoints = keypoints ) transformed_image = transformed [ 'image' ] transformed_keypoints = transformed [ 'keypoints' ] The augmented image with augmented keypoints If you set remove_invisible to False in keypoint_params , then Albumentations will return all keypoints, even if they lie outside the visible area. In the example image below, you can see that the keypoint for the right hip is located outside the image, but Albumentations still retuned it. The area outside the image is highlighted in yellow. When remove_invisible is set to False Albumentations will return all keypoints, even those located outside the image If keypoints have associated class labels, you need to create a list that contains those labels: class_labels = [ 'left_elbow' , 'right_elbow' , 'left_wrist' , 'right_wrist' , 'right_hip' , ] Also, you need to declare the name of the argument to transform that will contain those labels. For declaration, you need to use the label_fields parameters of A.KeypointParams . For example, we could use the class_labels name for the argument with labels. transform = A . Compose ([ A . RandomCrop ( width = 330 , height = 330 ), A . RandomBrightnessContrast ( p = 0.2 ), ], keypoint_params = A . KeypointParams ( format = 'xy' , label_fields = [ 'class_labels' ])) Next, you pass both keypoints' coordinates and class labels to transform . transformed = transform ( image = image , keypoints = keypoints , class_labels = class_labels ) transformed_image = transformed [ 'image' ] transformed_keypoints = transformed [ 'keypoints' ] transformed_class_labels = transformed [ 'class_labels' ] Note that label_fields expects a list, so you can set multiple fields that contain labels for your keypoints. So if you declare Compose like transform = A . Compose ([ A . RandomCrop ( width = 330 , height = 330 ), A . RandomBrightnessContrast ( p = 0.2 ), ], keypoint_params = A . KeypointParams ( format = 'xy' , label_fields = [ 'class_labels' , 'class_sides' ])) you can use those multiple arguments to pass info about class labels, like class_labels = [ 'left_elbow' , 'right_elbow' , 'left_wrist' , 'right_wrist' , 'right_hip' , ] class_sides = [ 'left' , 'right' , 'left' , 'right' , 'right' ] transformed = transform ( image = image , keypoints = keypoints , class_labels = class_labels , class_sides = class_sides ) transformed_class_sides = transformed [ 'class_sides' ] transformed_class_labels = transformed [ 'class_labels' ] transformed_keypoints = transformed [ 'keypoints' ] transformed_image = transformed [ 'image' ] Example input and output data for keypoints augmentation with two separate arguments for class labels Note Some augmentations may affect class labels and make them incorrect. For example, the HorizontalFlip augmentation mirrors the input image. When you apply that augmentation to keypoints that mark the side of body parts (left or right), those keypoints will point to the wrong side (since left on the mirrored image becomes right ). So when you are creating an augmentation pipeline look carefully which augmentations could be applied to the input data. HorizontalFlip may make keypoints' labels incorrect","title":"Step 4. Pass an image and keypoints to the augmentation pipeline and receive augmented images and boxes."},{"location":"getting_started/keypoints_augmentation/#examples","text":"Using Albumentations to augment keypoints","title":"Examples"},{"location":"getting_started/mask_augmentation/","text":"Mask augmentation for segmentation \u00b6 For instance and semantic segmentation tasks, you need to augment both the input image and one or more output masks. Albumentations ensures that the input image and the output mask will receive the same set of augmentations with the same parameters. The process of augmenting images and masks looks very similar to the regular image-only augmentation . You import the required libraries. You define an augmentation pipeline. You read images and masks from the disk. You pass an image and one or more masks to the augmentation pipeline and receive augmented images and masks. Steps 1 and 2. Import the required libraries and define an augmentation pipeline. \u00b6 Image augmentation for classification described Steps 1 and 2 in great detail. These are the same steps for the simultaneous augmentation of images and masks. import albumentations as A import cv2 transform = A . Compose ([ A . RandomCrop ( width = 256 , height = 256 ), A . HorizontalFlip ( p = 0.5 ), A . RandomBrightnessContrast ( p = 0.2 ), ]) Step 3. Read images and masks from the disk. \u00b6 Reading an image image = cv2 . imread ( \"/path/to/image.jpg\" ) image = cv2 . cvtColor ( image , cv2 . COLOR_BGR2RGB ) For semantic segmentation, you usually read one mask per image. Albumentations expects the mask to be a NumPy array. The height and width of the mask should have the same values as the height and width of the image. mask = cv2 . imread ( \"/path/to/mask.png\" ) For instance segmentation, you sometimes need to read multiple masks per image. Then you create a list that contains all the masks. mask_1 = cv2 . imread ( \"/path/to/mask_1.png\" ) mask_2 = cv2 . imread ( \"/path/to/mask_2.png\" ) mask_3 = cv2 . imread ( \"/path/to/mask_3.png\" ) masks = [ mask_1 , mask_2 , mask_3 ] Some datasets use other formats to store masks. For example, they can use Run-Length Encoding or Polygon coordinates. In that case, you need to convert a mask to a NumPy before augmenting it with Albumentations. Often dataset authors provide special libraries and tools to simplify the conversion. Step 4. Pass image and masks to the augmentation pipeline and receive augmented images and masks. \u00b6 If the image has one associated mask, you need to call transform with two arguments: image and mask . In image you should pass the input image, in mask you should pass the output mask. transform will return a dictionary with two keys: image will contain the augmented image, and mask will contain the augmented mask. transformed = transform ( image = image , mask = mask ) transformed_image = transformed [ 'image' ] transformed_mask = transformed [ 'mask' ] An image and a mask before and after augmentation. Inria Aerial Image Labeling dataset contains aerial photos as well as their segmentation masks. Each pixel of the mask is marked as 1 if the pixel belongs to the class building and 0 otherwise. If the image has multiple associated masks, you should use the masks argument instead of mask . In masks you should pass a list of masks. transformed = transform ( image = image , masks = masks ) transformed_image = transformed [ 'image' ] transformed_masks = transformed [ 'masks' ] Examples \u00b6 Using Albumentations for a semantic segmentation task Showcase. Cool augmentation examples on diverse set of images from various real-world tasks.","title":"Mask augmentation for segmentation"},{"location":"getting_started/mask_augmentation/#mask-augmentation-for-segmentation","text":"For instance and semantic segmentation tasks, you need to augment both the input image and one or more output masks. Albumentations ensures that the input image and the output mask will receive the same set of augmentations with the same parameters. The process of augmenting images and masks looks very similar to the regular image-only augmentation . You import the required libraries. You define an augmentation pipeline. You read images and masks from the disk. You pass an image and one or more masks to the augmentation pipeline and receive augmented images and masks.","title":"Mask augmentation for segmentation"},{"location":"getting_started/mask_augmentation/#steps-1-and-2-import-the-required-libraries-and-define-an-augmentation-pipeline","text":"Image augmentation for classification described Steps 1 and 2 in great detail. These are the same steps for the simultaneous augmentation of images and masks. import albumentations as A import cv2 transform = A . Compose ([ A . RandomCrop ( width = 256 , height = 256 ), A . HorizontalFlip ( p = 0.5 ), A . RandomBrightnessContrast ( p = 0.2 ), ])","title":"Steps 1 and 2. Import the required libraries and define an augmentation pipeline."},{"location":"getting_started/mask_augmentation/#step-3-read-images-and-masks-from-the-disk","text":"Reading an image image = cv2 . imread ( \"/path/to/image.jpg\" ) image = cv2 . cvtColor ( image , cv2 . COLOR_BGR2RGB ) For semantic segmentation, you usually read one mask per image. Albumentations expects the mask to be a NumPy array. The height and width of the mask should have the same values as the height and width of the image. mask = cv2 . imread ( \"/path/to/mask.png\" ) For instance segmentation, you sometimes need to read multiple masks per image. Then you create a list that contains all the masks. mask_1 = cv2 . imread ( \"/path/to/mask_1.png\" ) mask_2 = cv2 . imread ( \"/path/to/mask_2.png\" ) mask_3 = cv2 . imread ( \"/path/to/mask_3.png\" ) masks = [ mask_1 , mask_2 , mask_3 ] Some datasets use other formats to store masks. For example, they can use Run-Length Encoding or Polygon coordinates. In that case, you need to convert a mask to a NumPy before augmenting it with Albumentations. Often dataset authors provide special libraries and tools to simplify the conversion.","title":"Step 3. Read images and masks from the disk."},{"location":"getting_started/mask_augmentation/#step-4-pass-image-and-masks-to-the-augmentation-pipeline-and-receive-augmented-images-and-masks","text":"If the image has one associated mask, you need to call transform with two arguments: image and mask . In image you should pass the input image, in mask you should pass the output mask. transform will return a dictionary with two keys: image will contain the augmented image, and mask will contain the augmented mask. transformed = transform ( image = image , mask = mask ) transformed_image = transformed [ 'image' ] transformed_mask = transformed [ 'mask' ] An image and a mask before and after augmentation. Inria Aerial Image Labeling dataset contains aerial photos as well as their segmentation masks. Each pixel of the mask is marked as 1 if the pixel belongs to the class building and 0 otherwise. If the image has multiple associated masks, you should use the masks argument instead of mask . In masks you should pass a list of masks. transformed = transform ( image = image , masks = masks ) transformed_image = transformed [ 'image' ] transformed_masks = transformed [ 'masks' ]","title":"Step 4. Pass image and masks to the augmentation pipeline and receive augmented images and masks."},{"location":"getting_started/mask_augmentation/#examples","text":"Using Albumentations for a semantic segmentation task Showcase. Cool augmentation examples on diverse set of images from various real-world tasks.","title":"Examples"},{"location":"getting_started/setting_probabilities/","text":"Setting probabilities for transforms in an augmentation pipeline \u00b6 Each augmentation in Albumentations has a parameter named p that sets the probability of applying that augmentation to input data. The following augmentations have the default value of p set 1 (which means that by default they will be applied to each instance of input data): Compose , ReplayCompose , CenterCrop , Crop , CropNonEmptyMaskIfExists , FromFloat , CenterCrop , Crop , CropNonEmptyMaskIfExists , FromFloat , IAACropAndPad , Lambda , LongestMaxSize , Normalize , PadIfNeeded , RandomCrop , RandomCropNearBBox , RandomResizedCrop , RandomSizedBBoxSafeCrop , RandomSizedCrop , Resize , SmallestMaxSize , ToFloat . All other augmentations have the default value of p set 0.5, which means that by default, they will be applied to 50% of instances of input data. Let's take a look at the example: import albumentations as A import cv2 p1 = 0.95 p2 = 0.85 p3 = 0.75 transform = A . Compose ([ A . RandomRotate90 ( p = p2 ), A . OneOf ([ A . IAAAdditiveGaussianNoise ( p = 0.9 ), A . GaussNoise ( p = 0.6 ), ], p = p3 ) ], p = p1 ) image = cv2 . imread ( 'some/image.jpg' ) image = cv2 . cvtColor ( cv2 . COLOR_BGR2RGB ) transformed = transform ( image = image ) transformed_image = transformed [ 'image' ] We declare an augmentation pipeline. In this pipeline, we use three placeholder values to set probabilities: p1 , p2 , and p3 . Let's take a closer look at them. p1 \u00b6 p1 sets the probability that the augmentation pipeline will apply augmentations at all. If p1 is set to 0, then augmentations inside Compose will never be applied to the input image, so the augmentation pipeline will always return the input image unchanged. If p1 is set to 1, then all augmentations inside Compose will have a chance to be applied. The example above contains two augmentations inside Compose : RandomRotate90 and the OneOf block with two child augmentations (more on their probabilities later). Any value of p1 between 0 and 1 means that augmentations inside Compose could be applied with the probability between 0 and 100%. If p1 equals to 1 or p1 is less than 1, but the random generator decides to apply augmentations inside Compose probabilities p2 and p3 come into play. p2 \u00b6 Each augmentation inside Compose has a probability of being applied. p2 sets the probability of applying RandomRotate90 . In the example above, p2 equals 0.85, so RandomRotate90 has an 85% chance to be applied to the input image. p3 \u00b6 p3 sets the probability of applying the OneOf block. If the random generator decided to apply RandomRotate90 at the previous step, then OneOf will receive data augmented by it. If the random generator decided not to apply RandomRotate90 then OneOf will receive the input data (that was passed to Compose ) since RandomRotate90 is skipped. The OneOf block applies one of the augmentations inside it. That means that if the random generator chooses to apply OneOf then one child augmentation from it will be applied to the input data. To decide which augmentation within the OneOf block is used, Albumentations uses the following rule: The OneOf block normalizes the probabilities of all augmentations inside it, so their probabilities sum up to 1. Next, OneOf chooses one of the augmentations inside it with a chance defined by its normalized probability and applies it to the input data. In the example above IAAAdditiveGaussianNoise has probability 0.9 and GaussNoise probability 0.6. After normalization, they become 0.6 and 0.4. Which means that OneOf will decide that it should use IAAAdditiveGaussianNoise with probability 0.6 and GaussNoise otherwise. Example calculations \u00b6 Thus, each augmentation in the example above will be applied with the probability: RandomRotate90 : p1 * p2 IAAAdditiveGaussianNoise : p1 * p3 * (0.9 / (0.9 + 0.6)) GaussianNoise : p1 * p3 * (0.6 / (0.9 + 0.6))","title":"Setting probabilities for transforms in an augmentation pipeline"},{"location":"getting_started/setting_probabilities/#setting-probabilities-for-transforms-in-an-augmentation-pipeline","text":"Each augmentation in Albumentations has a parameter named p that sets the probability of applying that augmentation to input data. The following augmentations have the default value of p set 1 (which means that by default they will be applied to each instance of input data): Compose , ReplayCompose , CenterCrop , Crop , CropNonEmptyMaskIfExists , FromFloat , CenterCrop , Crop , CropNonEmptyMaskIfExists , FromFloat , IAACropAndPad , Lambda , LongestMaxSize , Normalize , PadIfNeeded , RandomCrop , RandomCropNearBBox , RandomResizedCrop , RandomSizedBBoxSafeCrop , RandomSizedCrop , Resize , SmallestMaxSize , ToFloat . All other augmentations have the default value of p set 0.5, which means that by default, they will be applied to 50% of instances of input data. Let's take a look at the example: import albumentations as A import cv2 p1 = 0.95 p2 = 0.85 p3 = 0.75 transform = A . Compose ([ A . RandomRotate90 ( p = p2 ), A . OneOf ([ A . IAAAdditiveGaussianNoise ( p = 0.9 ), A . GaussNoise ( p = 0.6 ), ], p = p3 ) ], p = p1 ) image = cv2 . imread ( 'some/image.jpg' ) image = cv2 . cvtColor ( cv2 . COLOR_BGR2RGB ) transformed = transform ( image = image ) transformed_image = transformed [ 'image' ] We declare an augmentation pipeline. In this pipeline, we use three placeholder values to set probabilities: p1 , p2 , and p3 . Let's take a closer look at them.","title":"Setting probabilities for transforms in an augmentation pipeline"},{"location":"getting_started/setting_probabilities/#p1","text":"p1 sets the probability that the augmentation pipeline will apply augmentations at all. If p1 is set to 0, then augmentations inside Compose will never be applied to the input image, so the augmentation pipeline will always return the input image unchanged. If p1 is set to 1, then all augmentations inside Compose will have a chance to be applied. The example above contains two augmentations inside Compose : RandomRotate90 and the OneOf block with two child augmentations (more on their probabilities later). Any value of p1 between 0 and 1 means that augmentations inside Compose could be applied with the probability between 0 and 100%. If p1 equals to 1 or p1 is less than 1, but the random generator decides to apply augmentations inside Compose probabilities p2 and p3 come into play.","title":"p1"},{"location":"getting_started/setting_probabilities/#p2","text":"Each augmentation inside Compose has a probability of being applied. p2 sets the probability of applying RandomRotate90 . In the example above, p2 equals 0.85, so RandomRotate90 has an 85% chance to be applied to the input image.","title":"p2"},{"location":"getting_started/setting_probabilities/#p3","text":"p3 sets the probability of applying the OneOf block. If the random generator decided to apply RandomRotate90 at the previous step, then OneOf will receive data augmented by it. If the random generator decided not to apply RandomRotate90 then OneOf will receive the input data (that was passed to Compose ) since RandomRotate90 is skipped. The OneOf block applies one of the augmentations inside it. That means that if the random generator chooses to apply OneOf then one child augmentation from it will be applied to the input data. To decide which augmentation within the OneOf block is used, Albumentations uses the following rule: The OneOf block normalizes the probabilities of all augmentations inside it, so their probabilities sum up to 1. Next, OneOf chooses one of the augmentations inside it with a chance defined by its normalized probability and applies it to the input data. In the example above IAAAdditiveGaussianNoise has probability 0.9 and GaussNoise probability 0.6. After normalization, they become 0.6 and 0.4. Which means that OneOf will decide that it should use IAAAdditiveGaussianNoise with probability 0.6 and GaussNoise otherwise.","title":"p3"},{"location":"getting_started/setting_probabilities/#example-calculations","text":"Thus, each augmentation in the example above will be applied with the probability: RandomRotate90 : p1 * p2 IAAAdditiveGaussianNoise : p1 * p3 * (0.9 / (0.9 + 0.6)) GaussianNoise : p1 * p3 * (0.6 / (0.9 + 0.6))","title":"Example calculations"},{"location":"getting_started/simultaneous_augmentation/","text":"Simultaneous augmentation of multiple targets: masks, bounding boxes, keypoints \u00b6 Albumentations can apply the same set of transformations to the input images and all the targets that are passed to transform : masks, bounding boxes, and keypoints. Please refer to articles Image augmentation for classification , Mask augmentation for segmentation , Bounding boxes augmentation for object detection , and Keypoints augmentation for the detailed description of each data type. Note Some transforms in Albumentation don't support bounding boxes or keypoints. If you try to use them you will get an exception. Please refer to this article to check whether a transform can augment bounding boxes and keypoints. Below is an example, how you can simultaneously augment the input image, mask, bounding boxes with their labels, and keypoints with their labels. Note that the only required argument to transform is image ; all other arguments are optional, and you can combine them in any way. Step 1. Define Compose with parameters that specify formats for bounding boxes and keypoints. \u00b6 transform = A . Compose ( [ A . RandomCrop ( width = 330 , height = 330 ), A . RandomBrightnessContrast ( p = 0.2 )], bbox_params = A . BboxParams ( format = \"coco\" , label_fields = [ \"bbox_classes\" ]), keypoint_params = A . KeypointParams ( format = \"xy\" , label_fields = [ \"keypoints_classes\" ]), ) Step 2. Load all required data from the disk. \u00b6 Please refer to articles Image augmentation for classification , Mask augmentation for segmentation , Bounding boxes augmentation for object detection , and Keypoints augmentation for more information about loading the input data. For example, here is an image from the COCO dataset . that has one associated mask, one bounding box with the class label person , and five keypoints that define body parts. An example image with mask, bounding boxes and keypoints Step 3. Pass all targets to transform and receive their augmented versions \u00b6 transformed = transform ( image = img , mask = mask , bboxes = bboxes , bbox_classes = bbox_classes , keypoints = keypoints , keypoints_classes = keypoints_classes , ) transformed_image = transformed [ \"image\" ] transformed_mask = transformed [ \"mask\" ] transformed_bboxes = transformed [ \"bboxes\" ] transformed_bbox_classes = transformed [ \"bbox_classes\" ] transformed_keypoints = transformed [ \"keypoints\" ] transformed_keypoints_classes = transformed [ \"keypoints_classes\" ] The augmented version of the image and its targets Examples \u00b6 Showcase. Cool augmentation examples on diverse set of images from various real-world tasks.","title":"Simultaneous augmentation of multiple targets: masks, bounding boxes, keypoints"},{"location":"getting_started/simultaneous_augmentation/#simultaneous-augmentation-of-multiple-targets-masks-bounding-boxes-keypoints","text":"Albumentations can apply the same set of transformations to the input images and all the targets that are passed to transform : masks, bounding boxes, and keypoints. Please refer to articles Image augmentation for classification , Mask augmentation for segmentation , Bounding boxes augmentation for object detection , and Keypoints augmentation for the detailed description of each data type. Note Some transforms in Albumentation don't support bounding boxes or keypoints. If you try to use them you will get an exception. Please refer to this article to check whether a transform can augment bounding boxes and keypoints. Below is an example, how you can simultaneously augment the input image, mask, bounding boxes with their labels, and keypoints with their labels. Note that the only required argument to transform is image ; all other arguments are optional, and you can combine them in any way.","title":"Simultaneous augmentation of multiple targets: masks, bounding boxes, keypoints"},{"location":"getting_started/simultaneous_augmentation/#step-1-define-compose-with-parameters-that-specify-formats-for-bounding-boxes-and-keypoints","text":"transform = A . Compose ( [ A . RandomCrop ( width = 330 , height = 330 ), A . RandomBrightnessContrast ( p = 0.2 )], bbox_params = A . BboxParams ( format = \"coco\" , label_fields = [ \"bbox_classes\" ]), keypoint_params = A . KeypointParams ( format = \"xy\" , label_fields = [ \"keypoints_classes\" ]), )","title":"Step 1. Define Compose with parameters that specify formats for bounding boxes and keypoints."},{"location":"getting_started/simultaneous_augmentation/#step-2-load-all-required-data-from-the-disk","text":"Please refer to articles Image augmentation for classification , Mask augmentation for segmentation , Bounding boxes augmentation for object detection , and Keypoints augmentation for more information about loading the input data. For example, here is an image from the COCO dataset . that has one associated mask, one bounding box with the class label person , and five keypoints that define body parts. An example image with mask, bounding boxes and keypoints","title":"Step 2. Load all required data from the disk."},{"location":"getting_started/simultaneous_augmentation/#step-3-pass-all-targets-to-transform-and-receive-their-augmented-versions","text":"transformed = transform ( image = img , mask = mask , bboxes = bboxes , bbox_classes = bbox_classes , keypoints = keypoints , keypoints_classes = keypoints_classes , ) transformed_image = transformed [ \"image\" ] transformed_mask = transformed [ \"mask\" ] transformed_bboxes = transformed [ \"bboxes\" ] transformed_bbox_classes = transformed [ \"bbox_classes\" ] transformed_keypoints = transformed [ \"keypoints\" ] transformed_keypoints_classes = transformed [ \"keypoints_classes\" ] The augmented version of the image and its targets","title":"Step 3. Pass all targets to transform and receive their augmented versions"},{"location":"getting_started/simultaneous_augmentation/#examples","text":"Showcase. Cool augmentation examples on diverse set of images from various real-world tasks.","title":"Examples"},{"location":"getting_started/transforms_and_targets/","text":"A list of transforms and their supported targets \u00b6 We can split all transforms into two groups: pixel-level transforms, and spatial-level transforms. Pixel-level transforms will change just an input image and will leave any additional targets such as masks, bounding boxes, and keypoints unchanged. Spatial-level transforms will simultaneously change both an input image as well as additional targets such as masks, bounding boxes, and keypoints. For the additional information, please refer to this section of \"Why you need a dedicated library for image augmentation\" . Pixel-level transforms \u00b6 Here is a list of all available pixel-level transforms. You can apply a pixel-level transform to any target, and under the hood, the transform will change only the input image and return any other input targets such as masks, bounding boxes, or keypoints unchanged. Blur CLAHE ChannelDropout ChannelShuffle ColorJitter Downscale Emboss Equalize FDA FancyPCA FromFloat GaussNoise GaussianBlur GlassBlur HistogramMatching HueSaturationValue ISONoise ImageCompression InvertImg MedianBlur MotionBlur MultiplicativeNoise Normalize Posterize RGBShift RandomBrightnessContrast RandomFog RandomGamma RandomRain RandomShadow RandomSnow RandomSunFlare RandomToneCurve Sharpen Solarize Superpixels ToFloat ToGray ToSepia Spatial-level transforms \u00b6 Here is a table with spatial-level transforms and targets they support. If you try to apply a spatial-level transform to an unsupported target, Albumentations will raise an error. Transform Image Masks BBoxes Keypoints CenterCrop \u2713 \u2713 \u2713 \u2713 CoarseDropout \u2713 \u2713 Crop \u2713 \u2713 \u2713 \u2713 CropAndPad \u2713 \u2713 \u2713 \u2713 CropNonEmptyMaskIfExists \u2713 \u2713 \u2713 \u2713 ElasticTransform \u2713 \u2713 Flip \u2713 \u2713 \u2713 \u2713 GridDistortion \u2713 \u2713 GridDropout \u2713 \u2713 HorizontalFlip \u2713 \u2713 \u2713 \u2713 IAAAffine \u2713 \u2713 \u2713 \u2713 IAAPiecewiseAffine \u2713 \u2713 \u2713 \u2713 Lambda \u2713 \u2713 \u2713 \u2713 LongestMaxSize \u2713 \u2713 \u2713 \u2713 MaskDropout \u2713 \u2713 NoOp \u2713 \u2713 \u2713 \u2713 OpticalDistortion \u2713 \u2713 PadIfNeeded \u2713 \u2713 \u2713 \u2713 Perspective \u2713 \u2713 \u2713 \u2713 RandomCrop \u2713 \u2713 \u2713 \u2713 RandomCropNearBBox \u2713 \u2713 \u2713 \u2713 RandomGridShuffle \u2713 \u2713 RandomResizedCrop \u2713 \u2713 \u2713 \u2713 RandomRotate90 \u2713 \u2713 \u2713 \u2713 RandomScale \u2713 \u2713 \u2713 \u2713 RandomSizedBBoxSafeCrop \u2713 \u2713 \u2713 RandomSizedCrop \u2713 \u2713 \u2713 \u2713 Resize \u2713 \u2713 \u2713 \u2713 Rotate \u2713 \u2713 \u2713 \u2713 ShiftScaleRotate \u2713 \u2713 \u2713 \u2713 SmallestMaxSize \u2713 \u2713 \u2713 \u2713 Transpose \u2713 \u2713 \u2713 \u2713 VerticalFlip \u2713 \u2713 \u2713 \u2713","title":"A list of transforms and their supported targets"},{"location":"getting_started/transforms_and_targets/#a-list-of-transforms-and-their-supported-targets","text":"We can split all transforms into two groups: pixel-level transforms, and spatial-level transforms. Pixel-level transforms will change just an input image and will leave any additional targets such as masks, bounding boxes, and keypoints unchanged. Spatial-level transforms will simultaneously change both an input image as well as additional targets such as masks, bounding boxes, and keypoints. For the additional information, please refer to this section of \"Why you need a dedicated library for image augmentation\" .","title":"A list of transforms and their supported targets"},{"location":"getting_started/transforms_and_targets/#pixel-level-transforms","text":"Here is a list of all available pixel-level transforms. You can apply a pixel-level transform to any target, and under the hood, the transform will change only the input image and return any other input targets such as masks, bounding boxes, or keypoints unchanged. Blur CLAHE ChannelDropout ChannelShuffle ColorJitter Downscale Emboss Equalize FDA FancyPCA FromFloat GaussNoise GaussianBlur GlassBlur HistogramMatching HueSaturationValue ISONoise ImageCompression InvertImg MedianBlur MotionBlur MultiplicativeNoise Normalize Posterize RGBShift RandomBrightnessContrast RandomFog RandomGamma RandomRain RandomShadow RandomSnow RandomSunFlare RandomToneCurve Sharpen Solarize Superpixels ToFloat ToGray ToSepia","title":"Pixel-level transforms"},{"location":"getting_started/transforms_and_targets/#spatial-level-transforms","text":"Here is a table with spatial-level transforms and targets they support. If you try to apply a spatial-level transform to an unsupported target, Albumentations will raise an error. Transform Image Masks BBoxes Keypoints CenterCrop \u2713 \u2713 \u2713 \u2713 CoarseDropout \u2713 \u2713 Crop \u2713 \u2713 \u2713 \u2713 CropAndPad \u2713 \u2713 \u2713 \u2713 CropNonEmptyMaskIfExists \u2713 \u2713 \u2713 \u2713 ElasticTransform \u2713 \u2713 Flip \u2713 \u2713 \u2713 \u2713 GridDistortion \u2713 \u2713 GridDropout \u2713 \u2713 HorizontalFlip \u2713 \u2713 \u2713 \u2713 IAAAffine \u2713 \u2713 \u2713 \u2713 IAAPiecewiseAffine \u2713 \u2713 \u2713 \u2713 Lambda \u2713 \u2713 \u2713 \u2713 LongestMaxSize \u2713 \u2713 \u2713 \u2713 MaskDropout \u2713 \u2713 NoOp \u2713 \u2713 \u2713 \u2713 OpticalDistortion \u2713 \u2713 PadIfNeeded \u2713 \u2713 \u2713 \u2713 Perspective \u2713 \u2713 \u2713 \u2713 RandomCrop \u2713 \u2713 \u2713 \u2713 RandomCropNearBBox \u2713 \u2713 \u2713 \u2713 RandomGridShuffle \u2713 \u2713 RandomResizedCrop \u2713 \u2713 \u2713 \u2713 RandomRotate90 \u2713 \u2713 \u2713 \u2713 RandomScale \u2713 \u2713 \u2713 \u2713 RandomSizedBBoxSafeCrop \u2713 \u2713 \u2713 RandomSizedCrop \u2713 \u2713 \u2713 \u2713 Resize \u2713 \u2713 \u2713 \u2713 Rotate \u2713 \u2713 \u2713 \u2713 ShiftScaleRotate \u2713 \u2713 \u2713 \u2713 SmallestMaxSize \u2713 \u2713 \u2713 \u2713 Transpose \u2713 \u2713 \u2713 \u2713 VerticalFlip \u2713 \u2713 \u2713 \u2713","title":"Spatial-level transforms"},{"location":"introduction/image_augmentation/","text":"What is image augmentation and how it can improve the performance of deep neural networks \u00b6 Deep neural networks require a lot of training data to obtain good results and prevent overfitting. However, it often very difficult to get enough training samples. Multiple reasons could make it very hard or even impossible to gather enough data: To make a training dataset, you need to obtain images and then label them. For example, you need to assign correct class labels if you have an image classification task. For an object detection task, you need to draw bounding boxes around objects. For a semantic segmentation task, you need to assign a correct class to each input image pixel. This process requires manual labor, and sometimes it could be very costly to label the training data. For example, to correctly label medical images, you need expensive domain experts. Sometimes even collecting training images could be hard. There are many legal restrictions for working with healthcare data, and obtaining it requires a lot of effort. Sometimes getting the training images is more feasible, but it will cost a lot of money. For example, to get satellite images, you need to pay a satellite operator to take those photos. To get images for road scene recognition, you need an operator that will drive a car and collect the required data. Image augmentation to the rescue \u00b6 Image augmentation is a process of creating new training examples from the existing ones. To make a new sample, you slightly change the original image. For instance, you could make a new image a little brighter; you could cut a piece from the original image; you could make a new image by mirroring the original one, etc. Here are some examples of transformations of the original image that will create a new training sample. By applying those transformations to the original training dataset, you could create an almost infinite amount of new training samples. How much does image augmentation improves the quality and performance of deep neural networks \u00b6 Basic augmentations techniques were used almost in all papers that describe the state-of-the-art models for image recognition. AlexNet was the first model that demonstrated exceptional capabilities of using deep neural networks for image recognition. For training, the authors used a set of basic image augmentation techniques. They resized original images to the fixed size of 256 by 256 pixels, and then they cropped patches of size 224 by 224 pixels as well as their horizontal reflections from those resized images. Also, they altered the intensities of the RGB channels in images. Successive state-of-the-art models such as Inception , ResNet , and EfficientNet also used image augmentation techniques for training. In 2018 Google published a paper about AutoAugment - an algorithm that automatically discovers the best set of augmentations for the dataset. They showed that a custom set of augmentations improves the performance of the model. Here is a comparison between a model that used only the base set of augmentations and a model that used a specific set of augmentations discovered by AutoAugment. The table shows Top-1 accuracy (%) on the ImageNet validation set; higher is better. Model Base augmentations AutoAugment augmentations ResNet-50 76.3 77.6 ResNet-200 78.5 80.0 AmoebaNet-B (6,190) 82.2 82.8 AmoebaNet-C (6,228) 83.1 83.5 The table demonstrates that a diverse set of image augmentations improves the performance of neural networks compared to a base set with only a few most popular transformation techniques. Augmentations help to fight overfitting and improve the performance of deep neural networks for computer vision tasks such as classification, segmentation, and object detection. The best part is that image augmentations libraries such as Albumentations make it possible to add image augmentations to any computer vision pipeline with minimal effort.","title":"What is image augmentation and how it can improve the performance of deep neural networks"},{"location":"introduction/image_augmentation/#what-is-image-augmentation-and-how-it-can-improve-the-performance-of-deep-neural-networks","text":"Deep neural networks require a lot of training data to obtain good results and prevent overfitting. However, it often very difficult to get enough training samples. Multiple reasons could make it very hard or even impossible to gather enough data: To make a training dataset, you need to obtain images and then label them. For example, you need to assign correct class labels if you have an image classification task. For an object detection task, you need to draw bounding boxes around objects. For a semantic segmentation task, you need to assign a correct class to each input image pixel. This process requires manual labor, and sometimes it could be very costly to label the training data. For example, to correctly label medical images, you need expensive domain experts. Sometimes even collecting training images could be hard. There are many legal restrictions for working with healthcare data, and obtaining it requires a lot of effort. Sometimes getting the training images is more feasible, but it will cost a lot of money. For example, to get satellite images, you need to pay a satellite operator to take those photos. To get images for road scene recognition, you need an operator that will drive a car and collect the required data.","title":"What is image augmentation and how it can improve the performance of deep neural networks"},{"location":"introduction/image_augmentation/#image-augmentation-to-the-rescue","text":"Image augmentation is a process of creating new training examples from the existing ones. To make a new sample, you slightly change the original image. For instance, you could make a new image a little brighter; you could cut a piece from the original image; you could make a new image by mirroring the original one, etc. Here are some examples of transformations of the original image that will create a new training sample. By applying those transformations to the original training dataset, you could create an almost infinite amount of new training samples.","title":"Image augmentation to the rescue"},{"location":"introduction/image_augmentation/#how-much-does-image-augmentation-improves-the-quality-and-performance-of-deep-neural-networks","text":"Basic augmentations techniques were used almost in all papers that describe the state-of-the-art models for image recognition. AlexNet was the first model that demonstrated exceptional capabilities of using deep neural networks for image recognition. For training, the authors used a set of basic image augmentation techniques. They resized original images to the fixed size of 256 by 256 pixels, and then they cropped patches of size 224 by 224 pixels as well as their horizontal reflections from those resized images. Also, they altered the intensities of the RGB channels in images. Successive state-of-the-art models such as Inception , ResNet , and EfficientNet also used image augmentation techniques for training. In 2018 Google published a paper about AutoAugment - an algorithm that automatically discovers the best set of augmentations for the dataset. They showed that a custom set of augmentations improves the performance of the model. Here is a comparison between a model that used only the base set of augmentations and a model that used a specific set of augmentations discovered by AutoAugment. The table shows Top-1 accuracy (%) on the ImageNet validation set; higher is better. Model Base augmentations AutoAugment augmentations ResNet-50 76.3 77.6 ResNet-200 78.5 80.0 AmoebaNet-B (6,190) 82.2 82.8 AmoebaNet-C (6,228) 83.1 83.5 The table demonstrates that a diverse set of image augmentations improves the performance of neural networks compared to a base set with only a few most popular transformation techniques. Augmentations help to fight overfitting and improve the performance of deep neural networks for computer vision tasks such as classification, segmentation, and object detection. The best part is that image augmentations libraries such as Albumentations make it possible to add image augmentations to any computer vision pipeline with minimal effort.","title":"How much does image augmentation improves the quality and performance of deep neural networks"},{"location":"introduction/why_albumentations/","text":"Why Albumentations \u00b6 A single interface to work with images, masks, bounding boxes, and key points. \u00b6 Albumentations provides a single interface to work with different computer vision tasks such as classification, semantic segmentation, instance segmentation, object detection, pose estimation, etc. Battle-tested \u00b6 The library is widely used in industry , deep learning research , machine learning competitions , and open source projects . High performance \u00b6 Albumentations optimized for maximum speed and performance. Under the hood, the library uses highly optimized functions from OpenCV and NumPy for data processing. We have a regularly updated benchmark that compares the speed of popular image augmentations libraries for the most common image transformations. Albumentations demonstrates the best performance in most cases. Diverse set of supported augmentations \u00b6 Albumentations supports more than 60 different image augmentations. Extensibility \u00b6 Albumentations allows to easily add new augmentations and use them in computer vision pipelines through a single interface along with built-in transformations. Rigorous testing \u00b6 Bugs in the augmentation pipeline could silently corrupt the input data. They can easily go unnoticed, but the performance of the models trained with incorrect data will degrade. Albumentations has an extensive test suite that helps to discover bugs during development. It is open source and MIT licensed \u00b6 You can find the source code on GitHub .","title":"Why Albumentations"},{"location":"introduction/why_albumentations/#why-albumentations","text":"","title":"Why Albumentations"},{"location":"introduction/why_albumentations/#a-single-interface-to-work-with-images-masks-bounding-boxes-and-key-points","text":"Albumentations provides a single interface to work with different computer vision tasks such as classification, semantic segmentation, instance segmentation, object detection, pose estimation, etc.","title":"A single interface to work with images, masks, bounding boxes, and key points."},{"location":"introduction/why_albumentations/#battle-tested","text":"The library is widely used in industry , deep learning research , machine learning competitions , and open source projects .","title":"Battle-tested"},{"location":"introduction/why_albumentations/#high-performance","text":"Albumentations optimized for maximum speed and performance. Under the hood, the library uses highly optimized functions from OpenCV and NumPy for data processing. We have a regularly updated benchmark that compares the speed of popular image augmentations libraries for the most common image transformations. Albumentations demonstrates the best performance in most cases.","title":"High performance"},{"location":"introduction/why_albumentations/#diverse-set-of-supported-augmentations","text":"Albumentations supports more than 60 different image augmentations.","title":"Diverse set of supported augmentations"},{"location":"introduction/why_albumentations/#extensibility","text":"Albumentations allows to easily add new augmentations and use them in computer vision pipelines through a single interface along with built-in transformations.","title":"Extensibility"},{"location":"introduction/why_albumentations/#rigorous-testing","text":"Bugs in the augmentation pipeline could silently corrupt the input data. They can easily go unnoticed, but the performance of the models trained with incorrect data will degrade. Albumentations has an extensive test suite that helps to discover bugs during development.","title":"Rigorous testing"},{"location":"introduction/why_albumentations/#it-is-open-source-and-mit-licensed","text":"You can find the source code on GitHub .","title":"It is open source and MIT licensed"},{"location":"introduction/why_you_need_a_dedicated_library_for_image_augmentation/","text":"Why you need a dedicated library for image augmentation \u00b6 At first glance, image augmentations look very simple; you apply basic transformations to an image: mirroring, cropping, changing brightness and contrast, etc. There are a lot of libraries that could do such image transformations. Here is an example of how you could use Pillow , a popular image processing library for Python, to make simple augmentations. from PIL import Image , ImageEnhance image = Image . open ( \"parrot.jpg\" ) mirrored_image = image . transpose ( Image . FLIP_LEFT_RIGHT ) rotated_image = image . rotate ( 45 ) brightness_enhancer = ImageEnhance . Brightness ( image ) brighter_image = brightness_enhancer . enhance ( factor = 1.5 ) However, this approach has many limitations, and it doesn't handle all cases with image augmentation. An image augmentation library such as Albumentations gives you a lot of advantages. Here is a list of few pitfalls that augmentation libraries can handle very well. The need to apply the same transform to an image and for labels for segmentation, object detection, and keypoint detection tasks. \u00b6 For image classification, you need to modify only an input image and keep output labels intact because output labels are invariant to image modifications. Note There are some exceptions to this rule. For example, an image could contain a cat and have an assigned label cat . During image augmentation, if you crop a part of an image that doesn't have a cat on it, then the output label cat becomes wrong and misleading. Usually, you deal with those situations by deciding which augmentations you could apply to a dataset without risking to have problems with incorrect labels. For segmentation, you need to apply some transformations both to an input image and an output mask. You also have to use the same parameters both for the image transformation and the mask transformation. Let's look at an example of a semantic segmentation task from Inria Aerial Image Labeling Dataset. The dataset contains aerial photos as well as masks for those photos. Each pixel of the mask is marked either as 1 if the pixel belongs to the class building and 0 otherwise. There are two types of image augmentations: pixel-level augmentations and spatial-level augmentations. Pixel-level augmentations change the values of pixels of the original image, but they don't change the output mask. Image transformations such as changing brightness or contrast of adjusting values of the RGB-palette of the image are pixel-level augmentations. We modify the input image by adjusting its brightness, but we keep the output mask intact. On the contrary, spatial-level augmentations change both the image and the mask. When you apply image transformations such as mirroring or rotation or cropping a part of the input image, you also need to apply the same transformation to the output label to preserve its correctness. We rotate both the input image and the output mask. We use the same set of transformations with the same parameters, both for the image and the mask. The same is true for object detection tasks. For pixel-level augmentations, you only need to change the input image. With spatial-level augmentations, you need to apply the same transformation not only to the image but for bounding boxes coordinates as well. After applying spatial-level augmentations, you need to update coordinates of bounding boxes to represent the correct locations of objects on the augmented image. Pixel-level augmentations such as brightness adjustment change only the input image but not the coordinates of bounding boxes. Spatial-level augmentations such as mirroring and cropping a part of the image change both the input image and the bounding boxes' coordinates. Albumentations knows how to correctly apply transformation both to the input data as well as the output labels. Working with probabilities \u00b6 During training, you usually want to apply augmentations with a probability of less than 100% since you also need to have the original images in your training pipeline. Also, it is beneficial to be able to control the magnitude of image augmentation, how much does the augmentation change the original image. If the original dataset is large, you could apply only the basic augmentations with probability around 10-30% and with a small magnitude of changes. If the dataset is small, you need to act more aggressively with augmentations to prevent overfitting of neural networks, so you usually need to increase the probability of applying each augmentation to 40-50% and increase the magnitude of changes the augmentation makes to the image. Image augmentation libraries allow you to set the required probabilities and the magnitude of values for each transformation. Declarative definition of the augmentation pipeline and unified interface \u00b6 Usually, you want to apply not a single augmentation, but a set of augmentations with specific parameters such as probability and magnitude of changes. Augmentation libraries allow you to declare such a pipeline in a single place and then use it for image transformation through a unified interface. Some libraries can store and load transformation parameters to formats such as JSON, YAML, etc. Here is an example definition of an augmentation pipeline. This pipeline will first crop a random 512px x 512px part of the input image. Then with probability 30%, it will randomly change brightness and contrast of that crop. Finally, with probability 50%, it will horizontally flip the resulting image. import albumentations as A transform = A . Compose ([ A . RandomCrop ( 512 , 512 ), A . RandomBrightnessContrast ( p = 0.3 ), A . HorizontalFlip ( p = 0.5 ), ]) Rigorous testing \u00b6 A bug in the augmentation pipeline could easily go unnoticed. A buggy pipeline could silently corrupt input data. There won't be any exceptions and code failures, but the performance of trained neural networks will degrade because they received a garbage input during training. Augmentation libraries usually have large test suites that capture regressions during development. Also large user base helps to find unnoticed bugs and report them to developers.","title":"Why you need a dedicated library for image augmentation"},{"location":"introduction/why_you_need_a_dedicated_library_for_image_augmentation/#why-you-need-a-dedicated-library-for-image-augmentation","text":"At first glance, image augmentations look very simple; you apply basic transformations to an image: mirroring, cropping, changing brightness and contrast, etc. There are a lot of libraries that could do such image transformations. Here is an example of how you could use Pillow , a popular image processing library for Python, to make simple augmentations. from PIL import Image , ImageEnhance image = Image . open ( \"parrot.jpg\" ) mirrored_image = image . transpose ( Image . FLIP_LEFT_RIGHT ) rotated_image = image . rotate ( 45 ) brightness_enhancer = ImageEnhance . Brightness ( image ) brighter_image = brightness_enhancer . enhance ( factor = 1.5 ) However, this approach has many limitations, and it doesn't handle all cases with image augmentation. An image augmentation library such as Albumentations gives you a lot of advantages. Here is a list of few pitfalls that augmentation libraries can handle very well.","title":"Why you need a dedicated library for image augmentation"},{"location":"introduction/why_you_need_a_dedicated_library_for_image_augmentation/#the-need-to-apply-the-same-transform-to-an-image-and-for-labels-for-segmentation-object-detection-and-keypoint-detection-tasks","text":"For image classification, you need to modify only an input image and keep output labels intact because output labels are invariant to image modifications. Note There are some exceptions to this rule. For example, an image could contain a cat and have an assigned label cat . During image augmentation, if you crop a part of an image that doesn't have a cat on it, then the output label cat becomes wrong and misleading. Usually, you deal with those situations by deciding which augmentations you could apply to a dataset without risking to have problems with incorrect labels. For segmentation, you need to apply some transformations both to an input image and an output mask. You also have to use the same parameters both for the image transformation and the mask transformation. Let's look at an example of a semantic segmentation task from Inria Aerial Image Labeling Dataset. The dataset contains aerial photos as well as masks for those photos. Each pixel of the mask is marked either as 1 if the pixel belongs to the class building and 0 otherwise. There are two types of image augmentations: pixel-level augmentations and spatial-level augmentations. Pixel-level augmentations change the values of pixels of the original image, but they don't change the output mask. Image transformations such as changing brightness or contrast of adjusting values of the RGB-palette of the image are pixel-level augmentations. We modify the input image by adjusting its brightness, but we keep the output mask intact. On the contrary, spatial-level augmentations change both the image and the mask. When you apply image transformations such as mirroring or rotation or cropping a part of the input image, you also need to apply the same transformation to the output label to preserve its correctness. We rotate both the input image and the output mask. We use the same set of transformations with the same parameters, both for the image and the mask. The same is true for object detection tasks. For pixel-level augmentations, you only need to change the input image. With spatial-level augmentations, you need to apply the same transformation not only to the image but for bounding boxes coordinates as well. After applying spatial-level augmentations, you need to update coordinates of bounding boxes to represent the correct locations of objects on the augmented image. Pixel-level augmentations such as brightness adjustment change only the input image but not the coordinates of bounding boxes. Spatial-level augmentations such as mirroring and cropping a part of the image change both the input image and the bounding boxes' coordinates. Albumentations knows how to correctly apply transformation both to the input data as well as the output labels.","title":"The need to apply the same transform to an image and for labels for segmentation, object detection, and keypoint detection tasks."},{"location":"introduction/why_you_need_a_dedicated_library_for_image_augmentation/#working-with-probabilities","text":"During training, you usually want to apply augmentations with a probability of less than 100% since you also need to have the original images in your training pipeline. Also, it is beneficial to be able to control the magnitude of image augmentation, how much does the augmentation change the original image. If the original dataset is large, you could apply only the basic augmentations with probability around 10-30% and with a small magnitude of changes. If the dataset is small, you need to act more aggressively with augmentations to prevent overfitting of neural networks, so you usually need to increase the probability of applying each augmentation to 40-50% and increase the magnitude of changes the augmentation makes to the image. Image augmentation libraries allow you to set the required probabilities and the magnitude of values for each transformation.","title":"Working with probabilities"},{"location":"introduction/why_you_need_a_dedicated_library_for_image_augmentation/#declarative-definition-of-the-augmentation-pipeline-and-unified-interface","text":"Usually, you want to apply not a single augmentation, but a set of augmentations with specific parameters such as probability and magnitude of changes. Augmentation libraries allow you to declare such a pipeline in a single place and then use it for image transformation through a unified interface. Some libraries can store and load transformation parameters to formats such as JSON, YAML, etc. Here is an example definition of an augmentation pipeline. This pipeline will first crop a random 512px x 512px part of the input image. Then with probability 30%, it will randomly change brightness and contrast of that crop. Finally, with probability 50%, it will horizontally flip the resulting image. import albumentations as A transform = A . Compose ([ A . RandomCrop ( 512 , 512 ), A . RandomBrightnessContrast ( p = 0.3 ), A . HorizontalFlip ( p = 0.5 ), ])","title":"Declarative definition of the augmentation pipeline and unified interface"},{"location":"introduction/why_you_need_a_dedicated_library_for_image_augmentation/#rigorous-testing","text":"A bug in the augmentation pipeline could easily go unnoticed. A buggy pipeline could silently corrupt input data. There won't be any exceptions and code failures, but the performance of trained neural networks will degrade because they received a garbage input during training. Augmentation libraries usually have large test suites that capture regressions during development. Also large user base helps to find unnoticed bugs and report them to developers.","title":"Rigorous testing"}]}